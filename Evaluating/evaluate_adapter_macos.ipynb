{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010c66b404eb43db94d3ac44fc532f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seanhuang/miniconda3/envs/cmlhw_mac/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # or the base model you trained on\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Load base model in 4-bit if you used quantization\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "# manually set rope_scaling to supported structure:\n",
    "config.rope_scaling = {\"type\": \"dynamic\", \"factor\": 2.0}\n",
    "config.use_cache = True\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"mps\",\n",
    "    config=config,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load your LoRA adapter\n",
    "adapter_path = \"./../Training/final_adapter_with_eval_1\"  # or wherever your adapter_model.safetensors is\n",
    "adapted_model= PeftModel.from_pretrained(base_model, adapter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to empty_cache other wise 2 models' results will bleed into each other\n",
    "# only slice and decode the new tokens \n",
    "def generate_summary(input_text, adapted_model, max_new_tokens=150):\n",
    "\n",
    "    prompt = f\"\"\"Summarize:\\n{input_text} Summary:\\n\"\"\"\n",
    "\n",
    "#     prompt = f\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>\n",
    "# Text:\n",
    "# {input_text}\n",
    "\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(adapted_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = adapted_model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = outputs[0][input_len:]  # exclude prompt\n",
    "    summary = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def generate_base_summary(input_text, base_model, max_new_tokens=150):\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"Summarize:\\n{input_text} Summary:\\n\"\"\"\n",
    "#     prompt = f\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>\n",
    "# Text:\n",
    "# {input_text}\n",
    "# \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = outputs[0][input_len:]  # exclude prompt\n",
    "    summary = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def chunk_text_with_overlap(txt_file=\"\", chunk_size=500, overlap=100, string=\"\"):\n",
    "    \"\"\"Splits text into overlapping chunks.\"\"\"\n",
    "    if txt_file:\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    if string:\n",
    "        text = string\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end_ptr = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end_ptr])\n",
    "        start += chunk_size - overlap  # Move forward while keeping overlap\n",
    "\n",
    "    # for chunk in chunks:\n",
    "    #     print(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def rate(summary):\n",
    "        prompt = f\"?Is this useful:{summary}\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = base_model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                max_new_tokens=10,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        new_tokens = outputs[0][input_len:]  # exclude prompt\n",
    "        score = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "        print(\"rating:\", score)\n",
    "        # try:\n",
    "        #     score  = float(score)\n",
    "        # except :\n",
    "        #     score = 0\n",
    "        return score \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_summary(text):\n",
    "    return (\n",
    "        text.replace('NULL', '')\n",
    "            .replace('<', '')\n",
    "            .replace('>', '')\n",
    "            .replace('\\n', '')\n",
    "            .replace('--', '')\n",
    "            .replace('Summary:', '')\n",
    "    )\n",
    "class IterativeSummarizer:\n",
    "    def __init__(self,model):\n",
    "         self.model = model\n",
    "     \n",
    "    def summarize(self, chunks, max_token):\n",
    "          if len(chunks) == 1:\n",
    "               return chunks[0]  # Base case: only one chunk remains\n",
    "\n",
    "          summarized_chunks = []\n",
    "\n",
    "          for chunk in chunks:\n",
    "               summary = generate_summary(chunk, adapted_model=self.model, max_new_tokens=max_token)\n",
    "               cleaned = clean_summary(summary)\n",
    "               summarized_chunks.append(cleaned)\n",
    "\n",
    "          # Combine and re-chunk\n",
    "          combined = \" \".join(summarized_chunks)\n",
    "          combined = combined.strip() \n",
    "          print(\"<----Combined--->\", combined)\n",
    "\n",
    "          # Recursively call after re-chunking\n",
    "          new_chunks = chunk_text_with_overlap(string=combined, chunk_size=900)\n",
    "          return self.summarize(new_chunks, max_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "('0e21835a42a6df2405496f62647058ff855743c1_normal.txt', PosixPath('Data/PDFs/0e21835a42a6df2405496f62647058ff855743c1_normal.txt'))\n",
      "('1dcf57a5007b56254583423ba31107d22459bccf_normal.txt', PosixPath('Data/PDFs/1dcf57a5007b56254583423ba31107d22459bccf_normal.txt'))\n",
      "('2009.08453v2_normal.txt', PosixPath('Data/PDFs/2009.08453v2_normal.txt'))\n",
      "('2010.05981v2_normal.txt', PosixPath('Data/PDFs/2010.05981v2_normal.txt'))\n",
      "('2010.11929v2_normal.txt', PosixPath('Data/PDFs/2010.11929v2_normal.txt'))\n",
      "('281928eff64137efdd144a833c81ad0ee45284c1_normal.txt', PosixPath('Data/PDFs/281928eff64137efdd144a833c81ad0ee45284c1_normal.txt'))\n",
      "('2FDPTMT2NZDE6RIJSZZXGBMD7LYL7YHV_ocr.txt', PosixPath('Data/PDFs/2FDPTMT2NZDE6RIJSZZXGBMD7LYL7YHV_ocr.txt'))\n",
      "('2G54QACZZK5MIIKK25USTLNPN66FST63_normal.txt', PosixPath('Data/PDFs/2G54QACZZK5MIIKK25USTLNPN66FST63_normal.txt'))\n",
      "('2YS3ALM6OTD5ENWN4Z5LOBWG735756DX_ocr.txt', PosixPath('Data/PDFs/2YS3ALM6OTD5ENWN4Z5LOBWG735756DX_ocr.txt'))\n",
      "('2a85b52768ea5761b773be49b09d15f0b95415b0_normal.txt', PosixPath('Data/PDFs/2a85b52768ea5761b773be49b09d15f0b95415b0_normal.txt'))\n",
      "('3P5D3UKXU2R6I2TK4OJSLL6LGIQJ4NY5_normal.txt', PosixPath('Data/PDFs/3P5D3UKXU2R6I2TK4OJSLL6LGIQJ4NY5_normal.txt'))\n",
      "('45W73IZ4UHYYGASU2Y4JO6Q7SC56OPTI_normal.txt', PosixPath('Data/PDFs/45W73IZ4UHYYGASU2Y4JO6Q7SC56OPTI_normal.txt'))\n",
      "('46ZGYJDQNL2COPU447ILTCZ6EUMSQOHA_normal.txt', PosixPath('Data/PDFs/46ZGYJDQNL2COPU447ILTCZ6EUMSQOHA_normal.txt'))\n",
      "('642c5aed3f342a15e2ae287d5350a5735bae9ebc_normal.txt', PosixPath('Data/PDFs/642c5aed3f342a15e2ae287d5350a5735bae9ebc_normal.txt'))\n",
      "('65VH6VWWJJCZNGG3XTA7XAPZRAIOGYEV_normal.txt', PosixPath('Data/PDFs/65VH6VWWJJCZNGG3XTA7XAPZRAIOGYEV_normal.txt'))\n",
      "('74e685ae7e7ff7d88ce66cc6cb6ba62d54c2300a_normal.txt', PosixPath('Data/PDFs/74e685ae7e7ff7d88ce66cc6cb6ba62d54c2300a_normal.txt'))\n",
      "('887c6fd22c2be24a023105b3fb23d5e29dfd8055_ocr.txt', PosixPath('Data/PDFs/887c6fd22c2be24a023105b3fb23d5e29dfd8055_ocr.txt'))\n",
      "('AGFUUCT2P73LRPEVWZK7X7PVK4Z4ZNPL_normal.txt', PosixPath('Data/PDFs/AGFUUCT2P73LRPEVWZK7X7PVK4Z4ZNPL_normal.txt'))\n",
      "('CUF7M7BNFDH3UZPGBG55JYJHN4LVA3CP_ocr.txt', PosixPath('Data/PDFs/CUF7M7BNFDH3UZPGBG55JYJHN4LVA3CP_ocr.txt'))\n",
      "('RRANJ75ZAZCFUGZ32P2DOPAJJRK7L3EG_ocr.txt', PosixPath('Data/PDFs/RRANJ75ZAZCFUGZ32P2DOPAJJRK7L3EG_ocr.txt'))\n",
      "('SE6VNMZC7SS4KLSVUOXM3QR4FK2WJHWZ_normal.txt', PosixPath('Data/PDFs/SE6VNMZC7SS4KLSVUOXM3QR4FK2WJHWZ_normal.txt'))\n",
      "('StockReport_2017-02_normal.txt', PosixPath('Data/PDFs/StockReport_2017-02_normal.txt'))\n",
      "('ZRLDRES3WQDNUZBVIGADP5JISMEUTD5C_ocr.txt', PosixPath('Data/PDFs/ZRLDRES3WQDNUZBVIGADP5JISMEUTD5C_ocr.txt'))\n",
      "('af754bdf786579eb81414d411ef7c19f4e62ace6_ocr.txt', PosixPath('Data/PDFs/af754bdf786579eb81414d411ef7c19f4e62ace6_ocr.txt'))\n",
      "At file 18of24\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<----Combined---> \n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'text' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m chunks = chunk_text_with_overlap(txt_file=pdf_names[i][\u001b[32m1\u001b[39m])\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chunks))\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28msum\u001b[39m = \u001b[43msummarizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_token\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m results.append((pdf_names[i][\u001b[32m0\u001b[39m], \u001b[38;5;28msum\u001b[39m))\n\u001b[32m     27\u001b[39m record = {\n\u001b[32m     28\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfile_name\u001b[39m\u001b[33m\"\u001b[39m: pdf_names[i][\u001b[32m0\u001b[39m],\n\u001b[32m     29\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28msum\u001b[39m \n\u001b[32m     30\u001b[39m         }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mIterativeSummarizer.summarize\u001b[39m\u001b[34m(self, chunks, max_token)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m<----Combined--->\u001b[39m\u001b[33m\"\u001b[39m, combined)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Recursively call after re-chunking\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m new_chunks = \u001b[43mchunk_text_with_overlap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m900\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.summarize(new_chunks, max_token)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mchunk_text_with_overlap\u001b[39m\u001b[34m(txt_file, chunk_size, overlap, string)\u001b[39m\n\u001b[32m     58\u001b[39m chunks = []\n\u001b[32m     59\u001b[39m start = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m start < \u001b[38;5;28mlen\u001b[39m(\u001b[43mtext\u001b[49m):\n\u001b[32m     61\u001b[39m     end_ptr = \u001b[38;5;28mmin\u001b[39m(start + chunk_size, \u001b[38;5;28mlen\u001b[39m(text))\n\u001b[32m     62\u001b[39m     chunks.append(text[start:end_ptr])\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'text' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "evaluate_data_folder = Path(\"./Data/PDFs\")\n",
    "pdf_names = []\n",
    "for txt_file in evaluate_data_folder.rglob(\"*.txt\"):  # Get all .txt files in the folder\n",
    "        tup = tuple([str(txt_file.name),txt_file])\n",
    "        pdf_names.append(tup)\n",
    "\n",
    "#print(pdf_names)\n",
    "pdf_names.sort(key=lambda x: x[0])\n",
    "print(len(pdf_names))\n",
    "for p in pdf_names:\n",
    "        print((p))\n",
    "summarizer = IterativeSummarizer(model=adapted_model)\n",
    "results = []\n",
    "page = 0\n",
    "page_size = 0\n",
    "\n",
    "\n",
    "with open(\"adapter_inference.jsonl\", \"a\") as f:\n",
    "        for i in range(19, len(pdf_names)):\n",
    "                print(f\"At file {i}of{len(pdf_names)}\")\n",
    "                chunks = chunk_text_with_overlap(txt_file=pdf_names[i][1])\n",
    "                print(len(chunks))\n",
    "                sum = summarizer.summarize(chunks,max_token=120)\n",
    "                results.append((pdf_names[i][0], sum))\n",
    "                record = {\n",
    "                        \"file_name\": pdf_names[i][0],\n",
    "                        \"summary\": sum \n",
    "                        }\n",
    "                f.write(json.dumps(record) + \"\\n\")\n",
    "#For pdf[0] after 14m on M1pro:\n",
    "#<----Combined--->    The text discusses effective data incident management in healthcare, emphasizing email notification and adherence to organization-specific procedures. It also highlights understanding the causal chain of data incidents, identifying threat actors and vulnerabilities, and managing existing risks. Organizations are advised to strengthen their data incident management by implementing security controls and sharing best practices. Vulnerabilities are shared with trusted entities. The European Union's funding for humanitarian aid is mentioned, with the Centre for Humanitarian Data undertaking humanitarian activities with key partners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!<----------------------->!!!!\n",
      "!!!!<----------------------->!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: On August 11, 2017, 20 units of Jack's New England Glam Chowder were shipped. The unit price was $9.65. The shipped date was 2017-08-11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: On 2017-08-28, two products were shipped. The first, Gudbrandsdalsost, was shipped in a quantity of 20 at a unit price of 36.0, totaling 720.0. The second, Outback Lager, was shipped in a quantity of 15 at a unit price of 15.0, totaling 225.0. The total price for all products shipped was 945.0. Shipping took place on 2017-08-28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: Order ID 10481 was placed by Ricardo Adocicados (Customer ID: RICAR) on 2017-03-20 and shipped by Laura Callahan via United Package (Shipper ID: 2) on 2017-03-25. The order included 26 units of Maxiak at a unit price of 160. The order was shipped to Av. Copacabana, 257, Cidade de Janeiro, Brazil, 02389-890.\n",
      "On August 11, 2017, 20 units of Jack's New England Glam Chowder were shipped at a unit price of $9.65. On August 28, 2017, two products were shipped: 20 units of Gudbrandsdalsost at a unit price of 36.0, totaling 720.0, and 15 units of Outback Lager at a unit price of 15.0, totaling 225.0. The total price for all products shipped was 945.0. Order ID 10481 was placed by Ricardo Adocicados (Customer ID: RICAR) on March 20, 2017, and shipped by Laura Callahan via United Package (Shipper ID: 2) on March 25, 2017. The order was shipped to Av. Copacabana, 257, Cidade de Janeiro, Brazil, 02389-890.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmlhw_mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
