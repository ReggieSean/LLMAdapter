{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb6a7957695480db776f9456d86f76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seanhuang/miniconda3/envs/cmlhw_mac/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # or the base model you trained on\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Load base model in 4-bit if you used quantization\n",
    "from transformers import BitsAndBytesConfig\n",
    "if not torch.backends.mps.is_available():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    # manually set rope_scaling to supported structure:\n",
    "    config.rope_scaling = {\"type\": \"dynamic\", \"factor\": 2.0}\n",
    "    config.use_cache = True\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        config = config,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    adapted_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        config=config,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "else:\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    # manually set rope_scaling to supported structure:\n",
    "    config.rope_scaling = {\"type\": \"dynamic\", \"factor\": 2.0}\n",
    "    config.use_cache = True\n",
    "\n",
    "    adapted_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"mps\",\n",
    "        config=config,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "# Load your LoRA adapter\n",
    "adapter_path = \"./../Training/final_adapter_with_eval_1\"  # or wherever your adapter_model.safetensors is\n",
    "adapted_model= PeftModel.from_pretrained(adapted_model, adapter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to empty_cache other wise 2 models' results will bleed into each other\n",
    "# only slice and decode the new tokens \n",
    "def generate_summary(input_text, adapted_model, max_new_tokens=150):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    prompt = f\"\"\"Summarize:\\n{input_text} Summary:\\n\"\"\"\n",
    "\n",
    "#     prompt = f\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>\n",
    "# Text:\n",
    "# {input_text}\n",
    "\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(adapted_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = adapted_model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = outputs[0][input_len:]  # exclude prompt\n",
    "    summary = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def generate_base_summary(input_text, base_model, max_new_tokens=150):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    prompt = f\"\"\"Summarize:\\n{input_text} Summary:\\n\"\"\"\n",
    "#     prompt = f\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>\n",
    "# Text:\n",
    "# {input_text}\n",
    "# \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = outputs[0][input_len:]  # exclude prompt\n",
    "    summary = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_json_from_file(file_path):\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = file.read().strip()\n",
    "\n",
    "        # Fix concatenated JSON objects\n",
    "        objs = data.split(\"}{\")\n",
    "\n",
    "        parsed_objs = []\n",
    "        for i, obj in enumerate(objs):\n",
    "            if not obj.startswith(\"{\"):\n",
    "                obj = \"{\" + obj\n",
    "            if not obj.endswith(\"}\"):\n",
    "                obj = obj + \"}\"\n",
    "            try:\n",
    "                parsed = json.loads(obj)\n",
    "                parsed_objs.append(parsed)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in file {file_path}, object {i}: {e}\")\n",
    "    return parsed_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks with formats 300\n",
      "chunks of recipts that need ocr 300\n",
      "chunks of well written long papers, no ocr 300\n",
      "chunks of all sorts 300\n",
      "chunks with filtered formats 99\n",
      "chunks with formatted receipts 100\n",
      "chunks with formatted reports 30\n",
      "chunks with formatted randoms 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# example_file = \"./../Training/../Training/InputNLabel/prompt_sensitive_translated/receipts_2.json\"\n",
    "# file_and_limits = {\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_formatted.json\": 30,\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_randoms.json\": 30,\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_receipts.json\": 30,\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_reports.json\": 10,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/formatted_2.json\": 300,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/random_2.json\": 300,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/receipts_2.json\": 300,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/reports_2.json\": 300,\n",
    "# }\n",
    "formatted_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/formatted_2.json\"\n",
    "receipt_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/receipts_2.json\"\n",
    "report_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/reports_2.json\"\n",
    "random_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/random_2.json\"\n",
    "\n",
    "filtered_receipts = \"./../Training/InputNLabel/filtered_labels/filtered_receipts.json\"\n",
    "filtered_reports = \"./../Training/InputNLabel/filtered_labels/filtered_reports.json\"\n",
    "filtered_randoms = \"./../Training/InputNLabel/filtered_labels/filtered_randoms.json\"\n",
    "filtered_formatted = \"./../Training/InputNLabel/filtered_labels/filtered_formatted.json\"\n",
    "\n",
    "formatted = get_json_from_file(formatted_chunks)\n",
    "receipts = get_json_from_file(receipt_chunks)\n",
    "reports = get_json_from_file(report_chunks)\n",
    "randoms = get_json_from_file(random_chunks)\n",
    "\n",
    "f_receipts = get_json_from_file(filtered_receipts)\n",
    "f_reports = get_json_from_file(filtered_reports)\n",
    "f_randoms = get_json_from_file(filtered_randoms)\n",
    "f_formatted = get_json_from_file(filtered_formatted)\n",
    "\n",
    "print(\"chunks with formats\",len(formatted))\n",
    "print(\"chunks of recipts that need ocr\", len(receipts))\n",
    "print(\"chunks of well written long papers, no ocr\",len(reports))\n",
    "print(\"chunks of all sorts\",len(randoms))\n",
    "\n",
    "print(\"chunks with filtered formats\",len(f_formatted))\n",
    "print(\"chunks with formatted receipts\", len(f_receipts))\n",
    "print(\"chunks with formatted reports\", len(f_reports))\n",
    "print(\"chunks with formatted randoms\", len(f_randoms))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing filtered chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARKET\n",
      "Pesan as 62-2104\n",
      "se\n",
      "etna\n",
      "saa\n",
      "10h\n",
      "czy corn Eueees ” a\n",
      "Raurommcorse =|\n",
      "Aur isto.\n",
      "|| B\n",
      "ferecth a ne bibs\n",
      "B\n",
      "Stl\n",
      "a\n",
      "ate:\n",
      "f\n",
      "ital\n",
      "S\n",
      "Ei iam:\n",
      "paid\n",
      "Riles,\n",
      "ase\n",
      "1\n",
      "ip facoet mor\n",
      "his Er i Amn\n",
      "rug;\n",
      "AIT retro a e\n",
      "Wrveuty ov tee fc \n",
      "3B Ghar asia tea\n",
      "biti vat\n",
      "canter\n",
      "fc} tars ave a ele Taos\n",
      "a's Pras Stig, Vid tke\n",
      "{Sin ie a scon\n",
      "Tam 5 ack at ne fons e\n",
      "HR the tan Pres hoar\n",
      "(Shave a musi.cofet\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Output: <NULL>\n"
     ]
    }
   ],
   "source": [
    "input = f_receipts[0]\n",
    "print(input[\"input\"])\n",
    "print(\"\\nOutput:\",input[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!<---------Next Summary----------->!!!!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#print(generate_base_summary(input_text,150))\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m!!!!<---------Next Summary----------->!!!!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mgenerate_summary\u001b[39m\u001b[34m(input_text, adapted_model, max_new_tokens)\u001b[39m\n\u001b[32m      6\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mSummarize:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00minput_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Summary:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#     prompt = f\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Text:\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# {input_text}\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[43madapted_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     16\u001b[39m         outputs = adapted_model.generate(\n\u001b[32m     17\u001b[39m             **inputs,\n\u001b[32m     18\u001b[39m             do_sample=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m             top_p=\u001b[32m0.9\u001b[39m\n\u001b[32m     22\u001b[39m         )\n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'device'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "<NULL>\n"
     ]
    }
   ],
   "source": [
    "print(f_reports[0][\"input\"])\n",
    "print(f_receipts[0][\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Runs of trained adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!<----------------------->!!!!\n",
      "KFC Nederland's restaurant number 222, located at KFC 'Jan van GaJen, had a transaction in 2019. The items purchased were 1 Up 3 Hot Wings for EUR 9.95, 1 FireStackCh MI for EUR 10.65, a Pepsi Max for EUR 0.65, and fries for EUR 0.65. The total cost was EUR 12.60. Payment was made with Master Card. The transaction was closed at 18:58. The customer was thanked for their visit and offered feedback, with a chance to receive 3 free Hot Wings on their next purchase (from EUR 6).\n"
     ]
    }
   ],
   "source": [
    "input = receipts[0]\n",
    "#print(generate_base_summary(input[\"input\"]))\n",
    "print(\"!!!!<----------------------->!!!!\")\n",
    "print(generate_summary(input[\"input\"],adapted_model=adapted_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!<----------------------->!!!!\n",
      "!!!!<----------------------->!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The product, Jack's New England Glam Chowder, was shipped on 2017-08-11 in a quantity of 20 units. Each unit costs 9.65.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: The text indicates a shipment made on 2017-08-28. Two products were shipped: Gudbrandsdalsost with a quantity of 20 and a unit price of 36.0, totaling 720.0, and Outback Lager with a quantity of 15 and a unit price of 15.0, totaling 225.0. The total price for the shipment is 945.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: Order ID 10481 was placed by customer Ricardo Adocicados (Customer ID: RICAR) on 2017-03-20 and shipped by United Package (Shipper ID: 2) on 2017-03-25. The order, which was handled by employee Laura Callahan, included 26 units of the product Maxiak with a unit price of 160. The order was shipped to Av. Copacabana, 257, 02389-890, de Janeiro, Brazil, in the South America region.\n",
      "The product, Jack's New England Glam Chowder, was shipped on 2017-08-11 in a quantity of 20 units at 9.65 each. The text also mentions two other products, Gudbrandsdalsost and Outback Lager, shipped on 2017-08-28. Order ID 10481, placed by customer Ricardo Adocicados on 2017-03-20, was handled by Laura Callahan and shipped by United Package on 2017-03-25. The order included 26 units of Maxiak with a unit price of 160. The shipment was to Av. Copacabana, 257, 02389-890, de Janeiro, Brazil, in the South America region.\n"
     ]
    }
   ],
   "source": [
    "#trained adapter 0 after 72 examples: there is still original text but a summary is now  more concise\n",
    "input_text = formatted[0][\"input\"] + formatted[1][\"input\"] + formatted[2][\"input\"]\n",
    "\n",
    "#input_text = f_receipts[0][\"input\"]\n",
    "print(\"!!!!<----------------------->!!!!\")\n",
    "#print(generate_base_summary(input_text))\n",
    "print(\"!!!!<----------------------->!!!!\")\n",
    "summary_0 = generate_summary(formatted[0][\"input\"],adapted_model=adapted_model,max_new_tokens=200)\n",
    "print(\"0:\",summary_0)\n",
    "summary_1 = generate_summary(formatted[1][\"input\"],adapted_model=adapted_model,max_new_tokens=200)\n",
    "print(\"1:\",summary_1)\n",
    "summary_2 = generate_summary(formatted[2][\"input\"],adapted_model=adapted_model,max_new_tokens=200)\n",
    "print(\"2:\", summary_2)\n",
    "print(generate_summary(summary_0 + summary_1 + summary_2, adapted_model=adapted_model, max_new_tokens=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!<----------------------->!!!!\n",
      "!!!!<----------------------->!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: GFNet\n",
      "80.3\n",
      "13\n",
      "5.1\n",
      "GFNet-10\n",
      "79.9\n",
      "13\n",
      "5.1\n",
      "GFNet-18\n",
      "80.3\n",
      "24\n",
      "6.2\n",
      "GFNet-50\n",
      "80.3\n",
      "13\n",
      "5.1\n",
      "GFNet-101\n",
      "80.3\n",
      "13\n",
      "5.1\n",
      "FNet\n",
      "79.6\n",
      "12\n",
      "4.3\n",
      "FNet-10\n",
      "79.5\n",
      "12\n",
      "4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: <NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: RVT-Ti∗and RVT-B∗achieve a better performance than DeiT in the robustness test. RVT-Ti∗can\n",
      "achieve 5.4% and 4.1% improvement compared with DeiT on CIFAR-10 and CIFAR-100. RVT-B∗can\n",
      "achieve 1.1% and 0.8% improvement compared with DeiT on CIFAR-10 and CIFAR-100. The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "<NULL> 1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "\n",
      "!!!!<----------------------->!!!!\n",
      "6.3.1\n",
      "Robustness in training data\n",
      "Training data robustness is generally stronger than the test data robustness, as the training data is more diverse and includes more examples of each class. The test data robustness is more limited to the examples that are seen during the test phase.\n",
      "6.3.2\n",
      "Robustness in the model\n",
      "The model's robustness can be improved by using techniques such as data augmentation, batch normalization, and weight decay. These techniques can help the model to be more robust to small perturbations and to generalize better to unseen data.\n",
      "6.3.3\n",
      "Robustness in the test data\n",
      "The test data robustness is more limited than the training data robustness, as\n"
     ]
    }
   ],
   "source": [
    "input_text = reports[0][\"input\"] + reports[1][\"input\"] + reports[2][\"input\"]\n",
    "\n",
    "#input_text = f_receipts[0][\"input\"]\n",
    "print(\"!!!!<----------------------->!!!!\")\n",
    "#print(generate_base_summary(input_text))\n",
    "print(\"!!!!<----------------------->!!!!\")\n",
    "summary_0 = generate_summary(reports[0][\"input\"],adapted_model=adapted_model,max_new_tokens=100)\n",
    "print(\"0:\",summary_0)\n",
    "summary_1 = generate_summary(reports[1][\"input\"],adapted_model=adapted_model,max_new_tokens=100)\n",
    "print(\"1:\",summary_1)\n",
    "summary_2 = generate_summary(reports[2][\"input\"],adapted_model=adapted_model,max_new_tokens=100)\n",
    "print(\"2:\", summary_2)\n",
    "print(generate_summary(summary_0 + summary_1 + summary_2, adapted_model=adapted_model, max_new_tokens=300))\n",
    "print(\"!!!!<----------------------->!!!!\")\n",
    "print(generate_summary(input_text,adapted_model=adapted_model, max_new_tokens=150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!<----------------------->!!!!\n",
      "The customer, Ricardo Adocicados, has placed an order for the following products:\n",
      "- Maxiak (26 units)\n",
      "- Gudbrandsdalsost (20 units)\n",
      "- Outback Lager (15 units)\n",
      "The order was shipped on March 25, 2017, from the United Package shipping company.\n",
      "The total price of the order is $945.00.\n",
      "The customer's shipping details are:\n",
      "- Ship Name: Ricardo Adocicados\n",
      "- Ship Address: Av. Copacabana, 257\n",
      "- Ship City: de Janeiro\n",
      "- Ship Postal Code: 02389-890\n",
      "- Ship County: Brazil\n",
      "- Ship Region: South America\n",
      "The customer's order details are:\n",
      "- Order Date:\n"
     ]
    }
   ],
   "source": [
    "print(\"!!!!<----------------------->!!!!\")\n",
    "print(generate_base_summary(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!<----------------------->!!!!\n",
      "The text is a shipping order from Ricardo Adocicados to United Package. The order was placed on 2017-03-20 and shipped on 2017-03-25. The order included two products: Maxiak with a quantity of 26 and unit price of 160.0, and Outback Lager with a quantity of 15 and unit price of 15.0. The total price was 945.0. The shipping details were sent to Ricardo Adocicados at Av. Copacabana, 257, Copacabana, de Janeiro, South America, 02389-890, Brazil. The order was handled by employee Laura Callahan and shipped by United Package.\n"
     ]
    }
   ],
   "source": [
    "print(\"!!!!<----------------------->!!!!\")\n",
    "print(generate_summary(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary  from one recursive pass of 3 summaries of original text \n",
    "\"\"\"The Jack's New England Glam Chowder was shipped on August 11, 2017. The product was sold in a unit of 20, with each unit priced at 9.65. The shipment took place on 2017-08-28. The products included were Gudbrandsdalsost with a quantity of 20 units at a unit price of 36.0, totaling 720.0, and Outback Lager with a quantity of 15 units at a unit price of 15.0, totaling 225.0. The overall total price was 945.0. Order ID 10481 was placed by Ricardo Adocicados (Customer ID: RICAR) from Av. Copacab\"\"\"\n",
    "\n",
    "#Summary  of adapted model from 1 pass of 3  chunks of original text \n",
    "\"\"\"The text is a shipping order from Ricardo Adocicados to United Package. The order was placed on 2017-03-20 and shipped on 2017-03-25. The order included two products: Maxiak with a quantity of 26 and unit price of 160.0, and Outback Lager with a quantity of 15 and unit price of 15.0. The total price was 945.0. The shipping details were sent to Ricardo Adocicados at Av. Copacabana, 257, Copacabana, de Janeiro, South America, 02389-890, Brazil. The order was handled by employee Laura Callahan and shipped by United Package.\"\"\"\n",
    "\n",
    "#Summary of base model from 3 chunks of original text\n",
    "\"\"\"\n",
    "The customer, Ricardo Adocicados, has placed an order for the following products:\n",
    "- Maxiak (26 units)\n",
    "- Gudbrandsdalsost (20 units)\n",
    "- Outback Lager (15 units)\n",
    "The order was shipped on March 25, 2017, from the United Package shipping company.\n",
    "The total price of the order is $945.00.\n",
    "The customer's shipping details are:\n",
    "- Ship Name: Ricardo Adocicados\n",
    "- Ship Address: Av. Copacabana, 257\n",
    "- Ship City: de Janeiro\n",
    "- Ship Postal Code: 02389-890\n",
    "- Ship County: Brazil\n",
    "- Ship Region: South America\n",
    "The customer's order details are:\n",
    "- Order Date:\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmlhw_mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
