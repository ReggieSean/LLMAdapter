{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # or the base model you trained on\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Load base model in 4-bit if you used quantization\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "# manually set rope_scaling to supported structure:\n",
    "config.rope_scaling = {\"type\": \"dynamic\", \"factor\": 2.0}\n",
    "config.use_cache = True\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    config=config,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# # Load your LoRA adapter\n",
    "# adapter_path = \"./../Training/final_adapter_with_eval_1\"  # or wherever your adapter_model.safetensors is\n",
    "# adapted_model= PeftModel.from_pretrained(base_model, adapter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to empty_cache other wise 2 models' results will bleed into each other\n",
    "# only slice and decode the new tokens \n",
    "def generate_summary(input_text, adapted_model, max_new_tokens=150):\n",
    "\n",
    "    prompt = f\"\"\"Summarize:\\n{input_text} Summary:\\n\"\"\"\n",
    "\n",
    "#     prompt = f\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>\n",
    "# Text:\n",
    "# {input_text}\n",
    "\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(adapted_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = adapted_model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = outputs[0][input_len:]  # exclude prompt\n",
    "    summary = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def generate_base_summary(input_text, base_model, max_new_tokens=150):\n",
    "    prompt = f\"\"\"Summarize:\\n{input_text} Summary:\\n\"\"\"\n",
    "#     prompt = f\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>\n",
    "# Text:\n",
    "# {input_text}\n",
    "# \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = outputs[0][input_len:]  # exclude prompt\n",
    "    summary = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def chunk_text_with_overlap(txt_file=\"\", chunk_size=500, overlap=100, string=\"\"):\n",
    "    \"\"\"Splits text into overlapping chunks.\"\"\"\n",
    "    if txt_file:\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    if string:\n",
    "        text = string\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end_ptr = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end_ptr])\n",
    "        start += chunk_size - overlap  # Move forward while keeping overlap\n",
    "\n",
    "    # for chunk in chunks:\n",
    "    #     print(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def rate(summary):\n",
    "        prompt = f\"?Is this useful:{summary}\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = base_model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                max_new_tokens=10,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        new_tokens = outputs[0][input_len:]  # exclude prompt\n",
    "        score = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "        print(\"rating:\", score)\n",
    "        # try:\n",
    "        #     score  = float(score)\n",
    "        # except :\n",
    "        #     score = 0\n",
    "        return score \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_summary(text):\n",
    "    return (\n",
    "        text.replace('NULL', '')\n",
    "            .replace('<', '')\n",
    "            .replace('>', '')\n",
    "            .replace('\\n', '')\n",
    "            .replace('--', '')\n",
    "            .replace('Summary:', '')\n",
    "    )\n",
    "class IterativeSummarizer:\n",
    "    def __init__(self,model):\n",
    "         self.model = model\n",
    "     \n",
    "    def summarize(self, chunks, max_token):\n",
    "          if len(chunks) == 1:\n",
    "               return chunks[0]  # Base case: only one chunk remains\n",
    "\n",
    "          summarized_chunks = []\n",
    "\n",
    "          for chunk in chunks:\n",
    "               summary = generate_base_summary(chunk, base_model=self.model, max_new_tokens=max_token)\n",
    "               cleaned = clean_summary(summary)\n",
    "               summarized_chunks.append(cleaned)\n",
    "\n",
    "          # Combine and re-chunk\n",
    "          combined = \" \".join(summarized_chunks)\n",
    "          combined = combined.strip() \n",
    "          print(\"<----Combined--->\", combined)\n",
    "\n",
    "          # Recursively call after re-chunking\n",
    "          new_chunks = chunk_text_with_overlap(string=combined, chunk_size=900)\n",
    "          return self.summarize(new_chunks, max_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "evaluate_data_folder = Path(\"./Data/PDFs\")\n",
    "pdf_names = []\n",
    "for txt_file in evaluate_data_folder.rglob(\"*.txt\"):  # Get all .txt files in the folder\n",
    "        tup = tuple([str(txt_file.name),txt_file])\n",
    "        pdf_names.append(tup)\n",
    "\n",
    "#print(pdf_names)\n",
    "pdf_names.sort(key=lambda x: x[0])\n",
    "print(len(pdf_names))\n",
    "for p in pdf_names:\n",
    "        print((p))\n",
    "summarizer = IterativeSummarizer(model=base_model)\n",
    "results = []\n",
    "page = 0\n",
    "page_size = 0\n",
    "with open(\"base_inference.jsonl\", \"a\") as f:\n",
    "        for i in range(6, len(pdf_names)):\n",
    "                print(f\"At file {i}of{len(pdf_names)}\")                \n",
    "                chunks = chunk_text_with_overlap(txt_file=pdf_names[i][1])\n",
    "                print(len(chunks))\n",
    "                sum = summarizer.summarize(chunks,max_token=100)\n",
    "                results.append((pdf_names[i][0], sum))\n",
    "                record = {\n",
    "                        \"file_name\": pdf_names[i][0],\n",
    "                        \"summary\": sum \n",
    "                        }\n",
    "                f.write(json.dumps(record) + \"\\n\")\n",
    "#For pdf[0] after 14m on M1pro:\n",
    "#<----Combined--->    The text discusses effective data incident management in healthcare, emphasizing email notification and adherence to organization-specific procedures. It also highlights understanding the causal chain of data incidents, identifying threat actors and vulnerabilities, and managing existing risks. Organizations are advised to strengthen their data incident management by implementing security controls and sharing best practices. Vulnerabilities are shared with trusted entities. The European Union's funding for humanitarian aid is mentioned, with the Centre for Humanitarian Data undertaking humanitarian activities with key partners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmlhw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
