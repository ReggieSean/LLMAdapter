MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy
on ImageNet without Tricks
Zhiqiang Shen
Marios Savvides
Carnegie Mellon University
Code: https://github.com/szq0214/MEAL-V2
Abstract
We introduce a simple yet effective distillation frame-
work that is able to boost the vanilla ResNet-50 to 80%+
Top-1 accuracy on ImageNet without tricks. We construct
such a framework through analyzing the problems in exist-
ing classiﬁcation system and simplify the base method en-
semble knowledge distillation via discriminators [42] by:
(1) adopting the similarity loss and discriminator only on
the ﬁnal outputs and (2) using the average of softmax prob-
abilities from all teacher ensembles as the stronger super-
vision. Intriguingly, three novel perspectives are presented
for distillation: (1) weight decay can be weakened or even
completely removed since the soft label also has a regular-
ization effect; (2) using a good initialization for students is
critical; and (3) one-hot/hard label is not necessary in the
distillation process if the weights are well initialized. We
show that such a straight-forward framework can achieve
state-of-the-art results without involving any commonly-
used techniques, such as architecture modiﬁcation; outside
training data beyond ImageNet; autoaug/randaug; cosine
learning rate; mixup/cutmix training; label smoothing; etc.
Our method obtains 80.67% top-1 accuracy on ImageNet
using a single crop-size of 224×224 with vanilla ResNet-50,
outperforming the previous state-of-the-arts by a signiﬁcant
margin under the same network structure. Our result can be
regarded as a strong baseline using knowledge distillation,
and to our best knowledge, this is also the ﬁrst method that
is able to boost vanilla ResNet-50 to surpass 80% on Ima-
geNet without architecture modiﬁcation or additional train-
ing data. On smaller ResNet-18, our distillation framework
consistently improves from 69.76% to 73.19%, which shows
tremendous practical values in real-world applications.
1. Introduction
Convolutional Neural Networks (CNNs) [28] have been
proven useful in many visual tasks, such as image classi-
ﬁcation [26, 15], object detection [11, 39], semantic seg-
+2.20
+2.25
+1.72
+1.49
+1.60
EfficientNet-B0
ResNet-50
Mobile-V3-L1.0
Mobile-V3-S1.0
Mobile-V3-S0.75
Figure 1. An illustration of improvement with our MEAL V2 on
ImageNet. The architectures from left to right are MobileNet V3-
Small 0.75 [18], MobileNet V3-Small 1.0 [18], MobileNet V3-
Large 1.0 [18], EfﬁcientNet-B0 [47] and ResNet-50 [15].
mentation [33], as well as some particular scenarios, like
transferring feature representation [54], learning detectors
from scratch [43], etc. In order to achieve highest possi-
ble accuracy, many training techniques and data augmenta-
tion methods have been proposed, such as mixup [56], cut-
mix [55], autoaug [3], randaug [4], ﬁx resolution discrep-
ancy [49], etc. Some works also focus on modifying the
network structures, e.g., SE module [20], Stem block [43],
Split-attention [57]. This paper is to similarly obtain the
best possible performance of a network, but our proposed
method is orthogonal to the above techniques. In general,
our method only relies on a teacher-student paradigm with
a powerful ensemble of teachers and a good initialization
of the student. It is simple, straight-forward, but effective
and can achieve state-of-the-art performance on large-scale
dataset. The advantages of our method are: 1) no architec-
ture modiﬁcation; 2) no outside training data beyond Ima-
geNet; 3) no complex learning rate scheduler like cosine lr;
4) no extra data augmentation like mixup, autoaug, etc.
The objective of this paper is to give a better understand-
ing on knowledge distillation and promote the capability
and robustness of the classiﬁcation networks through dis-
tillation. We ﬁrst analyze and introduce several critical fac-
tors and limitations that will degrade the performance in the
existing classiﬁcation systems. We ﬁnd the main drawback
1
arXiv:2009.08453v2  [cs.CV]  19 Mar 2021

--------------------------------------------------
in the conventional training strategy of a network, i.e. with
the one-hot label, is the inferior ability to distinguish the
semantically similar categories, as shown in Fig. 2 (1). Net-
works trained with one-hot label are incapable of handling
the semantically similar instances. We observe knowledge
distillation [17] is surprisingly effective in dealing with this
circumstance as the supervision from teacher networks is
smoothed and much lower than one-hot value. Therefore,
the distilled students will encourage representations of ex-
amples to lie in tight equally separated clusters and enforce
similar instances more distinguishable in feature space, sim-
ilar to label smoothing [36, 44]. We show multiple promis-
ing improvements on various network architectures using
distillation in Fig. 1.
The potential improvement of our
method can be larger if replacing with stronger teachers.
We also have a few interesting discoveries in our train-
ing process, for example, among them we would like to
emphasize that the one-hot/hard label is not necessary if
the weights are already well initialized and could not be
used in the distillation process [17, 42]. Some discussions
about this perspective are provided in our Appendix. Also,
weight decay can be weakened or even removed since the
soft label also has a regularization effect, and using a good
initialization is critical for distillation. While some previ-
ous studies deem that structure might be more crucial than
pre-trained parameters on some downstream tasks like ob-
ject detection [43], segmentation [24], etc., we still believe
that boosting the performance of standard and classical net-
work structures is interesting and useful, especially if the
networks are already small and compact, like MobileNet
V3, EfﬁcientNet-B0, since the proposed method can be ef-
fortlessly generalized to other elaborated or searched archi-
tectures. Our method can be considered as a post-process to
distill small and compact models for further boosting their
performance, while no modiﬁcation is required.
Our contributions in this paper are as follows:
• We provide empirical analysis and insights through in-
dividual class’s accuracy to expose the mechanism of
how knowledge distillation helps classiﬁcation. It is
not trivial to understand the principles behind it.
• We present a simple yet effective and practical frame-
work that can boost the performance of existing tiny
models by a signiﬁcant marginal. We show evidence
and provide a demonstration, and also give detailed
guidance on how to establish such a strong framework.
• We further visualize our model to explore where the
superior performance is from. Moreover, we transfer
trained parameters to other datasets like ﬁne-grained
recognition to show the transferability of our method.
2. Related Work
Image Classiﬁcation. Image classiﬁcation is a fundamen-
tal task in computer vision. AlexNet [26] is considered as
the seminal design that is proven feasible for deep neural
networks on the large-scale datasets. After that, many in-
novative network structures have been proposed. Szegedy
et al. [45] proposes an “Inception” design that concatenates
features maps produced by various sizes of kernels. He et
al. [15] creatively proposed residual blocks with skip con-
nections, which is ﬁrstly enable to train extremely deep
networks more than 100 layers.
Huang et al. [21] fur-
ther proposed densely layer-wise connections for build-
ing DenseNet.
Besides, some architectures are also tar-
geting at mobile device scenario, such as MobileNet se-
ries [19, 41, 18], ShufﬂeNet [58, 34], etc. With the devel-
opment of these modern neural network designs and auto-
matic architecture search [61, 47], this task has been one
of the fastest moving areas and achieved surprising results
which even surpasses human-level performance on large-
scale datasets like ImageNet [5] and OpenImage [27].
Knowledge Distillation. Hinton et al. [17] pioneered the
concept of distilling knowledge from a larger teacher net-
work or ensemble into a smaller compressed student. Math-
ematically, this paradigm of training the student on soft-
ened teacher predictive distribution is using the conven-
tional cross-entropy with predicted labels. The student is
encouraged to mimic the teacher output distribution, which
helps the student generalize much better on validation set
and in certain cases leads to the student performing even
better than the teacher itself. These studies argued that the
teacher distribution provided much richer information about
an image compared to just one-hot labels. Further studies
extended this concept by using internal feature representa-
tions [40, 42], adversarial training with discriminators [42]
and transfer ﬂow of solution procedure matrix as the student
initialization [53]. Some works also proposed online distil-
lation [51, 60] that do not rely on a pre-trained teacher, so
teacher and student can be learned simultaneously.
Network Compression. Knowledge distillation is a natu-
ral way to produce the compressed student network through
imitating the teacher’s softened prediction. Besides it, other
methods like weight quantization [22, 59, 23] and bina-
rization [2, 38], weight pruning [14, 13] and channel prun-
ing [29, 32, 16] can also achieve the compression purpose.
Knowledge distillation differs from them as the compressed
network is designed before training so there is no additional
operation required, such as reconstruction, retraining, etc.
3. Analysis: Problems in Existing Classiﬁca-
tion System
Consider a classiﬁcation task that distinguishes various
breeds of dogs, e.g. toy poodle, miniature poodle, etc., the
output predictive distribution of a higher capability teacher
always provides the student model with the extra informa-
tion of how alike one breed of dogs looks to the other. This
helps the student learn more generalized features of each
2

--------------------------------------------------
4%
0%
8%
12%
Semantically
dissimilar
Semantically
similar
ours
(1) Top-1 Accuracy
(2) Clustering
vanilla
tench
jay
toy poodle
standard poodle
(3) Supervision Distribution
tench
jay
toy poodle
standard poodle
tench
jay
toy poodle
standard poodle
Figure 2. (1) is the comparison of class-wise accuracy on four semantically similar and dissimilar classes between PyTorch ofﬁcial model
and ours. (2) is the feature embedding visualization using t-SNE [35] on these four classes. (3) is the visualization of supervision for
one-hot label and soft label in distillation.
dog breed compared of providing just one hot-label which
fails to supply any comparative knowledge. Also, in the
process of trying to mimic the distribution of a much deeper
teacher model the student tries to ﬁnd a compact solution
of transformation. This inherently enforces the student to
explore more informative knowledge and generalize better.
Impressively, in certain cases the student manages to out-
perform its teacher due to this superior generalization.
Training with one-hot labels accompanying with cross-
entropy loss is a “balanced” learning system, which means
the objective will enforce each class to be equidistant to all
remaining class’s distance, so the learned model is not sen-
sitive to the semantically similar classes (e.g., different dog
breeds) or dissimilar classes (e.g., dog and ﬁsh). Some situ-
ations it even gives risk to performing with respect to incor-
rect annotation of classes. By using dynamical soft labels
from knowledge distillation of a strong teacher, different ex-
amples from the same or different classes can have very dif-
ferent similarities to other classes, thus the student can cap-
ture additional subtle information and prevent from overﬁt-
ting. We illustrate training curves in Fig. 3 by using one-
hot/hard and soft labels. We use exactly the same training
hyper-parameters and settings, good initialized parameters,
learning rate schedule, etc. We found if the initialization
is already well-learned, training with one-hot label is easy
to be overﬁtting (blue curves), while with soft labels (or-
ange curves), the model can continue to learn new knowl-
edge and generalize better on the validation set. Moreover,
we select two semantically similar classes (toy poodle and
standard poodle) and two dissimilar classes (tench and jay),
and test their accuracy on ImageNet val set with the moder-
ate and extremely good ResNet-501. Results are shown in
Table 1, interestingly, it can be observed the overall accu-
racy gap mainly hinges on the semantically similar classes.
Intuitively, these classes are difﬁcult to distinguish in a clas-
siﬁcation system thus the bottleneck also lies here. In this
1PyTorch ofﬁcial model (76.15%) and ours (80.67%).
Figure 3. Comparison of Top-1 (left) and 5 (right) val accuracy by
using hard and soft labels. We use exactly the same training hyper-
parameters and settings, including the same initialized parameters,
learning rate schedule, etc., but the different supervision shapes.
Table 1. Comparison of class-wise accuracy on four semantically
similar and dissimilar classes. “Vanilla” is PyTorch ofﬁcial model.
Semantically dissimilar
Semantically similar
Accuracy
Model tench (%)
jay (%)
toy poodle (%) standard poodle (%)
Avg (%)
Vanilla
90
92
58
80
76.15
Ours
94+4%
92+0%
66 +8%
92+12%
80.67 +4.52%
paper, we aim to diagnose these problems, we will give in-
sights about how distillation method alleviates them below.
4. Solution: What Can KD Solve?
In this section, we discuss the following aspects that
knowledge distillation can handle in the modern classiﬁca-
tion system: (1) enlarge the distance of samples between
semantically similar classes; (2) overcome multiple objects
problem; (3) take advantage of random crop augmentation
and avoid its limitations.
4.1. Semantically Similar and Dissimilar Classes
As aforementioned in Table 1 and Fig. 2 (1), the perfor-
mance of good or moderate models primarily depends on
the semantically similar classes. To further diagnose how
this happens, we visualize the embedding distributions in
Fig. 2 (2). As expected, the clusters of toy and standard poo-
dle breeds are mixed up together, and tench and jay are sep-
3

--------------------------------------------------
GT: tench
Random Crop 
Augmentation
Multi-object
Figure 4. Illustration of random crop data augmentation strategy
on an image. This strategy randomly crops regions crossing a pre-
deﬁned scope, e.g. 8%∼100% of the whole image size on Ima-
geNet but it will involve massive noises and incorrect labels.
arate from each other. Impressively, the model trained with
knowledge distillation can split the similar classes represen-
tations to some extend and make the border of two clusters
clearer, which greatly facilitates the ﬁnal classifying. Let us
take this one step further to see how knowledge distillation
obtains this function, we visualize the supervisions from
teacher ensemble in Fig. 2 (3). This is the main difference in
our framework when learning with one-hot or distilled su-
pervisions. we are interested and illustrate the supervisions
from the training set since those are the ones used for dis-
tillation. We show the averaged probability of all samples
in each class. The prediction of major class is only 0.3∼0.4
comparing to 1.0 as one-hot labels. These softened labels
make models less conﬁdent and generalize much better, es-
pecially on those semantically similar classes.
4.2. Multi-Object/Random-Crop Issues
Multi-object is the scenario where there are at least two
categories in which the images can be classiﬁed, this is a
widely existing situation in ImageNet dataset as shown in
Fig. 4. The one-hot label of this image is tench but a human
is also contained in it and the area is even larger than the
tench. It causes a label mismatch problem if using standard
one-hot label for training. This paper argues that distilla-
tion can tackle this circumstance as the label is predicted by
a well pretrained teacher network and label distribution de-
pends on the content of input image instead of the assigned
labels. Also, the soft label can be a multi-peak distribution
which can model the mixed information of multiple objects.
Random crop data augmentation is an indispensable
technique heavily utilized in modern network training.
While as shown in Fig. 4, this strategy tends to involve a
large proportion of noise if cropping on the background area
or a small region of objects, which means it will always re-
sult in inaccurate labels for the augmented regions. In the
case of incorrect region by the random cropping, the global
ground-truth label is used for such input. Despite deep neu-
ral networks are highly tolerant to noises in labels but the in-
correctness will inevitably impair the effectiveness of learn-
ing process. Instead, knowledge distillation will predict the
true probability distribution for each input independently
and reﬂect what the input patch really is.
5. Framework Components
In this section, we begin by introducing each component
in our proposed framework, including: 1) teacher ensem-
ble; 2) KL-divergence loss; 3) the discriminator. Then, we
present the training details and techniques that we used and
did not use in our distillation training.
Teachers Ensemble is used to generate more accurate pre-
dictions for guiding the student training. Different from
MEAL [42] that selected one teacher through a teacher
selection module in each training iteration, we adopt the
average of softmax probabilities from multiple pre-trained
teachers as an ensemble. Let Tθ be the teacher network, the
output ensemble probability ˆpTθ
e can be described as:
ˆpTθ
e (X) = 1
K
K
X
t=1
pt
Tθ(X)
(1)
where pTθ
t
is the t-th teacher’s softmax prediction. X is
the inout image and K is the number of total teachers. e
denotes the ensembled probability.
KL-divergence is a measure metric of how one probability
distribution is different from another reference distribution.
In our approach, we train the student network Sθ by mini-
mizing the KL-divergence between its output pSθ(xi) and
the ensembled soft labels ˆpTθ(xi) generated by the teacher
ensemble. The loss function of KL-divergence can be for-
mulated as (temperature is used as 1 following [42]):
LKL(Sθ) = −1
N
N
X
i=1
ˆpTθ
e (xi) log(pSθ(xi)
ˆpTθ
e (xi)
)
(2)
where N is the number of samples. In practice, we can sim-
ply minimize the equivalent cross-entropy loss as follows:
LCE(Sθ) = −1
N
N
X
i=1
ˆpTθ
e (xi) logpSθ(xi)
(3)
Discriminator is a binary classiﬁer to distinguish the input
features are from teacher ensemble or student network. It
consists of a sigmoid function following the binary cross-
entropy loss. The loss can be formulated as:
LD = −1
N
N
X
i=1

yi · log ˆpD
i + (1 −yi) · log(1 −ˆpD
i )

(4)
4

--------------------------------------------------
Input
Student outputs
Teacher?
Student?
!
!!
!"
Teacher
Selection
Module
...
KL Loss
Student
(a) MEAL
Teacher output
Student outputs
!!
!"
1
% # $!
Input
KL Loss
Student
(b) Ours
Teacher Ensembles
Teacher?
Student?
!
Teacher mean output
Figure 5. An illustration of the comparison between MEAL [42] and our method. We use an ensemble of all teacher networks instead of
the teacher selection module as adopted in MEAL.
Table 2. Item-by-item comparison of techniques that we use and do not use in our distillation training.
What we do not use
What we use
architecture modiﬁcation

an ensemble of giant pre-trained teachers

outside training data

KL divergence loss

hard/one-hot labels during distillation

a good initialization for the student

cosine/linear decay learning rate

step decay/milestone learning rate (0.01-0.001)

weight-decay

cutout [6]/mixup [56]/cutmix [55] training

label smoothing [46]

autoaug [3]/randaug [4], etc.

warmup [12]

where yi is the binary label for the input features xi, y ∈
{0, 1}, and ˆpD
i is the corresponding probability vector.
We deﬁne a sigmoid function to model the individual
teacher or student probability:
ˆpD(x; θ) = σ(fθ({xT , xS}))
(5)
where fθ is a three-fc-layer subnetwork and θ is its param-
eters, σ(x) = 1/(1 + exp(−x)) is the logistic function. In
our model, we use the last output layer before softmax as
the representation for the discriminator input.
Consider that our teacher supervision is an ensemble of
multiple networks, it is not convenient to obtain the interme-
diate outputs. Also, to make the whole framework neater,
we only adopt the similarity loss and discriminator on the
ﬁnal outputs of networks for distillation. We show from our
experimental results that supervision from the last layer of
teacher ensemble is competent to distill a strong student.
5.1. Model Capacity and Weight Decay
Weight decay is a widely used regularization technique
in neural networks while it is not used in our framework. It
is worthwhile to discuss the motivation behind this choice.
Weight decay is basically delivering the same effects as L2
regularization. Since L2 will penalize the large parameters
in a network (as shown in Fig. 8), in our perspective, such an
operation will impair the capacity of a network. We illus-
trate a comparison in Fig. 6 (right) of using weight decay
and without it. It is curious to ask why previous models
need weigh decay and it also seems helpful. We conjecture
most of the previous networks are not yet saturated even
those are trained with massive data augmentation and more
training epochs, hence weight decay can help to prevent
from overﬁtting and learn more information, the lost capac-
ity is negligible and not so necessary. But for our initialized
model, the performance is already high and we guess it is
somewhat close to the upper bound of the network itself’s
capability, so the loss of capacity will be crucial for a net-
work and weight decay may be harmful. Moreover, since
our supervision from a strong teacher ensemble is fairly pre-
cise, the student should have enough capability to mimic
such distribution, weight decay may not be applicable as it
will reduce the complexity of a network. Also, the soft label
itself in distillation has the regularization effect, so weight
decay is not so necessary for distillation framework.
6. Experiments
Main Dataset. We conduct experiments on ILSVRC 2012
classiﬁcation dataset [5] that consists of 1,000 classes, with
a number of 1.2 million training images and 50,000 valida-
tion images. We adopt the basic data augmentation scheme
following [37], i.e., RandomResizedCrop and RandomHor-
izontalFlip, and apply the single-crop operation at test time.
Transfer Learning Datasets. We study the transferabil-
ity of our learned models on two mainstream tasks: the
multiple-object/ﬁne-grained classiﬁcation and object detec-
tion. We conduct experiments on the following datasets:
PASCAL VOC 2007 [8], CUB200-2011 [50], Birdsnap [1]
and CIFAR-10 [25] for classiﬁcation, and COCO [31] with
RetinaNet [30] for detection.
6.1. Experimental Settings
We use a mini-batch size of 512 with 8 GPUs for training
our models. SGD optimizer is adopted with a step learning
5

--------------------------------------------------
Table 3. Comparison of validation accuracy on ImageNet dataset for ResNet-50 architecture under single crop evaluation. (*) indicates that
they used horizontal ﬂip, shifted center crop and color jittering for training.
Network
Resolution
#Params
Top-1 (%)
Top-5 (%)
ResNet-50
224
25.6M
76.15
92.86
ResNet-50 + DropBlock, (kp=0.9) [10]
224
25.6M
78.13
94.02
ResNet-50 + DropBlock (kp=0.9) [10] + label smoothing (0.1)
224
25.6M
78.35
94.15
ResNet-50 + MEAL [42]
224
25.6M
78.21
94.01
ResNet-50 + Ours (MEAL V2)
224
25.6M
80.67
95.09
ResNet-50 + FixRes [49]
384
25.6M
79.0
94.6
ResNet-50 + FixRes (*) [49]
384
25.6M
79.1
94.6
ResNet-50 + Ours (MEAL V2)
380
25.6M
81.72
95.81
ResNet-50 + FixRes [49] + CutMix
320
25.6M
79.7
94.9
ResNet-50 + FixRes [49] + CutMix (*)
320
25.6M
79.8
94.9
ResNet-50 + Ours (MEAL V2) + CutMix
224
25.6M
80.98
95.35
Table 4. Comparison of validation accuracy on ImageNet for MobileNet V3-Small 0.75/1.0/Large 1.0 and EfﬁcientNet-B0 architectures.
Network
Resolution
#Params
Top-1 (%)
Top-5 (%)
MobileNet V3-Small 0.75 [18]
224
2.04M
65.40
–
+ Ours (MEAL V2)
224
2.04M
67.60
87.23
MobileNet V3-Small 1.0 [18]
224
2.54M
67.40
–
+ Ours (MEAL V2)
224
2.54M
69.65
88.71
MobileNet V3-Large 1.0 [18]
224
5.48M
75.20
–
+ Ours (MEAL V2)
224
5.48M
76.92
93.32
EfﬁcientNet-B0 [47]
224
5.29M
77.3 (76.8)
93.5 (93.2)
+ Ours (MEAL V2)
224
5.29M
78.29
93.95
rate decay scheduler. The initial learning rate is set to 0.01.
We train with a total number of 180 epochs and the learning
rate multiplied by 0.1 at 100 epoch. The weight decay is not
used (set to 0) in our training. We apply this strategy to all
our experiments regardless of what kind of teacher and stu-
dent architectures we choose. We use the models in timm2.
If the input size of a student network is 224×224, we choose
senet154 and resnet152 v1s as teachers according to the in-
put size of the pre-trained models. For 380 × 380, we use
efﬁcientnet b4 ns and efﬁcientnet b4 as teachers.
6.2. Main Results
On ResNet-50.
Our results on ResNet-50 are shown
in Table 3.
Under 224 × 224 input size, our method
achieves 80.67% Top-1 accuracy, outperforming the pre-
vious state-of-the-art method MEAL [42] by 2.46%. Fur-
thermore, our results are even better than ResNeSt-50 [57]
(fast) that requires to modify the network architecture and
learned with many training tricks. After enlarging the in-
put size to 380×380, our performance is further improved
to 81.72%, outperforming FixRes (*) [49] by 2.62% with
slightly smaller input.
On Small Networks. We choose MobileNet V3 Small-
0.75/1.0/Large-1.0 and EfﬁcientNet-B0 networks which are
already compact models to verify the effectiveness of our
proposed method. Our results are shown in Table 4, on Mo-
bileNet V3-Small 0.75 and 1.0, our method improves the
original models by 2.20% and 2.25% accuracy without any
architecture modiﬁcation. Such huge increases are fairly
surprising since the models are already compact, more im-
2https://github.com/rwightman/pytorch-image-mod
els/.
portantly, the gains are totally free during inference stage.
On MobileNet V3-Large 1.0 and EfﬁcientNet-B0, although
the improvement is not as large as Small 0.75 and 1.0, we
still obtain 1.72% and 1.49% increase on ImageNet. Note
that for EfﬁcientNet-B0, 77.3/93.5 accuracy is from their
paper [47] and 76.8/93.2 is the actual accuracy from their
pre-trained models in timm.
With More Data Augmentation. We’d like to further ex-
plore whether our models have been saturated on the tar-
get data by injecting more data augmentation like CutMix
in training. The results are shown in Table 3, we involve
CutMix and keep other settings the same as our basic ex-
periments, we obtain Top-1/5 80.98%/95.35%, which out-
perform the baseline MEAL V2 by 0.31%/0.26%. While
the improvement is not so large, it indicates that our model
is not yet over-ﬁtting and still has room to boost. Moreover,
our results are 1.18%/0.45% better than FixRes+CutMix (*)
under smaller input resolution (224 vs. 320). Intriguingly,
the results on ResNet-50 are very close to the teachers we
used in distillation (81.38%/95.39% and 80.86%/95.35%),
since the scale of our student is much smaller than the
teacher architectures, it’s surprising that the student can
catch up the teachers without additional training data.
Criterion for Choosing Teachers. One critical factor for
choosing teacher networks is the accuracy. From our exper-
iments, it shows that stronger teachers usually distill better
students. Another factor is the training settings on teachers,
such as input resolution, image color space, etc. These set-
ting should match the ones used during distillation so that
the teachers can provide correct probability of the input as
the supervision for students. Data augmentation can be dif-
ferent between the stages of training teachers and distilla-
6

--------------------------------------------------
"# = %. '
"# = %. %'
"# = %. %%'
"# = %. '
"# = %. %' "# = %. %%'
Figure 6. A comparison of training from random initialized parameters, inferior parameters and superior parameters. Left are the training
accuracy curves and middle are validation accuracy curves. The results indicate that a good initialization is crucial for the ﬁnal performance.
Left is the comparison of using weight decay (1e-4) and without it. We can observe that with weight decay, the training is surprisingly
unstable in the ﬁrst stage.
tion, like we can use CutMix to train teachers but not use it
in distillation, vice versa.
Table 5. Comparison with other state-of-the-art knowledge distil-
lation methods. We show the gap between teacher and student to
demonstrate the mimicking ability of each method. TE denotes the
ensembled result of our teachers. † denotes training with CutMix.
Method
Teacher(s)
Student
Gap with Teacher
CRD (ICLR’20) [48]
73.31/91.42
71.17/90.13
∆2.14/1.39
CRD+KD (ICLR’20) [48]
73.31/91.42
71.38/90.49
∆1.93/0.93
AE-KD (NeurIPS’20) [7] (#1)
75.67/92.50
67.81/88.21
∆7.86/4.29
AE-KD (NeurIPS’20) [7] (#3)
76.85/93.60
68.28/88.21
∆8.57/5.39
AE-KD (NeurIPS’20) [7] (#5)
77.52/93.85
69.14/88.93
∆8.38/4.92
Ours (MEAL V2)
T1: 81.38/95.39
T2: 80.86/95.35
TE: 82.67/96.13
80.67/95.09
∆0.71/0.30
∆0.19/0.26
∆2.00/1.04
Ours (MEAL V2)†
T1: 81.38/95.39
T2: 80.86/95.35
TE: 82.67/96.13
80.98/95.35
∆0.40/0.04
∆-0.13/0.00
∆1.69/0.78
Comparison with Other Distillation Methods. We com-
pare our distillation results with other state-of-the-art dis-
tillation methods in Table 5 on two aspects: (1) the mim-
icking ability through comparing the performance gap be-
tween teacher and student. Such comparison can directly re-
ﬂect the true superiority and learning ability, no matter what
kind of structures on teacher and student you use. It can be
observed that our gaps between teachers and students are
smaller than both CRD [48] and AE-KD [7] by a signiﬁcant
margin. (2) the absolute accuracy on the same structure of
student. If choosing ResNet-18 as the student, our method
obtains 73.19% Top-1 accuracy, outperforming CRD [48]
by 2.02%. Although different methods used different archi-
tectures as teachers, like AE-KD [7] and our method applied
multiple network ensemble as a teacher, such result still can
verify the effectiveness of our method in a certain extent.
6.3. Ablation Study and Analysis
There are many factors in knowledge distillation that de-
termines the performance of a student. Since we use the
same teacher ensemble for all ResNet-50, MobileNet V3
and EfﬁcientNet-B0 under 224×224 input, the results indi-
cate that the student architecture or capacity itself is a cru-
cial factor. If we compare MEAL V1 and V2 we can further
derive the conclusion that teacher’s performance, i.e. the
quality of supervision, is another factor for the student, gen-
erally, the stronger teachers can consistently distill stronger
students. In the following, we would like to exam the im-
pact of the initialization and discriminator.
Effects of initialization. To verify whether the initializa-
tion of a student has a big impact, we conduct the ablation
study through adopting (1) randomly initialized weights, (2)
pytorch moderate weights and (3) timm superior weights.
Results are illustrated in Fig. 6, we trained with additional
120 epochs with lr = 0.1 if the parameters are randomly
initialized.
Intriguingly, the convergence with randomly
initialized weights is not as good as using pre-trained pa-
rameters, especially the second and third stages when lr is
smaller. The ﬁnal accuracy (77.07%) is only slightly bet-
ter than that with hard label. Adopting the standard one-hot
label pre-trained weights can dramatically improve the re-
sults to 79.48%, but is still slightly worse than the superior
initialization. Models in timm are trained with massive data
augmentation techniques so the accuracy is higher. It seems
that our framework can inherit the knowledge in such ini-
tialization and promotes it to a better status of student. Since
the good initialization essentially is learned from hard label,
the knowledge learned by hard label is complementary with
that learned from soft label. When training from scratch,
the hard label is suggested to be involved with a pre-training
stage to encode more information even it is not accurate or
strong, then remove it in the latter part of distillation and
inherit weights with soft-label solely for better guidance.
We also examine the sensitivity of initialization for the
ﬁnal performance.
We chose tf efﬁcientnet b0 (Top-1/5:
76.85%/93.25%) and efﬁcientnet b0 (77.70%/93.53%) as
the student initialization in timm, respectively. They have
the same architecture but the performance is different due to
the discrepant training settings. Interestingly, we got Top-
1 78.29% and 78.23% respectively for the two initializa-
tions with the same teacher ensembles and training hyper-
parameters. It indicates that the ﬁnal performance is not
sensitive to the subtle difference of good initializations.
With or w/o the discriminator. The discriminator is used
to prevent the student from being overﬁtting on the training
data. It can slow down the moving of a student to mimic the
7

--------------------------------------------------
Figure 7. An illustration of weight distributions in the ﬁrst, middle (last conv in block 2) and last conv layers in a ResNet-50 model.
(1) Finetune w/ hard label
(2) Ours
Figure 8. Evolution of weights percentile. We monitor the mid-
dle convolutional layer (last conv in block 2) in ResNet-50 with
elements at 10%, 25%, 50%, 75%, 90% during whole training.
teachers’ output, which can be regarded as a regularization
effect. In the scenario of MEAL V2, our teacher ensembles
are usually powerful and strong, meanwhile, the student ar-
chitectures are usually smaller and more compact than the
teachers, meaning that the capability and learning ability
are also much worse than the pretrained teacher networks,
even we force the student to produce the same predictions
as strong teachers, the outputs between student and teacher
ensembles still have inevitable gaps which cannot be erad-
icated through the KL-divergence loss. That is to say, the
discriminator is very easy to distinguish that the feature is
from a student or teacher ensemble and the regularization
effect will be weakened. The performance comparison of
using and without the discriminator is shown in our Ap-
pendix. Even so, in MEAL V2 we still see slight improve-
ment on performance by using the discriminator.
6.4. Visualization
We provide two visualizations schemes to understand
why our trained model can achieve substantially better re-
sults. The ﬁrst is the histogram of weights in particular
convolutional layers as shown in Fig. 7, we choose the py-
torch pre-trained model for a comparison. We can see our
weights always have a wider scope of values and fewer el-
ements are close to zero. We argue the behavior of wider
scope on weights reﬂects the larger capacity of a network
since weights can have more potential values or status to
be. Our second is the evolution of weights percentile dur-
ing training, as in Fig. 8. The reference is a model trained
with one-hot labels and both of them use the same initial-
ization. Consistently, we observe the values of weights tend
to diffuse rather than the convergence. We conjecture this is
caused by weight decay as we have discussed earlier.
Table 6. Transfer accuracy on classiﬁcation task.
VOC2007
CUB200-2011
Birdsnap
CIFAR-10
From Scratch
72.20
58.90
66.97
94.71
Fine-tune:
Base model
94.00
80.70
75.28
97.24
Ours
95.10
83.70
75.55
97.26
Freeze backbone:
Base model
87.50
66.27
55.01
81.69
Ours
90.80
68.29
53.56
84.16
Table 7. Transfer accuracy on COCO detection using RetinaNet.
AP
AP50
AP75
APSmall
APMedium
APLarge
Fine-tune all layers:
Base model
37.234
56.436
39.769
22.899
41.167
47.576
Ours
37.501
56.829
40.148
21.483
41.264
48.655
Freeze ﬁrst stage of backbone:
Base model
37.253
56.636
39.912
22.143
41.567
47.322
Ours
37.501
56.850
40.245
22.471
41.410
48.853
7. Transfer Learning
Fine-tuning backbone. On the classiﬁcation task, we ﬁne-
tune the entire network of ResNet-50 using the parame-
ters of the pretrained model as initialization. Since PAS-
CAL VOC classiﬁcation is a multi-label problem, we apply
sigmoid cross-entropy objective for it, and softmax cross-
entropy for other datasets. On COCO detection with Reti-
naNet [30], we use exactly the same hyper-parameters in
detectron2 [52] but replacing the initialization with ours.
Fixing backbone. We freeze the entire backbone and solely
train the last linear layer. This is the linear evaluation to
verify the quality of learned representations. For detection
task, we freeze the ﬁrst stage of backbone instead of the
entire network. More details on ﬁnetuning and linear eval-
uation will be given in Appendix. Our results are shown in
Table 6 and 7. In the most cases and datasets, a consistent
improvement is achieved by using our trained parameters.
8. Conclusion
We have presented a new paradigm of knowledge dis-
tillation based on a teacher ensemble and a discriminator.
We show that such a simple framework can achieve promis-
ing results without tricks on a variety of network structures
including the extremely tiny and compact models. On Im-
ageNet, our method achieves 80.67% top-1 accuracy using
a single crop of 224×224 on the vanilla ResNet-50. Our
results show that existing networks’ potential has not been
fully exploited and there is still room to boost and enhance
through our framework. We hope the proposed method can
inspire more studies along this direction of boosting tiny
and compact models through knowledge distillation.
8

--------------------------------------------------
Appendix
A. Implementation Details of Transferring
Fine-tuning backbone. On classiﬁcation task, we ﬁne-tune
the entire network of ResNet-50 using the parameters of the
pre-trained model as initialization. We train for 200 epochs
with a batch size of 128 and an initial learning rate of 0.01.
Since PASCAL VOC classiﬁcation is a multi-label prob-
lem, we apply sigmoid cross-entropy objective for it, and
softmax cross-entropy for other datasets. We use SGD with
a momentum parameter of 0.9 and weight decay of 0.0001.
We perform standard random crops with resize and ﬂips as
data augmentation during ﬁne-tuning. The training image
size is 224×224, at test time, we resize images to 256 pixels
and take a 224×224 center crop. On COCO detection with
RetinaNet [30], we use exactly the same hyper-parameters
in detectron2 [52] but replacing the initialization with ours.
Fixing backbone. We freeze the entire backbone and solely
train the last linear layer. This is the linear evaluation pro-
tocol to verify the quality of learned representations. We
adopted the same training setting as ﬁne-tuning but using
a larger initial learning rate of 0.1.
For detection task,
we freeze the ﬁrst stage of backbone instead of the en-
tire network and follow the experimental settings of detec-
tron2 [52].
Overview of transfer learning datasets. As shown in Ta-
ble 8, we provide a brief overview of ﬁve datasets that we
used in our transfer learning experiments.
Table 8. Overview of ﬁve datasets used in our experiments for
transfer learning.
Dataset
#class
Property
#Train(+val) set
#Testing set
VOC 2007 [8]
20
Multi-object
5,011
4,952
CUB200-2011 [50]
200
Fine-grained
5,994
5,794
Birdsnap [1]
500
Fine-grained
47,386
2,443
CIFAR-10 [25]
10
Standard
50,000
10,000
MS COCO [31]
80
Detection
123,287
40,775
B. More Comparison
With or w/o the discriminator. As we have introduced in
the main text, the discriminator is used to prevent the stu-
dent from being overﬁtting on the training data. It can slow
down the moving of a student to mimic the teachers’ out-
put, which can be regarded as a regularization effect. In the
scenario of MEAL V2, our teacher ensembles are usually
large and heavy, while, the student architectures are smaller
and more compact than the teachers, meaning that the ca-
pability and learning ability are also much worse than the
pretrained teacher networks, even we force the student to
produce the same predictions as strong teachers, the outputs
between student and teacher ensembles still have inevitable
gaps which cannot be eradicated through the KL-divergence
Figure 9. Comparison of Top-1 accuracy with and without the dis-
criminator on ImageNet validation set.
loss. That is to say, the discriminator is easy to distinguish
that the feature is from a student or teacher ensemble and the
regularization effect from the discriminator will be weak-
ened. Fig. 9 illustrates the performance comparison with
and without the discriminator. Even so, in MEAL V2 we
still see slight improvement by using the discriminator.
C. Discussions
Why is the hard/one-hot label not necessary in knowl-
edge distillation? The one-hot labels in ImageNet are an-
notated by humans, thus there are certainly some incorrect
or missing annotations into them. Also, a non-negligible
proportion of images in ImageNet contain more than one
object within a single image, the one-hot label is determined
by the annotators among multiple objects which cannot rep-
resent the complete content of this image precisely. We ar-
gue that if the teacher ensembles are strong enough, which
can provide high-quality predictions for the input image, in-
volving such inaccurate hard labels will mislead the student
to a wrong optimum and incur inferior performance. More-
over, the distilled soft labels can overcome the noise and
mismatching issues caused by the random crop data aug-
mentation strategy adopted in deep model training.
Training w/ and w/o the good initialization. As we men-
tioned in the main paper, training without the good initial-
ization obtained inferior performance. However, after in-
volving the hard labels and training the initialization with
the hard labels by standard settings, our distillation frame-
work can boost such initialization model by ∼3% and the
ﬁnal performance is competitive. This procedure indicates
that hard labels and soft labels are complementary. Also, it
is equivalent to our proposed framework since our good ini-
tialization is trained with hard labels. Hence, our framework
can be regarded as a new procedure: hard label pre-training
+ soft label ﬁnetuning.
How about the generalization ability of our method on
large students? We tried to use some large models like
ResNeXt-101 32×48d for the students as used in teacher
networks, which means that the student has similar capabil-
ity with teachers. As expected, the improvement is not as
9

--------------------------------------------------
considerable as those of small students, we still see some
increase on performance. Generally, the soft supervision
from teacher ensembles is better than the human-annotated
hard labels. Especially when the scale and performance gap
between teachers and students are enormous, the improve-
ment will be more effective and notable. That is to say, in
most of our experimental cases, the stronger teachers can
consistently produce and distill stronger students.
Is there still room to improve the performance of vanilla
ResNet-50? It’s deﬁnitely Yes. Replacing the teacher en-
sembles we used with more and stronger networks could be
helpful, but the training cost will be increased accordingly.
Also, some of the common tricks like cosine decay learning
rate might be useful for the performance but it needs more
resources to verify and the framework will become not neat.
The current choices are the compromise and a trade-off un-
der the considerations of training efﬁciency, computational
resources, etc. Our purpose of this paper is primarily to
verify the effectiveness of our proposed perspective, rather
than the highest accuracy. Still, it will be very interesting
to explore the upper bound performance of a ﬁxed-structure
network, such as ResNet-50.
The relationship to the lottery ticket hypothesis [9]. Lot-
tery ticket hypothesis assumes that we can ﬁnd a sub-
network from a trained giant model, and retraining such
initialization of subnetwork can scale its accuracy back to
the original giant model. We also observe that the initial-
ization of the student is crucial in our framework for the
super teachers to play a role in knowledge distillation pro-
cess. While, the difference from lottery ticket hypothesis is
that our student is trained solely, rather than being selected
as a sub-network from a large pre-trained network.
References
[1] Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L
Alexander, David W Jacobs, and Peter N Belhumeur. Bird-
snap: Large-scale ﬁne-grained visual categorization of birds.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2011–2018, 2014. 5, 9
[2] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran
El-Yaniv, and Yoshua Bengio. Binarized neural networks:
Training deep neural networks with weights and activations
constrained to+ 1 or-1.
arXiv preprint arXiv:1602.02830,
2016. 2
[3] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-
van, and Quoc V Le. Autoaugment: Learning augmentation
policies from data. arXiv preprint arXiv:1805.09501, 2018.
1, 5
[4] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le.
Randaugment:
Practical automated data augmenta-
tion with a reduced search space.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops, pages 702–703, 2020. 1, 5
[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition, pages 248–255. Ieee, 2009. 2, 5
[6] Terrance DeVries and Graham W Taylor. Improved regular-
ization of convolutional neural networks with cutout. arXiv
preprint arXiv:1708.04552, 2017. 5
[7] Shangchen Du, Shan You, Xiaojie Li, Jianlong Wu, Fei
Wang, Chen Qian, and Changshui Zhang.
Agree to dis-
agree: Adaptive ensemble knowledge distillation in gradient
space. Advances in Neural Information Processing Systems,
33, 2020. 7
[8] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. International journal of computer
vision, 88(2):303–338, 2010. 5, 9
[9] Jonathan Frankle and Michael Carbin. The lottery ticket hy-
pothesis: Finding sparse, trainable neural networks. arXiv
preprint arXiv:1803.03635, 2018. 10
[10] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock:
A regularization method for convolutional networks.
In
Advances in Neural Information Processing Systems, pages
10727–10737, 2018. 6
[11] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
580–587, 2014. 1
[12] Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noord-
huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
Yangqing Jia, and Kaiming He.
Accurate, large mini-
batch sgd: Training imagenet in 1 hour.
arXiv preprint
arXiv:1706.02677, 2017. 5
[13] Song Han, Huizi Mao, and William J Dally.
Deep com-
pression: Compressing deep neural networks with pruning,
trained quantization and huffman coding.
arXiv preprint
arXiv:1510.00149, 2015. 2
[14] Song Han, Jeff Pool, John Tran, and William Dally. Learning
both weights and connections for efﬁcient neural network. In
Advances in neural information processing systems, pages
1135–1143, 2015. 2
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 1, 2
[16] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning
for accelerating very deep neural networks. In Proceedings
of the IEEE International Conference on Computer Vision,
pages 1389–1397, 2017. 2
[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
Distill-
ing the knowledge in a neural network.
arXiv preprint
arXiv:1503.02531, 2015. 2
[18] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh
Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,
Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-
bilenetv3. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 1314–1324, 2019. 1, 2,
6
10

--------------------------------------------------
[19] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017. 2
[20] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 7132–7141, 2018. 1
[21] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger.
Densely connected convolutional net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4700–4708, 2017. 2
[22] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-
Yaniv, and Yoshua Bengio.
Quantized neural networks:
Training neural networks with low precision weights and
activations.
The Journal of Machine Learning Research,
18(1):6869–6898, 2017. 2
[23] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry
Kalenichenko. Quantization and training of neural networks
for efﬁcient integer-arithmetic-only inference. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2704–2713, 2018. 2
[24] Simon J´egou, Michal Drozdzal, David Vazquez, Adriana
Romero, and Yoshua Bengio.
The one hundred layers
tiramisu: Fully convolutional densenets for semantic seg-
mentation. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition workshops, pages 11–
19, 2017. 2
[25] Alex Krizhevsky et al. Learning multiple layers of features
from tiny images. 2009. 5, 9
[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In Advances in neural information processing sys-
tems, pages 1097–1105, 2012. 1, 2
[27] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan
Popov, Matteo Malloci, Tom Duerig, et al. The open im-
ages dataset v4: Uniﬁed image classiﬁcation, object detec-
tion, and visual relationship detection at scale. arXiv preprint
arXiv:1811.00982, 2018. 2
[28] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
1
[29] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. arXiv
preprint arXiv:1608.08710, 2016. 2
[30] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll´ar. Focal loss for dense object detection. In Pro-
ceedings of the IEEE international conference on computer
vision, pages 2980–2988, 2017. 5, 8, 9
[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
European conference on computer vision, pages 740–755.
Springer, 2014. 5, 9
[32] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,
Shoumeng Yan, and Changshui Zhang. Learning efﬁcient
convolutional networks through network slimming. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 2736–2744, 2017. 2
[33] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 3431–3440, 2015. 1
[34] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
Shufﬂenet v2: Practical guidelines for efﬁcient cnn architec-
ture design. In Proceedings of the European conference on
computer vision (ECCV), pages 116–131, 2018. 2
[35] Laurens van der Maaten and Geoffrey Hinton.
Visualiz-
ing data using t-sne. Journal of machine learning research,
9(Nov):2579–2605, 2008. 3
[36] Rafael M¨uller, Simon Kornblith, and Geoffrey Hinton. When
does label smoothing help? In NeurIPS, 2019. 2
[37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
imperative style, high-performance deep learning library. In
Advances in neural information processing systems, pages
8026–8037, 2019. 5
[38] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
and Ali Farhadi. Xnor-net: Imagenet classiﬁcation using bi-
nary convolutional neural networks. In European conference
on computer vision, pages 525–542. Springer, 2016. 2
[39] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems, pages 91–99, 2015. 1
[40] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets. arXiv preprint arXiv:1412.6550,
2014. 2
[41] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen.
Mobilenetv2: Inverted
residuals and linear bottlenecks.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 4510–4520, 2018. 2
[42] Zhiqiang Shen, Zhankui He, and Xiangyang Xue.
Meal:
Multi-model ensemble via adversarial learning. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence, vol-
ume 33, pages 4886–4893, 2019. 1, 2, 4, 5, 6
[43] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,
Yurong Chen, and Xiangyang Xue. Dsod: Learning deeply
supervised object detectors from scratch. In Proceedings of
the IEEE international conference on computer vision, pages
1919–1927, 2017. 1, 2
[44] Zhiqiang Shen, Zechun Liu, Dejia Xu, Zitian Chen, Kwang-
Ting Cheng, and Marios Savvides.
Is label smoothing
truly incompatible with knowledge distillation: An empirical
study. In International Conference on Learning Representa-
tions, 2021. 2
[45] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
11

--------------------------------------------------
Vanhoucke, and Andrew Rabinovich.
Going deeper with
convolutions.
In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1–9, 2015.
2
[46] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception archi-
tecture for computer vision. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
2818–2826, 2016. 5
[47] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model
scaling for convolutional neural networks. In International
Conference on Machine Learning, pages 6105–6114, 2019.
1, 2, 6
[48] Yonglong Tian, Dilip Krishnan, and Phillip Isola.
Con-
trastive representation distillation. In ICLR, 2020. 7
[49] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv´e
J´egou. Fixing the train-test resolution discrepancy. In Ad-
vances in Neural Information Processing Systems, pages
8252–8262, 2019. 1, 6
[50] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The Caltech-UCSD Birds-200-2011 Dataset. Technical re-
port, 2011. 5, 9
[51] Devesh Walawalkar, Zhiqiang Shen, and Marios Savvides.
Online ensemble model compression using knowledge dis-
tillation. In ECCV, 2020. 2
[52] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen
Lo, and Ross Girshick. Detectron2. https://github.c
om/facebookresearch/detectron2, 2019. 8, 9
[53] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A
gift from knowledge distillation: Fast optimization, network
minimization and transfer learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4133–4141, 2017. 2
[54] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
How transferable are features in deep neural networks? In
Advances in neural information processing systems, pages
3320–3328, 2014. 1
[55] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
larization strategy to train strong classiﬁers with localizable
features. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pages 6023–6032, 2019. 1, 5
[56] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. arXiv preprint arXiv:1710.09412, 2017. 1, 5
[57] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi
Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R
Manmatha, et al. Resnest: Split-attention networks. arXiv
preprint arXiv:2004.08955, 2020. 1, 6
[58] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient convolutional neural net-
work for mobile devices. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
6848–6856, 2018. 2
[59] Chenzhuo Zhu, Song Han, Huizi Mao, and William J
Dally.
Trained ternary quantization.
arXiv preprint
arXiv:1612.01064, 2016. 2
[60] Xiatian Zhu, Shaogang Gong, et al. Knowledge distillation
by on-the-ﬂy native ensemble. In Advances in neural infor-
mation processing systems, pages 7517–7527, 2018. 2
[61] Barret Zoph and Quoc V Le. Neural architecture search with
reinforcement learning.
arXiv preprint arXiv:1611.01578,
2016. 2
12

--------------------------------------------------
