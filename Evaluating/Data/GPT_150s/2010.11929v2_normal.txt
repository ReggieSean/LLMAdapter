The Vision Transformer (ViT) applies a standard Transformer directly to sequences of image patches, eliminating the need for convolutions. Trained on large datasets like ImageNet-21k or JFT-300M, ViT outperforms ResNet-based models on multiple benchmarks with lower compute cost. Despite having less inductive bias, ViT scales well and transfers effectively to smaller datasets. Fine-tuning at higher resolutions further boosts accuracy. Self-supervised ViT shows promise, and the architecture benefits from further scaling and adaptation to other vision tasks.