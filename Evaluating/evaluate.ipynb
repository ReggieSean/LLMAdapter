{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 13:51:14.762890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0af390176c4bd78cd82a9d037cf314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293e9a38a90d494192fe01d6867a145a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # or the base model you trained on\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model in 4-bit if you used quantization\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "# manually set rope_scaling to supported structure:\n",
    "config.rope_scaling = {\"type\": \"dynamic\", \"factor\": 2.0}\n",
    "config.use_cache = True\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    config = config,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "adapted_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    config=config,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load your LoRA adapter\n",
    "adapter_path = \"./../Training/final_adapter_with_eval_0\"  # or wherever your adapter_model.safetensors is\n",
    "adapted_model= PeftModel.from_pretrained(adapted_model, adapter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to empty_cache other wise 2 models' results will bleed into each other\n",
    "# only slice and decode the new tokens \n",
    "def generate_summary(input_text, max_new_tokens=150):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    prompt = f\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>\n",
    "Text:\n",
    "{input_text}\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(adapted_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = adapted_model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = outputs[0][input_len:]  # exclude prompt\n",
    "    summary = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def generate_base_summary(input_text, max_new_tokens=150):\n",
    "    torch.cuda.empty_cache()\n",
    "    prompt = f\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>\n",
    "Text:\n",
    "{input_text}\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = outputs[0][input_len:]  # exclude prompt\n",
    "    summary = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_json_from_file(file_path):\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = file.read().strip()\n",
    "\n",
    "        # Fix concatenated JSON objects\n",
    "        objs = data.split(\"}{\")\n",
    "\n",
    "        parsed_objs = []\n",
    "        for i, obj in enumerate(objs):\n",
    "            if not obj.startswith(\"{\"):\n",
    "                obj = \"{\" + obj\n",
    "            if not obj.endswith(\"}\"):\n",
    "                obj = obj + \"}\"\n",
    "            try:\n",
    "                parsed = json.loads(obj)\n",
    "                parsed_objs.append(parsed)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in file {file_path}, object {i}: {e}\")\n",
    "    return parsed_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks with formats 300\n",
      "chunks of recipts that need ocr 300\n",
      "chunks of well written long papers, no ocr 300\n",
      "chunks of all sorts 300\n",
      "chunks with filtered formats 99\n",
      "chunks with formatted receipts 100\n",
      "chunks with formatted reports 30\n",
      "chunks with formatted randoms 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# example_file = \"./../Training/../Training/InputNLabel/prompt_sensitive_translated/receipts_2.json\"\n",
    "# file_and_limits = {\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_formatted.json\": 30,\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_randoms.json\": 30,\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_receipts.json\": 30,\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_reports.json\": 10,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/formatted_2.json\": 300,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/random_2.json\": 300,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/receipts_2.json\": 300,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/reports_2.json\": 300,\n",
    "# }\n",
    "formatted_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/formatted_2.json\"\n",
    "receipt_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/receipts_2.json\"\n",
    "report_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/reports_2.json\"\n",
    "random_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/random_2.json\"\n",
    "\n",
    "filtered_receipts = \"./../Training/InputNLabel/filtered_labels/filtered_receipts.json\"\n",
    "filtered_reports = \"./../Training/InputNLabel/filtered_labels/filtered_reports.json\"\n",
    "filtered_randoms = \"./../Training/InputNLabel/filtered_labels/filtered_randoms.json\"\n",
    "filtered_formatted = \"./../Training/InputNLabel/filtered_labels/filtered_formatted.json\"\n",
    "\n",
    "formatted = get_json_from_file(formatted_chunks)\n",
    "receipts = get_json_from_file(receipt_chunks)\n",
    "reports = get_json_from_file(report_chunks)\n",
    "randoms = get_json_from_file(random_chunks)\n",
    "\n",
    "f_receipts = get_json_from_file(filtered_receipts)\n",
    "f_reports = get_json_from_file(filtered_reports)\n",
    "f_randoms = get_json_from_file(filtered_randoms)\n",
    "f_formatted = get_json_from_file(filtered_formatted)\n",
    "\n",
    "print(\"chunks with formats\",len(formatted))\n",
    "print(\"chunks of recipts that need ocr\", len(receipts))\n",
    "print(\"chunks of well written long papers, no ocr\",len(reports))\n",
    "print(\"chunks of all sorts\",len(randoms))\n",
    "\n",
    "print(\"chunks with filtered formats\",len(f_formatted))\n",
    "print(\"chunks with formatted receipts\", len(f_receipts))\n",
    "print(\"chunks with formatted reports\", len(f_reports))\n",
    "print(\"chunks with formatted randoms\", len(f_randoms))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing filtered chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARKET\n",
      "Pesan as 62-2104\n",
      "se\n",
      "etna\n",
      "saa\n",
      "10h\n",
      "czy corn Eueees ” a\n",
      "Raurommcorse =|\n",
      "Aur isto.\n",
      "|| B\n",
      "ferecth a ne bibs\n",
      "B\n",
      "Stl\n",
      "a\n",
      "ate:\n",
      "f\n",
      "ital\n",
      "S\n",
      "Ei iam:\n",
      "paid\n",
      "Riles,\n",
      "ase\n",
      "1\n",
      "ip facoet mor\n",
      "his Er i Amn\n",
      "rug;\n",
      "AIT retro a e\n",
      "Wrveuty ov tee fc \n",
      "3B Ghar asia tea\n",
      "biti vat\n",
      "canter\n",
      "fc} tars ave a ele Taos\n",
      "a's Pras Stig, Vid tke\n",
      "{Sin ie a scon\n",
      "Tam 5 ack at ne fons e\n",
      "HR the tan Pres hoar\n",
      "(Shave a musi.cofet\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Output: <NULL>\n"
     ]
    }
   ],
   "source": [
    "input = f_receipts[0]\n",
    "print(input[\"input\"])\n",
    "print(\"\\nOutput:\",input[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!<---------Next Summary----------->!!!!\n",
      "Summary:\n",
      "<NULL>\n"
     ]
    }
   ],
   "source": [
    "input_text = input[\"input\"]\n",
    "#print(generate_base_summary(input_text,150))\n",
    "print(\"!!!!<---------Next Summary----------->!!!!\")\n",
    "print(generate_summary(input_text, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n",
      "Figure 3: ImageNet acc. vs model complexity.\n",
      "Table 6: Comparisons among the GFNet and other\n",
      "variants based on the transformer-like architecture\n",
      "on ImageNet. We show that GFNet outperforms the\n",
      "ResMLP [42], FNet [25] and models with local depth-\n",
      "wise convolutions. We also report the number of pa-\n",
      "rameters and theoretical complexity in FLOPs.\n",
      "Model\n",
      "Acc\n",
      "Param\n",
      "FLOPs\n",
      "(%)\n",
      "(M)\n",
      "(G)\n",
      "DeiT-S [43]\n",
      "79.8\n",
      "22\n",
      "4.6\n",
      "Local Conv (3 × 3)\n",
      "77.7\n",
      "15\n",
      "2.8\n",
      "Local Conv (5 × 5)\n",
      "78.1\n",
      "15\n",
      "2.9\n",
      "Local Conv (7 × 7)\n",
      "78.2\n",
      "15\n",
      "2.9\n",
      "ResML\n",
      "<NULL>\n"
     ]
    }
   ],
   "source": [
    "print(f_reports[0][\"input\"])\n",
    "print(f_receipts[0][\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2019-10-09\n",
      "Time: 06:58\n",
      "Currency: EUR\n",
      "Payment Method: Check\n",
      "Payment Amount: 12.60\n",
      "Discount: 1.04 9% BFW\n",
      "Subtotal: 12.60\n",
      "Total: 12.60\n",
      "Total with tax: 12.60\n",
      "Discount Amount: 1.04\n",
      "Tax Amount: 11.56\n",
      "Items: 3\n",
      "Total with tax: 12.60\n",
      "Tax Amount: 11.56\n",
      "Items: 3\n",
      "Total with tax: 12.60\n",
      "Discount Amount: 1.04\n",
      "Tax Amount: 11.56\n",
      "Items: 3\n",
      "Total with tax: 12.\n",
      "!!!!<----------------------->!!!!\n",
      "KFC.nl\n",
      "Bijdrage aan het 5ejaar van KFC Nederland\n",
      "Bedankt voor uw bezoek aan KFC؛\n",
      "Heeft 2 minuten nodig om een foto te maken?\n",
      "Geef feedback en krijg 3 gratis\n",
      "Hot Wings bij uw volgende\n",
      "bestelling (van min, 6 euro).\n",
      "Ga naar; w\n",
      "KFC.nl\n",
      "Bijdrage aan het 5ejaar van KFC Nederland\n",
      "Bedankt voor uw bezoek aan KFC؛\n",
      "Heeft 2 minuten nodig om een foto te maken?\n",
      "Geef feedback en krijg 3 gratis\n",
      "Hot Wings bij uw volgende\n",
      "bestelling (van min, 6\n"
     ]
    }
   ],
   "source": [
    "input = receipts[0]\n",
    "print(generate_base_summary(input[\"input\"]))\n",
    "print(\"!!!!<----------------------->!!!!\")\n",
    "print(generate_summary(input[\"input\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!<----------------------->!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarize :-08\n",
      "Shipped Date: 2017-08-11\n",
      "Products:\n",
      "Product: Jack's New England Glam Chowder\n",
      "‘Quantity: 20\n",
      "Unit Pie:\n",
      "9.65,\n",
      "\n",
      "--------------------------------------------------7-08-21\n",
      "Shipped Date: 2017-08-28\n",
      "Products:\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Product: Gudbrandsdalsost\n",
      "Quantity: 20\n",
      "Unit Price: 36.0\n",
      "Total: 720.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Product: Outback Lager\n",
      "Quantity: 15\n",
      "Unit Price: 15.0\n",
      "Total: 225.0\n",
      "Total Price:\n",
      "\n",
      "--------------------------------------------------\n",
      "Total Price: 945.0\n",
      "\n",
      "---------------------------------------Order ID: 10481\n",
      "Shipping Details:\n",
      "Ship Name: Ricardo Adocicados\n",
      "Ship Address: Av. Copacabana, 257\n",
      "Ship Cy: de Janeiro.\n",
      "Ship Region: South America\n",
      "Ship Postal Code: 02389-890\n",
      "Ship County: Brazil\n",
      "Customer Details:\n",
      "CustomerID: RICAR\n",
      "Customer Name:\n",
      "Ricardo Adocicados\n",
      "Employee Details:\n",
      "Employee Name: Laura Callahan\n",
      "Shipper Details:\n",
      "Shipper ID: 2\n",
      "Shipper Name: United Package\n",
      "Order Details:\n",
      "(Order Date: 2017-03-20\n",
      "Shipped Date: 2017-03-25\n",
      "Products:\n",
      "Product: Maxiak\n",
      "Quanty: 26\n",
      "Unit Pie: 160\n",
      "\n",
      "------------\n",
      " summary: \n",
      "This is an order summary for a shipment of products to Ricardo Adocicados in Rio de Janeiro, Brazil. The order includes Maxiak, Gudbrandsdalsost, and Outback Lager. The total price of the order is 945.0. The order was shipped on March 25, 2017. \n",
      "\n",
      "Here are the products and their quantities:\n",
      "\n",
      "* Maxiak: 26 units\n",
      "* Gudbrandsdalsost: 20 units\n",
      "* Outback Lager: 15 units\n",
      "\n",
      "The customer is Ricardo Adocicados, and the shipper is United Package. The order was placed on March 20, 2017, and shipped on March 25, 2017. \n",
      "\n",
      "Note\n",
      "!!!!<----------------------->!!!!\n",
      "summarize:-08\n",
      "Shipped Date: 2017-08-11\n",
      "Products:\n",
      "Product: Jack's New England Glam Chowder\n",
      "‘Quantity: 20\n",
      "Unit Pie:\n",
      "9.65,\n",
      "\n",
      "--------------------------------------------------7-08-21\n",
      "Shipped Date: 2017-08-28\n",
      "Products:\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Product: Gudbrandsdalsost\n",
      "Quantity: 20\n",
      "Unit Price: 36.0\n",
      "Total: 720.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Product: Outback Lager\n",
      "Quantity: 15\n",
      "Unit Price: 15.0\n",
      "Total: 225.0\n",
      "Total Price:\n",
      "\n",
      "--------------------------------------------------\n",
      "Total Price: 945.0\n",
      "\n",
      "---------------------------------------Order ID: 10481\n",
      "Shipping Details:\n",
      "Ship Name: Ricardo Adocicados\n",
      "Ship Address: Av. Copacabana, 257\n",
      "Ship Cy: de Janeiro.\n",
      "Ship Region: South America\n",
      "Ship Postal Code: 02389-890\n",
      "Ship County: Brazil\n",
      "Customer Details:\n",
      "CustomerID: RICAR\n",
      "Customer Name:\n",
      "Ricardo Adocicados\n",
      "Employee Details:\n",
      "Employee Name: Laura Callahan\n",
      "Shipper Details:\n",
      "Shipper ID: 2\n",
      "Shipper Name: United Package\n",
      "Order Details:\n",
      "(Order Date: 2017-03-20\n",
      "Shipped Date: 2017-03-25\n",
      "Products:\n",
      "Product: Maxiak\n",
      "Quanty: 26\n",
      "Unit Pie: 160\n",
      "\n",
      "--------------\n",
      "\n",
      "Summary:\n",
      "On August 11, 2017, the company shipped products to Ricardo Adoc\n"
     ]
    }
   ],
   "source": [
    "#trained adapter 0 after 72 examples: there is still original text but a summary is now  more concise\n",
    "input_text = formatted[0][\"input\"] + formatted[1][\"input\"] + formatted[2][\"input\"]\n",
    "#input_text = f_receipts[0][\"input\"]\n",
    "print(\"!!!!<----------------------->!!!!\")\n",
    "print(generate_base_summary(input_text))\n",
    "print(\"!!!!<----------------------->!!!!\")\n",
    "print(generate_summary(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmlhw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
