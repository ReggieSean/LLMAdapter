{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 19:31:25.882470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebfa4e2520a46a3a108ec22b156f362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"  # or the base model you trained on\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model in 4-bit if you used quantization\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "# Load your LoRA adapter\n",
    "adapter_path = \"./../Training/final_adapter_with_eval\"  # or wherever your adapter_model.safetensors is\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model.config.use_cache = True\n",
    "#model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(input_text, max_new_tokens=150):\n",
    "    prompt = f\"Summarize:\\n{input_text}\\n\\nSummary:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Try it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_json_from_file(file_path):\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = file.read().strip()\n",
    "\n",
    "        # Fix concatenated JSON objects\n",
    "        objs = data.split(\"}{\")\n",
    "\n",
    "        parsed_objs = []\n",
    "        for i, obj in enumerate(objs):\n",
    "            if not obj.startswith(\"{\"):\n",
    "                obj = \"{\" + obj\n",
    "            if not obj.endswith(\"}\"):\n",
    "                obj = obj + \"}\"\n",
    "            try:\n",
    "                parsed = json.loads(obj)\n",
    "                parsed_objs.append(parsed)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in file {file_path}, object {i}: {e}\")\n",
    "    return parsed_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs with formats 300\n",
      "PDFs of recipts that need ocr 300\n",
      "PDFs of well written long papers, no ocr 300\n",
      "PDFs of all sorts 300\n",
      "-08\n",
      "Shipped Date: 2017-08-11\n",
      "Products:\n",
      "Product: Jack's New England Glam Chowder\n",
      "‘Quantity: 20\n",
      "Unit Pie:\n",
      "9.65,\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# example_file = \"./../Training/../Training/InputNLabel/prompt_sensitive_translated/receipts_2.json\"\n",
    "# file_and_limits = {\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_formatted.json\": 30,\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_randoms.json\": 30,\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_receipts.json\": 30,\n",
    "#     \"./../Training/InputNLabel/filtered_labels/filtered_reports.json\": 10,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/formatted_2.json\": 300,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/random_2.json\": 300,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/receipts_2.json\": 300,\n",
    "#     \"./../Training/InputNLabel/prompt_sensitive_translated/reports_2.json\": 300,\n",
    "# }\n",
    "formatted_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/formatted_2.json\"\n",
    "receipt_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/receipts_2.json\"\n",
    "report_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/reports_2.json\"\n",
    "random_chunks = \"./../Training/InputNLabel/prompt_sensitive_translated/random_2.json\"\n",
    "\n",
    "formatted = get_json_from_file(formatted_chunks)\n",
    "receipts = get_json_from_file(receipt_chunks)\n",
    "reports = get_json_from_file(report_chunks)\n",
    "randoms = get_json_from_file(random_chunks)\n",
    "\n",
    "print(\"PDFs with formats\",len(formatted))\n",
    "print(\"PDFs of recipts that need ocr\", len(receipts))\n",
    "print(\"PDFs of well written long papers, no ocr\",len(reports))\n",
    "print(\"PDFs of all sorts\",len(randoms))\n",
    "\n",
    "\n",
    "print(formatted[0][\"input\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize:\n",
      "-08\n",
      "Shipped Date: 2017-08-11\n",
      "Products:\n",
      "Product: Jack's New England Glam Chowder\n",
      "‘Quantity: 20\n",
      "Unit Pie:\n",
      "9.65,\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary:\n",
      "The text indicates a shipment of 20 units of Jack's New England Glam Chowder, with a unit price of 9.65, took place on August 11, 2017. However, the product name and unit price are missing, which might be due to a formatting issue or a missing piece of information. The shipped date is provided as August 11, 2017. Without the product details, we cannot determine the specific product or its value.\n"
     ]
    }
   ],
   "source": [
    "input_text = formatted[0][\"input\"]\n",
    "print(generate_summary(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmlhw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
