{
  "input": ".\n[53] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv´e\nJ´egou. Fixing the train-test resolution discrepancy. arXiv\npreprint arXiv:1906.06423, 2019.\n[54] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas,\nNiki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling\nlocal self-attention for parameter efﬁcient visual backbones.\narXiv preprint arXiv:2103.12731, 2021.\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolo",
  "output": "The text lists three academic papers: \n\n1. \"Fixing the train-test resolution discrepancy\" by Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou, published on arXiv in 2019. \n\n2. \"Scaling local self-attention for parameter efficient visual backbones\" by Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens, published on arXiv in 2021. \n\n3. An unspecified paper involving Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polo.",
  "summary_type": "reports"
}{
  "input": "g\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 764–773, 2017. 1, 3\n[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 5\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterth",
  "output": "The text refers to three different academic works. The first one is \"Deformable Convolutional Networks\" by Zhang, Han Hu, and Yichen Wei, presented at the IEEE International Conference on Computer Vision in 2017. The second is \"Imagenet: A Large-Scale Hierarchical Image Database\" by Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, presented at the 2009 IEEE Conference on Computer Vision and Pattern Recognition. The third work is by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, and Thomas Unterth, but the text does not provide the title of this work.",
  "summary_type": "reports"
}{
  "input": "cientNet variants because\nwe report the wallclock time, whereas Srinivas et al. (2021)\nreport the “compute time” which will ignore cross-device\ncommunication. For very small models the inter-device\ncommunication costs can be non-negligible relative to the\ncompute time, especially for EfﬁcientNets which employ\ncross-replica batch normalization. For larger models this\ncost is generally negligible on hardware like TPUv3 with\nvery fast interconnects, so in practice one can expect that\nthe compute ti",
  "output": "The text discusses the difference between wallclock time and compute time in the context of EfficientNet variants. It notes that for small models, inter-device communication costs can be significant, particularly for EfficientNets that use cross-replica batch normalization. However, for larger models, this cost is usually negligible, especially on hardware like TPUv3 with fast interconnects.",
  "summary_type": "reports"
}{
  "input": "asurement in blind audio source separation. IEEE\nTransactions on Audio, Speech and Language Processing.\nWang, K.; et al. 2019. Haq: Hardware-aware automated\nquantization with mixed precision. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 8612–8620.\nWidrow, B.; Kollar, I.; and Liu, M.-C. 1996. Statistical theory\nof quantization. IEEE Transactions on instrumentation and\nmeasurement, 45(2): 353–361.\nWightman, R. 2019. PyTorch Image Models. https://github.\ncom",
  "output": "The text references several academic papers and resources related to audio processing, computer vision, and statistical theory of quantization. These include a paper on blind audio source separation published in IEEE Transactions on Audio, Speech and Language Processing, a 2019 paper titled \"Haq: Hardware-aware automated quantization with mixed precision\" presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition, a 1996 paper on the statistical theory of quantization published in IEEE Transactions on Instrumentation and Measurement, and PyTorch Image Models by R. Wightman in 2019.",
  "summary_type": "reports"
}{
  "input": "asks considered therein:\n5\n\n--------------------------------------------------\nTable 2: Main V-MoE & VIT models; Table 8 shows results for additional models and datasets.\nModel\nParams\nJFT prec@1\nIN/1shot IN/5shot IN/10shot IN/Fine-t.\nExaFLOPs TPUv3-days\nVIT-H/14\n656M\n56.68\n62.34\n76.95\n79.02\n88.08\n4.27k\n2.38k\nV-MoE-L/16, Every-2\n3.4B\n57.65\n62.41\n77.10\n79.01\n87.41\n2.17k\n1.20k\nV-MoE-H/14, Last-5\n2.7B\n60.12\n62.95\n78.08\n80.10\n88.23\n4.75k\n2.73k\nV-MoE-H/14, Every-2\n7.2B\n60.62\n63.38\n78.21\n80.33\n88.36\n5.",
  "output": "The text presents a comparison of different models: VIT-H/14, V-MoE-L/16, Every-2, V-MoE-H/14, Last-5, and V-MoE-H/14, Every-2. The models are evaluated based on parameters, JFT prec@1, IN/1shot, IN/5shot, IN/10shot, IN/Fine-t., ExaFLOPs, and TPUv3-days. The V-MoE-H/14, Every-2 model has the highest parameters (7.2B) and the highest scores in most categories.",
  "summary_type": "reports"
}{
  "input": "ngs of IEEE Conference\non Computer Vision and Pattern Recognition Workshops\n(CVPRW), 2020.\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and\nSalakhutdinov, R. Transformer-XL: Attentive language\nmodels beyond a ﬁxed-length context. In Annual Meet-\nings of the Association for Computational Linguistics,\n2019.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and\nKaiser, L. Universal Transformers. In Proceedings of\nInternational Conference on Learning Representations\n(ICLR), 2019.\nDeng,",
  "output": "The text mentions several academic papers presented at various conferences. One is from the 2020 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Another is \"Transformer-XL: Attentive language models beyond a fixed-length context,\" presented at the Annual Meetings of the Association for Computational Linguistics in 2019. The third is \"Universal Transformers,\" presented at the 2019 International Conference on Learning Representations (ICLR).",
  "summary_type": "reports"
}{
  "input": "sequence with 256\ntokens. In the centroid attention block, we ﬁrst initialize\nour shorter sequence by reshaping the N length patch to-\nkens into a\n√\nN ×\n√\nN order. We then apply a depth-wise\nModels\n#layers\n#tokens\nembedding\ndimension\ncentroid @\nDeiT-tiny\n12\n196\n192\n-\nOurs-1\n196\n6\nOurs-2\n256\n5\nOurs-3\n256\n7\nDeiT-small\n12\n196\n384\n-\nOurs-4\n196\n6\nOurs-5\n256\n5\nOurs-6\n256\n7\nTable 5: Architecture design of different models for image\nclassiﬁcation tasks on ImageNet, centroid @ means re-\nplacing the self-",
  "output": "The text discusses the architecture design of different models for image classification tasks on ImageNet. It mentions a sequence with 256 tokens and a centroid attention block. The block begins by reshaping the N length patch tokens into a √N × √N order. The text also provides a table comparing different models, including DeiT-tiny, DeiT-small, and six versions of 'Ours', with varying numbers of layers, tokens, and embedding dimensions.",
  "summary_type": "reports"
}{
  "input": "petitive with\nCNN based architectures given sufﬁciently large amount\nof training data. ViT is composed of several parts: Image\nTokenization, Positional Embedding, Classiﬁcation Token,\nthe Transformer Encoder, and a Classiﬁcation Head. These\nsubjects are discussed in more detail below.\nImage Tokenization: A standard transformer takes as\ninput a sequence of vectors, called tokens. For traditional\nNLP based transformers the word ordering provides a nat-\nural order to sequence the data, but this is",
  "output": "The text discusses the Vision Transformer (ViT), which can compete with CNN-based architectures given enough training data. ViT consists of several parts: Image Tokenization, Positional Embedding, Classification Token, the Transformer Encoder, and a Classification Head. Image Tokenization involves inputting a sequence of vectors, known as tokens, into a standard transformer. This process is different from traditional NLP-based transformers, which use word ordering to sequence data.",
  "summary_type": "reports"
}{
  "input": "m, Nm −j∗\nm), ..., (j∗\nM, NM −j∗\nM)].\nSecond Stage. After obtaining the optimal the distribution\nof global-local modules in all blocks at the first stage, we\nfix this distribution and search the detailed architecture of\nall modules. Similar to the first stage, we adopt SPOS [13]\nto find the optimal architecture S∗in the search space. The\nmain difference is changed search space and correspond-\ningly the random index of a block is an array with four el-\nements, instead of a single number jm. The r",
  "output": "The text describes a two-stage process. In the first stage, the optimal distribution of global-local modules in all blocks is determined. In the second stage, this distribution is fixed and the detailed architecture of all modules is searched. The method SPOS is used to find the optimal architecture in the search space. The main difference in the second stage is the changed search space and the random index of a block, which is an array with four elements instead of a single number.",
  "summary_type": "reports"
}{
  "input": "pernet adaptation and evolutionary search, that are iteratively executed. NAT also uses an online accuracy predictor model to improve its\ncomputational efﬁciency.\n(i) subnets that span the entire objective trade-off front, and (ii)\na task-speciﬁc supernet. The latter can now be utilized for all\nfuture deployment-speciﬁc NAS, i.e., new and different hardware\nor objectives, without any additional training.\nThe core of NAT’s efﬁciency lies in only adapting the subnets\nof the supernet that will lie",
  "output": "NAT (Neural Architecture Transformer) uses pernet adaptation and evolutionary search iteratively, along with an online accuracy predictor model to enhance computational efficiency. It utilizes subnets spanning the entire objective trade-off front and a task-specific supernet. This supernet can be used for all future deployment-specific NAS (Neural Architecture Search) without additional training. The efficiency of NAT is based on adapting only the subnets of the supernet that will lie.",
  "summary_type": "reports"
}{
  "input": "erature has shown that features obtained from supervised training of\nCNNs may over-emphasize texture rather than encoding high-level information.\nIn self-supervised learning in particular, texture as a low-level cue may provide\nshortcuts that prevent the network from learning higher level representations. To\naddress these problems we propose to use classic methods based on anisotropic\ndiffusion to augment training using images with suppressed texture. This simple\nmethod helps retain important ed",
  "output": "The text discusses how features from supervised training of Convolutional Neural Networks (CNNs) may focus too much on texture instead of encoding high-level information. This is particularly true in self-supervised learning where texture can provide shortcuts, preventing the network from learning higher level representations. To solve these issues, the authors suggest using classic methods based on anisotropic diffusion to augment training with images that have suppressed texture.",
  "summary_type": "reports"
}{
  "input": "that when the normalization is combined with an inner product (as we do here), this is equivalent\nto cosine similarity. Some contrastive learning approaches [3] use a cosine similarity explicitly in their loss\nformulation. We decouple the normalization here to highlight the beneﬁts it provides.\n16\n\n--------------------------------------------------\nThus, for the gradient of Lsup\nin (where Xip = Xin\nip ):\n∥(zp −(zi • zp)zi∥|Pip −Xin\nip |\n≈|Pip −Xin\nip |\n=\n\f\f\f\f\f\f\f\n1\nP\np′∈P (i)\nexp (zi • zp′/τ) +\nP",
  "output": "The text discusses the equivalence of normalization combined with an inner product to cosine similarity, as used in some contrastive learning approaches. The benefits of decoupling the normalization are highlighted. The text also includes a mathematical expression for the gradient of Lsup.",
  "summary_type": "reports"
}{
  "input": "arning Sys-\ntems, 28(10):2222–2232, October 2017. ISSN 2162-2388.\ndoi: 10.1109/TNNLS.2016.2582924. Conference Name:\nIEEE Transactions on Neural Networks and Learning\nSystems.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770–778, 2016.\nHinton, G., Vinyals, O., and Dean, J.\nDistilling\nthe knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015.\nHu, H., Gu, J",
  "output": "The text references several academic papers and articles. The first one, published in IEEE Transactions on Neural Networks and Learning Systems in October 2017, does not have a provided title. The second paper, \"Deep residual learning for image recognition,\" was presented at the IEEE conference on computer vision and pattern recognition in 2016. The third paper, \"Distilling the knowledge in a neural network,\" is an arXiv preprint from 2015. The text ends abruptly after mentioning authors Hu, H., and Gu, J., without providing further information about their work.",
  "summary_type": "reports"
}{
  "input": "s\n1-10-1\n2-9-1\n1-1-9-1\n1-2-8-1\n1-10-1\n2-9-1\n1-1-9-1\n1-2-8-1\nFull Attention\n7.1, 1.35\n6.8, 1.45\n6.7, 2.29 6.4, 2.39\n27.6, 4.84 26.3, 5.05\n26.0, 6.74 24.6, 6.95\nVision Longformer\n7.1, 1.27\n6.8, 1.29\n6.7, 1.33 6.4, 1.35\n27.6, 4.67 26.3, 4.71\n26.0, 4.82 24.6, 4.86\nLinformer [46]\n7.8, 1.57\n7.7, 1.6\n8.2, 1.69 8.0, 1.73\n28.3, 5.27 27.1, 5.35\n27.4, 5.55 26.3, 5.62\nPartial Linformer\n7.3, 1.31\n7.2, 1.37\n7.7, 1.46 7.6, 1.52\n27.8, 4.76 26.7, 4.88\n27.0, 5.08 25.8, 5.21\nSRA/64 [47]\n14.2, 0.99 13.9, 0.99\n13.8,",
  "output": "The text appears to be a series of numerical data related to different models or systems, including \"Full Attention\", \"Vision Longformer\", \"Linformer [46]\", \"Partial Linformer\", and \"SRA/64 [47]\". Each model has a series of paired numbers, possibly representing performance metrics or measurements. There are also a series of numerical sequences at the beginning, but their meaning or context is not clear from the provided text.",
  "summary_type": "reports"
}{
  "input": "123, 2019. 1, 2, 6, 7, 8, 21\n[6] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical data\naugmentation with no separate search. arXiv preprint arXiv:1909.13719, 2019. 1, 6, 8, 21, 22\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In 2009 IEEE conference on computer vision and pattern\nrecognition, 2009. 1, 5, 6\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:",
  "output": "The text mentions various academic works. \"Randaugment: Practical data augmentation with no separate search\" by Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le, is a preprint on arXiv from 2019. \"Imagenet: A large-scale hierarchical image database\" by Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, was presented at the 2009 IEEE conference on computer vision and pattern recognition. The text also refers to a work by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, titled \"Bert\".",
  "summary_type": "reports"
}{
  "input": "approximated global self-attention features. Once trained, we\nconditionally re-parameterize it as an atomic operation as shown on the right. At inference phase, X-volution\nis actually a dynamic convolution operator, and its weight is composed of the attention map that needs to be\ndynamically calculated and the convolution weight that has been trained and solidiﬁed.\nto a compatible pattern of convolution. Finally, we describe that in the inference phase how to\nconditionally merge the branches of",
  "output": "The text discusses a trained global self-attention feature that is re-parameterized as an atomic operation. During the inference phase, this feature, referred to as X-volution, acts as a dynamic convolution operator. Its weight is made up of a dynamically calculated attention map and a trained, solidified convolution weight. The text also mentions a process of conditionally merging branches in the inference phase to match a compatible convolution pattern.",
  "summary_type": "reports"
}{
  "input": "riments. Stochastic Depth is also applied here as what proposed on ImageNet. We\nembed ViP into two popular frameworks for object detection and instance segmentation, RetinaNet\n[44] and Cascade Mask-RCNN [6]. When incorporating ViP into these frameworks, ViP serves as\nthe backbone followed by a Feature Pyramid Network (FPN) [43] reﬁning the multi-scale whole\nrepresentations. All weights within the backbone are ﬁrst pre-trained on ImageNet-1K, while those\noutside the backbone are initialized with",
  "output": "Stochastic Depth is applied to ViP, which is embedded into two popular object detection and instance segmentation frameworks, RetinaNet and Cascade Mask-RCNN. ViP acts as the backbone, followed by a Feature Pyramid Network (FPN) that refines multi-scale representations. All weights within the backbone are pre-trained on ImageNet-1K, while those outside the backbone are initialized differently.",
  "summary_type": "reports"
}{
  "input": "Semi-Supervised Recognition under a Noisy and Fine-grained Dataset\nCheng Cui*, Zhi Ye*, Yangxi Li*, Xinjian Li*,\nMinYang*, Kai Wei *, Bing Dai*, Yanmei Zhao*, Zhongji Liu*, Rong Pang*\nNational Internet Emergency Center(CNCERT), Baidu\n{laxlyezhi, weikai105b, cuicheng0101, yangminbupt}@gmail.com\nAbstract\nSimi-Supervised Recognition Challenge-FGVC7 is a\nchallenging ﬁne-grained recognition competition. One of\nthe difﬁculties of this competition is how to use unlabeled\ndata. We adopted pseudo-tag dat",
  "output": "The text discusses the Semi-Supervised Recognition Challenge-FGVC7, a fine-grained recognition competition. The challenge lies in utilizing unlabeled data. The authors have adopted a pseudo-tag data approach to address this issue.",
  "summary_type": "reports"
}{
  "input": "practically not\nfeasible and, more importantly, not necessary. Firstly, it is evident\nfrom existing NAS approaches [26], [62] that different objectives\n(#Params, #MAdds, latency on different hardware, etc.) require\n\n--------------------------------------------------\n8\nTABLE 2: Hyperparameter Settings\nCategory\nParameter\nSetting\nGlobal\nArchive size\n300\nNumber of iterations\n30\nAccuracy predictor\nTrain size\n100\nEnsemble size\n500\nEvolutionary search\nPopulation size\n100\nNumber of generations per itera",
  "output": "The text discusses the impracticality and unnecessary nature of certain aspects in existing NAS approaches. It also provides a table of hyperparameter settings, including global parameters like archive size and number of iterations, accuracy predictor parameters like train size and ensemble size, and evolutionary search parameters like population size and number of generations per iteration.",
  "summary_type": "reports"
}{
  "input": "position encodings.\n\n--------------------------------------------------\nPerceiver: General Perception with Iterative Attention\nFigure 11. Example attention maps from the ﬁrst cross-attend of an ImageNet network trained with 2D Fourier feature position\nencodings.\n\n--------------------------------------------------\nPerceiver: General Perception with Iterative Attention\nFigure 12. Example attention maps from the eighth (ﬁnal) cross-attend of an ImageNet network trained with 2D Fourier feature\nposi",
  "output": "The text provides examples of attention maps from the first and final cross-attend of an ImageNet network trained with 2D Fourier feature position encodings, as part of a study titled \"Perceiver: General Perception with Iterative Attention\".",
  "summary_type": "reports"
}{
  "input": "capacity itself is a cru-\ncial factor. If we compare MEAL V1 and V2 we can further\nderive the conclusion that teacher’s performance, i.e. the\nquality of supervision, is another factor for the student, gen-\nerally, the stronger teachers can consistently distill stronger\nstudents. In the following, we would like to exam the im-\npact of the initialization and discriminator.\nEffects of initialization. To verify whether the initializa-\ntion of a student has a big impact, we conduct the ablation\nstudy",
  "output": "The text discusses the factors affecting student performance in the context of the MEAL V1 and V2 systems. It suggests that teacher's performance and the quality of supervision are crucial. It also mentions the potential impact of the student's initialization, which is being investigated through an ablation study.",
  "summary_type": "reports"
}{
  "input": "For the h-th SA head, the attention map is computed as\nAh,:,: = Softmax(QhKh\n⊤/\n√\nd) with Qh and Kh from the\ncorresponding head. When the context is clear, we omit the\nsubscript h.\n3.2. Attention Collapse\nMotivated by the success of deep CNNs [9, 30, 35, 36],\nwe conduct systematic study in the changes of the perfor-\nmance of ViTs as depth increases. Without loss of gen-\nerality, we ﬁrst ﬁx the hidden dimension and the number\nof heads to 384 and 12 respectively1, following the com-\nmon practice",
  "output": "The text discusses the computation of the attention map for a specific self-attention (SA) head in a Vision Transformer (ViT). The attention map, denoted as Ah, is calculated using the Softmax function applied to the product of Qh and Kh, divided by the square root of d. The text also mentions a study on the performance changes of ViTs as depth increases, with a fixed hidden dimension and number of heads at 384 and 12 respectively.",
  "summary_type": "reports"
}{
  "input": "r greedy super-\nnet is more discriminative since it can use less images to\nidentify whether a path is good or weak. Nevertheless, as\nleft Figure 4 shows, too few evaluation images might have\nweak correlation while too many evaluation images mean\ngreater evaluation cost. But 1000 evaluation images enjoy a\nbalanced trade-off between the rank correlation and evalua-\ntion cost. Note that we also report the results w.r.t. ranking\nusing the ACC of 1000 images, which is smaller than that\nusing loss. Th",
  "output": "A greedy super-net is more discriminative as it can use fewer images to determine the quality of a path. However, using too few or too many evaluation images can lead to weak correlation or higher evaluation cost respectively. A balance is achieved with 1000 evaluation images, which provides a good trade-off between rank correlation and evaluation cost. The results also show that the ACC of 1000 images is smaller than that using loss.",
  "summary_type": "reports"
}{
  "input": "t,\napply the Normalizer-Free setup of Brock et al. (2021) to\nan SE-ResNeXt-D, with modiﬁed width and depth patterns,\nand a second spatial convolution. Second, apply AGC to\nevery parameter except for the linear weight of the classiﬁer\nlayer. For batch size 1024 to 4096, set λ = 0.01, and make\nuse of strong regularization and data augmentation. See\nTable 1 for additional information on each model variant.\n6. Experiments\n6.1. Evaluating NFNets on ImageNet\nWe now turn our attention to evaluating our",
  "output": "The text discusses the application of the Normalizer-Free setup of Brock et al. (2021) to an SE-ResNeXt-D, with modified width and depth patterns, and a second spatial convolution. It also mentions the application of AGC to every parameter except for the linear weight of the classifier layer. For batch sizes ranging from 1024 to 4096, λ is set to 0.01, with strong regularization and data augmentation being utilized. Additional information on each model variant can be found in Table 1. The text then transitions to discussing experiments evaluating NFNets on ImageNet.",
  "summary_type": "reports"
}{
  "input": "77] both use JFT as unlabeled data like\nMeta Pseudo Labels. Similar to Section 3.2, we only com-\npare Meta Pseudo Labels to results that are obtained with\nResNet-50 and without distillation.\nResults.\nTable 3 presents the results. As can be seen from\nthe table, Meta Pseudo Labels boosts the top-1 accuracy of\nResNet-50 from 76.9% to 83.2%, which is a large margin\nof improvement for ImageNet, outperforming both UDA\nand Noisy Student. Meta Pseudo Labels also outperforms\nBillion-scale SSL [68, 79] in",
  "output": "The text discusses the use of Meta Pseudo Labels with ResNet-50, without distillation, and its comparison with other methods. The results show that Meta Pseudo Labels significantly improved the top-1 accuracy of ResNet-50 from 76.9% to 83.2% on ImageNet, surpassing both UDA and Noisy Student. It also outperformed Billion-scale SSL.",
  "summary_type": "reports"
}{
  "input": "[14]. However, the relative\nimprovement of larger Mixer models are even more pronounced. The performance gap between\nMixer-L/16 and ViT-L/16 shrinks with data scale. It appears that Mixer beneﬁts from the growing\ndataset size even more than ViT. One could speculate and explain it again with the difference in\ninductive biases: self-attention layers in ViT lead to certain properties of the learned functions that are\nless compatible with the true underlying distribution than those discovered with M",
  "output": "The performance improvement of larger Mixer models is more pronounced than that of ViT models. The performance gap between Mixer-L/16 and ViT-L/16 decreases with increasing data scale. Mixer seems to benefit more from larger datasets than ViT. This could be due to the difference in inductive biases: the self-attention layers in ViT may result in learned functions that are less compatible with the true underlying distribution than those discovered with Mixer.",
  "summary_type": "reports"
}{
  "input": "s are not able to\nbeneﬁt from either the largest dataset, or compute resources. Figure 1, left and center, show the Ti/16\nmodel tending towards a high error rate, even when trained on a large number of images.\nThird, large models beneﬁt from additional data, even beyond 1B images. When scaling up the\nmodel size, the representation quality can be limited by smaller datasets; even 30-300M images is\nnot sufﬁcient to saturate the largest models. In Figure 1, center, the error rate of L/16 model on t",
  "output": "The text discusses the performance of different models in relation to dataset size and compute resources. It mentions that some models tend to have a high error rate even when trained on a large number of images. It also states that larger models benefit from additional data, even beyond 1 billion images, as smaller datasets can limit the representation quality. The text suggests that even 30-300 million images may not be sufficient to saturate the largest models.",
  "summary_type": "reports"
}{
  "input": "und that the learnable pooling performs the\nbest. We believe that the learnable weighting is more ef-\nﬁcient because each embedded patch does not contain the\nsame amount of entropy. This allows our model to give\nhigher weight to patches that contain more information rel-\nevant to the classiﬁer. Additionally, the sequence pooling\nallows our model to better utilize information across spa-\ntially sparse data. We will further study the effects of this\npooling in the ablation study (Sec 4.4).\n3.4. Sm",
  "output": "The text discusses the performance of learnable pooling in a model, stating it performs the best due to its efficiency. This is because each embedded patch doesn't contain the same amount of entropy, allowing the model to give higher weight to patches with more relevant information for the classifier. The sequence pooling also enables the model to better use information across spatially sparse data. Further study on the effects of this pooling will be conducted in the ablation study.",
  "summary_type": "reports"
}{
  "input": "tion to 12\nobjective problem in Section F.\n4.6\nUtility on Dense Image Prediction\nDense image prediction is another series of important computer\nvision tasks, that assigns a label to each pixel in the input image\n[72], [73]. Success in these tasks relies on both feature extraction\nvia a backbone CNN, e.g. ResNet [3], and feature aggregation, e.g.\nFPN [74], at multiple scales. In this section, we use NAT to design\nefﬁcient backbone feature extractors for semantic segmentation, to\ndemonstrate its u",
  "output": "Dense image prediction, a crucial computer vision task, assigns a label to each pixel in an input image. Success in these tasks depends on feature extraction through a backbone CNN, such as ResNet, and feature aggregation at multiple scales, like FPN. The text mentions using NAT to design efficient backbone feature extractors for semantic segmentation.",
  "summary_type": "reports"
}{
  "input": "g ones, starting at 30M (around 10% of the original dataset), it does.\nin the sense of reducing validation accuracy on the pre-training dataset, but also in reduced transfer\nperformance as training continued. As an initial attempt at tackling this, we used RandAugment [14]\nwith N = 2 transformations of magnitude M = 10. This is shown in Figure 38. Interestingly,\nRandAug typically helps expert models while harming dense models. With this applied, for each\narchitecture, there is an expert model wh",
  "output": "The text discusses the impact of reducing validation accuracy on the pre-training dataset and its effect on reduced transfer performance. An initial solution attempted was the use of RandAugment with N = 2 transformations of magnitude M = 10. It was observed that RandAugment typically benefits expert models while negatively affecting dense models.",
  "summary_type": "reports"
}{
  "input": "d-to-Head comparison on image segmentation (ResNet-50 as backbone) on ADE20K.\nHead\noutput stride backbone: 8\noutput stride backbone: 16\nmean IoU\npixel Acc.\nparams\nGFLOPs\nmean IoU\npixel Acc.\nparams\nGFLOPs\nbaseline [23]: 3×3 conv\n37.87\n78.17\n35.42\n131.37\n36.84\n77.84\n35.42\n39.52\nDeepLabv3 [24]: ASPP\n40.91\n79.92\n41.48\n151.17\n40.34\n79.44\n41.48\n44.47\nPSPNet [23]: PPM\n41.24\n80.01\n49.06\n165.42\n39.75\n79.17\n49.06\n48.08\nPyConvSegNet: PyConvPH\n41.54\n80.18\n34.40\n116.84\n40.43\n79.45\n34.40\n36.08\nPyConv results",
  "output": "The text presents a head-to-head comparison of image segmentation using ResNet-50 as a backbone on ADE20K. Four methods are compared: baseline (3x3 conv), DeepLabv3 (ASPP), PSPNet (PPM), and PyConvSegNet (PyConvPH). The comparison includes metrics like mean IoU, pixel accuracy, parameters, and GFLOPs. The PyConvSegNet method shows the highest mean IoU and pixel accuracy with the lowest parameters and GFLOPs.",
  "summary_type": "reports"
}{
  "input": "the advantages of Transformers: dynamic\nattention, global context fusion, and better generalization.\nOur results demonstrate that this approach attains state-of-\nart performance when CvT is pre-trained with ImageNet-\n1k, while being lightweight and efﬁcient: CvT improves the\nperformance compared to CNN-based models (e.g. ResNet)\nand prior Transformer-based models (e.g. ViT, DeiT) while\nutilizing fewer FLOPS and parameters. In addition, CvT\nachieves state-of-the-art performance when evaluated at",
  "output": "The text discusses the benefits of Transformers, specifically CvT (Convolutional vision Transformers), which include dynamic attention, global context fusion, and better generalization. The results show that CvT, when pre-trained with ImageNet-1k, achieves state-of-the-art performance, surpassing CNN-based models like ResNet and previous Transformer-based models like ViT and DeiT. CvT is also more efficient and lightweight, using fewer FLOPS and parameters.",
  "summary_type": "reports"
}{
  "input": "d Aditya V. Nori. Semi-supervised learning via\ncompact latent space clustering. In International Conference on Machine Learning, 2018. 3\n[25] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain\nGelly, and Neil Houlsby. Large scale learning of general visual representations for transfer.\narXiv preprint arXiv:1912.11370, 2019. 1\n[26] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better?\nIn Proceedings of the IEEE conference",
  "output": "Aditya V. Nori presented a paper on semi-supervised learning via compact latent space clustering at the International Conference on Machine Learning in 2018. Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby published a preprint on arXiv in 2019 about large scale learning of general visual representations for transfer. Simon Kornblith, Jonathon Shlens, and Quoc V Le questioned whether better ImageNet models transfer better in the proceedings of the IEEE conference.",
  "summary_type": "reports"
}{
  "input": "ms 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5754–5764,\n2019.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\narXiv preprint arXiv:2106.04560, 2021.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu,\nJianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic segmentatio",
  "output": "The text lists several academic papers and their authors. The first is from NeurIPS 2019 by Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer, titled \"Scaling vision transformers\". The second is a preprint on arXiv by the same authors. The third is a paper on colorful image colorization by Richard Zhang, Phillip Isola, and Alexei A Efros, presented at ECCV in 2016. The last paper is by Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang, titled \"Rethinking semantic segmentation\".",
  "summary_type": "reports"
}{
  "input": "18] Tsung-Yi Lin, Piotr Dollár, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J.\nBelongie. Feature pyramid networks for object detection. In IEEE Conference on Computer\nVision and Pattern Recognition, 2017. 6\n[19] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In\nEuropean Conference on Computer Vision, 2014. 2, 6\n[20] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan",
  "output": "Tsung-Yi Lin, Piotr Dollár, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J. Belongie presented a paper on Feature Pyramid Networks for object detection at the IEEE Conference on Computer Vision and Pattern Recognition in 2017. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick discussed Microsoft COCO: common objects in context at the European Conference on Computer Vision in 2014. Ze Liu, Yutong Lin, Yue Cao, Han Hu, and Yixuan are also mentioned but their work is not specified.",
  "summary_type": "reports"
}{
  "input": "mputer Vision and Pattern\nRecognition, pages 1580–1589, 2020.\n[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n770–778, 2016.\n[18] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\nin a neural network. arXiv preprint arXiv:1503.02531, 2015.\n[19] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-",
  "output": "The text refers to various academic papers and their authors. Paper [17] by K. He, X. Zhang, S. Ren, and J. Sun discusses deep residual learning for image recognition, presented at the IEEE conference on computer vision and pattern recognition in 2016. Paper [18] by G. Hinton, O. Vinyals, and J. Dean discusses distilling knowledge in a neural network, available as an arXiv preprint. Paper [19] by A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam is about Mobilenets, but the text cuts off before providing more information.",
  "summary_type": "reports"
}{
  "input": ",\n(ii) between different spatial locations, or both at once. In CNNs, (ii) is implemented with N × N\nconvolutions (for N > 1) and pooling. Neurons in deeper layers have a larger receptive ﬁeld [1, 28].\nAt the same time, 1×1 convolutions also perform (i), and larger kernels perform both (i) and (ii).\nIn Vision Transformers and other attention-based architectures, self-attention layers allow both (i)\nand (ii) and the MLP-blocks perform (i). The idea behind the Mixer architecture is to clearly sepa",
  "output": "The text discusses the implementation of different spatial locations in Convolutional Neural Networks (CNNs) using N × N convolutions and pooling. It mentions that deeper layer neurons have a larger receptive field. 1×1 convolutions perform one function, while larger kernels perform two. In Vision Transformers and other attention-based architectures, self-attention layers and MLP-blocks perform different functions. The Mixer architecture aims to clearly separate these functions.",
  "summary_type": "reports"
}{
  "input": "Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In\nInternational Conference on Machine Learning, pages 6105–6114. PMLR, 2019.\n[53] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In\nInternational Conference on Machine Learning, pages 9438–9447. PMLR, 2020.\n[54] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou.\nTraining data-efﬁcient image transformers & distillation through a",
  "output": "The text mentions three different studies presented at the International Conference on Machine Learning. The first is \"Efficientnet: Rethinking model scaling for convolutional neural networks\" (2019), which discusses a new approach to model scaling in convolutional neural networks. The second study is \"Sparse Sinkhorn attention\" (2020) by Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. The third study, by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou, focuses on training data-efficient image transformers and distillation.",
  "summary_type": "reports"
}{
  "input": "nd Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners.\narXiv preprint\narXiv:2005.14165, 2020.\n[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit:\nCross-attention multi-scale vision transformer for image\nclassiﬁcation. arXiv preprint arXiv:2103.14899, 2021.\n[5] Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George\nPapandreou, Barret Zoph, Florian Schroff, Hartwig Adam,\nand Jon Shlens. Searching for efﬁcient multi-scale architec-\ntures fo",
  "output": "The text mentions several academic papers. The first, by Neelakantan, Shyam, Sastry, Askell, et al., is titled \"Language models are few-shot learners\" and was published on arXiv in 2020. The second paper, by Chen, Fan, and Panda, is titled \"Crossvit: Cross-attention multi-scale vision transformer for image classification\" and was also published on arXiv in 2021. The third paper, by Chen, Collins, Zhu, Papandreou, Zoph, Schroff, Adam, and Shlens, is about searching for efficient multi-scale architectures, but the full title is not provided.",
  "summary_type": "reports"
}{
  "input": "lly a bottleneck\nstructure [10], which can be deﬁned as a stack of 1 × 1, 3 × 3, and 1 × 1 convolution layers\nwith residual learning (shown in Figure 1a). The 1 × 1 layers are responsible for reducing and\nthen increasing channel dimensions, leaving the 3 × 3 layer a bottleneck with smaller input/output\nchannel dimensions. The CNN backbones are generally faster and require less inference time thanks\nto parameter sharing, local information aggregation, and dimension reduction. However, due to\nthe",
  "output": "A bottleneck structure, defined as a stack of 1x1, 3x3, and 1x1 convolution layers with residual learning, is generally faster and requires less inference time due to parameter sharing, local information aggregation, and dimension reduction. The 1x1 layers reduce and then increase channel dimensions, making the 3x3 layer a bottleneck with smaller input/output channel dimensions.",
  "summary_type": "reports"
}{
  "input": "dation set.\n5.1. Implementation Details\nWe use data sharding for distributed training on Ima-\ngeNet, evenly partitioning the data across GPUs. At each\ntraining iteration, a mini-batch of training data is sampled\nfrom the corresponding shard (without replacement). We\napply the transformations from the learned Auto Augmen-\ntation policy to each individual image. Then we further ap-\nply standard transformations including: random size crop,\nrandom horizontal ﬂip, color jittering, and changing the\nli",
  "output": "The text discusses the implementation details of a distributed training process on ImageNet using data sharding. The data is evenly partitioned across GPUs and a mini-batch of training data is sampled from the corresponding shard at each training iteration. The learned Auto Augmentation policy is applied to each individual image, followed by standard transformations such as random size crop, random horizontal flip, color jittering, and changing the light.",
  "summary_type": "reports"
}{
  "input": "r than stated in Figure 5.\n4. Transfer Learning Results\nIn this section, we will present transfer learning results\nof TResNet models on four well-known single-label classi-\nﬁcation downstream datasets. We will also present transfer\nlearning results on multi-label classiﬁcation and object de-\ntection tasks.\n4.1. Single-Label Classiﬁcation\nWe evaluated TResNet on four commonly used, compet-\nitive transfer learning datasets: Stanford-Cars [16], CIFAR-\n10 [17], CIFAR-100 [17] and Oxford-Flowers [29]",
  "output": "The text presents transfer learning results of TResNet models on four single-label classification datasets: Stanford-Cars, CIFAR-10, CIFAR-100, and Oxford-Flowers. It also mentions that transfer learning results on multi-label classification and object detection tasks will be presented.",
  "summary_type": "reports"
}{
  "input": "in several of the ﬁrst module’s attention maps).\narchitectural inductive bias to do so. In other words, under\nthese conditions, networks that use 2D convolutions cannot\nexploit the local neighorhood structure of inputs to reason\nabout space, but must reason in terms of features, just like\nstructure-agnostic architectures like Transformers and Per-\nceivers. The results of this experiment are shown in Table 2.\nAs the Transformer and Perceiver effectively treat any input\nas a permuted input, their",
  "output": "The text discusses an experiment involving networks that use 2D convolutions. These networks, under certain conditions, cannot utilize the local neighborhood structure of inputs to reason about space. Instead, they must reason in terms of features, similar to structure-agnostic architectures like Transformers and Perceivers. The results of this experiment are presented in Table 2. Transformers and Perceivers treat any input as a permuted input.",
  "summary_type": "reports"
}{
  "input": "ture advantages of both CNN and transform-\ners but also retains the representation capability of local fea-\ntures and global representations to the maximum extent.\n3. Conformer\n3.1. Overview\nLocal features and global representations are important\ncounterparts, which have been extensively studied in the\nlong history of visual descriptors. Local features and their\ndescriptors [33, 26, 34], which are compact vector represen-\ntations of local image neighborhoods, have been the build-\ning blocks of m",
  "output": "The text discusses the Conformer, a model that combines the advantages of both CNN and transformers. It retains the representation capability of local features and global representations to the maximum extent. Local features and global representations, extensively studied in the history of visual descriptors, are important counterparts. Local features and their descriptors, compact vector representations of local image neighborhoods, are the building blocks of the model.",
  "summary_type": "reports"
}{
  "input": "Prodpoly-ResNet50\n98.78 (Ò 0.50)\n98.95 (Ò 0.31)\nIn Fig. 13, we compare the reconstruction error of the proposed\nmethod to the baseline spiral convolutions along with other popular\ngraph learnable operators, i.e., the Graph Attention Network\n(GAT) [90], FeastNet [91], Mixture model CNNs (MoNet) [92],\nConvolutional Mesh Autoencoders (COMA) [88] which are based\non the spectral graph ﬁlters of ChebNet [93], as well as with\nPrincipal Component Analysis (PCA), which is quite popular\nin shape analysis",
  "output": "The text compares the reconstruction error of Prodpoly-ResNet50 to other graph learnable operators such as the Graph Attention Network (GAT), FeastNet, Mixture model CNNs (MoNet), Convolutional Mesh Autoencoders (COMA), and Principal Component Analysis (PCA). These operators are based on the spectral graph filters of ChebNet.",
  "summary_type": "reports"
}{
  "input": "cally this is either a learned embedding\nor tokens are given weights from two sine waves with high\nfrequencies, which is sufﬁcient for the model to learn that\nthere exists a positional relationship between these tokens.\nClassiﬁcation Token: Vision transformers typically add\nan extra learnable [class] token to the sequence of the\nembedded patches, representing the class parameter of an\nentire image and its state after transformer encoder can be\nused for classiﬁcation. [class] token contains the l",
  "output": "The text discusses two aspects of vision transformers. Firstly, it mentions that tokens are either given a learned embedding or weights from two high-frequency sine waves, enabling the model to understand the positional relationship between tokens. Secondly, it explains that vision transformers often include an extra learnable [class] token in the sequence of embedded patches. This token represents the class parameter of an entire image and can be used for classification after the transformer encoder processes it.",
  "summary_type": "reports"
}{
  "input": "magenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 1\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An Image is Worth 16x",
  "output": "The text mentions two academic works. The first is about \"magenet\", a large-scale hierarchical image database, presented at CVPR in 2009. The second is a paper by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova titled \"Bert: Pre-training of deep bidirectional transformers for language understanding\", which was published as an arXiv preprint in 2018. The text ends abruptly while introducing a third work by Alexey Dosovitskiy and others, with the incomplete title \"An Image is Worth 16x\".",
  "summary_type": "reports"
}{
  "input": "rmers enhanced by the proposed\nlocality mechanism outperform their baselines.\ntures, which is essential for the success of CNNs. Mean-\nwhile, the local, repetitive connections save many parame-\nters compared with fully connected layers. Yet, one prob-\nlem is that a larger receptive ﬁeld can only be achieved by\ncombining layers, despite alternative attempts at enlarging\nthe receptive ﬁeld [51].\nA parallel, thriving research strand incorporates global\nconnectivity into the network via self-attenti",
  "output": "The text discusses the advantages of a proposed locality mechanism in enhancing the performance of certain systems. It highlights the importance of local, repetitive connections in saving parameters compared to fully connected layers. However, it mentions a problem where a larger receptive field can only be achieved by combining layers. The text also refers to a parallel research strand that incorporates global connectivity into the network via self-attention.",
  "summary_type": "reports"
}{
  "input": "ith the state-of-the-art. We ﬁrst report a simple ViT-B\n(our baseline) that uses ImageNet-21K pre-training. Our\nMViT-B with 16 frames has 64.7% top-1 accuracy, which is\nbetter than the SlowFast R101 [30] which shares the same\nsetting (K400 pre-training and 3×1 view testing). With more\ninput frames, our MViT-B achieves 67.7% and the deeper\nMViT-B-24 achieves 68.7% using our K600 pre-trained\nmodel of above. In general, Table 6 veriﬁes the capability of\ntemporal modeling for MViT.\nmodel\npretrain\nmA",
  "output": "The text discusses the performance of different video processing models. The ViT-B baseline model uses ImageNet-21K pre-training. The MViT-B model with 16 frames achieves 64.7% top-1 accuracy, outperforming the SlowFast R101 model. When more input frames are used, MViT-B reaches 67.7% accuracy, and the deeper MViT-B-24 model achieves 68.7% accuracy using a K600 pre-trained model. The results confirm the effectiveness of temporal modeling for MViT.",
  "summary_type": "reports"
}{
  "input": "to changes in image resolution at test time,\nand are therefore more amenable to process variable-size images.\n• For image classiﬁcation, we demonstrate that our models are on par with state-of-the-\nart vision transformers for multiple model sizes using a simple columnar architecture,\ni.e., in which we keep the resolution constant across layers. In particular, our XCiT-L24\nmodel achieves 86.0% top-1 accuracy on ImageNet, outperforming its CaiT-M24 [68] and\nNFNet-F2 [10] counterparts with comparab",
  "output": "The text discusses image resolution changes and the processing of variable-size images. It states that for image classification, the models used are comparable to state-of-the-art vision transformers for various model sizes, using a simple columnar architecture where the resolution remains constant across layers. Specifically, the XCiT-L24 model achieves 86.0% top-1 accuracy on ImageNet, surpassing its CaiT-M24 and NFNet-F2 counterparts.",
  "summary_type": "reports"
}{
  "input": "nique portion of the\ndataset. To accomplish this goal, we propose WGM to reweight losses of different experts.\nThe expert selection can be denoted as a probability matrix Pm×n ∈Rm×n, whose element Pj,k ∈\n[0, 1] represents the probability of assigning the j-th sample to the k-th expert. WGM takes Pm×n as\ninput and outputs a weight matrix Wm×n. WGM ﬁrstly obtain an assignment matrix Am×n, which\nis a binary matrix with one-hot row vectors, then get Wm×n via smoothing and normalizing. For\nexample, A",
  "output": "The text proposes the use of Weighted Gradient Method (WGM) to reweight losses of different experts in a dataset. The expert selection is represented as a probability matrix Pm×n, where each element Pj,k represents the probability of assigning the j-th sample to the k-th expert. WGM takes this matrix as input and outputs a weight matrix Wm×n. This is achieved by first obtaining an assignment matrix Am×n, a binary matrix with one-hot row vectors, then smoothing and normalizing to get Wm×n.",
  "summary_type": "reports"
}{
  "input": "he median over 50 trials exceeds the\ntarget of 75.9%.\n2.1\nNesterov momentum at batch size 32k\nThis section describes how we used the standard Nesterov momentum optimizer to train the ResNet-\n50 v1.5 on ImageNet to 75.9% validation accuracy in 2,512 update steps at a batch size of 32,768,\nmatching the best published LARS result at this batch size. Although we implemented our own\ntraining program, the only logical changes we made to the published LARS pipeline were to the\noptimizer and the optimiz",
  "output": "The text discusses the use of the Nesterov momentum optimizer to train the ResNet-50 v1.5 on ImageNet to 75.9% validation accuracy in 2,512 update steps at a batch size of 32,768. This matched the best published LARS result at this batch size. The only changes made to the published LARS pipeline were to the optimizer.",
  "summary_type": "reports"
}{
  "input": "possible. This requires that, for any two\nblocks b(i)\nj\nand b(i)\nk in the same layer, we have\nw(i)\nj\n⊆w(i)\nk\nor w(i)\nk\n⊆w(i)\nj .\n(6)\nSuch within layer weight sharing makes the weight updates\nof w(i)\nj\nand w(i)\nk\nentangled with each other. The training of\nany block will affect the weights of others for their inter-\nsected portion, as demonstrated in Fig. 5. This is different\nfrom the classical weight sharing strategy in one-shot NAS,\nwhere the building blocks in the same layer are isolated. In\no",
  "output": "The text discusses a weight sharing strategy in a certain layer of a neural network. In this strategy, the weights of any two blocks in the same layer are either subsets of each other or vice versa. This causes the weight updates of these blocks to be interconnected, meaning the training of one block affects the weights of others. This is different from the traditional weight sharing strategy in one-shot Neural Architecture Search (NAS), where the building blocks in the same layer are isolated.",
  "summary_type": "reports"
}{
  "input": "d Momentum instead of RMSProp. Both simpliﬁcations reduce the total number of hyper-\nparameters as (1) cosine decay has no hyperparameters associated with it and (2) Momentum has one less than RMSProp.\nResNet (2015)\nResNet-RS (2021)\nEfﬁcientNets (2019)\nEpochs Trained\n90\n350\n350\nLR Decay Schedule\nStepwise\nCosine\nExponential Decay\nOptimizer\nMomentum\nMomentum\nRMSProp\nEMA of Weights\n✓\n✓\nLabel Smoothing\n✓\n✓\nStochastic Depth\n✓\n✓\nRandAugment\n✓\n✓\nDropout on FC\n✓\n✓\nSmaller Weight Decay\n✓\n✓\nSqueeze-Excita",
  "output": "The text compares three different neural network models: ResNet (2015), ResNet-RS (2021), and EfficientNets (2019). It mentions the number of epochs trained, the learning rate (LR) decay schedule, the optimizer used, and various features such as EMA of weights, label smoothing, stochastic depth, RandAugment, dropout on FC, smaller weight decay, and Squeeze-Excita. It also notes that using Momentum instead of RMSProp and cosine decay simplifies the models by reducing the total number of hyperparameters.",
  "summary_type": "reports"
}{
  "input": "W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[18] X. Ding, X. Zhang, J. Han, and G. Ding. Repmlp: Re-parameterizing convolutions into fully-connected\nlayers for image recognition. arXiv preprint arXiv:2105.01883, 2021.\n[19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 w",
  "output": "W. Chang, K. Lee, and K. Toutanova developed BERT, a pre-training model for deep bidirectional transformers for language understanding, as per their 2018 arXiv preprint. X. Ding, X. Zhang, J. Han, and G. Ding proposed RepMLP in 2021, a method of re-parameterizing convolutions into fully-connected layers for image recognition, also published on arXiv. A. Dosovitskiy et al. discussed the value of an image in a partially cited work.",
  "summary_type": "reports"
}{
  "input": "s.\nWe have considered different optimizers and\ncross-validated different learning rates and weight decays. Transformers are\nsensitive to the setting of optimization hyper-parameters. Therefore, during\ncross-validation, we tried 3 different learning rates (5.10−4, 3.10−4, 5.10−5) and\n3 weight decay (0.03, 0.04, 0.05). We scale the learning rate according to the\nbatch size with the formula: lrscaled =\nlr\n512 × batchsize, similarly to Goyal et\nal. [19] except that we use 512 instead of 256 as the b",
  "output": "The text discusses the sensitivity of Transformers to optimization hyper-parameters. Different optimizers, learning rates, and weight decays were considered and cross-validated. Three different learning rates (5.10−4, 3.10−4, 5.10−5) and three weight decays (0.03, 0.04, 0.05) were tested. The learning rate was scaled according to the batch size using a formula, similar to the method used by Goyal et al., but with 512 instead of 256.",
  "summary_type": "reports"
}{
  "input": ". First note that if we\nchoose each stochastic component to be distributed standard\nGumbel independently, ϵj ∼i.i.d. G(0, 1) ∀j, then the pre-\ndictive probabilities pc have a closed form solution that is\nprecisely the popular softmax cross-entropy model used in\ntraining neural network classiﬁcation models [42, 9]:\npc = P(arg max\nj∈[K]\nuj(x) = c)\n=\nexp(µc)\nPK\nj=1 exp(µj)\n⇐⇒ϵj ∼i.i.d. G(0, 1) ∀j\n(2)\nIn other words, this generative process with Gumbel noise\ndistribution is already an implicit stand",
  "output": "The text discusses that if each stochastic component is independently distributed standard Gumbel, the predictive probabilities have a closed form solution that aligns with the softmax cross-entropy model used in training neural network classification models. This generative process with Gumbel noise distribution is an implicit standard.",
  "summary_type": "reports"
}{
  "input": "mall 0.75 and 1.0, we\nstill obtain 1.72% and 1.49% increase on ImageNet. Note\nthat for EfﬁcientNet-B0, 77.3/93.5 accuracy is from their\npaper [47] and 76.8/93.2 is the actual accuracy from their\npre-trained models in timm.\nWith More Data Augmentation. We’d like to further ex-\nplore whether our models have been saturated on the tar-\nget data by injecting more data augmentation like CutMix\nin training. The results are shown in Table 3, we involve\nCutMix and keep other settings the same as our basi",
  "output": "The text discusses an increase in ImageNet accuracy, with 1.72% and 1.49% improvements noted. It mentions that the actual accuracy of EfficientNet-B0 from pre-trained models in timm is 76.8/93.2, differing from the 77.3/93.5 accuracy reported in a paper. The text also explores the potential saturation of their models by introducing more data augmentation like CutMix in training, but the results of this are not provided.",
  "summary_type": "reports"
}{
  "input": "s. Swin Transformer depends on\ntorch.roll() to perform cyclic shift and its reverse on features. This operation is memory unfriendly and\nrarely supported by popular inference frameworks such as NVIDIA TensorRT, Google Tensorﬂow-\nLite, and Snapdragon Neural Processing Engine SDK (SNPE), etc. This hinders the deployment of\nSwin either on the server-side or on end devices in a production environment. In contrast, Twins\nmodels don’t require such an operation and only involve matrix multiplications t",
  "output": "Swin Transformer relies on torch.roll() for cyclic shift and its reverse on features, an operation not commonly supported by popular inference frameworks like NVIDIA TensorRT, Google Tensorflow-Lite, and Snapdragon Neural Processing Engine SDK (SNPE). This limitation impedes Swin's deployment in production environments, either on servers or end devices. Conversely, Twins models don't need this operation and only involve matrix multiplications.",
  "summary_type": "reports"
}{
  "input": "namicViT.\n1\nIntroduction\nThese years have witnessed the great progress in computer vision brought by the evolution of CNN-\ntype architectures [12, 18]. Some recent works start to replace CNN by using transformer for many\nvision tasks, like object detection [36, 20] and classiﬁcation [25]. Just like what has been done to the\nCNN-type architectures in the past few years, it is also desirable to accelerate the transformer-like\nmodels to make them more suitable for real-time applications.\nOne common",
  "output": "The text discusses the recent advancements in computer vision due to the evolution of CNN-type architectures. It mentions that some recent works have started to replace CNN with transformers for various vision tasks such as object detection and classification. The text suggests that there is a need to accelerate transformer-like models to make them more suitable for real-time applications.",
  "summary_type": "reports"
}{
  "input": "sults. While being better in ImageNet1K, our model is on\npar with DeiT models on all the downstream classiﬁcation\ntasks. This result assures that our models still have good\ngeneralization ability rather than only ﬁt to ImageNet1K.\n4.3. Ablation Studies\nIn this section, we ﬁrst compare the different fusion ap-\nproaches (Section 3.3), and then analyze the effects of dif-\nferent parameters of our architecture design, including the\npatch sizes, the channel width and depth of the small branch\nand num",
  "output": "The text discusses a model that performs well in ImageNet1K and is comparable to DeiT models in all downstream classification tasks, indicating good generalization ability. It also mentions a section that compares different fusion approaches and analyzes the effects of various parameters of the architecture design, such as patch sizes, channel width and depth of the small branch.",
  "summary_type": "reports"
}{
  "input": "ognition (CVPR), 2018,\npp. 7132–7141. 7\n[73] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular\nmargin loss for deep face recognition,” in Conference on Computer Vision\nand Pattern Recognition (CVPR), 2019, pp. 4690–4699. 8, 9, 10\n[74] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao, “Ms-celeb-1m: A dataset\nand benchmark for large-scale face recognition,” in European Conference\non Computer Vision (ECCV).\nSpringer, 2016, pp. 87–102. 8\n[75] J. Deng, J. Guo, D. Zhang, Y. Deng, X. Lu,",
  "output": "The text refers to several academic papers presented at various conferences. Paper [73] by J. Deng, J. Guo, N. Xue, and S. Zafeiriou, titled \"Arcface: Additive angular margin loss for deep face recognition,\" was presented at the Conference on Computer Vision and Pattern Recognition (CVPR) in 2019. Paper [74] by Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao, titled \"Ms-celeb-1m: A dataset and benchmark for large-scale face recognition,\" was presented at the European Conference on Computer Vision (ECCV) in 2016. The text also begins to reference another paper by J. Deng, J. Guo, D. Zhang, Y. Deng, X. Lu, but it is cut off.",
  "summary_type": "reports"
}{
  "input": "However, both have\nsophisticated designs (with non-trivial implementation) and add extra complexity.\nDifferent from these methods, every block in NesT processes information independently via standard\ntransformer layers, and only communicate and mix global information during block aggregation\nstep via some simple spatial operations (e.g. convolution and pooling). One key ingredient of block\naggregation is to perform it in image plane so that information can be exchanged between nearby\nblocks. The",
  "output": "Unlike other methods that have complex designs, every block in NesT processes information independently using standard transformer layers. They only mix and communicate global information during the block aggregation step through simple spatial operations like convolution and pooling. A crucial aspect of block aggregation is performing it in the image plane, allowing information exchange between nearby blocks.",
  "summary_type": "reports"
}{
  "input": "propose the random-shifting training strategy for Vi-\nsion Longformer, to further accelerate the training speed of\nVision Longformer. More speciﬁcally, instead of attending\nto all 8 neighbor patches, one patch can only attend to itself\nand one random neighbor patch during training. To achieve\nFigure 9. Illustration of the 8 modes in the random-shifting train-\ning strategy. For mode i (1 <= i <= 8), the query chunk (dark\nbrown) attends to itself and the i’th neighbor chunk.\nFigure 10. The random",
  "output": "The text proposes a random-shifting training strategy for Vision Longformer to increase its training speed. Specifically, instead of attending to all 8 neighboring patches, a patch can only attend to itself and one random neighboring patch during training.",
  "summary_type": "reports"
}{
  "input": "ields an improvement of 1.9\nBLEU scores over the Transformer model with vanilla\nDropout.\nAlthough the search cost of AutoDropout can be high, a\nsimple use case of AutoDropout is to drop our found patterns\ninto existing pipelines in the same way that AutoAugment\npolicies (Cubuk et al. 2019a) were used to improve state-of-\nthe-art models.\nRelated works.\nOur work has the same philosophy with\nexisting neural architecture search and AutoAugment lines of\nresearch (Pham et al. 2018; Liu, Simonyan, and",
  "output": "The text discusses an improvement of 1.9 BLEU scores over the Transformer model with vanilla Dropout, achieved through AutoDropout. Despite its high search cost, AutoDropout can be used to enhance existing pipelines, similar to how AutoAugment policies were used to improve models. The work aligns with existing research on neural architecture search and AutoAugment.",
  "summary_type": "reports"
}{
  "input": "l complexity to image scale. (c) Our proposed Cycle\nFully-Connected Layer (Cycle FC) has linear complexity\nthe same as channel FC and a larger receptive ﬁeld than\nChannel FC.\nThe MLP-like models [49, 50, 35] are much simpler\nthan Transformer-based models in: They replace the self-\nattention sub-layers by the purely fully-connected lay-\ners1 (equal to the 1 × 1 convolution), removing the at-\n1MLP and FC are are used interchangeably.\n1\narXiv:2107.10224v1  [cs.CV]  21 Jul 2021\n\n--------------------",
  "output": "The proposed Cycle Fully-Connected Layer (Cycle FC) has linear complexity similar to channel FC and a larger receptive field than Channel FC. MLP-like models are simpler than Transformer-based models as they replace the self-attention sub-layers with purely fully-connected layers, equivalent to the 1 × 1 convolution, thereby removing the attention mechanism. MLP and FC are used interchangeably.",
  "summary_type": "reports"
}{
  "input": "on each input\nimage patches at training time. Our motivation comes from the difference of ViTs and CNNs that\nViTs not only extract intra-patch features but also concern the inter-patch relations. We think the\ntraditional augmentation which randomly transforms the whole image could provide enough intra-\npatch augmentation. However, it lacks the diversity on inter-patch augmentation, as all of patches\nhave the same transformation at one time. To impose more inter-patch diversity, we retain the ori",
  "output": "The text discusses the difference between Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs) in image processing. ViTs extract both intra-patch features and inter-patch relations. Traditional augmentation, which randomly transforms the whole image, provides sufficient intra-patch augmentation but lacks diversity in inter-patch augmentation. The authors suggest retaining the original image to increase inter-patch diversity.",
  "summary_type": "reports"
}{
  "input": "bly effective at numerous downstream tasks such as object\ndetection [37], instance segmentation [22] and image captioning [1]. Similarly, in natural language\nprocessing, Transformers rule the roost [13, 43, 42, 4]. Their effectiveness at capturing short and long\nrange information have led to state-of-the-art results across tasks such as question answering [45]\nand language understanding [58].\nIn computer vision, Transformers were initially employed as long range information aggregators\nacross sp",
  "output": "Transformers are highly effective in various tasks such as object detection, instance segmentation, and image captioning. They also dominate in natural language processing due to their ability to capture short and long range information, leading to state-of-the-art results in question answering and language understanding tasks. In computer vision, Transformers are used as long range information aggregators.",
  "summary_type": "reports"
}{
  "input": "y\ncheck marks (✓) in the Required additional modules columns such as KD, AT, PKT,\nRKD, HND, SPKD, Tf-KD, GHND and L2 can be reimplemented simply by adding\nthe new loss modules to the registry in the framework (Section 2.2).\nDifferent from the existing frameworks [52,29,12,28,42,49], all the methods in Ta-\nble 4 are reimplemented independently from models in torchvision so that users can\neasily switch models by specifying a model name and its parameters in a conﬁguration\nﬁle. Taking image classiﬁ",
  "output": "The text discusses the reimplementation of additional modules like KD, AT, PKT, RKD, HND, SPKD, Tf-KD, GHND and L2 by adding new loss modules to the registry in the framework. It mentions that this process is different from existing frameworks, with all methods independently reimplemented from models in torchvision. This allows users to easily switch models by specifying a model name and parameters in a configuration file.",
  "summary_type": "reports"
}{
  "input": "nts of the inputs, and\nmodify the inputs by propagating information between\nthe input pairs. As a result, it maps N inputs to N outputs\nand casts a quadratic O(N 2) memory and time complex-\nity. We propose centroid attention, a generalization of\nself-attention that maps N inputs to M outputs (M ≤N),\nsuch that the key information in the inputs is summarized\nin the smaller number of outputs (called centroids). We\ndesign centroid attention by amortizing the gradient de-\nscent update rule of a clust",
  "output": "The text discusses a proposed method called centroid attention, which is a generalization of self-attention. It maps N inputs to M outputs (where M is less than or equal to N), summarizing key information from the inputs into fewer outputs, referred to as centroids. This method is designed by amortizing the gradient descent update rule of a cluster, and it reduces the quadratic O(N 2) memory and time complexity.",
  "summary_type": "reports"
}{
  "input": "with training improvements makes accurate comparisons\nbetween architectures difﬁcult. The large improvements\ncoming from training strategies, when not being controlled\nfor, can overshadow architectural differences.\nHow should one compare different architectures?\nSince training methods and scale typically improve perfor-\nmance (Lee et al., 2020; Kaplan et al., 2020), it is critical\nto control for both aspects when comparing different ar-\nchitectures. Controlling for scale can be achieved through",
  "output": "The text discusses the difficulty in accurately comparing different architectures due to large improvements from training strategies. It suggests controlling for training methods and scale to improve performance and make accurate comparisons between different architectures.",
  "summary_type": "reports"
}{
  "input": "ongside model sparsity in\norder to scale models up. In order to scale the base architecture to which we add sparse mixture of\nexpert layers, we make the following changes based on [68]:\n• Low precision: We use bfloat16 instead of float32 to store the gradient moving average.\n• Learning-rate decay: We replace the linear schedule with an inverse square root schedule\n(rsqrt).\n• Weight decay: We apply weight decay to the kernel weights in the model with value 0.03\n(while biases are not regularized),",
  "output": "To scale up models, the base architecture is modified with sparse mixture of expert layers, based on reference [68]. Changes include using bfloat16 instead of float32 for storing the gradient moving average, replacing the linear schedule with an inverse square root schedule for learning-rate decay, and applying weight decay to the kernel weights in the model with a value of 0.03, while biases are not regularized.",
  "summary_type": "reports"
}{
  "input": "9]\n219M\n75.2\n-\nMnasNet-A3 [63]\n403M\n76.7\n93.3\nEfﬁcientNet-B0 [64]\n399M\n76.3\n93.2\nDNA-b [37]\n406M\n77.5\n93.3\nBossNet-M2\n403M\n77.4\n93.6\nTable 2: ImageNet results of state-of-the-art NAS models\non MBConv search space.\nMethod\nSearch Cost\nτ\nρ\nR\nSPOS [22]\n8.5 Gds\n-0.18\n-0.27\n-0.29\nDARTS [43]\n50 Gds\n0.08\n0.14\n0.06\nMnasNet [63]\n288 Tds\n0.61\n0.77\n0.78\nDNA [37] (EffNetB0)\n8.5 Gds\n0.62\n0.77\n0.83\nDNA [37] (MBNetV1)\n8.5 Gds\n0.23\n0.27\n0.37\nBossNAS\n10 Gds\n0.65\n0.78\n0.85\nTable 3: Comparison of the effectiveness",
  "output": "The text provides ImageNet results of state-of-the-art NAS models on MBConv search space, including MnasNet-A3, EfficientNet-B0, DNA-b, and BossNet-M2. It also compares the effectiveness of different methods such as SPOS, DARTS, MnasNet, DNA (EffNetB0), DNA (MBNetV1), and BossNAS, detailing their search cost and three other parameters (τ, ρ, R).",
  "summary_type": "reports"
}{
  "input": "ablation studies for pre-training and analyze the representations learned by BEIT.\n5\n\n--------------------------------------------------\nModels\nCIFAR-100\nImageNet\nTraining from scratch (i.e., random initialization)\nViT384 (Dosovitskiy et al., 2020)\n48.5*\n77.9\nDeiT (Touvron et al., 2020)\nn/a\n81.8\nSupervised Pre-Training on ImageNet-1K (using labeled data)\nViT384 (Dosovitskiy et al., 2020)\n87.1\n77.9\nDeiT (Touvron et al., 2020)\n90.8\n81.8\nSelf-Supervised Pre-Training on ImageNet-1K (without labeled",
  "output": "The text presents ablation studies for pre-training and analyzes the representations learned by BEIT. It compares the performance of models ViT384 and DeiT on CIFAR-100 and ImageNet datasets. The models are trained from scratch, with supervised pre-training on ImageNet-1K using labeled data, and self-supervised pre-training on ImageNet-1K without labeled data. The results are presented in percentages.",
  "summary_type": "reports"
}{
  "input": "ary\n74.50\n326M\nuniform-E\nevolutionary\n74.17\n320M\ngreedy\nevolutionary\n74.85\n320M\nFrom Table 7, we can see that a greedy supernet improves consistently the classiﬁcation accuracy in terms of different\nsearchers. This validates the superiority of our greedy supernet since it helps searchers to probe better architectures. More-\nover, to comprehensively investigate the effect of supernets, we implement systematic sampling 4 to sample 30 paths from\n50×20 = 1000 paths, which are discovered by the evolu",
  "output": "Table 7 shows that a greedy supernet consistently improves classification accuracy across different searchers, validating its superiority. To further investigate the effect of supernets, systematic sampling 4 is implemented to sample 30 paths from 1000 paths discovered by the evolutionary process.",
  "summary_type": "reports"
}{
  "input": "et, CIFAR-10, CIFAR-\n100, Cars, and Flowers dataset. On ImageNet, we achieve\n85.7% top-1 accuracy while training 3x - 9x faster and being\nup to 6.8x smaller than previous models (Figure 1). Our Ef-\nﬁcientNetV2 and progressive learning also make it easier to\ntrain models on larger datasets. For example, ImageNet21k\n(Russakovsky et al., 2015) is about 10x larger than ImageNet\nILSVRC2012, but our EfﬁcientNetV2 can ﬁnish the training\nwithin two days using moderate computing resources of 32\nTPUv3 cor",
  "output": "EfficientNetV2 achieves 85.7% top-1 accuracy on ImageNet, training 3x - 9x faster and being up to 6.8x smaller than previous models. It also facilitates training on larger datasets, such as ImageNet21k, which is 10x larger than ImageNet ILSVRC2012. The training can be completed within two days using moderate computing resources of 32 TPUv3.",
  "summary_type": "reports"
}{
  "input": "okens of different sizes (Ps and Pl, Ps < Pl) and\nfuse the tokens at the end by an efﬁcient module based on\ncross attention of the CLS tokens. Our design includes dif-\nferent numbers of regular transformer encoders in the two\nbranches (i.e. N and M) to balance computational costs.\n3.1. Overview of Vision Transformer\nVision Transformer (ViT) [11] ﬁrst converts an image\ninto a sequence of patch tokens by dividing it with a cer-\ntain patch size and then linearly projecting each patch into\ntokens. A",
  "output": "The text discusses a design that uses tokens of different sizes (Ps and Pl, Ps < Pl) and fuses them using an efficient module based on cross attention of the CLS tokens. The design includes varying numbers of regular transformer encoders in two branches (N and M) to balance computational costs. It also provides an overview of the Vision Transformer (ViT), which first converts an image into a sequence of patch tokens by dividing it with a certain patch size and then projects each patch into tokens linearly.",
  "summary_type": "reports"
}{
  "input": "eports the TPUv3 compute time for a batch size of 32 per core. The\ninference speed is also computed on a single TPUv3 core.\ntrain with Nesterov’s Accelerated Gradient [39, 55] during\npretraining and ﬁnetuning. We pretrain on 256 × 256 size\nimages and ﬁnetune on different image sizes, as shown in\nTable 6. Our wider H4 and hybrid-H4 models achieves bet-\nter accuracy than the Vision Transformer and a 4× wide\nResNet-152 from [29] and are also faster at inference on\nlarger images. We ﬁnetune for 8 ep",
  "output": "The text reports the TPUv3 compute time for a batch size of 32 per core and the inference speed on a single TPUv3 core. It uses Nesterov’s Accelerated Gradient during pretraining and finetuning. Pretraining is done on 256 × 256 size images and finetuning on different image sizes. The wider H4 and hybrid-H4 models achieve better accuracy than the Vision Transformer and a 4× wide ResNet-152, and are faster at inference on larger images. The finetuning process lasts for 8 epochs.",
  "summary_type": "reports"
}{
  "input": "LBERT (Lan et al. 2019)). Model compres-\nsion (Xu et al. 2020; Sun et al. 2019) can also make trans-\nformer more parameter efﬁcient.\nThe two existing methods both have their own limitations.\nFor huge models, one typical and effective method to scale\ntrainable parameters is replacing part of the feed-forward\nnetwork (FFN) layer in transformer blocks with MoE lay-\ners. In each MoE layer, to reﬁne one single token represen-\ntation, only a few experts are activated, so the MoE based\ntransformer hold",
  "output": "The text discusses the use of model compression to make transformer models more parameter efficient. It mentions two existing methods, each with their own limitations. For large models, a common method is to replace part of the feed-forward network layer in transformer blocks with MoE (Mixture of Experts) layers. In each MoE layer, only a few experts are activated to refine a single token representation, making the MoE-based transformer more efficient.",
  "summary_type": "reports"
}{
  "input": ", Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic\ndepth. In European conference on computer vision, pages 646–661. Springer, 2016.\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining",
  "output": "Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger presented a paper on deep networks with stochastic depth at the European conference on computer vision, published by Springer in 2016. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov published a preprint on arXiv in 2019 about Roberta, a robustly optimized BERT pretraining approach. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining are also mentioned but their work is not specified.",
  "summary_type": "reports"
}{
  "input": "batch\nsize of 128 on each GPU card. We use 8 NVIDIA RTX\nA6000 GPUs to train all models for 300 epochs and the to-\ntal batch size is 1024. All other training settings and hyper-\nparameters are adopted from Deit [35] for fair comparisons.\nFor those results in ablation study, we train these models\nfor 100 epochs with batch size 256 on each GPU and use 4\nGPUs with learning rate at 0.001.\n4.2. Ablation Study\nOur baseline model Pure-MLP Baseline is composed of\none patch converter and a sequence of ch",
  "output": "The text discusses the training of models using 8 NVIDIA RTX A6000 GPUs for 300 epochs with a batch size of 128 on each GPU, resulting in a total batch size of 1024. The training settings and hyperparameters are adopted from Deit for fair comparisons. In an ablation study, models are trained for 100 epochs with a batch size of 256 on each of 4 GPUs, with a learning rate of 0.001. The baseline model, Pure-MLP Baseline, consists of one patch converter and a sequence of channels.",
  "summary_type": "reports"
}{
  "input": "-M (both give similar\nGPU throughput). Comparison results appear in Table 10.\nMethod\nBabkbone\nmAP %\nFCOS\nResNet50\n42.8\nFCOS\nTResNet-M\n44.0\nTable 10. Comparison of TResNet-M to ResNet50 on MS-\nCOCO object detection task. Results were obtained using mm-\ndetection package, with FCOS as the object detection method .\nWe can see from Table 10 that TResNet-M outperform\nResNet50 on this object-detection task, increasing COCO\nmAP score from 42.8 to 44.0. This is consistent with the\nimprovement we saw in",
  "output": "The text compares the performance of TResNet-M and ResNet50 on the MS-COCO object detection task using the FCOS method. The results show that TResNet-M outperforms ResNet50, increasing the COCO mAP score from 42.8 to 44.0.",
  "summary_type": "reports"
}{
  "input": "5), but they are not stable at large batch sizes and do not\nmatch the performance of EfﬁcientNets (Tan & Le, 2019),\nthe current state of the art (Gong et al., 2020). This paper\nbuilds on this line of work and seeks to address these central\nlimitations. Our main contributions are as follows:\n• We propose Adaptive Gradient Clipping (AGC), which\nclips gradients based on the unit-wise ratio of gradient\nnorms to parameter norms, and we demonstrate that\nAGC allows us to train Normalizer-Free Networks",
  "output": "The text discusses the limitations of certain models at large batch sizes and their inability to match the performance of EfficientNets, the current state of the art. The paper proposes Adaptive Gradient Clipping (AGC), a method that clips gradients based on the unit-wise ratio of gradient norms to parameter norms. The authors demonstrate that AGC enables the training of Normalizer-Free Networks.",
  "summary_type": "reports"
}{
  "input": "layers.\nIn Proceedings of International Conference on Learning\nRepresentations (ICLR), 2020.\nCorreia, G. M., Niculae, V., and Martins, A. F. Adaptively\nsparse Transformers. In Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), 2019.\nCubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Ran-\ndaugment: Practical automated data augmentation with a\nreduced search space. In Proceedings of IEEE Conference\non Computer Vision and Pattern Recognition Workshops\n(CVPRW), 2020.\nDai, Z.,",
  "output": "The text mentions several academic papers presented at various conferences. These include a paper on layers presented at the International Conference on Learning Representations (ICLR) in 2020, a paper on adaptively sparse Transformers presented at the Conference on Empirical Methods in Natural Language Processing (EMNLP) in 2019, and a paper on RandAugment, a practical automated data augmentation method with a reduced search space, presented at the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) in 2020.",
  "summary_type": "reports"
}{
  "input": "d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697,\n2021.\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.\nIeee, 2009.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
  "output": "The text mentions two academic papers. The first one, by d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun, is titled \"Convit: Improving vision transformers with soft convolutional inductive biases\" and was published as an arXiv preprint in 2021. The second paper, by Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, is titled \"Imagenet: A large-scale hierarchical image database\" and was presented at the 2009 IEEE conference on computer vision and pattern recognition. The text ends with the beginning of a citation for a paper by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, but the title and details of this paper are not provided.",
  "summary_type": "reports"
}{
  "input": "retraining, etc.\n3. Analysis: Problems in Existing Classiﬁca-\ntion System\nConsider a classiﬁcation task that distinguishes various\nbreeds of dogs, e.g. toy poodle, miniature poodle, etc., the\noutput predictive distribution of a higher capability teacher\nalways provides the student model with the extra informa-\ntion of how alike one breed of dogs looks to the other. This\nhelps the student learn more generalized features of each\n2\n\n--------------------------------------------------\n4%\n0%\n8%\n12%\nSe",
  "output": "The text discusses a classification task that differentiates various breeds of dogs. A teacher with higher capability provides the student model with additional information about how similar one dog breed looks to another, aiding the student in learning more generalized features of each breed.",
  "summary_type": "reports"
}{
  "input": "local self-attention because it improves accuracy without\nsacriﬁcing performance.\nAnother difference with SASA is our implementation\nof downsampling. We replace attention followed by post-\nattention strided average pooling by a single strided attention\nlayer that subsamples queries similar to strided convolutions,\nas shown in Figure 2. Note that we use the same neighbor-\nhood as is extracted in the stride 1 case (Figure 1). This\nchange does not impact accuracy while also reducing the\nFLOPs 4× in",
  "output": "The text discusses the use of local self-attention for improving accuracy without compromising performance. It also mentions a different implementation of downsampling in SASA, where attention followed by post-attention strided average pooling is replaced with a single strided attention layer that subsamples queries similar to strided convolutions. This modification doesn't affect accuracy but reduces the FLOPs 4×.",
  "summary_type": "reports"
}{
  "input": "nsformer (ViT) in performance and efﬁciency by intro-\nducing convolutions into ViT to yield the best of both de-\nsigns. This is accomplished through two primary modiﬁca-\ntions: a hierarchy of Transformers containing a new convo-\nlutional token embedding, and a convolutional Transformer\nblock leveraging a convolutional projection. These changes\nintroduce desirable properties of convolutional neural net-\nworks (CNNs) to the ViT architecture (i.e. shift, scale,\nand distortion invariance) while main",
  "output": "The text discusses the improvement of Vision Transformer (ViT) by introducing convolutions into it. This is achieved through two main modifications: a hierarchy of Transformers with a new convolutional token embedding, and a convolutional Transformer block using a convolutional projection. These modifications incorporate desirable properties of convolutional neural networks (CNNs) into the ViT architecture, such as shift, scale, and distortion invariance.",
  "summary_type": "reports"
}{
  "input": "ottleneck\nlayers have higher GPU usage than BasicBlock layers, but\nusually give better accuracy. However, BasicBlock layers\nhave larger receptive ﬁeld, so they might be more suited to\nbe placed at the early stages of a network.\nWe found that the uniform block selection of ResNet\nmodels is not optimal, and a better speed-accuracy trade-off\ncan be obtained using a novel design, which uses a mixture\nof BasicBlock and Bottleneck layers. Since BasicBlock lay-\ners have a larger receptive ﬁeld, we plac",
  "output": "Bottleneck layers have higher GPU usage and usually provide better accuracy than BasicBlock layers, which have a larger receptive field and may be more suitable for early network stages. The uniform block selection of ResNet models is not optimal. A better speed-accuracy trade-off can be achieved with a new design that combines BasicBlock and Bottleneck layers.",
  "summary_type": "reports"
}{
  "input": "exclusive grouping.\nquest for explanations an ill-posed endeavor. The ﬁeld of\nadversarial perturbations is a good example of this seman-\ntic gap for explanability [24]. Despite having input sam-\nples that preserve all the perceptually relevant information,\nadversarially perturbed samples can be misclassiﬁed with\nhigh probability. The effectiveness of adversarial attacks\nprovides a strong body of evidence that patterns extracted\nby neural networks are effective but fundamentally differ-\nent from",
  "output": "The field of adversarial perturbations exemplifies the semantic gap for explainability. Despite having input samples that maintain all perceptually relevant information, adversarially perturbed samples can often be misclassified. The success of adversarial attacks indicates that the patterns identified by neural networks are efficient but fundamentally different.",
  "summary_type": "reports"
}{
  "input": "er. Speciﬁcally, the PAWS-NN\nresults in Table 1 are reported by directly applying a soft nearest neighbours classiﬁer to the pre-trained representations. To\ndetermine a class prediction for an image x, we compare its representation, z = fθ(x), to the representations of the available\nlabeled training samples, zS ∈RM×d, and subsequently choose the class label with the highest probability under the similarity\nclassiﬁer; i.e., argmaxk∈[1000] [πd (z, zS)]k. All results are reported using a single cen",
  "output": "The PAWS-NN results in Table 1 are obtained by applying a soft nearest neighbours classifier to pre-trained representations. To predict a class for an image x, its representation is compared to the representations of available labeled training samples. The class label with the highest probability under the similarity classifier is then chosen. All results are reported using a single center.",
  "summary_type": "reports"
}{
  "input": "s the training loss function, while x\nand y denote the input data and the labels, respectively.\nSubsequently, architectures α are searched based on the\nranking of their ratings with these shared network weights.\nWithout loss of generality, we choose the evaluation loss\nfunction Lval as the rating metric; the searching phase can\nbe formulated as: α∗= arg min\n∀α∈A\nLval(W∗, α; x, y). However,\nthe architecture ranking based on the shared weights W∗\ndoes not necessarily represents the correct ranking",
  "output": "The text discusses a process where the training loss function is used with input data (x) and labels (y). Architectures (α) are then searched based on the ranking of their ratings with shared network weights. The evaluation loss function (Lval) is chosen as the rating metric. The searching phase is formulated as: α∗= arg min ∀α∈A Lval(W∗, α; x, y). However, the architecture ranking based on the shared weights (W∗) may not accurately represent the correct ranking.",
  "summary_type": "reports"
}{
  "input": "and original models depends\non the experiment but in essence, a mixture of convolutional\nand pooling layers are used. A more detailed speciﬁcation\nof all networks used in this work can be found in Section B\nof the supplementary material.\nUnder this last perspective, an important meta-parameter\nof the architecture is the point at which the auxiliary clas-\nsiﬁer attaches to the original model. Having a junction in\nearlier layers allows both branches to work with generic,\nlower-level features but l",
  "output": "The text discusses an experiment involving a mixture of convolutional and pooling layers in original models. A detailed specification of all networks used can be found in Section B of the supplementary material. An important meta-parameter of the architecture is the point at which the auxiliary classifier attaches to the original model, with earlier junctions allowing both branches to work with generic, lower-level features.",
  "summary_type": "reports"
}{
  "input": "Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-\nheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,\nJifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\nChen Change Loy, and Dahua Lin.\nMMDetection: Open\nmmlab detection toolbox and benchmark.\narXiv preprint\narXiv:1906.07155, 2019. 7\n[7] MMSegmentation\nContributors.\nMMSegmentation:\nOpenmmlab\nsemantic\nsegmentation\ntoolbox\nand\nbenchmark.\nh",
  "output": "The text mentions a group of individuals who are presumably authors of two works. The first work is titled \"MMDetection: Open mmlab detection toolbox and benchmark,\" published as an arXiv preprint in 2019. The second work is titled \"MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark,\" with the contributors not specified.",
  "summary_type": "reports"
}{
  "input": "magenet-val1k.\n4.5.4\nLonger training schedules\nAs shown in Table 8 , increasing the number of training epochs from 300 to\n400 improves the performance of CaiT-S-36. However, increasing the number\nof training epochs from 400 to 500 does not change performance signiﬁcantly\n(83.44 with 400 epochs 83.42 with 500 epochs). This is consistent with the ob-\nservation of the DeiT [63] paper, which notes a saturation of performance from\n400 epochs for the models trained without distillation.\n5\nVisualizatio",
  "output": "The text discusses the impact of increasing training epochs on the performance of CaiT-S-36. It states that increasing the number of training epochs from 300 to 400 improves the model's performance. However, further increasing the epochs from 400 to 500 does not significantly change the performance. This observation aligns with the DeiT paper, which notes a performance saturation from 400 epochs for models trained without distillation.",
  "summary_type": "reports"
}{
  "input": "than that of other compar-\nison methods under the same FLOPs or latency level, but\nreduces approximate 40% of supernet training cost.\nBy\nsearching on a larger space, we can also obtain new state-\nof-the-art architectures.\n1For example, in a same supernet, MobileNetV2 [28] can achieve\n72.0% Top-1 accuracy on ImageNet dataset while an extreme case of al-\nmost all identity operations only has 24.1% [3].\n2. Related Work\nOne-shot NAS methods mainly aim to train an over-\nparameterized network (a.k.a",
  "output": "The text discusses a method that not only performs better than other comparison methods under the same FLOPs or latency level, but also reduces approximately 40% of supernet training cost. By searching on a larger space, new state-of-the-art architectures can be obtained. For instance, in the same supernet, MobileNetV2 can achieve 72.0% Top-1 accuracy on ImageNet dataset, while an extreme case of almost all identity operations only has 24.1%. The text also mentions one-shot NAS methods which aim to train an over-parameterized network.",
  "summary_type": "reports"
}{
  "input": "at\nDraw a mini-batch M from Dn, and update θ via\nθ ←θ −ηEx∼M\n\u0014\n∇θ\n\u0012\nmax\ni∈[m] L(x′\ni, θ)\n\u0013\u0015\n,\nwhere {x′\ni}m\ni=1 are drawn i.i.d. from P(·|x) for each x in the mini batch M. See Equation 3.\nuntil convergence\n3. Related Methods and Discussion\nMaxUp is closely related to both data augmentation and ad-\nversarial training. It can be viewed as an adversarial vari-\nant of data augmentation, in that it minimizes the worse\ncase loss on the perturbed data, instead of an average loss\nlike typical data augm",
  "output": "The text describes an algorithmic process where a mini-batch M is drawn from Dn, and θ is updated accordingly. The process continues until convergence. The method, MaxUp, is related to data augmentation and adversarial training, and can be seen as an adversarial variant of data augmentation. It minimizes the worst-case loss on the perturbed data, unlike typical data augmentation which minimizes an average loss.",
  "summary_type": "reports"
}{
  "input": "competitive image classiﬁcation perfor-\nmance at large scale. The ViT design adapts Transformer\n*This work is done when Haiping Wu was an intern at Microsoft.\n†Corresponding author\nCvT\nViT\nBiT\n(a)\n78\n80\n82\n84\n86\n88\nImageNet top-1 accuracy (%)\n20M\n32M\n277M\n86M\n307M\n25M\n928M\nCvT\nViT\nBiT\n20\n40\n60\n80\nModel Paramters (M)\n(b)\n80.0\n80.5\n81.0\n81.5\n82.0\n82.5\nImageNet top-1 accuracy (%)\nCvT\nDeiT\nT2T\nPVT\nTNT\nFigure 1: Top-1 Accuracy on ImageNet validation com-\npared to other methods with respect to model p",
  "output": "The text discusses the competitive image classification performance at a large scale of the ViT (Vision Transformer) design. It includes a comparison of top-1 accuracy on ImageNet validation among different models such as CvT, ViT, BiT, DeiT, T2T, PVT, and TNT, with respect to model parameters. The work was done when Haiping Wu was an intern at Microsoft.",
  "summary_type": "reports"
}{
  "input": "r forward propagation. Backward passes proceed in reverse\norder through the same graph. This approach is the core of popular deep learning frameworks [1]\nand is associated with the very concept of “architecture”. In contrast, implicit models do not have\nprescribed computation graphs. They instead posit a speciﬁc criterion that the model must satisfy\n(e.g., the endpoint of an ODE ﬂow, or the root of an equation). Importantly, the algorithm that drives\nthe model to fulﬁll this criterion is not pre",
  "output": "The text discusses two approaches in deep learning models. The first approach, used by popular deep learning frameworks, involves forward and backward propagation through a prescribed computation graph, which is associated with the concept of \"architecture\". The second approach, used by implicit models, does not have a prescribed computation graph. Instead, it posits a specific criterion that the model must satisfy, such as the endpoint of an ODE flow or the root of an equation. The algorithm that drives the model to fulfill this criterion is not predetermined.",
  "summary_type": "reports"
}{
  "input": "hich this parameterization has a\nhigher ﬁtting error eﬁt tend to perform poorly. See text for details.\n3.3. The RegNet Design Space\nTo gain further insight into the model structure, we show\nthe best 20 models from AnyNetXE in a single plot, see Fig-\nure 8 (top-left). For each model, we plot the per-block width\nwj of every block j up to the network depth d (we use i and\nj to index over stages and blocks, respectively). See Fig-\nure 6 for reference of our model visualization.\nWhile there is signiﬁ",
  "output": "The text discusses the RegNet Design Space, where the top 20 models from AnyNetXE are plotted. Each model's per-block width is plotted up to the network depth. The models with higher fitting errors tend to perform poorly. The text uses 'i' and 'j' to index over stages and blocks respectively.",
  "summary_type": "reports"
}{
  "input": "h. The RandAug-\nment magnitude is set to 10 for image resolution 224 or\nsmaller, 20 for image resolution larger than 320 and 15 oth-\nerwise. All other hyperparameters are kept the same as per\nthe original EfﬁcientNets. Figure 5 demonstrates a marked\nimprovement of the re-scaled EfﬁcientNets (EfﬁcientNet-\nRS) on the speed-accuracy Pareto curve over the original\nEfﬁcientNets.\n3Activations are typically stored during training as they are\nused in backpropagation. At inference, activations can be dis",
  "output": "The RandAugment magnitude is set to 10 for image resolutions of 224 or smaller, 20 for resolutions larger than 320, and 15 otherwise. All other hyperparameters remain the same as in the original EfficientNets. The re-scaled EfficientNets (EfficientNet-RS) show a notable improvement on the speed-accuracy Pareto curve over the original EfficientNets. Activations are usually stored during training for use in backpropagation, and can be discarded at inference.",
  "summary_type": "reports"
}{
  "input": "e of the\nconvolutional prior.\n\n--------------------------------------------------\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n0\n100\n200\n300\nEpochs\n3\n4\n5\n6\n7\nNon-locality\nDeiT\n0\n100\n200\n300\nEpochs\n3\n4\n5\n6\n7\nConViT\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\nLayer 9\nLayer 10\nLayer 11\nLayer 12\n(a) DeiT-Ti and ConViT-Ti\n0\n100\n200\n300\nEpochs\n3\n4\n5\n6\n7\nNon-locality\nDeiT\n0\n100\n200\n300\nEpochs\n3\n4\n5\n6\n7\nConViT\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5",
  "output": "The text discusses ConViT, a method for improving Vision Transformers with Soft Convolutional Inductive Biases. It compares the performance of ConViT and DeiT over several epochs and layers.",
  "summary_type": "reports"
}{
  "input": "(3) In detail, the token extraction and the bi-directional pathway\ndesigned in ViP are quite different from their pipelines.\n4\nExperiments\n4.1\nImage Classiﬁcation on ImageNet-1K\nExperimental Settings. For image classiﬁcation, we evaluate our models on ImageNet-1K [16],\nwhich consists of 1.28M training images and 50K validation images categorized into 1,000 classes.\nThe network is trained for 300 epochs using AdamW [39] and a half-cosine annealing learning rate\n6\n\n--------------------------------",
  "output": "The text discusses the differences in token extraction and bi-directional pathway design in ViP compared to other pipelines. It also mentions an experiment on image classification using ImageNet-1K, which includes 1.28M training images and 50K validation images divided into 1,000 classes. The network is trained for 300 epochs using AdamW and a half-cosine annealing learning rate.",
  "summary_type": "reports"
}{
  "input": "ation bound can be written as follows:\nEϵi∼N(0,σ)[LD(w+ϵ)] ≤Eϵi∼N(0,σ)[LS(w+ϵ)]+\nv\nu\nu\nt\n1\n4k log\n\u0010\n1 + ∥w∥2\n2\nkσ2\n\u0011\n+ 1\n4 + log n\nδ + 2 log (6n + 3k)\nn −1\n(13)\nIn the above bound, we have ϵi ∼N(0, σ). Therefore, ∥ϵ∥2\n2 has chi-square distribution and by\nLemma 1 in Laurent & Massart (2000), we have that for any positive t:\nP(∥ϵ∥2\n2 −kσ2 ≥2σ2√\nkt + 2tσ2) ≤exp(−t)\n(14)\nTherefore, with probability 1 −1/√n we have that:\n∥ϵ∥2\n2 ≤σ2(2 ln(√n) + k + 2\nq\nk ln(√n)) ≤σ2k\n \n1 +\nr\nln(n)\nk\n!2\n≤ρ2\n15\n\n--------",
  "output": "The text presents a mathematical expression for an 'ation bound'. It states that ϵi follows a normal distribution with mean 0 and standard deviation σ. The square of the norm of ϵ follows a chi-square distribution. According to Lemma 1 in Laurent & Massart (2000), a probability is defined for any positive t. With a probability of 1 - 1/√n, the square of the norm of ϵ is less than or equal to a given expression.",
  "summary_type": "reports"
}{
  "input": ", with input size 224. Tiny-4stage / 4 means that the model has\ncomparable size with DeiT-Tiny, has 4 stages and uses patch size 4x4 in the initial pixel space. 1-2-8-1 means that the model contains 4\nstages, each stage has 1/2/8/1 MSA-FFN blocks, respectively. *Partial* means that the last two stages, which contain most of the attention\nblocks, still use full attention. Vision Longformer does not have *Partial* version because its window size is set as 15 (comparable with\nthe ViT(DeiT)/16 featu",
  "output": "The text discusses a model comparable in size to DeiT-Tiny, with 4 stages and a 4x4 patch size in the initial pixel space. The model contains 4 stages, each with 1/2/8/1 MSA-FFN blocks respectively. The term \"Partial\" refers to the last two stages using full attention. Vision Longformer doesn't have a \"Partial\" version due to its window size being set at 15, comparable with the ViT(DeiT)/16 feature.",
  "summary_type": "reports"
}{
  "input": "nLearner with Video Vision Transformer\n5.1\nNetwork architecture implementation\nViViT [2] is a direct extension of ViT for videos, which uses spatio-temporal patches from videos\nas its tokens. The size of the space-time patches are typically 16x16x2, which are given to the\nTransformer layers similar to ViT. ViViT and ViT share the architecture. For our experiments, we\ninsert the TokenLearner module within the ViViT architecture, identically to how we inserted it\nwithin ViT in Figure 3. ViViT has",
  "output": "ViViT is an extension of ViT for videos, using spatio-temporal patches from videos as tokens. The size of these patches is typically 16x16x2, and they are given to the Transformer layers in a similar way to ViT. The architecture of ViViT and ViT are shared. In the experiments, the TokenLearner module is inserted within the ViViT architecture, in the same way it was inserted within ViT.",
  "summary_type": "reports"
}{
  "input": "where h = ⌊k\n2⌋\n4(b + 2h)2c\nBlocked local (ours)\nHW\nb2 (b + 2h)2c\n(b + 2h) × (b + 2h)\n4(b + 2h)2c\nTable 1. Scaling behavior of self-attention mechanisms. f is the\nnumber of heads, b is the size of the block, c is the total number of\nchannels, and h is the size of the halo\na factor of k2 due to replicating the pixel contents for each of\nthe k2 neighborhoods it participates in. This solution quickly\nleads to out-of-memory errors. Global attention (Row 4) is\nat the other end of the spectrum, where",
  "output": "The text discusses the scaling behavior of self-attention mechanisms, where f represents the number of heads, b is the block size, c is the total number of channels, and h is the halo size. It mentions a factor of k2 due to pixel content replication for each of the k2 neighborhoods. The text also refers to issues with out-of-memory errors and mentions global attention.",
  "summary_type": "reports"
}{
  "input": "led dot-product attention is a key component in Multi-Head Self\nAttention layer (MHSA) of transformer. MHSA ﬁrst generates set of query Q ∈RN×d, key\nK ∈RN×d, value V ∈RN×d with the corresponding projection. Then the query vector q ∈Rd is\nmatched against the each key vector in K. The output is the weighted sum of a set of N value vectors\nv based on the matching score. This process is called scaled dot-product attention:\nAttention(Q, K, V ) = Softmax(QKT /\n√\nd)V\n(1)\nFor preventing extremly small g",
  "output": "The text discusses the role of led dot-product attention in the Multi-Head Self Attention layer (MHSA) of a transformer. MHSA first generates a set of query Q, key K, and value V with the corresponding projection. The query vector q is then matched against each key vector in K. The output is the weighted sum of a set of N value vectors v, based on the matching score. This process is referred to as scaled dot-product attention, represented by the formula: Attention(Q, K, V ) = Softmax(QKT / √d)V.",
  "summary_type": "reports"
}{
  "input": "ut. Vis., 60(2):91–110, 2004.\n3\n[34] Timo Ojala, Matti Pietik¨ainen, and Topi M¨aenp¨a¨a. Multires-\nolution gray-scale and rotation invariant texture classiﬁcation\nwith local binary patterns. IEEE Trans. Pattern Anal. Mach.\nIntell., 24(7):971–987, 2002. 3\n[35] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll´ar. Designing network design\nspaces. arXiv preprint arXiv:2003.13678, 2020. 6, 11\n[36] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networ",
  "output": "The text mentions several academic references. One is a study by Timo Ojala, Matti Pietikäinen, and Topi Mäenpää on multiresolution gray-scale and rotation invariant texture classification using local binary patterns, published in IEEE Trans. Pattern Anal. Mach. Intell. in 2002. Another is a preprint on arXiv by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár about designing network design spaces, published in 2020. The text also refers to work by Karen Simonyan and Andrew Zisserman on deep convolutional networks.",
  "summary_type": "reports"
}{
  "input": "w the im-\nage feature is obtained for the linear classiﬁcation head.\nModel\nTiny\nSmall\nCLS\nAve Pool\nCLS\nAve Pool\nViL-1,2,8,1-APE\n75.72\n75.98\n81.65\n81.99\nViL-1,1,8,2-APE\n76.18\n76.25\n82.12\n82.08\nTable 12. For ViL models that has only one attention block in the\nlast stage (ViL 1-2-8-1), the average pooled feature from all tokens\nworks better than the feature of the CLS token. When there are\nmore than 2 attention blocks in the last stage (ViL 1-1-8-2), the\ndifference between these two features disapp",
  "output": "The text discusses the performance of ViL models with different configurations. For ViL models with one attention block in the last stage (ViL 1-2-8-1), the average pooled feature from all tokens performs better than the feature of the CLS token. However, when there are more than two attention blocks in the last stage (ViL 1-1-8-2), the difference between these two features is not specified.",
  "summary_type": "reports"
}{
  "input": "input size): a fully-ISAB model scales as\nO(LMN), rather than O(MN + LN 2), like the Perceiver\n(where M is the index dimension of the input, N the index\ndimension of the latent, and L the network depth).\nPMA is used to map an input array to an output array with a\nsized determined by the task (e.g. 1 point for classiﬁcation\nor 4 points for a 4-way clustering task). It is used to map\nto a target output size and not to induce a latent space. In\ncontrast, the Perceiver’s latent space has a size tha",
  "output": "A fully-ISAB model scales as O(LMN), unlike the Perceiver which scales as O(MN + LN 2), where M is the input's index dimension, N is the latent's index dimension, and L is the network depth. PMA maps an input array to an output array with a size determined by the task, such as 1 point for classification or 4 points for a 4-way clustering task. It maps to a target output size, not to induce a latent space, contrasting with the Perceiver's latent space.",
  "summary_type": "reports"
}{
  "input": "cture on object detection and the exact numbers of the results.\nAs our proposed PyConv uses different levels of kernel sizes in parallel, it can provide signiﬁcant\nbeneﬁts for object detection task, where the objects can appear in the image at different scales. For\nobject detection, we integrate our PyConv in a powerful approach, Single Shot Detector (SSD) [48].\nSSD is a very efﬁcient single stage framework for object detection, which performs the detection at\nmultiple feature maps resolutions.",
  "output": "The proposed PyConv, which uses various kernel sizes simultaneously, offers significant advantages for object detection tasks due to its ability to detect objects at different scales. PyConv is integrated into the Single Shot Detector (SSD), an efficient single-stage framework for object detection that operates at multiple feature map resolutions.",
  "summary_type": "reports"
}{
  "input": "AS, when equipped with the stopping principle of candidate pool, the supernet training is stopped at\napproximately 46-th epoch. Thus the accumulated number of examples calculated for a whole optimization step is equal to\n#optimization=1.23M × 46,\nwhere 1.23M refers to the quantity of training dataset. As for the path ﬁltering, we evaluate 10 paths based on 1000 validation\nimages, and select 5 paths for training, whose batch size is 1024. In this way, the number of images for evaluation amounts\nt",
  "output": "The supernet training, equipped with the stopping principle of candidate pool, is halted around the 46th epoch. The total number of examples calculated for an entire optimization step equals 1.23M (the size of the training dataset) multiplied by 46. For path filtering, 10 paths are evaluated using 1000 validation images, and 5 paths are chosen for training with a batch size of 1024. This results in a certain number of images for evaluation.",
  "summary_type": "reports"
}{
  "input": "ow, the correct design of block aggregation is critical.\nNesT improves consistently given bigger model size. Most variants of NesT in Figure 6 outperform\ncompared methods with far better throughput. For example, NesT3-T (S = 2) leads to 94.5%\nCIFAR10 accuracy with 5384 images/s throughout, 10× faster than Swin-B with 94.6% accuracy.\nImageNet We test NesT on standard ImageNet 2012 benchmarks [16] with commonly used 300\nepoch training on TPU in Table 2. The input size is 224 × 224 and no extra pre",
  "output": "The design of block aggregation is crucial. NesT consistently improves with a larger model size and outperforms other methods in throughput. For instance, NesT3-T (S = 2) achieves 94.5% CIFAR10 accuracy with a throughput of 5384 images/s, which is 10 times faster than Swin-B with 94.6% accuracy. NesT is also tested on standard ImageNet 2012 benchmarks with commonly used 300 epoch training on TPU. The input size is 224 × 224.",
  "summary_type": "reports"
}{
  "input": "re as in ResNet. We also investigate the ben-\neﬁts of PiT compared to ViT and conﬁrm that ResNet-style\ndimension setting also improves the performance of ViT.\nFinally, to analyze the effect of PiT compared to ViT, we\narXiv:2103.16302v2  [cs.CV]  18 Aug 2021\n\n--------------------------------------------------\n(a) ResNet-50\n56×56×256\n224×224\n×3\n28×28×512\n14×14×1024\n7×7×2048\n(b) ViT-S/16\n(14×14)×384\nSpatial tokens\n1×384\nClass token\n224×224\n×3\n(14×14)×288\nSpatial tokens\n1×144\nClass token\n224×224\n×3",
  "output": "The text discusses the benefits of PiT (Perceiver IO Transformer) compared to ViT (Vision Transformer) and confirms that ResNet-style dimension setting improves the performance of ViT. It also provides some specifications for ResNet-50 and ViT-S/16, including spatial tokens and class tokens.",
  "summary_type": "reports"
}{
  "input": "Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting the design of spatial attention in vision\ntransformers. arXiv preprint arXiv:2104.13840, 2021. 1, 2,\n3, 4\n[5] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-\naolin Wei, Huaxia Xia, and Chunhua Shen. Conditional po-\nsitional encodings for vision transformers.\narXiv preprint\narXiv:2102.10882, 2021. 2\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei.\nImagenet: A large-scale hierarchical im-\nage database. In Proc. IEEE C",
  "output": "Huaxia Xia and Chunhua Shen revisited the design of spatial attention in vision transformers in their 2021 preprint on arXiv. Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen also worked on conditional positional encodings for vision transformers in another 2021 arXiv preprint. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei have contributed to the creation of ImageNet, a large-scale hierarchical image database.",
  "summary_type": "reports"
}{
  "input": "he\nframework (e.g., by forking the repository) to conduct their knowledge distillation stud-\nies, and their studies can be integrated to the framework by sending a pull request.\nThis will help the research community ensure the reproducibility of the work, and ad-\nvance the deep learning research while supporting fair method comparison on bench-\nmarks. Speciﬁcally, researchers can publish the log, conﬁguration, and pretrained model\nweights for their champion performance, that will help them ensur",
  "output": "The text discusses a framework that allows researchers to conduct knowledge distillation studies. These studies can be integrated into the framework via a pull request, aiding in reproducibility and advancing deep learning research. It also supports fair method comparison on benchmarks. Researchers can publish the log, configuration, and pretrained model weights for their top performance.",
  "summary_type": "reports"
}{
  "input": "normalized projection net-\nwork representations provide an intrinsic mechanism for hard positive/negative mining during train-\ning. For losses such as the triplet loss or max-margin, hard mining is known to be crucial to their\nperformance. For contrastive loss, we show analytically that hard mining is intrinsic and thus re-\nmoves the need for complicated hard mining algorithms.\nAs shown in Sec. 6, the gradients of both Lsup\nout and Lsup\nin\nare given by Eq. 12. Additionally, note\nthat the self-su",
  "output": "Normalized projection network representations inherently facilitate hard positive/negative mining during training. This is essential for performance with losses like the triplet loss or max-margin. For contrastive loss, hard mining is intrinsically included, eliminating the need for complex hard mining algorithms. The gradients of both Lsup out and Lsup in are provided by Equation 12.",
  "summary_type": "reports"
}{
  "input": "alization [68] and im-\nage scale augmentation (short size of a image is picked ran-\ndomly from 640 to 800). 1x learning rate schedule is used.\nWe conduct Faster-RCNNs and Cascade-RCNNs experi-\nments using Detectron2 [60]. For comparison, we simply\nreplaced the vanilla ResNet backbones with our ResNeSt,\nwhile using the default settings for the hyper-parameters\nand detection heads [20,60].\nCompared to the baselines using standard ResNet, Our\nbackbone is able to boost mean average precision by arou",
  "output": "The text discusses the use of Faster-RCNNs and Cascade-RCNNs experiments with Detectron2. The standard ResNet backbones were replaced with ResNeSt, maintaining the default settings for hyper-parameters and detection heads. This replacement resulted in an increase in mean average precision compared to the standard ResNet baselines.",
  "summary_type": "reports"
}{
  "input": "lasses\nCIFAR-10 (Krizhevsky & Hinton, 2009)\n50,000\n10,000\n10\nCIFAR-100 (Krizhevsky & Hinton, 2009)\n50,000\n10,000\n100\nFlowers (Nilsback & Zisserman, 2008)\n2,040\n6,149\n102\nCars (Krause et al., 2013)\n8,144\n8,041\n196\nFor this experiment, we use the checkpoints trained on Ima-\ngeNet ILSVRC2012. For fair comparison, no ImageNet21k\nimages are used here. Our ﬁnetuning settings are mostly the\nsame as ImageNet training with a few modiﬁcations similar\nto (Dosovitskiy et al., 2021; Touvron et al., 2021): We",
  "output": "The text discusses an experiment using checkpoints trained on ImageNet ILSVRC2012, with no ImageNet21k images used for a fair comparison. The finetuning settings are mostly the same as ImageNet training with a few modifications. It also mentions four datasets: CIFAR-10 with 50,000 training images and 10,000 test images across 10 classes, CIFAR-100 with the same number of images but 100 classes, Flowers with 2,040 training images and 6,149 test images across 102 classes, and Cars with 8,144 training images and 8,041 test images across 196 classes.",
  "summary_type": "reports"
}{
  "input": "odels obtaining an accuracy of 89.2%.2\n1. Introduction\nThe vast majority of recent models in computer vision are\nvariants of deep residual networks (He et al., 2016b;a),\ntrained with batch normalization (Ioffe & Szegedy, 2015).\nThe combination of these two architectural innovations has\nenabled practitioners to train signiﬁcantly deeper networks\nwhich can achieve higher accuracies on both the training\nset and the test set. Batch normalization also smoothens the\nloss landscape (Santurkar et al., 2",
  "output": "The text discusses the prevalence of deep residual networks in computer vision models, often trained with batch normalization. These two architectural innovations have allowed for the training of significantly deeper networks, resulting in higher accuracies on both training and test sets. Batch normalization also smoothens the loss landscape.",
  "summary_type": "reports"
}{
  "input": "y. Journal of Machine Learning Research,\n2019.\nGupta, S. and Akin, B. Accelerator-aware neural network\ndesign using automl. On-device Intelligence Workshop in\nSysML, 2020.\nGupta, S. and Tan, M.\nEfﬁcientnet-edgetpu:\nCre-\nating accelerator-optimized neural networks with au-\ntoml.\nhttps://ai.googleblog.com/2019/08/efﬁcientnet-\nedgetpu-creating.html, 2019.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition.\nCVPR, pp. 770–778,\n2016.\nHoffer, E., Weinstein, B., Hubara,",
  "output": "The text mentions various research papers and articles related to machine learning and neural network design. Gupta and Akin discussed accelerator-aware neural network design using automl in a 2020 SysML workshop. Gupta and Tan created accelerator-optimized neural networks with automl, as detailed in a 2019 Google AI blog post. He, Zhang, Ren, and Sun explored deep residual learning for image recognition in a 2016 CVPR paper. The text also mentions Hoffer, Weinstein, and Hubara, but doesn't provide details about their work.",
  "summary_type": "reports"
}{
  "input": "ve the ﬁxed-size position encoding [7], and introduce\nzero padding position encoding into PVT. As shown in Fig-\nure 1(b), we add a 3 × 3 depth-wise convolution [15] with\nthe padding size of 1 between the ﬁrst fully-connected (FC)\nlayer and GELU [14] in feed-forward networks.\nLinear Spatial Reduction Attention. To further reduce\nthe computation cost of PVT, we propose linear spatial re-\nduction attention (SRA) as illustrated in Figure 2. Different\nfrom SRA [31], linear SRA enjoys linear computati",
  "output": "The text discusses the introduction of zero padding position encoding into PVT, involving a 3x3 depth-wise convolution with a padding size of 1 between the first fully-connected layer and GELU in feed-forward networks. It also mentions the proposal of linear spatial reduction attention (SRA) to reduce the computation cost of PVT, which is different from SRA and offers linear computation.",
  "summary_type": "reports"
}{
  "input": "arXiv:2102.05918v2  [cs.CV]  11 Jun 2021\n\n--------------------------------------------------\nScaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision\nText \nEncoder\nImage \nEncoder\nNoisy Image-Text \nData\nContrastive Learning\n(Zero-shot) Visual Tasks\n   Fine-grained Image-Text Retrieval\nPre-training\n“Roppongi Hills Spider at night”\n“original picture of \nmonet haystack”\n“monet haystack png”\n“haystack series \nmonet art institute of \nchicago”\n...\n“snow”\n(A) Text -> Im",
  "output": "The text discusses a method of scaling up visual and vision-language representation learning using noisy text supervision. This involves the use of a text encoder, an image encoder, and noisy image-text data. The process includes contrastive learning and pre-training for fine-grained image-text retrieval. Examples of the process include images labeled as \"Roppongi Hills Spider at night\", \"original picture of monet haystack\", \"monet haystack png\", \"haystack series monet art institute of chicago\", and \"snow\".",
  "summary_type": "reports"
}{
  "input": "al learning rate from 0.1 to 0.01.\nThe second option achieved better overall results and that is the one we report. We also tried training with LSQ from a randomly\ninitialized model, but that performed the worst of all approaches.\n2https://sigsep.github.io/datasets/musdb.html\n3https://github.com/sigsep/website/blob/master/content/datasets/assets/tracklist.csv\n4https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/\n5https://github.com/pytorch/fairseq/tree/master/exa",
  "output": "The text discusses the results of different training approaches for a model. Lowering the learning rate from 0.1 to 0.01 yielded the best results, which are reported. Training with LSQ from a randomly initialized model performed the worst. The text also includes several web links related to the topic.",
  "summary_type": "reports"
}{
  "input": "spondingly, the tokens associated\nwith image and feature patches are referred to as lo-\ncal tokens afterwards.\n3. We replace the vanilla full self-attention with an efﬁ-\ncient attention mechanism, denoted by a, which will\nbe described in detail in Sections 3.2 and 3.3.\n4. We use either an Absolute 2-D Positional Embedding\n(APE for short, separately encoding x and y coordi-\nnates and concatenating them) or a Relative Positional\nBias (RPB for short) to replace the original absolute\n1-D positional",
  "output": "The text discusses changes made to a system. Tokens related to image and feature patches are now called local tokens. The vanilla full self-attention is replaced with an efficient attention mechanism, which will be explained further in Sections 3.2 and 3.3. The system now uses either an Absolute 2-D Positional Embedding (APE), which separately encodes x and y coordinates and concatenates them, or a Relative Positional Bias (RPB) to replace the original absolute 1-D positional.",
  "summary_type": "reports"
}{
  "input": "interactions between the pruned tokens and other\ntokens. We also ﬁnd that merely zero-out the tokens to be dropped using the binary mask ˆD is not\nfeasible, because in the calculation of self-attention matrix [26]\nA = Softmax\n\u0012QKT\n√\nC\n\u0013\n(8)\nthe zeroed tokens will still inﬂuence other tokens through the Softmax operation. To this end, we\ndevise a strategy called attention masking which can totally eliminate the effects of the dropped\ntokens. Speciﬁcally, we compute the attention matrix by:\nP = QK",
  "output": "The text discusses the issue of pruned tokens influencing other tokens in the calculation of a self-attention matrix, even when zeroed out using a binary mask. To address this, an attention masking strategy is proposed, which can completely remove the effects of the dropped tokens. The attention matrix is then computed by P = QK.",
  "summary_type": "reports"
}{
  "input": "that none of the models\nconverge with shorter training schedules.\nMultilabel datasets.\nWe can also apply our method to\nmultilabel datasets which may have more than one class in\neach image. The same latent variable formulation can be\nused but with temperature parameterized sigmoid smoothing\nfunction (see Appendix A). Imagenet-21k and JFT are two\nlarge-scale multilabel image classiﬁcation datasets.\nImagenet-21k is a larger version of the standard ILSVRC-\n2012 Imagenet benchmark [10, 26, 2, 9]. It",
  "output": "The text discusses the application of a certain method to multilabel datasets, which can have more than one class in each image. This method uses a latent variable formulation with a temperature parameterized sigmoid smoothing function. The text also mentions two large-scale multilabel image classification datasets, Imagenet-21k and JFT. Imagenet-21k is a larger version of the standard ILSVRC-2012 Imagenet benchmark. The text also notes that none of the models converge with shorter training schedules.",
  "summary_type": "reports"
}{
  "input": "eir detailed conﬁgurations are shown in Tab. 8. We\ntrain all models with 8 NVIDIA Telsa-V100 GPUs.\nTable 8: Default training hyper-parameters for our experiments.\nH-param.\nStandard\nAdvanced\nEpoch\n300\n300\nBatch size\n256\n512\nLR\n5e-3 · batch_size\n256\n5e-3· batch_size\n512\nLR decay\ncosine\ncosine\nWeight decay\n0.05\n0.05\nWarmup epochs\n5\n5\nDropout\n0\n0\nStoch. Depth\n0.1\n0.1 · #Blocks\n12\nMixUp\n0.2\n0.8\nCutMix\n0\n1.0\nErasing prob.\n0.25\n0.25\nRandAug\n9/0.5\n9/0.5\nA.2\nFine-tuning with larger image resolutions\nOn I",
  "output": "The text describes the training of models with 8 NVIDIA Telsa-V100 GPUs, using the hyper-parameters listed in Table 8. Two configurations, Standard and Advanced, are used for the experiments. Both configurations use 300 epochs, with batch sizes of 256 and 512 respectively. The learning rate (LR) is 5e-3 times the batch size for both configurations. Other parameters such as LR decay, weight decay, warmup epochs, dropout, stochastic depth, MixUp, CutMix, erasing probability, and RandAug are also specified for each configuration.",
  "summary_type": "reports"
}{
  "input": "ear activation functions could be used. In the\nfollowing experiments, we use the combination of h-swish\nand SE as the non-linear activation function after depth-\nwise convolution. Additionally, the reduction ratio of the\nsqueeze-and-excitation module is chosen such that only 4\nchannels are kept after the squeeze operation. This choice\nof design achieves a good balance between the number of\nparameters and the model accuracy. Thus, local informa-\ntion is also important in vision transformers. A wi",
  "output": "The text discusses the use of h-swish and SE as non-linear activation functions after depth-wise convolution in certain experiments. The squeeze-and-excitation module's reduction ratio is chosen to retain only 4 channels after the squeeze operation, achieving a balance between the number of parameters and model accuracy. The text also emphasizes the importance of local information in vision transformers.",
  "summary_type": "reports"
}{
  "input": "Drop\nCutout\n1.6\n1.9\n11.3\n11.8\nPyramidNet+ShakeDrop\nAA\n1.4\n1.6\n10.3\n10.6\nTable 9:\nResults for the CIFAR-10/CIFAR-100 experiments, using ρ = 0.05 for all mod-\nels/datasets/augmentations\nboth types of updates becomes weaker. Fortunately, the model trained without the second order\nterms reaches a lower test error, showing that the most efﬁcient method is also the one providing the\nbest generalization on this example. The reason for this is quite unclear and should be analyzed in\nfollow up work.\nC.5",
  "output": "The text presents results for CIFAR-10/CIFAR-100 experiments using ρ = 0.05 for all models/datasets/augmentations. The model trained without the second order terms achieves a lower test error, indicating that the most efficient method also provides the best generalization. The reason for this is unclear and needs further analysis.",
  "summary_type": "reports"
}{
  "input": "To verify whether the initializa-\ntion of a student has a big impact, we conduct the ablation\nstudy through adopting (1) randomly initialized weights, (2)\npytorch moderate weights and (3) timm superior weights.\nResults are illustrated in Fig. 6, we trained with additional\n120 epochs with lr = 0.1 if the parameters are randomly\ninitialized.\nIntriguingly, the convergence with randomly\ninitialized weights is not as good as using pre-trained pa-\nrameters, especially the second and third stages when",
  "output": "An ablation study was conducted to assess the impact of a student's initialization, using (1) randomly initialized weights, (2) pytorch moderate weights, and (3) timm superior weights. The results, shown in Fig. 6, indicate that the convergence with randomly initialized weights is not as effective as using pre-trained parameters, particularly during the second and third stages. An additional 120 epochs were trained with a learning rate of 0.1 if the parameters were randomly initialized.",
  "summary_type": "reports"
}{
  "input": "odel [25] using the visualization method proposed in [3]. These results demonstrate the ﬁnal\nprediction in vision transformers is only based on a subset of most informative tokens, which suggests\na large proportion of tokens can be removed without hurting the performance.\ndownsampling strategy for vision transformers to further leverage the advantages of self-attention\n(our experiments also show unstructured sparsiﬁcation can lead to better performance for vision\ntransformers compared to structu",
  "output": "The text discusses a study using a model and visualization method to show that the final prediction in vision transformers relies only on a subset of most informative tokens. It suggests that many tokens can be removed without affecting performance. The text also mentions a downsampling strategy for vision transformers to enhance the benefits of self-attention. The experiments indicate that unstructured sparsification can improve performance for vision transformers compared to structured methods.",
  "summary_type": "reports"
}{
  "input": "on, Ulrike von Luxburg, Samy\nBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in\nNeural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008, 2017.\n[28] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convoluti",
  "output": "The text refers to a conference, the Annual Conference on Neural Information Processing Systems 2017, held in Long Beach, CA, USA from December 4-9, 2017. The conference was edited by Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett. It also mentions a work titled \"Pyramid vision transformer: A versatile backbone for dense prediction without convolution\" by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.",
  "summary_type": "reports"
}{
  "input": "residual learning for image recognition. In CVPR, 2016.\n1\n[23] Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang.\nLearning deep representation for imbalanced classiﬁcation.\nIn CVPR, 2016. 2\n[24] Chen Huang, Yining Li, Change Loy Chen, and Xiaoou Tang.\nDeep imbalanced learning for face recognition and attribute\nprediction. IEEE TPAMI, 2019. 2\n[25] Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang.\nLearning deep representation for imbalanced classiﬁcation.\nIn CVPR, 2016. 1, 5, 14\n[26]",
  "output": "The text discusses various works by Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. They have conducted research on residual learning for image recognition, deep representation learning for imbalanced classification, and deep imbalanced learning for face recognition and attribute prediction. These studies were published in CVPR 2016 and IEEE TPAMI 2019.",
  "summary_type": "reports"
}{
  "input": "with\nVQ-VAE-2. In Advances in Neural Information Processing Systems, volume 32. Curran Associates,\nInc., 2019.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet\nlarge scale visual recognition challenge. IJCV, 2015.\n13\n\n--------------------------------------------------\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words",
  "output": "The text mentions a study called VQ-VAE-2 published in Advances in Neural Information Processing Systems. It also refers to the ImageNet large scale visual recognition challenge by Olga Russakovsky and others. Lastly, it mentions work by Rico Sennrich, Barry Haddow, and Alexandra Birch on neural machine translation of rare words.",
  "summary_type": "reports"
}{
  "input": "sual and vision-language representation learning with noisy text supervision. arXiv\npreprint arXiv:2102.05918, 2021.\n[22] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big transfer\n(BiT): General visual representation learning. In ECCV, 2020.\n[23] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report,\nUniversity of Toronto, 2009.\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutiona",
  "output": "The text discusses various studies and reports on visual and vision-language representation learning. It mentions a preprint on arXiv about learning with noisy text supervision. It also references a study called \"Big Transfer (BiT): General visual representation learning\" presented at ECCV 2020. Additionally, it cites a technical report from the University of Toronto about learning multiple layers of features from tiny images, and a study on ImageNet classification with deep convolution.",
  "summary_type": "reports"
}{
  "input": "ctor of |h|. The number of heads |h| controls the size\nof the lambdas λn ∈R|k|×|d|/|h| relative to the total size of the queries qn ∈R|hk|.\n5Attention maps typically need to be stored for back-propagation (Kitaev et al., 2020).\n6\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\ndef lambda layer(queries, keys, embeddings, values):\n\"\"\"Multi−query lambda layer.\"\"\"\n# b: batch, n: input length, m: context length,\n# k: query/key depth, v: value depth,\n#",
  "output": "The text discusses a multi-query lambda layer in a context of a conference paper published at ICLR 2021. It mentions the role of the number of heads |h| in controlling the size of the lambdas λn relative to the total size of the queries qn. It also notes that attention maps typically need to be stored for back-propagation.",
  "summary_type": "reports"
}{
  "input": "ich consists of the design choices for\nconvolutional operation types {MBConv, Fused-MBConv},\nnumber of layers, kernel size {3x3, 5x5}, expansion ratio\n{1, 4, 6}. On the other hand, we reduce the search space size\nby (1) removing unnecessary search options such as pooling\nskip ops, since they are never used in the original Efﬁcient-\nNets; (2) reusing the same channel sizes from the backbone\nas they are already searched in (Tan & Le, 2019a). Since the\nsearch space is smaller, we can apply reinforc",
  "output": "The text discusses design choices for convolutional operations, including types (MBConv, Fused-MBConv), number of layers, kernel size (3x3, 5x5), and expansion ratio (1, 4, 6). The search space size is reduced by removing unnecessary options like pooling skip ops, which aren't used in the original Efficient-Nets, and reusing the same channel sizes from the backbone, as they have already been searched in a previous study. This smaller search space allows for the application of reinforcement.",
  "summary_type": "reports"
}{
  "input": "learning\nperformance on object detection.\n4.2.2\nSAME DOMAIN\nWe also observe consistent performance improvements in the same domain setups.\nImageNet. In Table 9, we show results by using Anisotropic ImageNet for supervised classiﬁcation.\nWe observe that Anisotropic ImageNet improves performance in both ImageNet classiﬁcation and\nobject detection. For Gaussian blurring experiments, we closely follow Chen et al. (2020b) and\nadd a Gaussian blur operator with random radius from 10 to 20 and train in",
  "output": "The text discusses performance improvements in object detection and ImageNet classification using Anisotropic ImageNet. It also mentions the use of a Gaussian blur operator with a random radius from 10 to 20 in Gaussian blurring experiments, following the methods of Chen et al. (2020b).",
  "summary_type": "reports"
}{
  "input": "ets is provided in Section E.\n\n--------------------------------------------------\n11\nFig. 11: Top row: NATNets obtained from tri-objective search to maximize ImageNet top-1 accuracy, minimize model size (#Params), and minimize {#MAdds,\nCPU latency, GPU latency} from left to right. Pareto surfaces emerge at higher model complexity regime (i.e. top right corner) suggesting that trade-offs exist\nbetween model size (#params) and model efﬁciency (#MAdds and latency). Bottom row: 2D projections from a",
  "output": "The text discusses NATNets obtained from a tri-objective search to maximize ImageNet top-1 accuracy, minimize model size (#Params), and minimize other factors such as #MAdds, CPU latency, and GPU latency. It suggests that trade-offs exist between model size and model efficiency. The text also mentions 2D projections from a certain context, but the information is incomplete.",
  "summary_type": "reports"
}{
  "input": "Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16\nwords: Transformers for image recognition at scale. In ICLR, 2021.\n[21] D. Eigen, M. Ranzato, and I. Sutskever. Learning factored representations in a deep mixture of\nexperts. arXiv preprint arXiv:1312.4314, 2013.\n[22] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models\nwith simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021.\n[23] D. M. Gavr",
  "output": "Dehghani et al. discuss the use of transformers for image recognition at scale in their 2021 ICLR paper. Eigen, Ranzato, and Sutskever, in their 2013 preprint, focus on learning factored representations in a deep mixture of experts. Fedus, Zoph, and Shazeer, in their 2021 preprint, present Switch transformers, which scale to trillion parameter models with simple and efficient sparsity. The text ends abruptly after mentioning D. M. Gavr.",
  "summary_type": "reports"
}{
  "input": "†Corresponding author.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2107.00645v2  [cs.CV]  26 Oct 2021\n\n--------------------------------------------------\nPatch Embedding\nGlobal Filter Layer\nFeed Forward Network (FFN)\n𝑁×\nGlobal Average Pooling\nLinear\nClass\n2D FFT\n×\nlearnable\nglobal filters\nfrequency domain\nfeatures\n2D IFFT\nGlobal Filter Layer\nLayer Norm\nLayer Norm\nMLP\nFFN\nFigure 1: The overall architecture of the Global Filter Network. Our architecture is based",
  "output": "The text describes the overall architecture of the Global Filter Network, which includes components such as Patch Embedding, Global Filter Layer, Feed Forward Network (FFN), Global Average Pooling, Linear Class, 2D FFT, learnable global filters, frequency domain features, 2D IFFT, Layer Norm, and MLP. This architecture was presented at the 35th Conference on Neural Information Processing Systems (NeurIPS 2021) and is detailed in the paper arXiv:2107.00645v2 [cs.CV] dated 26 Oct 2021.",
  "summary_type": "reports"
}{
  "input": "be maintained. In this way, our method can be\na generic plugin in most ViTs of both ﬂat and deep-narrow\nstructures from the very beginning of training.\nConcretely, Evo-ViT 1 , a self-motivated slow-fast token\nevolution approach for dynamic ViTs is proposed in this\nwork. We claim that since transformers have insights into\nglobal dependencies among image tokens and learn for clas-\nsiﬁcation, it is naturally able to distinguish informative to-\nkens from placeholder tokens for each instance, which i",
  "output": "The text discusses a method called Evo-ViT 1, a self-motivated slow-fast token evolution approach for dynamic Vision Transformers (ViTs). This method can be used in most ViTs, regardless of their structure, from the start of training. The text suggests that transformers, due to their ability to understand global dependencies among image tokens and learn for classification, can naturally distinguish informative tokens from placeholder tokens for each instance.",
  "summary_type": "reports"
}{
  "input": "eiT. It is possible\nto design a self-attention network with high base perfor-\nmance and elite performance.\n3.3.8\nAdjusting the shape of network\nThere are still many differences between Net7 and ResNet-\n50.\nFirst, the shape of Net7 is different from ResNet-\n50. Their depths, widths, bottleneck ratios and block num-\nbers in network stages are different. Second, they normal-\nize the features in different positions. Net7 only normal-\nizes input features in a block, while ResNet-50 normalizes\nfeature",
  "output": "The text discusses the design of a self-attention network with high base and elite performance. It highlights the differences between two networks, Net7 and ResNet-50, in terms of their shape, depth, width, bottleneck ratios, and block numbers in network stages. Additionally, it notes the difference in their normalization of features, with Net7 normalizing input features in a block, while ResNet-50 normalizes features in different positions.",
  "summary_type": "reports"
}{
  "input": "plmented KD outperformed their proposed state-of-the-art method, CRD (71.17%), and\nachieved the comparable accuracy with their CRD+KD (71.38%) method.\n3.3\nReproducing COCO experiments\nTo demonstrate that our framework can 1) be applied to different tasks, and 2) col-\nlaborate with model architectures that are not implemented in torchvision, we apply\nthe generalized head network distillation (GHND) to bottleneck-injected R-CNN ob-\nject detectors for split computing [25], using COCO 2017 dataset.",
  "output": "The text discusses how the implemented KD method outperformed the state-of-the-art CRD method, achieving comparable accuracy with the CRD+KD method. It also mentions the application of the generalized head network distillation (GHND) to bottleneck-injected R-CNN object detectors for split computing, using the COCO 2017 dataset, to demonstrate the framework's versatility and compatibility with different tasks and model architectures.",
  "summary_type": "reports"
}{
  "input": ". These approaches rely on heuristics (e.g., EA)\nto efﬁciently navigate the search space allowing practitioners to\nvisualize the trade-off between the objectives and to choose a\nsuitable network a posteriori to the search. NAT falls into the\nlatter category and uses an accuracy prediction model and weight\nsharing for efﬁcient architecture transfer to new tasks.\n3\nPROPOSED APPROACH\nNeural Architecture Transfer consists of three main components:\nan accuracy predictor, an evolutionary search routin",
  "output": "The text discusses approaches that use heuristics, such as Evolutionary Algorithms (EA), to efficiently navigate the search space. This allows practitioners to visualize the trade-off between objectives and choose a suitable network after the search. Neural Architecture Transfer (NAT) is one such approach that uses an accuracy prediction model and weight sharing for efficient architecture transfer to new tasks. NAT consists of three main components: an accuracy predictor, an evolutionary search routine, and a third component that is not mentioned in the provided text.",
  "summary_type": "reports"
}{
  "input": "s a special serial case of the resid-\nual structure. (b) The CNN (e.g., ResNet); (c) A special\nhybrid structure where the transformer block is embedded\nto bottlenecks. (d) The visual transformers (e.g., ViT); (e)\nA special case where the bottlenecks are embedded to the\ntransformer blocks.\noverlap, by a linear projection layer, which is a 4×4\nconvolution with stride 4. A class token is then pretended\nto the patch embeddings for classiﬁcation.\nConsidering\nthat the CNN branch (3×3 convolution) enco",
  "output": "The text discusses various structures in machine learning, including the residual structure, CNN (like ResNet), a hybrid structure with an embedded transformer block in bottlenecks, visual transformers (like ViT), and a case where bottlenecks are embedded in transformer blocks. It also mentions a linear projection layer involving a 4x4 convolution with stride 4 and a class token added to patch embeddings for classification. The CNN branch uses a 3x3 convolution.",
  "summary_type": "reports"
}{
  "input": "print arXiv:1611.03530, 2016.\nZhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D.\nmixup: Beyond empirical risk minimization. ICLR, 2018.\nZhang, Y., Pezeshki, M., Brakel, P., Zhang, S., Bengio, C.\nL. Y., and Courville, A. Towards end-to-end speech recog-\nnition with deep convolutional neural networks. arXiv\npreprint arXiv:1701.02720, 2017.\nZhao, R., Ouyang, W., Li, H., and Wang, X. Saliency\ndetection by multi-context deep learning. CVPR, 2015.\nZhou, B., Khosla, A., Lapedriza, A., Oliva, A., a",
  "output": "The text lists various academic papers: \"mixup: Beyond empirical risk minimization\" by Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D., published at ICLR in 2018; \"Towards end-to-end speech recognition with deep convolutional neural networks\" by Zhang, Y., Pezeshki, M., Brakel, P., Zhang, S., Bengio, C. L. Y., and Courville, A., available on arXiv preprint arXiv:1701.02720 in 2017; and \"Saliency detection by multi-context deep learning\" by Zhao, R., Ouyang, W., Li, H., and Wang, X. S., presented at CVPR in 2015. The text also mentions an unspecified work by Zhou, B., Khosla, A., Lapedriza, A., and Oliva, A.",
  "summary_type": "reports"
}{
  "input": "Mixup+KD\n200\n224\n74.18\n77.51\n64.41\n12.98 / 22.49\nM5\nMobileNet-V1\nLS+Mixup+KD\n360\n224\n74.37\n76.39\n65.02\n12.98 / 22.49\nM6\nSE-MobileNet-V1 (r=16)\nLS+Mixup+KD\n360\n224\n76.42\n71.67\n56.73\n10.33 / 19.71\nM7\nSE-MobileNet-V1 (r=2)\nLS+Mixup+KD\n360\n224\n76.82\n70.67\n55.76\n9.9 / 19.32\nM8\nSE-MobileNet-V1 (r=2)\nLS+Mixup+KD+DropBlock\n900\n224\n77.30\n68.12\n49.99\n9.9 / 19.32\nTable 9. Ablation study for assembling the network tweaks and regularization with MobileNet on ILSVRC2012 dataset. In order to measure\nthroughput",
  "output": "The text presents an ablation study for assembling network tweaks and regularization with MobileNet on the ILSVRC2012 dataset. Different models (Mixup+KD, LS+Mixup+KD, and LS+Mixup+KD+DropBlock) were used with varying parameters. The models were evaluated based on different metrics, with SE-MobileNet-V1 (r=2) with LS+Mixup+KD+DropBlock achieving the highest score of 77.30.",
  "summary_type": "reports"
}{
  "input": "different types of ﬁlters with varying size and depth, which are\nable to capture different levels of details in the scene. On top of these improved\nrecognition capabilities, PyConv is also efﬁcient and, with our formulation, it\ndoes not increase the computational cost and parameters compared to standard\nconvolution. Moreover, it is very ﬂexible and extensible, providing a large space of\npotential network architectures for different applications. PyConv has the potential\nto impact nearly every co",
  "output": "PyConv is a type of filter with varying size and depth, capable of capturing different levels of scene details. It has improved recognition capabilities and is efficient, not increasing computational cost and parameters compared to standard convolution. It's flexible and extensible, offering a wide range of potential network architectures for various applications. PyConv has the potential to significantly impact numerous areas.",
  "summary_type": "reports"
}{
  "input": "input data. Spatial relationships are essential for sensory\nreasoning (Kant, 1781) and this limitation is clearly unsat-\nisfying. In the attention literature, position information is\ntypically injected by tagging position encodings onto the\ninput features (Vaswani et al., 2017); we pursue this strategy\nhere as well. While position information is typically used\nto encode sequence position in the context of language, it\ncan also be used to encode spatial, temporal, and modality\nidentity.\nScalable",
  "output": "The text discusses the importance of spatial relationships for sensory reasoning. It mentions that position information is typically added to input features in attention literature, a strategy also used in this context. Position information, commonly used to encode sequence position in language, can also encode spatial, temporal, and modality identity.",
  "summary_type": "reports"
}{
  "input": "s, Jeffrey Dean, Matthieu Devin, et al.\nTensorﬂow:\nLarge-scale machine learning on heterogeneous distributed\nsystems. arXiv preprint arXiv:1603.04467, 2016.\n[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101–mining discriminative components with random\nforests. In European Conference on Computer Vision, pages\n446–461. Springer, 2014.\n[3] Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and\nRogerio Feris. Big-little net: An efﬁcient multi-scale fea-\nture representation for visu",
  "output": "The text mentions three different academic works. The first is a paper on TensorFlow, a tool for large-scale machine learning on heterogeneous distributed systems, authored by Jeffrey Dean, Matthieu Devin, and others. The second is a study by Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool on Food-101, which uses random forests to mine discriminative components, presented at the European Conference on Computer Vision. The third is a work by Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and Rogerio Feris on Big-little net, a multi-scale feature representation for visual data.",
  "summary_type": "reports"
}{
  "input": "nsformer over CNN\nmainly come with strong data augmentation and training tricks. This can promote the ongoing and\nfuture research of Vision Transformer.\n14\n\n--------------------------------------------------",
  "output": "The advantages of Vision Transformer over Convolutional Neural Networks primarily stem from robust data augmentation and training techniques, which can further advance current and future research in Vision Transformer.",
  "summary_type": "reports"
}{
  "input": "arning applied to document recognition. Proceed-\nings of the IEEE, 86(11):2278–2324, 1998.\nLeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Na-\nture, 521(7553):436–444, 2015.\nLee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh,\nY. W. Set Transformer: A framework for attention-based\npermutation-invariant neural networks. In Proceedings of\nInternational Conference on Machine Learning (ICML),\n2019.\nLee, M. A., Zhu, Y., Zachares, P., Tan, M., Srinivasan, K.,\nSavarese, S., Fei-Fei, L., Gar",
  "output": "The text mentions several scholarly works related to machine learning and deep learning. These include a 1998 paper on machine learning applied to document recognition, a 2015 article by LeCun, Bengio, and Hinton on deep learning, and a 2019 conference paper by Lee, Lee, Kim, Kosiorek, Choi, and Teh on the Set Transformer, a framework for attention-based permutation-invariant neural networks. Other authors mentioned include Lee, Zhu, Zachares, Tan, Srinivasan, Savarese, and Fei-Fei.",
  "summary_type": "reports"
}{
  "input": "aluate\nit under the transfer learning setup in [20] on three different\ndatasets; CIFAR-10, CIFAR-100 and ChestX-Ray14 [43].\n5.4.1\nCIFAR-10 and CIFAR-100\nBoth CIFAR-10 and -100 datasets have 50,000 and 10,000\nimages for training and testing, respectively. CIFAR-100\nextends CIFAR-10 by adding 90 more classes resulting in\n10× fewer training examples per class. For training on both\ndatasets, the models are initialized with weights pre-trained\non ImageNet. The model is then ﬁne-tuned using SGD with\nm",
  "output": "The text discusses the evaluation of a model under the transfer learning setup on three datasets: CIFAR-10, CIFAR-100, and ChestX-Ray14. Both CIFAR-10 and CIFAR-100 datasets have 50,000 training images and 10,000 testing images. CIFAR-100 extends CIFAR-10 by adding 90 more classes, resulting in 10 times fewer training examples per class. The models are trained using weights pre-trained on ImageNet and fine-tuned using SGD.",
  "summary_type": "reports"
}{
  "input": "cause a prohibitively larger memory requirement\nbecause of the (D + h ∗THW/fqfk) term. Hence, Multi-\nscale Transformer starts with a small number of heads which\nis increased as the resolution factor THW/fqfk decreases,\nto hold the effect of (D +h∗THW/fqfk) roughly constant.\nD. Additional Implementation Details\nWe implement our model with PySlowFast [28]. Code\nand models are available at: https://github.com/\nfacebookresearch/SlowFast.\nD.1. Details: Kinetics Action Classiﬁcation\nArchitecture deta",
  "output": "The Multi-scale Transformer model starts with a small number of heads, which is increased as the resolution factor decreases, to keep the effect of the memory requirement term constant. The model is implemented with PySlowFast, and the code and models are available on GitHub.",
  "summary_type": "reports"
}{
  "input": "naries Section, the class attention\nAcls is calculated by Eqn. 3. We select k tokens whose\nscores in the class attention are among the top k as the infor-\nmative tokens. The remaining N −k tokens are recognized\nas placeholder tokens that contain less information. Differ-\nent from token pruning, the placeholder tokens are kept and\nfast-updated rather than dropped.\nFor better capability of capturing the underlying informa-\ntion among tokens in different layers, we propose a global\nclass attention",
  "output": "The text discusses a method of calculating Acls using Eqn. 3, where the top k tokens are selected as informative based on their scores in the class attention. The remaining N-k tokens are considered placeholder tokens with less information. These tokens are not dropped but are kept and fast-updated. A global class attention is proposed to better capture the underlying information among tokens in different layers.",
  "summary_type": "reports"
}{
  "input": "e that lambda layers (or linear attention) can successfully be used complemen-\ntary to pixel patches.\nC.6\nCONNECTIONS TO HYPERNETWORKS AND EXPERT MODELS\nLambdaNetworks generate their own computations, i.e. lambdas such that yn = λnqn. As such,\nthey can alternatively be viewed as an extension of HyperNetworks (Ha et al., 2016) that dynamically\ngenerate their computations based on contextual information.\nLastly, LambdaNetworks share some connections with sparsely-activated expert models (Shazeer\ne",
  "output": "The text discusses the successful use of lambda layers or linear attention in conjunction with pixel patches. It explains that LambdaNetworks generate their own computations, which can be seen as an extension of HyperNetworks that dynamically generate computations based on contextual information. The text also mentions a connection between LambdaNetworks and sparsely-activated expert models.",
  "summary_type": "reports"
}{
  "input": "able B.3: Penalty λ and group size g for the v1 and v2 DIFFQ models reported on Table B.2\nMOBILENET\nRESNET-18\nWIDERESNET\nλ\ng\nλ\ng\nλ\ng\nCIFAR-10\nDIFFQ V1\n1\n16\n0.1\n8\n5\n16\nDIFFQ V2\n5\n8\n5\n4\n5\n16\nCIFAR-100\nDIFFQ V1\n1\n16\n0.05\n4\n5\n16\nDIFFQ V2\n5\n16\n5\n8\n1\n16\n8 bits, DIFFQ achieves the same accuracy as the uncompressed baseline, for a slightly smaller model than QAT 8bits. As we\nlower the number of bits, we again see a clear advantage for DIFFQ, with both a smaller model (5.7MB against 6.1MB) than\nQAT 4bits",
  "output": "The text presents penalty λ and group size g for the v1 and v2 DIFFQ models in MOBILENET, RESNET-18, and WIDERESNET for CIFAR-10 and CIFAR-100. The DIFFQ achieves the same accuracy as the uncompressed baseline for a slightly smaller model than QAT 8bits. As the number of bits is lowered, DIFFQ shows a clear advantage with a smaller model (5.7MB against 6.1MB) than QAT 4bits.",
  "summary_type": "reports"
}{
  "input": "two basic operations. First,\nis an attention operation [4] for modeling inter-element re-\nlations. Second, is a multi-layer perceptron (MLP), which\nmodels relations within an element. Intertwining these oper-\nations with normalization [2] and residual connections [44]\nallows transformers to generalize to a wide variety of tasks.\nRecently, transformers have been applied to key com-\nputer vision tasks such as image classiﬁcation. In the spirit\nof architectural universalism, vision transformers [25",
  "output": "Transformers involve two basic operations: an attention operation for modeling inter-element relations, and a multi-layer perceptron (MLP) for modeling relations within an element. The combination of these operations with normalization and residual connections allows transformers to be applied to a wide variety of tasks. Recently, they have been used in key computer vision tasks such as image classification.",
  "summary_type": "reports"
}{
  "input": "pendix B.5. There are three additional\nmajor design decisions that affect the cost (and potentially the quality) of our model:\nNumber of MoE layers. Following [39], we place the MoEs on every other layer (we refer to these\nas V-MoE Every-2). In addition, we experimented with using fewer MoE layers, by placing them on\nthe last-n even blocks (thus we dub these V-MoE Last-n). In Appendix E.1 we observe that, although\nusing fewer MoE layers decreases the number of parameters of the model, it has typ",
  "output": "The text discusses three major design decisions that impact the cost and possibly the quality of a model. The number of MoE layers is one such decision. The model places MoE layers on every other layer, referred to as V-MoE Every-2, and experiments with fewer MoE layers by placing them on the last-n even blocks, called V-MoE Last-n. The text notes that using fewer MoE layers reduces the model's parameters.",
  "summary_type": "reports"
}{
  "input": "arison of class-token and global average pooling classiﬁers. Both work similarly\nwell, but require different learning-rates.\nPos. Emb.\nDefault/Stem\nEvery Layer\nEvery Layer-Shared\nNo Pos. Emb.\n0.61382\nN/A\nN/A\n1-D Pos. Emb.\n0.64206\n0.63964\n0.64292\n2-D Pos. Emb.\n0.64001\n0.64046\n0.64022\nRel. Pos. Emb.\n0.64032\nN/A\nN/A\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\nImageNet 5-shot linear.\nthe difference in performance is fully explained by the requirem",
  "output": "The text presents the results of an ablation study on positional embeddings with the ViT-B/16 model evaluated on ImageNet 5-shot linear. It compares class-token and global average pooling classifiers, noting they perform similarly but require different learning rates. The table shows various results for different types of positional embeddings, including Default/Stem, Every Layer, Every Layer-Shared, and No Pos. Emb. The text also mentions that the difference in performance is fully explained by the requirements.",
  "summary_type": "reports"
}{
  "input": "tails). Since X[k] repeats on intervals of length N, it\nis sufﬁce to take the value of X[k] at N consecutive points k = 0, 1, . . . , N −1. Speciﬁcally, X[k]\nrepresents to the spectrum of the sequence x[n] at the frequency ωk = 2πk/N.\nIt is also worth noting that DFT is a one-to-one transformation. Given the DFT X[k], we can recover\nthe original signal x[n] by the inverse DFT (IDFT):\nx[n] = 1\nN\nN−1\nX\nk=0\nX[k]ej(2π/N)kn.\n(3.2)\nFor real input x[n], it can be proved that (see Appendix A) its DFT is",
  "output": "The text discusses the Discrete Fourier Transform (DFT), stating that X[k] repeats on intervals of length N and represents the spectrum of the sequence x[n] at the frequency ωk = 2πk/N. It also mentions that DFT is a one-to-one transformation, and the original signal x[n] can be recovered using the inverse DFT (IDFT). The text also suggests that for real input x[n], its DFT can be proved.",
  "summary_type": "reports"
}{
  "input": "architecture search.\narXiv preprint\narXiv:2103.12424, 2021. 3\n[32] Min Lin, Qiang Chen, and Shuicheng Yan. Network in net-\nwork. arXiv preprint arXiv:1312.4400, 2013. 2\n[33] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll´ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980–2988, 2017. 7\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Law",
  "output": "The text refers to various academic papers: one on architecture search (arXiv:2103.12424, 2021), another titled \"Network in network\" by Min Lin, Qiang Chen, and Shuicheng Yan (arXiv:1312.4400, 2013), and a third on \"Focal loss for dense object detection\" presented at the IEEE international conference on computer vision (2017). The last line seems to refer to another paper by Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Law, but no title or details are provided.",
  "summary_type": "reports"
}{
  "input": "associ-\nated alt-texts are discarded. To ensure that we don’t train on\ntest images, we also remove duplicates or near-duplicates\nof test images in all downstream evaluation datasets (e.g.,\nILSVRC-2012, Flickr30K, and MSCOCO). See Appendix\nA for more details.\nText-based ﬁltering.\nWe exclude alt-texts that are shared\nby more than 10 images. These alt-texts are often irrelevant\nto the content of the images (e.g., “1920x1080”, “alt img”,\nand “cristina”). We also discard alt-texts that contain any\nra",
  "output": "The text discusses the process of filtering and discarding certain images and alt-texts during training. Duplicates or near-duplicates of test images are removed from evaluation datasets like ILSVRC-2012, Flickr30K, and MSCOCO. Alt-texts shared by more than 10 images are excluded as they are often irrelevant to the image content. Alt-texts containing certain unspecified content are also discarded.",
  "summary_type": "reports"
}{
  "input": "ls. A SSAL objective is real-\nized through one or more additional targets that are derived\nfrom the original supervised classiﬁcation task, following\narchitectural principles found in multi-task learning. SSAL\nbranches impose low-level priors into the optimization pro-\ncess (e.g., grouping). The ability of using SSAL branches\nduring inference, allow models to converge faster, focus-\ning on a richer set of class-relevant features. We show that\nSSAL models consistently outperform the state-of-the-",
  "output": "The Self-Supervised Auxiliary Learning (SSAL) objective is achieved through additional targets derived from the original supervised classification task, following multi-task learning principles. SSAL branches incorporate low-level priors into the optimization process, such as grouping. Using SSAL branches during inference enables models to converge faster and focus on a richer set of class-relevant features. It is demonstrated that SSAL models consistently outperform the state-of-the-art.",
  "summary_type": "reports"
}{
  "input": "53]\nC. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, “Learning\na discriminative feature network for semantic segmentation,” in\nCVPR, 2018, pp. 1857–1866.\n[54]\nH. Zhang, K. J. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and\nA. Agrawal, “Context encoding for semantic segmentation,” in\nCVPR, 2018, pp. 7151–7160.\n[55]\nZ. Zhong, Z. Q. Lin, R. Bidart, X. Hu, I. B. Daya, Z. Li, W. Zheng,\nJ. Li, and A. Wong, “Squeeze-and-attention networks for semantic\nsegmentation,” in CVPR, 2020, pp. 13 062–13 071.",
  "output": "C. Yu et al. presented a study on \"Learning a discriminative feature network for semantic segmentation\" at CVPR 2018. H. Zhang et al. also presented their research on \"Context encoding for semantic segmentation\" at the same conference. In CVPR 2020, Z. Zhong et al. discussed \"Squeeze-and-attention networks for semantic segmentation\".",
  "summary_type": "reports"
}{
  "input": "rate multiple types of data that appear in vision, such as\npixels [61, 3, 46, 68], point clouds [65], sequence condition-\ning information [64], and graphs [32]. Self-attention may\nalso be regarded as an adaptive nonlinearity paralleling a\nlong history of nonlinear processing techniques in computer\nvision, such as bilateral ﬁltering [41] and non-local means\n[4].\nSeveral recent papers [3, 45, 12, 68, 52] have attempted\nusing self-attention primitives to improve image classiﬁca-\ntion accuracy over",
  "output": "The text discusses the use of self-attention in processing various types of data in vision such as pixels, point clouds, sequence conditioning information, and graphs. It's compared to adaptive nonlinearity, a technique used in computer vision. Recent studies have attempted to use self-attention primitives to enhance image classification accuracy.",
  "summary_type": "reports"
}{
  "input": "ccu-\nracy is the percentage of images that are certiﬁably correct.\nFollowing Salman et al. (2019), we calculate the certiﬁed\naccuracy of all the classiﬁers for various radius and report\nthe best results overall of the classiﬁers. We use the codes\nprovided by Cohen et al. (2019) to calculate certiﬁed ac-\ncuracy.4\nFollowing Salman et al. (2019), we select the best hyperpa-\nrameters with grid search. The only two hyperparameters\nof our MaxUp+Gauss are the sample size m and the vari-\nance σ2 of the",
  "output": "The text discusses the calculation of certified accuracy, which is the percentage of images that are correctly identified. The certified accuracy of all classifiers is calculated for various radii, using codes provided by Cohen et al. (2019). The best results are reported. The best hyperparameters are selected using grid search, as per Salman et al. (2019). The only two hyperparameters of MaxUp+Gauss are the sample size (m) and the variance (σ2).",
  "summary_type": "reports"
}{
  "input": "020; Shao\net al., 2020). Additionally, Zhang et al. (2019a) and De &\nSmith (2020) observed that the performance of unnormal-\nized ResNets can be improved with additional regulariza-\ntion. However only recovering these two beneﬁts of batch\nnormalization is not sufﬁcient to achieve competitive test\naccuracies on challenging benchmarks (De & Smith, 2020).\nIn this work, we adopt and build on “Normalizer-Free\nResNets” (NF-ResNets) (Brock et al., 2021), a class of pre-\nactivation ResNets (He et al., 2",
  "output": "The text discusses the performance of unnormalized ResNets, stating that it can be improved with additional regularization according to Zhang et al. (2019a) and De & Smith (2020). However, these improvements are not sufficient to achieve competitive test accuracies on challenging benchmarks. The authors of the text are working on \"Normalizer-Free ResNets\" (NF-ResNets), a class of pre-activation ResNets, to address this issue.",
  "summary_type": "reports"
}{
  "input": "n (4), p(1)\ni\nand p(2)\ni\ncorrespond to the two large crop views, and p(3)\ni , . . . , p(8)\ni\ncorrespond to the six small crop views.\nThus, from equation (4), the target for p(1)\ni\nis the sharpened positive view prediction ρ(p(2)\ni ), and similarly, the target for p(2)\ni\nis\nthe sharpened positive view prediction ρ(p(1)\ni ). For the small views, p(3)\ni , . . . , p(8)\ni , we use both ρ(p(1)\ni ) and ρ(p(2)\ni ) as positive\nview predictions and average those to produce a single target. This is similar",
  "output": "The text describes an equation where n(4), p(1)i and p(2)i represent two large crop views, while p(3)i to p(8)i represent six small crop views. The target for p(1)i is the sharpened positive view prediction ρ(p(2)i), and vice versa for p(2)i. For the small views, both ρ(p(1)i) and ρ(p(2)i) are used as positive view predictions and averaged to produce a single target.",
  "summary_type": "reports"
}{
  "input": "t the Top-1 accuracy on ImageNet-1K in Ta-\nble 1 and Cifar10 in Appendix. Observe that WideNet-H\nachieves the best performance and signiﬁcantly outperforms\nViT and ViT-MoE models on ImageNet-1K. Compared with\nthe strongest baseline, our WideNet-H outperforms ViT-B\nby 1.5% with less trainable parameters. Even if we use the\nsmallest model, WideNet-B, it still achieves comparable per-\nformance with ViT-L and ViT-MoE-B with over 4× less\ntrainable parameters. When we scale up to WideNet-L, it\nhas sur",
  "output": "WideNet-H demonstrates superior performance on ImageNet-1K and Cifar10, significantly outperforming ViT and ViT-MoE models. It surpasses ViT-B by 1.5% while having fewer trainable parameters. The smallest model, WideNet-B, matches the performance of ViT-L and ViT-MoE-B with over 4x fewer trainable parameters.",
  "summary_type": "reports"
}{
  "input": "ny improvements for the\nlarger scales.\nPyramids\nAll layers in Mixer retain the same, isotropic design. Recent improvements on the ViT\narchitecture hint that this might not be ideal [52]. We tried using the token-mixing MLP to reduce the\nnumber of tokens by mapping from S input tokens to S′ < S output tokens. While ﬁrst experiments\nshowed that on JFT-300M such models signiﬁcantly reduced training time without losing much\nperformance, we were unable to transfer these ﬁndings to ImageNet or ImageNe",
  "output": "All layers in Mixer maintain the same isotropic design. Recent enhancements to the ViT architecture suggest this may not be optimal. Experiments were conducted using the token-mixing MLP to decrease the number of tokens, which significantly reduced training time on JFT-300M without substantial performance loss. However, these results could not be replicated on ImageNet or ImageNe.",
  "summary_type": "reports"
}{
  "input": "earner do not drop (e.g., TokenLearner-B/16 vs.\nViT-B/16), despite the difference in the number of tokens.\nTokenLearner on larger models.\nWe also evaluated our TokenLearner inserted into a ‘large’\nmodel: ViT-L. In addition to the standard L/16 model that splits the scene into 16x16 patches, the\nsame model with ﬁner tokens were used including L/14, L/10, and L/8. Note that the model size stays\nthe same, with 464M parameters, and only the inputs change for these models. As discussed above,\nthe num",
  "output": "The text discusses an evaluation of TokenLearner inserted into a 'large' model, ViT-L. This model was tested with different token sizes including L/14, L/10, and L/8, in addition to the standard L/16 model. The model size remained the same with 464M parameters, and only the inputs changed for these models. The performance of TokenLearner did not drop despite the difference in the number of tokens.",
  "summary_type": "reports"
}{
  "input": "computer vision to predict\nthe class. The transformer thus process batches of (N + 1) tokens of dimension\nD, of which only the class vector is used to predict the output. This architecture\nforces the self-attention to spread information between the patch tokens and\nthe class token: at training time the supervision signal comes only from the\nclass embedding, while the patch tokens are the model’s only variable input.\n5\n\n--------------------------------------------------\nFixing the positional enc",
  "output": "The transformer in computer vision processes batches of (N + 1) tokens of dimension D, using only the class vector to predict the output. This architecture compels the self-attention to distribute information between the patch tokens and the class token. During training, the supervision signal originates solely from the class embedding, while the patch tokens serve as the model's only variable input. The text also mentions fixing the positional enc, but it is cut off.",
  "summary_type": "reports"
}{
  "input": "r set of class-relevant features. We show that\nSSAL models consistently outperform the state-of-the-art\nwhile also providing structured predictions that are more\ninterpretable.\n1. Introduction\nMachine learning models tackling classiﬁcation prob-\nlems are isolated in nature i.e., they are deﬁned, and op-\nerate under a closed world paradigm [2] where all possible\ninputs belong to one out of multiple but ﬁnite pre-deﬁned\nclasses. This simpliﬁcation goes against emerging needs\nfor more interpretable",
  "output": "The text discusses SSAL models in machine learning that handle classification problems. These models operate under a closed world paradigm where all inputs belong to one of several pre-defined classes. SSAL models are noted for their superior performance and their ability to provide structured, interpretable predictions.",
  "summary_type": "reports"
}{
  "input": "9216\n9728\n10240\n1024//16\n1024//8\n1280//20\n1280//16\n1280//10\n1408//22\n1408//16\n1408//11\n1536//24\n1536//16\n1536//12\n1664//26\n1664//16\n1664//13\n1792//28\n1792//16\n1792//14\n1920//30\n1920//24\n1920//16\n1920//15\n2048//32\n2048//16\nSA-width // num heads\nViT-L\n24 layers\n4096\n4608\n5120\n5632\n6144\n6656\n7168\n7680\n8192\n8704\n9216\n9728\n10240\nViT-H\n32 layers\n4096\n4608\n5120\n5632\n6144\n6656\n7168\n7680\n8192\n8704\n9216\n9728\n10240\nMLP-width\nViT-g\n40 layers\n4096\n4608\n5120\n5632\n6144\n6656\n7168\n7680\n8192\n8704\n9216\n9728\n10240",
  "output": "The text presents a series of numerical values related to three different models: ViT-L with 24 layers, ViT-H with 32 layers, and ViT-g with 40 layers. Each model has a range of values from 4096 to 10240. Additionally, there are values related to SA-width divided by the number of heads, with various combinations of numbers.",
  "summary_type": "reports"
}{
  "input": "ing or special regularization mechanisms.\n3. Methods\nIn this section, we describe the algorithmic components\nfrom SSAL and how they integrate into a traditional clas-\nsiﬁcation problem for training and prediction. There are\nfour main components to discuss: grouping criterion, ar-\nchitectural design, training objectives and joint prediction.\nFor each of these components, we introduce emergent meta-\nparameters that need to be considered during evaluation.\n3.1. Grouping Criterion\nWe propose that th",
  "output": "The text discusses the algorithmic components of SSAL and their integration into a traditional classification problem for training and prediction. The four main components include the grouping criterion, architectural design, training objectives, and joint prediction. The text also mentions emergent meta-parameters that need consideration during evaluation.",
  "summary_type": "reports"
}{
  "input": "Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A.\nOptimal transport for domain adaptation. IEEE transac-\ntions on pattern analysis and machine intelligence, 2016.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Li,\nF. F. Imagenet: a large-scale hierarchical image database.\nCVPR, 2009.\nDeVries, T. and Taylor, G. W. Improved regularization of\nconvolutional neural networks with cutout. arXiv preprint\narXiv:1708.04552, 2017.\nErkan, G. and Radev, D. R. Lexrank: Graph-based lexical\ncent",
  "output": "The text provides references to several academic papers and articles. These include \"Optimal transport for domain adaptation\" by Courty, Flamary, Tuia, and Rakotomamonjy, published in IEEE transactions on pattern analysis and machine intelligence in 2016; \"Imagenet: a large-scale hierarchical image database\" by Deng, Dong, Socher, Li, Li, and Li, presented at CVPR in 2009; \"Improved regularization of convolutional neural networks with cutout\" by DeVries and Taylor, an arXiv preprint from 2017; and a work by Erkan and Radev on Lexrank, a graph-based lexical cent.",
  "summary_type": "reports"
}{
  "input": "d generalization ability of our ViTAE.\n4.6\nVisual inspection of ViTAE\n40\n60\n80\n100\n0\n1\n2\n3\n4\n5\n6\n7\n8\nAttention Distance (pixel)\nLayers\nT2T-ViT\nViTAE\nFigure 3: The average per-layer attention distance\nof T2T-ViT-7 and our ViTAE-T.\nTo further analyze the property of our ViTAE,\nwe ﬁrst calculate the average attention distance\nof each layer in ViTAE-T and the baseline T2T-\nViT-7 on the ImageNet test set, respectively. The\nresults are shown in Figure 3. It can be observed\nthat with the usage of PCM,",
  "output": "The text discusses an analysis of the properties of ViTAE, a visual inspection tool. The average attention distance of each layer in ViTAE-T and the baseline T2T-ViT-7 was calculated on the ImageNet test set. The results are shown in Figure 3. The text implies that the usage of PCM has some impact, but the specific effect is not mentioned.",
  "summary_type": "reports"
}{
  "input": "tisfactory result, 0.76 ρ,\nsmoothly and quickly within only 20 epochs on CIFAR-100,\nand continues to be stable for the subsequent 10 epochs.\nA.4. Visualization of Human-designed Architec-\ntures in HyTra\nThe\narchitectures\nof\nResNet50-T,\nViT-T/16\nand\nBoTNet50-T from our HyTra search space are illustrated\nin Fig. 10. Their architectures follow as closely as possible\nto the architectures of their prototypes.\n14\n\n--------------------------------------------------",
  "output": "The text discusses a satisfactory result of 0.76 ρ, achieved smoothly and quickly within only 20 epochs on CIFAR-100, and remained stable for the next 10 epochs. It also mentions the visualization of human-designed architectures in HyTra, specifically ResNet50-T, ViT-T/16, and BoTNet50-T. These architectures are designed to closely follow their prototypes.",
  "summary_type": "reports"
}{
  "input": "rameters, at\nany point in time, a signiﬁcant fraction of the weights could\nbe quantized to a suboptimal value due to the oscillations\nimplied by the STE method. We conﬁrm such behavior exper-\nimentally using EfﬁcientNet-b3 with QAT4 on ImageNet (see\nFigure 1(b) in the Appendix). In the following section, we\nintroduce DIFFQ, a method based on independent additive\npseudo quantization noise, that does not suffer from such a\nbias, while approximating well enough quantization noise to\nperform efﬁcien",
  "output": "The text discusses the limitations of the STE method in weight quantization, which can lead to suboptimal values. This is confirmed through experiments using EfficientNet-b3 with QAT4 on ImageNet. The text then introduces DIFFQ, a new method that uses independent additive pseudo quantization noise, which doesn't have the same bias and approximates quantization noise efficiently.",
  "summary_type": "reports"
}{
  "input": "xtensive connections [34], and\nmore sophisticated forms of convolution [70, 18, 84]. With\nCNNs serving as backbone networks for a variety of vision\ntasks, these architectural advances have led to performance\nimprovements that have broadly lifted the entire ﬁeld.\nOn the other hand, the evolution of network architectures\nin natural language processing (NLP) has taken a different\npath, where the prevalent architecture today is instead the\nTransformer [64].\nDesigned for sequence modeling and\ntransdu",
  "output": "The text discusses the evolution of network architectures in vision tasks and natural language processing (NLP). Convolutional Neural Networks (CNNs) have advanced and improved performance in vision tasks, while the Transformer architecture is prevalent in NLP.",
  "summary_type": "reports"
}{
  "input": "and the softmax of the student model.\nLet Zt be the logits of the teacher model, Zs the logits of the student model.\nWe denote by τ the temperature for the distillation, λ the coefﬁcient balanc-\ning the Kullback–Leibler divergence loss (KL) and the cross-entropy (LCE) on\nground truth labels y, and ψ the softmax function. The distillation objective is\nLglobal = (1 −λ)LCE(ψ(Zs), y) + λτ 2KL(ψ(Zs/τ), ψ(Zt/τ)).\n(2)\nHard-label distillation.\nWe introduce a variant of distillation where we take\nthe ha",
  "output": "The text discusses a model distillation process. It defines Zt as the logits of the teacher model and Zs as the logits of the student model. The temperature for the distillation is denoted by τ, the coefficient balancing the Kullback-Leibler divergence loss (KL) and the cross-entropy (LCE) on ground truth labels y is denoted by λ, and the softmax function is denoted by ψ. The distillation objective is given by the equation Lglobal = (1 −λ)LCE(ψ(Zs), y) + λτ 2KL(ψ(Zs/τ), ψ(Zt/τ)). The text also mentions the introduction of a variant of distillation called hard-label distillation.",
  "summary_type": "reports"
}{
  "input": "ation quality may pro-\nfoundly affect ﬁnal fusion weights. Since it is still a feature\nfusion problem, an intuitive way is to have another attention\nmodule to fuse input features. We call this two-stage ap-\nproach iterative Attentional Feature Fusion (iAFF), which\nis illustrated in Fig. 2(b). Then, the initial integration X⊎Y\nin Eq. (4) can be reformulated as\nX ⊎Y = M(X + Y) ⊗X + (1 −M(X + Y)) ⊗Y (5)\n4.3. Examples: InceptionNet, ResNet, and FPN\nTo validate the proposed AFF/iAFF as a uniform and",
  "output": "The text discusses a two-stage approach called Iterative Attentional Feature Fusion (iAFF) which is used to fuse input features, affecting final fusion weights. This approach is illustrated in a formula. The text also mentions that examples of this approach include InceptionNet, ResNet, and FPN.",
  "summary_type": "reports"
}{
  "input": "uch networks in memory for the\nteacher and the student would vastly exceed the available\n6\n\n--------------------------------------------------\nMethod\n# Params\nExtra Data\nImageNet\nImageNet-ReaL [6]\nTop-1\nTop-5\nPrecision@1\nResNet-50 [24]\n26M\n−\n76.0\n93.0\n82.94\nResNet-152 [24]\n60M\n−\n77.8\n93.8\n84.79\nDenseNet-264 [28]\n34M\n−\n77.9\n93.9\n−\nInception-v3 [62]\n24M\n−\n78.8\n94.4\n83.58\nXception [11]\n23M\n−\n79.0\n94.5\n−\nInception-v4 [61]\n48M\n−\n80.0\n95.0\n−\nInception-resnet-v2 [61]\n56M\n−\n80.1\n95.1\n−\nResNeXt-101 [78]",
  "output": "The text presents a comparison of different neural network models based on their parameters and performance. The models include ResNet-50, ResNet-152, DenseNet-264, Inception-v3, Xception, Inception-v4, Inception-resnet-v2, and ResNeXt-101. Performance metrics include Top-1, Top-5, and Precision@1. The number of parameters for each model is also provided.",
  "summary_type": "reports"
}{
  "input": "tion on MS-COCO dataset. KSSNet [40],\nis the known SOTA, based on ResNet101 backbone.\nWe can see from Table 9 that the TResNet-based solu-\ntion signiﬁcantly outperforms previous top solution for MS-\nCOCO multi-label dataset, increasing the known SOTA by\na large margin, from 83.7 mAP to 86.4 mAP. All additional\nevaluation metrics also show improvement.\n4.3. Object Detection\nWhile our main focus was on various classiﬁcation tasks,\nwe wanted to further test TResNet on another popular com-\nputer vis",
  "output": "The TResNet-based solution significantly outperforms the previous top solution, KSSNet, for the MS-COCO multi-label dataset, increasing the state-of-the-art (SOTA) from 83.7 mAP to 86.4 mAP. All additional evaluation metrics also show improvement. The team also plans to test TResNet on other popular computer vision tasks.",
  "summary_type": "reports"
}{
  "input": "d interpretation or action.\nClosed world models are thereby semantically disconnected\nfrom the patterns we may deem reasonable, making the\nFigure 1. Overview of a SSAL model. Starting from a common\nfeature representation h, a supervised goal f and an auxiliary\nbranch g are used for training and prediction. Training objective\nfor g is derived from the original labels used for f following a\nmutually exclusive grouping.\nquest for explanations an ill-posed endeavor. The ﬁeld of\nadversarial perturbat",
  "output": "The text discusses closed world models, which are semantically disconnected from patterns deemed reasonable, making explanations difficult. It also describes a SSAL model which starts from a common feature representation h, uses a supervised goal f and an auxiliary branch g for training and prediction. The training objective for g is derived from the original labels used for f following a mutually exclusive grouping.",
  "summary_type": "reports"
}{
  "input": "r fast inference image classiﬁcation. We consider dif-\nferent measures of efﬁciency on different hardware plat-\nforms, so as to best reﬂect a wide range of application\nscenarios.\nOur extensive experiments empirically vali-\ndate our technical choices and show they are suitable to\nmost architectures. Overall, LeViT signiﬁcantly outper-\nforms existing convnets and vision transformers with re-\nspect to the speed/accuracy tradeoff. For example, at 80%\nImageNet top-1 accuracy, LeViT is 5 times faster",
  "output": "The text discusses LeViT, a tool for fast inference image classification. It measures efficiency on various hardware platforms to cater to a wide range of application scenarios. Extensive experiments have validated its technical choices, showing suitability for most architectures. LeViT outperforms existing convnets and vision transformers in terms of speed/accuracy tradeoff, being 5 times faster at 80% ImageNet top-1 accuracy.",
  "summary_type": "reports"
}{
  "input": "sub-optimal initial architecture can skew the scaling\nresults. For example, the compound scaling rule derived\nfrom a small grid search around EfﬁcientNet-B0, which\nwas obtained by architecture search using a ﬁxed FLOPs\nbudget and a speciﬁc image resolution. However, since\nthis image resolution can be sub-optimal for that FLOPs\nbudget, the resulting scaling strategy can be sub-optimal.\nIn contrast, our work designs scaling strategies by training\nmodels across a variety of widths, depths and imag",
  "output": "The text discusses how a sub-optimal initial architecture can affect scaling results, using the example of the compound scaling rule derived from EfficientNet-B0. This was obtained through an architecture search using a fixed FLOPs budget and a specific image resolution, which could be sub-optimal for that budget, leading to a sub-optimal scaling strategy. The text then mentions that their work designs scaling strategies by training models across various widths, depths, and image resolutions.",
  "summary_type": "reports"
}{
  "input": "---------------------------------------------\nPerceiver: General Perception with Iterative Attention\nFigure 26. Example attention maps from the ﬁrst cross-attend over the video input subset of an AudioSet network trained on video and\nmel-spectrogram.\n\n--------------------------------------------------\nPerceiver: General Perception with Iterative Attention\nFigure 27. Example attention maps from the ﬁrst cross-attend over the mel-spectrogram input subset of an AudioSet network trained on\nvideo and",
  "output": "The text provides information about the Perceiver model's general perception with iterative attention. It mentions example attention maps from the first cross-attend over the video input subset and the mel-spectrogram input subset of an AudioSet network trained on video and mel-spectrogram.",
  "summary_type": "reports"
}{
  "input": "utput features as shown in Eqn. (2):\nFk = CKA(fk, fout)\nCKA(fin, fout),\n(2)\n15\n\n--------------------------------------------------\nTable 10: Model architecture conﬁgurations.\nModel\n#Blocks Hidden dim #Head #Params Training Resolution\nReﬁned-ViT-Base\n12\n768\n12\n86M\n224\nReﬁned-ViT-S\n16\n384\n12\n25M\n224\nReﬁned-ViT-M\n32\n420\n12\n55M\n224\nReﬁned-ViT-L\n32\n512\n16\n81M\n224\nwhere fk denotes the token features at layer k, fin denotes the token features at the ﬁrst transformer\nblock and fout denotes the features",
  "output": "The text presents an equation (Fk = CKA(fk, fout)) and a table detailing the configurations of four different models: Refined-ViT-Base, Refined-ViT-S, Refined-ViT-M, and Refined-ViT-L. The models vary in the number of blocks, hidden dimensions, number of heads, parameters, and resolution. The equation's variables represent token features at different layers of a transformer block.",
  "summary_type": "reports"
}{
  "input": "worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[20] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan.\nThe third pascal recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and\nparaphrasing, pages 1–9. Association for Computational Linguistics, 2007.\n11\n\n--------------------------------------------------\n[21] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu.",
  "output": "The text mentions two academic works. The first is a preprint on arXiv titled \"Transformers for image recognition at scale\" from 2020. The second is a paper titled \"The third pascal recognizing textual entailment challenge\" presented at the ACL-PASCAL workshop on textual entailment and paraphrasing in 2007, authored by Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The text also lists the names Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu, but provides no context for these names.",
  "summary_type": "reports"
}{
  "input": "ew(B, G, C//G, K*K, H, W)\n# kernel generation, Eqn.(6)\nkernel = span(reduce(o(x))) # B,KxKxG,H,W\nkernel = kernel.view(B, G, K*K, H, W).unsqueeze(2)\n# Multiply-Add operation, Eqn.(4)\nout = mul(kernel, x_unfolded).sum(dim=3) # B,G,C/G,H,W\nout = out.view(B, C, H, W)\nreturn out\n1, 2, · · · , G, is specially tailored for the pixel Xi,j ∈RC\n(the subscript of C is omitted for notation brevity) located\nat the corresponding coordinate (i, j), but shared over the\nchannels. G counts the number of groups wh",
  "output": "The text describes a process of kernel generation and multiply-add operation in a programming context. A kernel is generated and reshaped, then a multiply-add operation is performed on the kernel and an unfolded variable 'x'. The output is then reshaped. The process involves pixel Xi,j in RC and uses the number of groups 'G'. The subscript of 'C' is omitted for notation brevity.",
  "summary_type": "reports"
}{
  "input": "Table 3), indicating that the commonly used\nweight decay may not effectively regularize ViTs and MLP-Mixers.\nSparser active neurons in MLP-Mixers. Given the recursive formulation Equation (3), we identify\nanother intrinsic measure of MLP-Mixers that contribute to the Hessian: the number of activated\nneurons. Indeed, Bk is determined by the activated neurons whose values are greater than zero,\nsince the ﬁrst-order derivative of GELU becomes much smaller when the input is negative. As\na result, th",
  "output": "The text suggests that the commonly used weight decay may not effectively regularize Vision Transformers (ViTs) and MLP-Mixers. It also mentions that the number of activated neurons, which are determined by values greater than zero, is an intrinsic measure of MLP-Mixers contributing to the Hessian. The first-order derivative of GELU becomes much smaller when the input is negative.",
  "summary_type": "reports"
}{
  "input": "P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. In AISTATS,\n2015.\n[34] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable\nunsupervised learning of hierarchical representations. In International Conference on Machine\nLearning (ICML), 2009.\n[35] H. Lee, P. Pham, Y. Largman, and A. Y. Ng. Unsupervised feature learning for audio classiﬁca-\ntion using convolutional deep belief networks. In Advances in Neural Information Processing\nSystems, 2009.",
  "output": "P. Gallagher, Z. Zhang, and Z. Tu presented a study on deeply-supervised nets at AISTATS in 2015. In 2009, H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng discussed convolutional deep belief networks for scalable unsupervised learning of hierarchical representations at the International Conference on Machine Learning. The same year, H. Lee, P. Pham, Y. Largman, and A. Y. Ng presented a study on unsupervised feature learning for audio classification using convolutional deep belief networks at the Advances in Neural Information Processing Systems.",
  "summary_type": "reports"
}{
  "input": "ilable.\nConvolutional arithmetic circuits (ConvACs) are also related\nto our work. Arithmetic circuits are networks with two types of\nnodes: sum nodes (weighted sum of their inputs), and product\nnodes (computing the product of their inputs). Those two types\nof nodes are sufﬁcient to express a polynomial expansion. On\n[40], the authors want to characterize the depth efﬁciency of\n(deep) convolutional neural networks. The CP decomposition is\nused to factorize the weights of a shallow convolutional n",
  "output": "Convolutional arithmetic circuits (ConvACs) are networks featuring sum nodes, which compute the weighted sum of their inputs, and product nodes, which calculate the product of their inputs. These nodes can express a polynomial expansion. The authors aim to characterize the depth efficiency of deep convolutional neural networks, using the CP decomposition to factorize the weights of a shallow convolutional network.",
  "summary_type": "reports"
}{
  "input": "yin Zhou.\nTransunet: Transformers make strong encoders for medi-\ncal image segmentation. arXiv preprint arXiv:2102.04306,\n2021. 2\n[14] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In Proc. ICML, pages 1691–1703.\nPMLR, 2020. 2\n[15] Yunpeng Chen, Haoqi Fang, Bing Xu, Zhicheng Yan, Yannis\nKalantidis, Marcus Rohrbach, Shuicheng Yan, and Jiashi\nFeng. Drop an octave: Reducing spatial redundancy in con-\nvolutional",
  "output": "The text refers to three different academic sources. The first one is a 2021 preprint by Yin Zhou titled \"Transunet: Transformers make strong encoders for medical image segmentation\". The second source is a 2020 paper by Mark Chen et al., titled \"Generative pre-training from pixels\", presented at the ICML conference. The third source is a work by Yunpeng Chen et al., titled \"Drop an octave: Reducing spatial redundancy in convolutional\".",
  "summary_type": "reports"
}{
  "input": "T-B [95]\n81.8\n17.6\n86.6\nDeiT-B ↑3842 [95]\n83.1\n55.5\n87.0\nMViT-B-16, max-pool\n82.5\n7.8\n37.0\nMViT-B-24, max-pool\n83.1\n10.9\n53.5\nMViT-B-24-wide-3202, max-pool 84.3\n32.7\n72.9\nMViT-B-16\n83.0\n7.8\n37.0\nMViT-B-24-wide-3202\n84.8\n32.7\n72.9\nTable 17. Comparison to prior work on ImageNet.\nRegNet\nand EfﬁcientNet are ConvNet examples that use different training\nrecipes. DeiT/MViT are ViT-based and use identical recipes [95].\nWe show models of different depth, MViT-B-Depth, (16,\n24, and 32), where MViT-B-16 is",
  "output": "The text presents a comparison of different models' performance on ImageNet. T-B, DeiT-B, MViT-B-16, MViT-B-24, and MViT-B-24-wide-3202 are all models tested, with varying results. DeiT/MViT are ViT-based and use identical recipes. The MViT models are shown in different depths (16, 24, and 32). RegNet and EfficientNet are ConvNet examples that use different training recipes.",
  "summary_type": "reports"
}{
  "input": "100 (Krizhevsky\n& Geoffrey, 2009), Tiny-ImageNet (Chrabaszcz et al., 2017),\nand ImageNet (Deng et al., 2009) datasets. We ﬁrst study\nthe generalization performance and adversarial robustness\nof our method (Section 6.1). Next, we show that our method\ncan be used in conjunction with the existing augmentation\nmethod (AugMix) to simultaneously improve the corruption\nrobustness and generalization performance (Section 6.2).\nFinally, we perform ablation studies for our method (Sec-\ntion 6.3).\n6.1. Gene",
  "output": "The text discusses a study on the generalization performance and adversarial robustness of a method using 100 (Krizhevsky & Geoffrey, 2009), Tiny-ImageNet (Chrabaszcz et al., 2017), and ImageNet (Deng et al., 2009) datasets. The method can also be used with the existing augmentation method (AugMix) to improve corruption robustness and generalization performance. Ablation studies for the method are also performed.",
  "summary_type": "reports"
}{
  "input": "distance, while the\nclip function assigns the same encoding when the relative\ndistance is larger than β.\nNumber of buckets. The number of buckets largely af-\nfects model parameters, computational complexities and\nperformance. In order to ﬁnd a balance, we explore the in-\nﬂuence of varying the number of buckets for the contextual\nProduct method. Fig. 3 shows the change of top-1 accuracy\nalong with the number of buckets. The accuracy increase\n6\n\n--------------------------------------------------\n0",
  "output": "The text discusses the impact of the number of buckets on model parameters, computational complexities, and performance in the context of the Product method. It mentions an experiment exploring the influence of varying the number of buckets, with Figure 3 illustrating the change in top-1 accuracy in relation to the number of buckets. The accuracy increases as the number of buckets increases.",
  "summary_type": "reports"
}{
  "input": "ing the kernels size\nprovides more reliable details about larger objects and/or context information.\n(2) Efﬁciency. In comparison with the standard convolution, PyConv maintains, by default, a similar\nnumber of model parameters and requirements in computational resources, as shown in Equation 1.\nFurthermore, PyConv offers a high degree of parallelism due to the fact that the pyramid levels can\nbe independently computed in parallel. Thus, PyConv can also offer the possibility of customizable\nheav",
  "output": "The text discusses the efficiency of PyConv in comparison to standard convolution. It mentions that PyConv maintains a similar number of model parameters and computational resources requirements. It also highlights the high degree of parallelism offered by PyConv, as the pyramid levels can be computed independently in parallel, providing the potential for customization.",
  "summary_type": "reports"
}{
  "input": "given sufﬁcient pre-training data) it is more efﬁcient to\ntrain larger models with a shorter epoch budget than to train\nsmaller models for longer, consistent with the observations\nin (Kaplan et al., 2020).\nWe ﬁne-tune NFNet models for 15,000 steps at a batch\nsize of 2048 using a learning rate of 0.1, which is warmed\nup from zero over 5000 steps, then annealed to zero with\ncosine decay through the rest of training. We use SAM with\nρ = 0.05, weight decay of 10−5, a DropOut rate of 0.25, and\na stoc",
  "output": "The text discusses the efficiency of training larger models with a shorter epoch budget compared to smaller models over a longer period, as observed in Kaplan et al., 2020. It also details the fine-tuning of NFNet models for 15,000 steps at a batch size of 2048 using a learning rate of 0.1. This rate is warmed up from zero over 5000 steps, then annealed to zero with cosine decay through the rest of the training. SAM with ρ = 0.05, weight decay of 10−5, a DropOut rate of 0.25, and a stochastic element are used.",
  "summary_type": "reports"
}{
  "input": "the PSA layer can achieve a strictly convolutional\nattention map by setting the centers of attention ∆h to\nPatches\nClass\ntoken\nImage \nembedding\nNonlocal \nGPSA\nFFN\nSA\nFFN\nSA\nFFN\nLocal \n𝑊𝑞𝑟𝑦\n𝑊𝑘𝑒𝑦\n𝑣𝑝𝑜𝑠\n𝑟𝑖𝑗\n∗\n∗\nsoftmax\nsoftmax\n+\n1 −𝜎(𝜆)\n𝜎(𝜆)\nnormalize\nGated Positional\n Self-Attention\nConViT\n𝑋𝑖\n𝑋𝑗\n𝐴𝑖𝑗\nGPSA\nFFN\nFigure 4. Architecture of the ConViT. The ConViT (left) is a ver-\nsion of the ViT in which some of the self-attention (SA) layers\nare replaced with gated positional self-attention layers (GPSA",
  "output": "The ConViT architecture is a version of the ViT where some self-attention (SA) layers are replaced with gated positional self-attention layers (GPSA). The PSA layer can create a strictly convolutional attention map by setting the centers of attention ∆h.",
  "summary_type": "reports"
}{
  "input": "s prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 × 4 downsized version of the 16 × 16\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\nwell, though L2 was slightly worse. We report ﬁnal results only for option 1) because it has",
  "output": "During pretraining, three prediction settings were tested: 1) predicting only the mean with 3bit color, 2) predicting a downsized version of the 16x16 patch with 3bit colors in parallel, and 3) regression on the full patch using L2. All methods performed well, though L2 was slightly worse. Final results were reported only for the first option.",
  "summary_type": "reports"
}{
  "input": "rd. Are we done with\nimagenet? CoRR, abs/2006.07159, 2020. 6\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV (1), volume\n12346 of Lecture Notes in Computer Science, pages 213–\n229. Springer, 2020. 1, 2\n[6] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. CoRR,\nabs/2",
  "output": "The text refers to various scholarly works. One is titled \"Are we done with imagenet?\" published in CoRR, abs/2006.07159, 2020. Another work by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko is about end-to-end object detection with transformers, presented in ECCV (1), volume 12346 of Lecture Notes in Computer Science, pages 213–229 by Springer, 2020. The last mentioned work by Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao is about a pre-trained image processing transformer, published in CoRR.",
  "summary_type": "reports"
}{
  "input": "Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[11] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer. Multiscale vision\ntransformers. arXiv preprint arXiv:2104.11227, 2021.\n[12] C. Feichtenhofer. X3D: expanding architectures for efﬁcient video recognition. In Proceeding",
  "output": "The text references three academic papers: \"An image is worth 16x16 words: Transformers for image recognition at scale\" by Kolesnikov et al., published on arXiv in 2020; \"Multiscale vision transformers\" by Fan et al., also published on arXiv in 2021; and \"X3D: expanding architectures for efficient video recognition\" by Feichtenhofer, with no specified publication details.",
  "summary_type": "reports"
}{
  "input": "stafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929,\n2020.\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi\nAzar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent:\nA new approach to self-supervised learning",
  "output": "The text mentions two academic papers. The first one, \"An image is worth 16x16 words: Transformers for image recognition at scale\" by Stafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and others, was published as a preprint on arXiv in 2020. The second paper, \"Bootstrap your own latent: A new approach to self-supervised learning\" is authored by Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko.",
  "summary_type": "reports"
}{
  "input": "------------\nRevisiting ResNets: Improved Training and Scaling Strategies\nHu, H., Zhang, Z., Xie, Z., and Lin, S.\nLocal rela-\ntion networks for image recognition.\narXiv preprint\narXiv:1904.11491, 2019.\nHu, J., Shen, L., and Sun, G. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pp. 7132–7141,\n2018.\nHuang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger,\nK. Q. Deep networks with stochastic depth. In European\nconference on com",
  "output": "The text lists several academic references related to image recognition and deep networks. These include \"Revisiting ResNets: Improved Training and Scaling Strategies\" by Hu, Zhang, Xie, and Lin, \"Local relation networks for image recognition\" by Hu, Shen, and Sun, and \"Deep networks with stochastic depth\" by Huang, Sun, Liu, Sedra, and Weinberger.",
  "summary_type": "reports"
}{
  "input": "phase. Figure 2 provides an intuitive interpretation. Given the output\npatch tokens X1, ..., XN and the corresponding labels [y1, ..., yN], the token labeling objective can\nbe deﬁned as\nLtl = 1\nN\nN\nX\ni=1\nH(Xi, yi).\n(2)\nRecall that H is the cross-entropy loss. Therefore, the total loss function can be written as\nLtotal = H(Xcls, ycls) + β · Ltl,\n(3)\n= H(Xcls, ycls) + β · 1\nN\nN\nX\ni=1\nH(Xi, yi),\n(4)\nwhere β is a hyper-parameter to balance the two terms. In our experiment, we empirically set it to\n0",
  "output": "The text discusses a token labeling objective defined as Ltl = 1/N * Σ H(Xi, yi) from i=1 to N, where H is the cross-entropy loss. The total loss function is written as Ltotal = H(Xcls, ycls) + β * Ltl, which can be expanded to H(Xcls, ycls) + β * 1/N * Σ H(Xi, yi) from i=1 to N. Here, β is a hyper-parameter used to balance the two terms, which is empirically set to 0 in the experiment.",
  "summary_type": "reports"
}{
  "input": "ent. To\nadjust for the staleness of their gradients, we need to reweigh\nthe gradient properly as explained later.\nOur implementation maintains two queues: a queue\nqunﬁnished of unﬁnished jobs and a queue qﬁnished of ﬁnished\njobs. Whenever the qunﬁnished contains less than its capacity\nC, the controller generates n = C −|qunﬁnished| new dropout\npatterns r1, r2, ..., rn and ﬁlls up qunﬁnished with the pairs\n(ri, P(ri; θi)), where θi is the value of the controller’s param-\neters at the time ri is s",
  "output": "The text discusses an implementation that maintains two queues: one for unfinished jobs (qunﬁnished) and one for finished jobs (qﬁnished). When qunﬁnished is less than its capacity (C), the controller generates new dropout patterns and fills up qunﬁnished with pairs of dropout patterns and the value of the controller's parameters. The gradients' staleness is adjusted by reweighing them properly.",
  "summary_type": "reports"
}{
  "input": "y and makes\nlearning easier. We also notice that the temperature usually\nquickly decrease to only around 1.2x of the converged\nvalues in the ﬁrst 100k steps, and then slowly converges\nuntil the end of training.\nTable 8. Ablation study of key architecture parameters. Baseline\nmodel (ﬁrst row) is trained with embedding dimension 640, using\nall negatives in the batch, and a learnable softmax temperature.\nModel\nMSCOCO\nImangeNet KNN\nI2T R@1\nT2I R@1\nR@1\nB5 + BERT-base\n51.7\n37.5\n64.6\nw/ embedding dim=3",
  "output": "The text discusses a learning process where the temperature decreases to around 1.2x of the converged values in the first 100k steps, then slowly converges until the end of training. An ablation study of key architecture parameters is mentioned, with a baseline model trained with an embedding dimension of 640, using all negatives in the batch, and a learnable softmax temperature. The model's performance on MSCOCO, ImageNet KNN, I2T R@1, and T2I R@1 is also mentioned, with a specific model (B5 + BERT-base) achieving certain scores.",
  "summary_type": "reports"
}{
  "input": "n addition, the local context modeling of\nthe proposed Convolutional Projection compensates for the\nloss of information incurred by resolution reduction.\n3.3. Methodological Discussions\nRemoving Positional Embeddings:\nThe introduction of\nConvolutional Projections for every Transformer block,\ncombined with the Convolutional Token Embedding, gives\nus the ability to model local spatial relationships through the\nnetwork. This built-in property allows dropping the position\nembedding from the network",
  "output": "The text discusses the use of Convolutional Projections in every Transformer block, along with Convolutional Token Embedding, to model local spatial relationships in a network. This method compensates for information loss due to resolution reduction and allows the removal of position embedding from the network.",
  "summary_type": "reports"
}{
  "input": "rk, we revisit the design of the spatial attention in vision\ntransformers. Our ﬁrst ﬁnding is that the global sub-sampled attention in PVT is highly effective,\nand with the applicable positional encodings [9], its performance can be on par or even better\nthan state-of-the-art vision transformers (e.g., Swin). This results in our ﬁrst proposed architecture,\ntermed Twins-PCPVT. On top of that, we further propose a carefully-designed yet simple spatial\nattention mechanism, making our architectures",
  "output": "The text discusses a revision of the spatial attention design in vision transformers. It highlights the effectiveness of the global sub-sampled attention in PVT, which, with the right positional encodings, can match or surpass the performance of leading vision transformers like Swin. This led to the creation of a new architecture called Twins-PCPVT. Additionally, a new spatial attention mechanism was proposed to enhance the architecture.",
  "summary_type": "reports"
}{
  "input": "ROUGE-1\nROUGE-2\nROUGE-L\nTransformer\n523.2\n192\n32.987\n15.286\n30.771\nOurs-Random\n262.9\n230\n30.310\n12.752\n27.823\nOurs-MP\n262.9\n230\n34.651\n16.468\n32.415\nTable 1: Results on Gigaword text summarization task (MBS=Maximal Batch Size, MP = Mean-Pooling). The MACs\n(Multiply-add ACcumulation) is only computed for the encoder, assuming the length of sequence is 45 (the maximal\nlength of sequence in the dataset). Though centroid transformer with random initialization (Ours-Random) performs\nworse than the ba",
  "output": "The text presents results on the Gigaword text summarization task. The Transformer model scored 523.2, 192, 32.987, 15.286, 30.771. The model with random initialization (Ours-Random) scored 262.9, 230, 30.310, 12.752, 27.823. The Mean-Pooling model (Ours-MP) scored 262.9, 230, 34.651, 16.468, 32.415. The Multiply-add Accumulation (MACs) is only calculated for the encoder, assuming a sequence length of 45.",
  "summary_type": "reports"
}{
  "input": "sion alignment.\nFinally, the feature maps are added with patch embeddings,\nas shown in Fig. 2(b). When fed back from the transformer\nbranch to the CNN branch, the patch embeddings require\nto be up-sampled (Fig. 2(a)) to align the spatial scale. The\nchannel dimension is then aligned with that of CNN feature\nmaps through the 1×1 convolution, and added to the fea-\nResNet-101\n(a) Class activation maps\nDeiT-S\nOurs-CNN\nOurs-Trans.\n(b) Attention maps\nFigure 4: Feature analysis. (a) Class activation map",
  "output": "The text discusses a process where feature maps are combined with patch embeddings, as depicted in Fig. 2(b). The patch embeddings need to be up-sampled to match the spatial scale when they are returned from the transformer branch to the CNN branch. The channel dimension is then aligned with the CNN feature maps using a 1x1 convolution before being added to the feature. The text also mentions a feature analysis involving class activation maps and attention maps.",
  "summary_type": "reports"
}{
  "input": "95.07\n96.03\nSAM-ﬁnal\n99.65\n97.10\n95.96\n96.18\nALIGN\n99.65\n96.19\n96.13\n95.88\n6. Ablation Study\nIn the ablation study, we compare model performance\nmostly on MSCOCO zero-shot retrieval and ImageNet K-\nNearest-neighbor (KNN) tasks.3 We ﬁnd these two met-\n2ViT (Dosovitskiy et al., 2021) uses different hyper-parameters\nfor different tasks and hence is not included in comparison.\n3For each image in the validation set of ImageNet, we retrieve\nits nearest neighbors from the training set w/ pre-trained i",
  "output": "The text discusses an ablation study comparing model performance primarily on MSCOCO zero-shot retrieval and ImageNet K-Nearest-neighbor (KNN) tasks. The study retrieves the nearest neighbors from the training set of ImageNet for each image in the validation set. ViT (Dosovitskiy et al., 2021) is not included in the comparison due to its use of different hyper-parameters for different tasks.",
  "summary_type": "reports"
}{
  "input": "A ∈RN×N and Aij represents for the attention\nscore between the Qi and Kj. Then, the self-attention op-\neration is applied on the value vectors to produce an output\nmatrix\nO = AV,\n(11)\nwhere O ∈RN×Dh. For a multi-head self-attention layer\nwith D/Dh heads, the outputs can be calculated by a linear\nprojection for the concatenated self-attention outputs\nX\n′ = [O1; O2; ...; OD/Dh]Wproj,\n(12)\nwhere Wproj ∈RD×D is a learnable parameter and [·] de-\nnotes the concatenation operation.\nS1.2. Position-wise",
  "output": "The text discusses a self-attention operation applied on value vectors to produce an output matrix (O = AV), where A represents the attention score between Qi and Kj. For a multi-head self-attention layer with D/Dh heads, the outputs are calculated by a linear projection for the concatenated self-attention outputs (X′ = [O1; O2; ...; OD/Dh]Wproj), where Wproj is a learnable parameter and [·] denotes the concatenation operation.",
  "summary_type": "reports"
}{
  "input": "ization performance\n(c) Model performance\nFigure 3. Effects of the spatial dimensions in vision transformer (ViT) [9]. We compare our Pooling-based Vision Transformer (PiT)\nwith original ViT at various aspects. PiT outperforms ViT in capability, generalization performance, and model performance.\nSpatial tokens\nClass token\n(𝑤×ℎ)×𝑑\n1×𝑑\n𝑤×ℎ×𝑑\nReshape\n𝑤\n2 × ℎ\n2 ×2𝑑\nDepth-wise\nConvolution\nReshape\nSpatial tokens\n(𝑤\n2 × ℎ\n2)×2𝑑\n1×2𝑑\nFully-connected layer\nClass token\nFigure 4. Pooling layer of PiT archi",
  "output": "The text discusses the comparison between the Pooling-based Vision Transformer (PiT) and the original Vision Transformer (ViT). PiT is shown to outperform ViT in terms of capability, generalization performance, and model performance. The text also describes the pooling layer of the PiT architecture.",
  "summary_type": "reports"
}{
  "input": "a factor of r. Our superpixel operation bears sim-\nilarity to the concept of tiled convolution [28], a particular\nrealization of locally connected layers. This idea has also\nbeen particularly effective for image super-resolution [35]\nin the form of “subpixel\" convolution.\n3.2. Channel Multiplexing\nWhile the spatial multiplexing operation described above\nis effective, it still suffers from some limitations. Firstly, the\ngroup convolutions in spatial multiplexing are more com-\nputationally expens",
  "output": "The text discusses a superpixel operation that is similar to the concept of tiled convolution, a type of locally connected layers. This idea has been effective for image super-resolution through \"subpixel\" convolution. The text also mentions channel multiplexing, a spatial multiplexing operation that, despite its effectiveness, has limitations due to the computational expense of group convolutions.",
  "summary_type": "reports"
}{
  "input": "n, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\ntion. arXiv, 2020.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens van der Maaten.\nExploring the limits of weakly supervised\npretraining. In ECCV,",
  "output": "Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf published a paper on object-centric learning with slot attention in 2020. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee presented ViLBERT, a pretraining task-agnostic visiolinguistic representation for vision-and-language tasks, at NeurIPS in 2019. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten explored the limits of weakly supervised pretraining at ECCV.",
  "summary_type": "reports"
}{
  "input": "ford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019. 6\n[8] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi.\nOn the relationship between self-attention and convolutional\nlayers. arXiv preprint arXiv:1911.03584, 2019. 2\n[9] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmentation\nstrategies from data. In Proceedings of the IEEE conference\non computer vision and pattern",
  "output": "The text mentions three academic papers. The first, by Ford and Sutskever, is titled \"Generating long sequences with sparse transformers\" and was published on arXiv in 2019. The second paper, by Cordonnier, Loukas, and Jaggi, is titled \"On the relationship between self-attention and convolutional layers\" and was also published on arXiv in 2019. The third paper, by Cubuk, Zoph, Mane, Vasudevan, and Le, is titled \"Autoaugment: Learning augmentation strategies from data\" and was presented at the IEEE conference on computer vision and pattern.",
  "summary_type": "reports"
}{
  "input": "object detection\nframeworks: Cascade Mask R-CNN [29, 6], ATSS [79],\nRepPoints v2 [12], and Sparse RCNN [56] in mmdetec-\ntion [10]. For these four frameworks, we utilize the same\nsettings: multi-scale training [8, 56] (resizing the input such\nthat the shorter side is between 480 and 800 while the longer\nside is at most 1333), AdamW [44] optimizer (initial learn-\ning rate of 0.0001, weight decay of 0.05, and batch size of\n16), and 3x schedule (36 epochs). For system-level compar-\nison, we adopt a",
  "output": "The text discusses the use of four object detection frameworks: Cascade Mask R-CNN, ATSS, RepPoints v2, and Sparse RCNN in mmdetection. The same settings are applied across these frameworks, including multi-scale training (resizing the input so the shorter side is between 480 and 800 while the longer side is at most 1333), the AdamW optimizer (with an initial learning rate of 0.0001, weight decay of 0.05, and batch size of 16), and a 3x schedule (36 epochs). These settings are used for a system-level comparison.",
  "summary_type": "reports"
}{
  "input": "which alternates be-\ntween two partitioning conﬁgurations in consecutive Swin\nTransformer blocks.\nAs illustrated in Figure 2, the ﬁrst module uses a regular\nwindow partitioning strategy which starts from the top-left\npixel, and the 8 × 8 feature map is evenly partitioned into\n2 × 2 windows of size 4 × 4 (M = 4). Then, the next mod-\nule adopts a windowing conﬁguration that is shifted from\nthat of the preceding layer, by displacing the windows by\n(⌊M\n2 ⌋, ⌊M\n2 ⌋) pixels from the regularly partitio",
  "output": "The text describes a process that alternates between two partitioning configurations in consecutive Swin Transformer blocks. The first module uses a regular window partitioning strategy starting from the top-left pixel, partitioning the 8x8 feature map into 2x2 windows of size 4x4. The next module uses a windowing configuration that is shifted from the preceding layer, displacing the windows by (⌊M/2⌋, ⌊M/2⌋) pixels from the regular partition.",
  "summary_type": "reports"
}{
  "input": "(Imagenet-21k and JFT). We param-\neterize V (x) = v(x)1⊺\nR ⊙V where v(x) is a vector of\ndimension R, 1R is a vector of ones of dimension R and V\nis a K×R matrix of learnable parameters which is not a func-\ntion of x. Sampling the correlated noise component can be\nsimpliﬁed, V (x)ϵR = (v(x)1⊺\nK ⊙V )ϵR = v(x) ⊙(V ϵR).\nThe total parameter count of this parameter-efﬁcient ver-\nsion is O(DK + KR) which typically reduces the memory\nand computational requirements dramatically for large-scale\nAlgorithm",
  "output": "The text discusses a parameterization method for V(x) in the context of large-scale algorithms. V(x) is defined as a vector of dimension R, and V is a K×R matrix of learnable parameters. The total parameter count of this efficient version is O(DK + KR), which significantly reduces memory and computational requirements.",
  "summary_type": "reports"
}{
  "input": "bels assigned to the cue conﬂict images here will be\nexclusively based on the texture information, e.g., the image of chimpanzee shape but with lemon\ntexture will be labelled as lemon, shown in Figure 2(b). By this way, the texture information is highly\nrelated to the “ground-truth” while the shape information only serves as a nuisance factor during\nlearning. Similarly, to learn a shape-biased model, the label assignment of cue conﬂict images will\nbe based on shape only, e.g., the image of chimp",
  "output": "The labels for cue conflict images will be based solely on texture information, such as an image with the shape of a chimpanzee but the texture of a lemon being labelled as a lemon. This makes texture information highly relevant to the \"ground truth\", while shape information is just a nuisance factor during learning. Conversely, to learn a shape-biased model, labels for cue conflict images will be based only on shape.",
  "summary_type": "reports"
}{
  "input": "omputation in self-attention. Several methods have been proposed\nto make Transformer computations more efﬁcient for high resolution input. Reformer [32], Clus-\nterform [57], Adaptive Clustering Transformer [73] and Asymmetric Clustering [10] propose to\nuse Locality Sensitivity Hashing to cluster keys or queries and reduce quadratic computation into\nlinear computation. Lightweight convolution [62] explore convolution architectures for replacing\nTransformers but only explore applications in NLP. R",
  "output": "Several methods have been proposed to improve the efficiency of Transformer computations for high resolution input. These include Reformer, Clusterform, Adaptive Clustering Transformer, and Asymmetric Clustering, which use Locality Sensitivity Hashing to cluster keys or queries and reduce quadratic computation to linear computation. Lightweight convolution explores convolution architectures as a replacement for Transformers, but only in NLP applications.",
  "summary_type": "reports"
}{
  "input": "t\narXiv:2103.12424, 2021. 8\n[33] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom\nGoldstein.\nVisualizing the loss landscape of neural nets.\nNeurIPS, 2017. 5\n[34] Liam Li and Ameet Talwalkar. Random search and repro-\nducibility for neural architecture search. In UAI, 2019. 3,\n8\n[35] Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Yuwen\nXiong, Rui Hu, and Raquel Urtasun. Polytransform: Deep\npolygon transformer for instance segmentation. In CVPR,\n2020. 8\n[36] Paul Michel, Omer Levy, and Gra",
  "output": "The text includes references to various research papers. [33] discusses visualizing the loss landscape of neural nets, authored by Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein, presented at NeurIPS in 2017. [34] is a paper by Liam Li and Ameet Talwalkar on random search and reproducibility for neural architecture search, presented at UAI in 2019. [35] is a paper titled \"Polytransform: Deep polygon transformer for instance segmentation\" by Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Yuwen Xiong, Rui Hu, and Raquel Urtasun, presented at CVPR in 2020. The text ends with the beginning of another reference [36] by Paul Michel, Omer Levy, and Gra.",
  "summary_type": "reports"
}{
  "input": "under 330 FLOPs),\nwe just ditch it for good.\nBesides, evolutionary algorithms need to initialize pop-\nulation with size Npop before implementing iterative muta-\ntion and crossover. Current methods usually random sam-\nple Npop paths under the constraint as initial population. In\ncontrast, our method makes the initialization with the help\nof candidate pool P, and select its Top-Npop paths instead.\nAs Figure 3 shows, searching with candidate pool can boost\nthe evolutionary performance for supplyin",
  "output": "The text discusses a method of initializing population size (Npop) in evolutionary algorithms. Current methods typically random sample Npop paths under a constraint as the initial population. However, the text suggests a different approach, which uses a candidate pool (P) to initialize and selects its Top-Npop paths. This method, as shown in Figure 3, can enhance the evolutionary performance.",
  "summary_type": "reports"
}{
  "input": "x\n64\n128\n256\n512\n1024\n2048\n4096\nRegNetX-16GF\ndi = [2, 6, 13, 1]\nwi = [256, 512, 896, 2048]\ng = 128, b = 1, e = 20.0%\nwa = 56, w0 = 216, wm = 2.1\n0\n3\n6\n9\n12\n15\n18\n21\nblock index\n64\n128\n256\n512\n1024\n2048\n4096\nRegNetX-32GF\ndi = [2, 7, 13, 1]\nwi = [336, 672, 1344, 2520]\ng = 168, b = 1, e = 19.5%\nwa = 70, w0 = 320, wm = 2.0\nﬂops\nparams\nacts\nbatch\ninfer\ntrain\nerror\n(B)\n(M)\n(M)\nsize\n(ms)\n(hr)\n(top-1)\nREGNETX-200MF\n0.2\n2.7\n2.2\n1024\n10\n2.8\n31.1±0.09\nREGNETX-400MF\n0.4\n5.2\n3.1\n1024\n15\n3.9\n27.3±0.15\nREGNETX",
  "output": "The text provides specifications for three versions of RegNetX: 16GF, 32GF, and 200MF/400MF. For RegNetX-16GF, di = [2, 6, 13, 1], wi = [256, 512, 896, 2048], g = 128, b = 1, e = 20.0%, wa = 56, w0 = 216, wm = 2.1. For RegNetX-32GF, di = [2, 7, 13, 1], wi = [336, 672, 1344, 2520], g = 168, b = 1, e = 19.5%, wa = 70, w0 = 320, wm = 2.0. For RegNetX-200MF, the parameters are 0.2 flops, 2.7M params, 2.2M acts, 1024 batch size, 10ms infer, 2.8hr train, 31.1±0.09 top-1 error. For RegNetX-400MF, the parameters are 0.4 flops, 5.2M params, 3.1M acts, 1024 batch size, 15ms infer, 3.9hr train, 27.3±0.15 top-1 error.",
  "summary_type": "reports"
}{
  "input": "able parameters. Therefore, with\none attention layer and one single stronger MoE layer learn-\ning complex representations, and independent normalization\nlayers for diversiﬁed semantic representations, going wider\ninstead of deeper is a more parameter-efﬁcient and effective\nframework.\nCompared with simply scaling along the width, going\nwider instead of deeper is a more parameter-efﬁcient frame-\nwork, which makes the models small enough to be adapted\nto downstream tasks without advanced parallelis",
  "output": "The text discusses a framework that uses one attention layer and one single stronger MoE layer for learning complex representations, with independent normalization layers for diversified semantic representations. This framework is more parameter-efficient when it is widened rather than deepened. This approach makes the models small enough to be adapted to downstream tasks without requiring advanced parallelism.",
  "summary_type": "reports"
}{
  "input": "ed Task Papers,\npp. 304–323, 2018.\nBossard, L., Guillaumin, M., and Van Gool, L. Food-101 –\nmining discriminative components with random forests.\nIn European Conference on Computer Vision, 2014.\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P.,\nand Joulin, A. Unsupervised learning of visual features\nby contrasting cluster assignments. In Advances in Neural\nInformation Processing Systems, 2020.\nChen, J., Hu, H., Wu, H., Jiang, Y., and Wang, C. Learning\nthe best pooling strategy for vis",
  "output": "The text mentions several academic papers related to computer vision and machine learning. Bossard, Guillaumin, and Van Gool's paper discusses mining discriminative components with random forests in the context of food recognition. Caron, Misra, Mairal, Goyal, Bojanowski, and Joulin's paper focuses on unsupervised learning of visual features by contrasting cluster assignments. Chen, Hu, Wu, Jiang, and Wang's paper is about learning the best pooling strategy for visual data.",
  "summary_type": "reports"
}{
  "input": "compare to their best EfﬁcientNet-L2 image encoder.\n3.1\nMain results\nTable 2 presents comparison of the largest Mixer models to state-of-the-art models from the literature.\n“ImNet” and “ReaL” columns refer to the original ImageNet validation [13] and cleaned-up ReaL [5]\n4\n\n--------------------------------------------------\nTable 2: Transfer performance, inference throughput, and training cost. The rows are sorted by\ninference throughput (ﬁfth column). Mixer has comparable transfer accuracy to s",
  "output": "The text discusses a comparison of the largest Mixer models to other top models from literature. The comparison is presented in Table 2, which includes transfer performance, inference throughput, and training cost. The rows in the table are sorted by inference throughput. The Mixer models have comparable transfer accuracy.",
  "summary_type": "reports"
}{
  "input": "ention,\nwe have explored different sizes of the latent layer, by adjusting the expansion factor e in the sequence:\nlinear N × e × N — GELU — linear e × N × N. For this experiment we used average pooling to\naggregating the patches before the classiﬁcation layer.\nexpansion factor ×e\n×0.25\n×0.5\n×1\n×2\n×3\n×4\nImnet-val top-1 acc.\n78.6\n79.2\n79.2\n79.3\n78.8\n78.8\nWe observe that a large expansion factor is detrimental in the patch communication, possibly because\nwe should not introduce too much capacity i",
  "output": "The text discusses an experiment where different sizes of the latent layer were explored by adjusting the expansion factor e in a sequence. Average pooling was used to aggregate patches before the classification layer. The results show that a large expansion factor negatively impacts patch communication, suggesting that too much capacity should not be introduced.",
  "summary_type": "reports"
}{
  "input": "nction for the\nInplace-ABN, we chose to use Leaky-ReLU instead of\nResNet50’s plain ReLU.\nUsing Inplace-ABN in TResNet models offers the fol-\nlowing advantages:\n• BatchNorm layers are major consumers of GPU mem-\nory. Replacing BatchNorm layers with Inplace-ABN\nenables to signiﬁcantly increase the maximal possible\nbatch size, which can improve the GPU utilization.\n• For TResNet models, Leaky-ReLU provides better ac-\ncuracy than plain ReLU. While some modern activa-\ntion, like Swish and Mish [28],",
  "output": "Inplace-ABN was chosen over ResNet50's plain ReLU for TResNet models due to its advantages. These include reducing GPU memory consumption by replacing BatchNorm layers, thereby increasing the maximum possible batch size and improving GPU utilization. Additionally, Leaky-ReLU has been found to provide better accuracy than plain ReLU for TResNet models.",
  "summary_type": "reports"
}{
  "input": "eng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115(3):211–252, 2015. 2, 6, 8\n[50] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nattention with relative position representations. arXiv preprint\narXiv:1803.02155, 2018. 2, 3\n[51] Laurent Sifre. Rigid-motion scattering for image classiﬁca-\ntion. Ph. D. thesis, 2014.",
  "output": "The text refers to three different academic works: \n\n1. \"Imagenet large scale visual recognition challenge\" by Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al., published in the International Journal of Computer Vision in 2015. \n\n2. \"Self-attention with relative position representations\" by Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani, a preprint on arXiv from 2018. \n\n3. \"Rigid-motion scattering for image classification\", a Ph.D. thesis by Laurent Sifre from 2014.",
  "summary_type": "reports"
}{
  "input": "0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncumulative prob.\nflops=800M\n[35.8|44.5] AnyNetXA\n[35.1|38.5] AnyNetXE\n[34.6|36.8] RegNetX\n30\n35\n40\n45\n50\n55\n60\nepochs=50\n[30.0|38.8] AnyNetXA\n[30.0|32.5] AnyNetXE\n[29.4|31.5] RegNetX\n40\n45\n50\n55\n60\n65\n70\nstages=5\n[40.4|49.9] AnyNetXA\n[38.4|42.8] AnyNetXE\n[37.9|41.4] RegNetX\n45\n50\n55\n60\n65\n70\nerror\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncumulative prob.\n[42.9|53.1] AnyNetRA\n[42.0|45.7] AnyNetRE\n[41.9|44.3] RegNetR\n50\n60\n70\n80\nerror\n[47.0|61.2] AnyNetVA\n[46.8|56.4] AnyNetVE\n[46.0|49.2] Re",
  "output": "The text appears to be data or results from some sort of experiment or test. It includes values for cumulative probability, flops, epochs, stages, and error for different models or versions named AnyNetXA, AnyNetXE, RegNetX, AnyNetRA, AnyNetRE, and RegNetR. The models are associated with different numerical values in brackets, possibly indicating a range or two different measurements.",
  "summary_type": "reports"
}{
  "input": "tion\nfrom a sequence-to-sequence perspective with transformers. In CVPR, 2021. 3\n[35] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi\nFeng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021. 3\n[36] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159,\n2020. 1\n12\n\n--------------------------------------",
  "output": "The text discusses various research papers on transformers in the field of computer vision. The first paper discusses sequence-to-sequence perspective with transformers, presented in CVPR 2021. The second paper by Daquan Zhou et al., titled \"Deepvit: Towards deeper vision transformer\", is a preprint on arXiv. The third paper by Xizhou Zhu et al., titled \"Deformable detr: Deformable transformers for end-to-end object detection\", is also a preprint on arXiv.",
  "summary_type": "reports"
}{
  "input": ", 7, 8\n[14] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Jun-\nyuan Xie, and Mu Li. Bag of tricks for image classiﬁca-\ntion with convolutional neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2019. 7\n[15] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\nhardt, and Dawn Song. Natural adversarial examples. arXiv\npreprint arXiv:1907.07174, 2019. 8\n[16] Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon\nHan, Sangdoo Yun, Gyu",
  "output": "The text mentions three different studies. The first one by Tong He et al., presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) in June 2019, discusses image classification with convolutional neural networks. The second study by Dan Hendrycks et al., titled \"Natural adversarial examples\", was published as an arXiv preprint in 2019. The third study involves Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, and Sangdoo Yun, but the text does not provide further details about this study.",
  "summary_type": "reports"
}{
  "input": "-long-term-dependency-language-modeling-dataset/\n5https://github.com/pytorch/fairseq/tree/master/examples/quant noise\n\n--------------------------------------------------\nThe dataset has been obtained from the torchvision package6. The input images are augmented with a random crop of\nsize 32 with padding of 4, and a random horizontal ﬂip. The RGB pixel values are normalized to mean 0 and standard deviation\n1. We use the default split between train and valid as obtained from the torchvision packag",
  "output": "The dataset, sourced from the torchvision package, includes input images that have been augmented with a random crop of size 32 with padding of 4, and a random horizontal flip. The RGB pixel values in the images are normalized to a mean of 0 and standard deviation of 1. The default split between training and validation data is used as provided by the torchvision package.",
  "summary_type": "reports"
}{
  "input": "-\nnamic convolutions. arXiv preprint arXiv:1901.10430, 2019.\n3\n[39] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. Lite transformer with long-short range attention. arXiv\npreprint arXiv:2004.11886, 2020. 3, 4\n[40] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-\ning Guo. Learning texture transformer network for image\nsuper-resolution. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n5791–5800, 2020. 2\n[41] Li Yuan, Yunpeng Chen, T",
  "output": "The text mentions several academic papers. The first one discusses dynamic convolutions and is a preprint on arXiv from 2019. The second paper, also a preprint on arXiv from 2020, is about a Lite transformer with long-short range attention. The third paper, presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition, is about learning texture transformer network for image super-resolution. The text also begins to mention a fourth paper by Li Yuan, Yunpeng Chen, and T, but the details are incomplete.",
  "summary_type": "reports"
}{
  "input": "an Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[6] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to\nvision transformers. arXiv preprint arXiv:2104.05707, 2021.\n[7] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv",
  "output": "An Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo published a preprint on arXiv titled \"Swin transformer: Hierarchical vision transformer using shifted windows\" in 2021. Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool also published a preprint on arXiv in 2021 titled \"Localvit: Bringing locality to vision transformers\". Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang published a preprint introducing convolutions to vision transformers, titled \"CVT\".",
  "summary_type": "reports"
}{
  "input": ") + ψ(y, x)) + γ(φ(x, y) + φ(y, x))\n= (βψ + γφ)(x, y) + (βψ + γφ)(y, x).\nA.2. Proof of Proposition 5\nProposition 5. Algorithm 1 converges to a local-minimum\nwith respect to the update rule at most n(n−1)/2+1 steps.\nProof. Let C(0) has a minimum at (i1, j1). Then, ∀t =\n0, 1, ..., Π(t)\nwin[i1, j1]\n=\n1.\nNext, let’s deﬁne I2\n=\n{(i, j)|i ̸= i1, j ̸= j1} and (i2, j2) = argmin(i,j)∈I2\nC(1)[i, j]. If Π(0)[i2, j1] = 1, C(0)[i2, j1] will be added\nby the large value, and thus, Π(1)\nwin[i2, j2] = 1. Otherwi",
  "output": "Proposition 5 states that Algorithm 1 converges to a local minimum with respect to the update rule in at most n(n−1)/2+1 steps. The proof begins by assuming a minimum at (i1, j1) for C(0). Then, it defines I2 as the set of all (i, j) not equal to (i1, j1) and (i2, j2) as the argument minimum of C(1)[i, j] over I2. If Π(0)[i2, j1] = 1, C(0)[i2, j1] will increase by a large value, making Π(1)win[i2, j2] = 1.",
  "summary_type": "reports"
}{
  "input": "n, 2016. 8\n[51] Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia\nGkioxari, and Kaiming He. Data distillation: Towards omni-\n10\n\n--------------------------------------------------\nsupervised learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2018. 1, 8\n[52] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V\nLe. Regularized evolution for image classiﬁer architecture\nsearch. In Proceedings of the AAAI conference on artiﬁcial\nintelligence, volume 3",
  "output": "In 2018, Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, and Kaiming He presented a study on data distillation towards omni-supervised learning at the IEEE Conference on Computer Vision and Pattern Recognition. In another study, Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le discussed regularized evolution for image classifier architecture search at the AAAI conference on artificial intelligence.",
  "summary_type": "reports"
}{
  "input": "Model Size\nImage Size\nImageNet\nTraining from scratch (i.e., random initialization)\nViT384-B (Dosovitskiy et al., 2020)\n86M\n3842\n77.9\nViT384-L (Dosovitskiy et al., 2020)\n307M\n3842\n76.5\nDeiT-B (Touvron et al., 2020)\n86M\n2242\n81.8\nDeiT384-B (Touvron et al., 2020)\n86M\n3842\n83.1\nSupervised Pre-Training on ImageNet-22K (using labeled data)\nViT384-B (Dosovitskiy et al., 2020)\n86M\n3842\n84.0\nViT384-L (Dosovitskiy et al., 2020)\n307M\n3842\n85.2\nSelf-Supervised Pre-Training on ImageNet-1K (without labeled da",
  "output": "The text presents data on different models and their performance. ViT384-B and ViT384-L (Dosovitskiy et al., 2020) models, trained from scratch, have 86M and 307M sizes respectively, both with an image size of 3842, and achieved 77.9 and 76.5 scores respectively. DeiT-B and DeiT384-B (Touvron et al., 2020) models, also with 86M size and image sizes of 2242 and 3842 respectively, scored 81.8 and 83.1. When supervised pre-training on ImageNet-22K was used, ViT384-B and ViT384-L models achieved scores of 84.0 and 85.2 respectively.",
  "summary_type": "reports"
}{
  "input": "reasonable performance on ImageNet-1K [14] alone. More importantly, when\npre-trained on large-scale weakly labeled JFT-300M dataset [15], ViT achieves comparable results\nto state-of-the-art (SOTA) ConvNets, indicating that Transformer models potentially have higher\ncapacity at scale than ConvNets.\nWhile ViT has shown impressive results with enormous JFT 300M training images, its performance\nstill falls behind ConvNets in the low data regime. For example, without extra JFT-300M pre-training,\nthe",
  "output": "The text discusses the performance of the Vision Transformer (ViT) model. When pre-trained on the large-scale, weakly labeled JFT-300M dataset, ViT achieves results comparable to state-of-the-art ConvNets, suggesting Transformers may have higher capacity at scale. However, without extra JFT-300M pre-training, ViT's performance lags behind ConvNets in low data situations.",
  "summary_type": "reports"
}{
  "input": "tep\nstep\nstep\ncosine\nstep\ncosine\ncosine\ncosine\ndecay rate\n0.1\n0.1\n0.1\n0.02t/400\ndecay epochs\n30\n30\n30\n1\nWeight decay\n10−4\n10−4\n10−4\n0.05\n10−4\n0.01\n0.02\n0.02\nWarmup epochs\n\u0017\n\u0017\n\u0017\n5\n5\n5\n5\n5\nLabel smoothing ε\n\u0017\n\u0017\n\u0017\n0.1\n0.1\n0.1\n\u0017\n\u0017\nDropout\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\nStoch. Depth\n\u0017\n\u0017\n\u0017\n0.1\n\u0017\n0.05\n0.05\n\u0017\nRepeated Aug\n\u0017\n\u0017\n\u0013\n\u0013\n\u0017\n\u0013\n\u0013\n\u0017\nGradient Clip.\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\nH. ﬂip\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\nRRC\n\u0017\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\nRand Augment\n\u0017\n\u0017\n\u0017\n9/0.5\n\u0017\n7/0.5\n7/0.5\n6/0.5\nAuto Augment\n\u0017\n\u0017\n\u0017\n\u0017\n\u0013\n\u0017\n\u0017\n\u0017\nMixup alpha\n\u0017\n\u0017\n\u0017\n0.8\n0.2\n0.2\n0.1\n0",
  "output": "The text appears to be a list of parameters and their values related to a machine learning model. The parameters include step, cosine, decay rate, decay epochs, weight decay, warmup epochs, label smoothing ε, dropout, stochastic depth, repeated augmentation, gradient clip, horizontal flip, RRC, random augment, auto augment, and mixup alpha. The values for these parameters vary, with some being numerical values and others being unspecified.",
  "summary_type": "reports"
}{
  "input": "ns. Scaling local self-attention for parameter efﬁcient visual backbones. CoRR,\nabs/2103.12731, 2021. 1, 2, 3, 5\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\ntion Processing Systems, 2017. 1, 2, 3, 6, 7, 8, 9\n[33] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu. Eca-net:\nEfﬁcient channel attention for deep convolutional neura",
  "output": "The text mentions three academic references. The first is about scaling local self-attention for parameter efficient visual backbones, published in 2021. The second is a work by Ashish Vaswani and others titled \"Attention is all you need\", published in Advances in Neural Information Processing Systems in 2017. The third reference is about ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks, authored by Qilong Wang and others.",
  "summary_type": "reports"
}{
  "input": "accuracy drop, we propose an\nimproved method of progressive learning, which\nadaptively adjusts regularization (e.g. data aug-\nmentation) along with image size.\nWith progressive learning, our EfﬁcientNetV2 sig-\nniﬁcantly outperforms previous models on Im-\nageNet and CIFAR/Cars/Flowers datasets. By\npretraining on the same ImageNet21k, our Efﬁ-\ncientNetV2 achieves 87.3% top-1 accuracy on\nImageNet ILSVRC2012, outperforming the re-\ncent ViT by 2.0% accuracy while training 5x-11x\nfaster using the same",
  "output": "The text discusses an improved method of progressive learning that adaptively adjusts regularization along with image size. This method has been used in EfficientNetV2, which outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, surpassing the recent ViT by 2.0% accuracy while training 5x-11x faster.",
  "summary_type": "reports"
}{
  "input": "ontrastive learning to the fully supervised setting. Analytically and\nempirically, we show that a na¨ıve extension performs much worse than our proposed version.\n2\n\n--------------------------------------------------\n2. We show that our loss provides consistent boosts in top-1 accuracy for a number of datasets. It is\nalso more robust to natural corruptions.\n3. We demonstrate analytically that the gradient of our loss function encourages learning from hard\npositives and hard negatives.\n4. We show",
  "output": "The text discusses the application of contrastive learning in a fully supervised setting, stating that a naive extension performs poorly compared to their proposed version. The authors claim that their loss function improves top-1 accuracy across various datasets and is more robust to natural corruptions. They also analytically demonstrate that the gradient of their loss function promotes learning from hard positives and hard negatives.",
  "summary_type": "reports"
}{
  "input": "d data from ImageNet (see Section 4), we use the\nLARS optimizer [82] with their default parameters, i.e., momentum 0.9 and learning rate 0.001, training for 20,000 steps with\na batch size of 4,096. We ﬁnetune using this optimizer instead of SGD in Noisy Student [77] because unlike Noisy Student,\nthe student model in Meta Pseudo Labels never trains directly on any labeled example, and hence can beneﬁt from a more\n“aggressive” ﬁnetuning process with stronger optimiziers.\nNumerical Hyper-parameters",
  "output": "The ImageNet data is processed using the LARS optimizer with default parameters (momentum 0.9 and learning rate 0.001), training for 20,000 steps with a batch size of 4,096. This optimizer is used for finetuning instead of SGD in Noisy Student because the student model in Meta Pseudo Labels never trains directly on any labeled example, thus benefiting from a more aggressive finetuning process with stronger optimizers.",
  "summary_type": "reports"
}{
  "input": "s would like to reuse parameters deﬁned in the conﬁguration ﬁle such as root\ndirectory path for datasets, parameters and model names as part of checkpoint ﬁle paths\nfor better data management. In a conﬁguration ﬁle, there are three main components to\nbe deﬁned: datasets, teacher and student models, and training. Each of the key compo-\nnents is deﬁned by using abstracted and registered modules described in Sections 2.1\nand 2.2. A conﬁguration ﬁle gives users a summary of the experiment, and shows",
  "output": "The text discusses the desire to reuse parameters defined in a configuration file for better data management. These parameters include root directory path for datasets, parameters, and model names. The configuration file consists of three main components: datasets, teacher and student models, and training. These components are defined using abstracted and registered modules. The configuration file provides a summary of the experiment.",
  "summary_type": "reports"
}{
  "input": "performance gap in unnormal-\nized resnets. In 9th International Conference on Learning\nRepresentations, ICLR, 2021.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. In International conference on machine\nlearning, pp. 1597–1607. PMLR, 2020.\nCubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Ran-\ndaugment: Practical automated data augmentation with a\nreduced search space. In Proceedings of the IEEE/CVF\nConference on Compu",
  "output": "The text references several academic papers. One discusses the performance gap in unnormalized resnets, presented at the 9th International Conference on Learning Representations (ICLR) in 2021. Another paper by Chen, Kornblith, Norouzi, and Hinton presents a simple framework for contrastive learning of visual representations, presented at an International conference on machine learning. The third paper by Cubuk, Zoph, Shlens, and Le discusses Randaugment, a practical automated data augmentation with a reduced search space, presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
  "summary_type": "reports"
}{
  "input": "--------------------------------------------\nPublished as a conference paper at ICLR 2021\nModel\nLayers\nHidden size D\nMLP size\nHeads\nParams\nViT-Base\n12\n768\n3072\n12\n86M\nViT-Large\n24\n1024\n4096\n16\n307M\nViT-Huge\n32\n1280\n5120\n16\n632M\nTable 1: Details of Vision Transformer model variants.\nWe also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\nthree groups: Natura",
  "output": "The text provides details of Vision Transformer (ViT) model variants, including ViT-Base, ViT-Large, and ViT-Huge, with varying layers, hidden sizes, MLP sizes, heads, and parameters. It also mentions an evaluation on the 19-task VTAB classification suite, which evaluates low-data transfer to diverse tasks, using 1,000 training examples per task.",
  "summary_type": "reports"
}{
  "input": "we plot the accuracy percentage at each\ntraining batch throughout the training process of a teacher and a student in Meta Pseudo Labels. We also plot the same data for\na supervised model. From the ﬁgure, we have two observations:\n• On CIFAR-10-4K (Figure 4-Left), the student’s training accuracy in Meta Pseudo Labels is much lower that of the same\nnetwork in Supervised Learning. As CIFAR-10-4K has very few labeled data, if the teacher converges quickly like in\nSupervised Learning, it will not ge",
  "output": "The text discusses a comparison of accuracy percentages during the training process of a teacher and a student in Meta Pseudo Labels, as well as a supervised model. Two observations are noted: On CIFAR-10-4K, the student's training accuracy in Meta Pseudo Labels is significantly lower than in Supervised Learning. This is attributed to the fact that CIFAR-10-4K has very few labeled data, and if the teacher converges quickly as in Supervised Learning, it will not complete.",
  "summary_type": "reports"
}{
  "input": "proximation is:\nxn “\n´\nU T\nrnsz\n¯\n˚ xn´1 ` xn´1\n(6)\nfor n “ 2, . . . , N with x1 “ U T\nr1sz and x “ CxN ` β. The\nparameters C P Roˆk, Urns P Rdˆk for n “ 1, . . . , N are\nlearnable.\nz\nU[1]\n∗\n+\nU[2]\nG(z)\n∗\n+\nU[3]\n∗\n+\nC\nβ\nFig. 2: Schematic illustration of the CCP (for third order approxima-\ntion). Symbol ˚ refers to the Hadamard product.\nModel 2: NCP (Nested coupled CP decomposition)\nInstead of explicitly separating the interactions between layers,\nwe can utilize a joint hierarchical decomposition",
  "output": "The text discusses two mathematical models. The first model, CCP (third order approximation), uses the formula xn = U T rnsz xn´1 + xn´1 for n = 2, . . . , N with x1 = U T r1sz and x = CxN + β. The parameters C and Urns are learnable. The second model, NCP (Nested coupled CP decomposition), doesn't separate interactions between layers, but instead uses a joint hierarchical decomposition.",
  "summary_type": "reports"
}{
  "input": "bution of a much deeper\nteacher model the student tries to ﬁnd a compact solution\nof transformation. This inherently enforces the student to\nexplore more informative knowledge and generalize better.\nImpressively, in certain cases the student manages to out-\nperform its teacher due to this superior generalization.\nTraining with one-hot labels accompanying with cross-\nentropy loss is a “balanced” learning system, which means\nthe objective will enforce each class to be equidistant to all\nremaining",
  "output": "The text discusses a teaching model where the student aims to find a compact solution of transformation, encouraging the student to explore more informative knowledge and generalize better. In some cases, the student can outperform the teacher due to superior generalization. The text also mentions training with one-hot labels and cross-entropy loss as a balanced learning system, where the objective ensures each class is equidistant to all others.",
  "summary_type": "reports"
}{
  "input": "s because in Axial-ViT models, each Transformer block with global\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\nattention and although the sequence length that self-attention operates on is smaller in axial case,\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\non TPUs (Figure 13, right).\nD.7\nATTENTION",
  "output": "In Axial-ViT models, each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column self-attention. Despite the sequence length being smaller in the axial case, there is an extra MLP per Axial-ViT block. The AxialResNet appears reasonable in terms of accuracy/compute trade-off, but its naive implementation is extremely slow on TPUs.",
  "summary_type": "reports"
}{
  "input": "W., and\nPlumbley, M. D. Panns: Large-scale pretrained audio\nneural networks for audio pattern recognition. IEEE/ACM\nTransactions on Audio, Speech, and Language Process-\ning, 2020.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nAdvances in neural information processing systems, 25,\n2012.\nKumar, M. P., Ton, P., and Zisserman, A. Obj cut. In\nProceedings of IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2005.\nLan,",
  "output": "The text refers to several academic works: \"Panns: Large-scale pretrained audio neural networks for audio pattern recognition\" by W. and Plumbley, M. D., published in IEEE/ACM Transactions on Audio, Speech, and Language Processing in 2020; \"Imagenet classification with deep convolutional neural networks\" by Krizhevsky, A., Sutskever, I., and Hinton, G. E., published in Advances in neural information processing systems, 25, in 2012; and \"Obj cut\" by Kumar, M. P., Ton, P., and Zisserman, A., presented at the Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) in 2005.",
  "summary_type": "reports"
}{
  "input": "tive extensions over past work (outlined in Sec 3.3), several of which are compared in Table 2.\nH-DeiT-S is a hierarchical version of DeiT-S obtained by simply using Asa within our hierarchical\narchitecture and provides 1.2 gain. Conv-3 (naive convolution (conv) with 3 × 3 kernel) aggregates\nspatial and channel information, where as Group Conv-3 splits input features and performs convs\nusing different kernels – it is cheaper and more effective. When group size = channel dim., we\nget depth-wise c",
  "output": "The text discusses improvements over previous work, some of which are compared in Table 2. H-DeiT-S is a hierarchical version of DeiT-S that uses Asa within a hierarchical architecture, providing a 1.2 gain. Conv-3, a naive convolution with a 3x3 kernel, aggregates spatial and channel information. Group Conv-3, on the other hand, splits input features and performs convolutions using different kernels, which is more cost-effective and efficient. When the group size equals the channel dimension, depth-wise convolution is achieved.",
  "summary_type": "reports"
}{
  "input": "ecay of 5e-2, 10 warmup and cooldown\nepochs. We used the same training script and therefore aug-\nmentations as the ImageNet-1k experiments. We also re-\nsized all images to 224×224.\nResults The results are presented in Table 4. We report\nresults from ResMLP, ViT and DeiT as well.\n4.5. Object Detection\nDataset MS COCO [21] is a widely-used benchmark for\nevaluating object detection model.\nIt has 118k images\nfor training and 5k images for evaluating performances\nof object detectors. We follow standa",
  "output": "The text discusses a training script used for ImageNet-1k experiments, with a decay of 5e-2 and 10 warmup and cooldown epochs. All images were resized to 224×224. The results, presented in Table 4, include data from ResMLP, ViT, and DeiT. The text also mentions the use of the MS COCO dataset, a popular benchmark for evaluating object detection models, which contains 118k training images and 5k evaluation images.",
  "summary_type": "reports"
}{
  "input": "at an image as 1D sequence of\nvisual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual\nstructures and dealing with scale variance. Alternatively, they require large-scale\ntraining data and longer training schedules to learn the IB implicitly. In this\npaper, we propose a novel Vision Transformer Advanced by Exploring intrinsic\nIB from convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid\nreduction modules to downsample and embed the input image into tok",
  "output": "The text discusses a novel Vision Transformer called ViTAE, which is advanced by exploring intrinsic inductive bias (IB) from convolutions. ViTAE uses several spatial pyramid reduction modules to downsample and embed the input image. This approach contrasts with other methods that view an image as a 1D sequence of visual tokens, lack an intrinsic IB in modeling local visual structures, and require large-scale training data and longer training schedules to learn the IB implicitly.",
  "summary_type": "reports"
}{
  "input": "of inferior\nquality. Since the weights are highly shared in the same\nsupernet, training on these weak paths does have negative\ninﬂuence on the evaluation of those potentially-good paths.\nTo alleviate this disturbance, an intuitive idea is to block the\ntraining of these weak paths.\nFor simplifying the analysis, we assume the search space\nA can be partitioned into two subsets Agood and Aweak by\nan Oracle good but unknown supernet No, where\nA = Agood\n[\nAweak, Agood\n\\\nAweak = ∅,\n(4)\nand Agood indic",
  "output": "The text discusses the negative impact of training on weak paths in a supernet, which can affect the evaluation of potentially good paths. To mitigate this, the text suggests blocking the training of these weak paths. It further simplifies the analysis by assuming that the search space A can be divided into two subsets, Agood and Aweak, by an unknown but good supernet No.",
  "summary_type": "reports"
}{
  "input": "have adopted the recent CaiT variant [57] as\na starting point. This transformer-based architecture achieves state-of performance with Imagenet-\ntraining only (achieving 86.5% top-1 accuracy on Imagenet-val for the best model). Most importantly,\nthe training is relatively stable with increasing depth.\nIn our exploration phase, our objective was to radically simplify this model. For this purpose, we\nhave considered the Cait-S24 model for faster iterations. This network consists of 24-layer with a",
  "output": "The recent CaiT variant, a transformer-based architecture, has been adopted due to its state-of-the-art performance with Imagenet-training only, achieving 86.5% top-1 accuracy on Imagenet-val for the best model. The training is stable even with increasing depth. The aim was to simplify this model, using the Cait-S24 model, a 24-layer network, for faster iterations.",
  "summary_type": "reports"
}{
  "input": "pproaches [36,50,52,12,1,31,42,49,53] introduce auxiliary modules, which are\nused only in training session. Such auxiliary modules use tensors from intermediate\nlayers in models, and introducing the modules to the models often results in branching\ntheir feedforward path as shown in Figs. 1 and 2. This paradigm, however, is also one of\nthe backgrounds that researchers decide to hard-code the models (e.g., modify the orig-\ninal implementations of models in torchvision every time they change the pl",
  "output": "The text discusses the use of auxiliary modules in training sessions, which are introduced to models and often result in branching their feedforward path. This approach is one of the reasons researchers decide to hard-code the models, modifying the original implementations every time they change.",
  "summary_type": "reports"
}{
  "input": "All subnets share the weights\nof their common parts. The supernet is the largest model\nin the space, and its architecture is shown in Fig. 3. In par-\nticular, the supernet stacks the maximum number of trans-\nformer blocks with the largest embedding dimension, Q-K-\nV dimension and MLP ratio as deﬁned in the space. During\ntraining, all possible subnets are uniformly sampled, and the\ncorresponding weights are updated.\nAccording to the constraints on model parameters, we\npartition the large-scale se",
  "output": "All subnets share the weights of their common parts. The supernet is the largest model and its architecture includes the maximum number of transformer blocks with the largest embedding dimension, Q-K-V dimension, and MLP ratio. During training, all possible subnets are uniformly sampled and their weights are updated. The text also mentions partitioning the large-scale model based on certain constraints on parameters.",
  "summary_type": "reports"
}{
  "input": "ains a projection 1×1 conv to adapt the number of feature maps and their spatial\nresolution for the summation with the output of the block). This is similar to projection shortcut\nin [26]. Therefore, for the original ResNet, the downsampling is not performed by the ﬁrst stage (as\nthe max pooling performs this before), the next three main stages perform the downsampling on their\nﬁrst block. In our networks, all four main stages perform the downsampling in their ﬁrst block.\nThis change does not in",
  "output": "The text discusses a modification to the original ResNet where all four main stages perform the downsampling in their first block, unlike the original where only the next three main stages do so after the first stage. This is achieved through a 1x1 conv projection that adjusts the number of feature maps and their spatial resolution for summation with the block's output, similar to the projection shortcut in [26].",
  "summary_type": "reports"
}{
  "input": "structure); (3)\nNL blocks are inserted as additional blocks into a ResNet\nbackbone as opposed to replacing existing convolutional\nblocks as done by BoTNet. Section 4.6 offers a comparison\nbetween BoTNet, NLNet as well as a NL-like version of\nBoTNet where we insert BoT blocks in the same manner as\nNL blocks instead of replacing.\n3. Method\nstage\noutput\nResNet-50\nBoTNet-50\nc1 512 × 512\n7×7, 64, stride 2\n7×7, 64, stride 2\nc2 256 × 256\n3×3 max pool, stride 2\n3×3 max pool, stride 2\n\n\n1×1, 64\n3×3,",
  "output": "NL blocks are added to a ResNet backbone as extra blocks, unlike BoTNet which replaces existing convolutional blocks. Section 4.6 compares BoTNet, NLNet and a NL-like version of BoTNet where BoT blocks are inserted in the same way as NL blocks. The method stage output for ResNet-50 and BoTNet-50 is also outlined, with both using a 7x7, 64, stride 2 for c1 512x512 and a 3x3 max pool, stride 2 for c2 256x256.",
  "summary_type": "reports"
}{
  "input": "ViTAE: Vision Transformer Advanced by Exploring\nIntrinsic Inductive Bias\nYufei Xu1∗\nQiming Zhang1∗\nJing Zhang1\nDacheng Tao2\n1The University of Sydney\n2JD Explore Academy, JD.com\nAbstract\nTransformers have shown great potential in various computer vision tasks owing to\ntheir strong capability in modeling long-range dependency using the self-attention\nmechanism. Nevertheless, vision transformers treat an image as 1D sequence of\nvisual tokens, lacking an intrinsic inductive bias (IB) in modeling lo",
  "output": "The text discusses ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias, a study by Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. The study explores the potential of transformers in computer vision tasks due to their ability to model long-range dependency using the self-attention mechanism. However, it notes that vision transformers treat an image as a 1D sequence of visual tokens, which lacks an intrinsic inductive bias in modeling.",
  "summary_type": "reports"
}{
  "input": "e study the training bottlenecks of Efﬁcient-\nNet (Tan & Le, 2019a), and introduce our training-aware\nNAS and scaling, as well as EfﬁcientNetV2 models.\n3.1. Review of EfﬁcientNet\nEfﬁcientNet (Tan & Le, 2019a) is a family of models that are\noptimized for FLOPs and parameter efﬁciency. It leverages\nNAS to search for the baseline EfﬁcientNet-B0 that has\nbetter trade-off on accuracy and FLOPs. The baseline model\nis then scaled up with a compound scaling strategy to obtain\na family of models B1-B7. W",
  "output": "The text discusses the study of training bottlenecks in EfficientNet, introducing training-aware NAS and scaling, as well as EfficientNetV2 models. EfficientNet is a family of models optimized for FLOPs and parameter efficiency, using NAS to search for the baseline EfficientNet-B0 that offers a better trade-off on accuracy and FLOPs. This baseline model is then scaled up using a compound scaling strategy to create a family of models B1-B7.",
  "summary_type": "reports"
}{
  "input": "-------------------------------------------------\nTable 1: Model details of two variants of ViTAE.\nModel\nReduction Cell\nNormal Cell\nParams Macs\nDilation\nCells Heads Embed Cells\n(M)\n(G)\nViTAE-T [1, 2, 3, 4] ↓\n3\n4\n256\n7\n4.8\n1.5\nViTAE-S [1, 2, 3, 4] ↓\n3\n6\n384\n14\n23.6\n5.6\nWe use two variants of ViTAE in\nour experiments for a fair com-\nparison of other models with sim-\nilar model sizes. The details of\nthem are summarized in Table 1.\nIn the ﬁrst RC, the default convo-\nlution kernel size is 7 × 7 with",
  "output": "The text provides details of two variants of ViTAE model used in experiments for comparison with other models. The first variant, ViTAE-T, has 3 reduction cells, 4 normal cells, 256 MACs, 7 heads, 4.8 million parameters, and 1.5 billion cells. The second variant, ViTAE-S, has 3 reduction cells, 6 normal cells, 384 MACs, 14 heads, 23.6 million parameters, and 5.6 billion cells. The default convolution kernel size in the first RC is 7x7.",
  "summary_type": "reports"
}{
  "input": "1\n2\n3\n0\n1\nFigure 2: Overview of the proposed Hierarchical Visual Transformer. To reduce the redundancy in the full-length patch\nsequence and construct a hierarchical representation, we propose to progressively pool visual tokens to shrink the sequence\nlength. To this end, we partition the ViT [11] blocks into several stages. At each stage, we insert a pooling layer after the ﬁrst\nTransformer block to perform down-sampling. In addition to the pooling layer, we perform predictions using the result",
  "output": "The text describes a Hierarchical Visual Transformer that aims to reduce redundancy in full-length patch sequences and construct a hierarchical representation. This is achieved by progressively pooling visual tokens to decrease the sequence length. The ViT blocks are partitioned into several stages, with a pooling layer inserted after the first Transformer block at each stage for down-sampling. Predictions are also made using the result of the pooling layer.",
  "summary_type": "reports"
}{
  "input": "eprint arXiv:1906.01787,\n2019a.\nWang, S., Li, B., Khabsa, M., Fang, H., and Ma, H. Lin-\nformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768, 2020.\nWang, Y., Sun, Y., Liu, Z., Sarma, S. E., Bronstein, M. M.,\nand Solomon, J. M. Dynamic graph cnn for learning\non point clouds. Acm Transactions On Graphics (tog),\n38(5):1–12, 2019b.\nWu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang,\nX., and Xiao, J. 3d shapenets: A deep representation\nfor volumetric shapes. In Proceeding",
  "output": "The text mentions several academic papers. Wang, Li, Khabsa, Fang, and Ma authored a paper titled \"Linformer: Self-attention with linear complexity\" in 2020. Wang, Sun, Liu, Sarma, Bronstein, and Solomon wrote \"Dynamic graph cnn for learning on point clouds\" published in ACM Transactions On Graphics in 2019. Wu, Song, Khosla, Yu, Zhang, Tang, and Xiao authored \"3D Shapenets: A deep representation for volumetric shapes\".",
  "summary_type": "reports"
}{
  "input": "Jiashi Feng, and S. Yan.\nTokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021. 3\n[55] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and\nYoungjoon Yoo. CutMix: Regularization strategy to train strong classiﬁers with localizable\nfeatures. arXiv preprint arXiv:1905.04899, 2019. 3, 5, 18\n[56] Hongyi Zhang, Moustapha Ciss´e, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond\nempirical risk minimization. arXiv prepri",
  "output": "The text refers to three different academic papers. The first one by Jiashi Feng and S. Yan, titled \"Tokens-to-token vit: Training vision transformers from scratch on imagenet\", was published on arXiv in 2021. The second paper, \"CutMix: Regularization strategy to train strong classifiers with localizable features\" by Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo, was also published on arXiv in 2019. The third paper, presumably titled \"mixup: Beyond empirical risk minimization\", is authored by Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz, but the text does not provide the year of publication or the platform it was published on.",
  "summary_type": "reports"
}{
  "input": "to map an input array (inter-\npreted as a set) to a low-dimensional array and immediately\nmap it back to the input space. Stacking these blocks leads\nto an architecture that scales linearly in compute/memory\nwith input size like the Perceiver’s cross-attention module,\nbut without the advantage of the Perceiver’s latent array\n(which completely decouples the cost of the latent Trans-\nformer from the input size): a fully-ISAB model scales as\nO(LMN), rather than O(MN + LN 2), like the Perceiver\n(whe",
  "output": "The text discusses a method of mapping an input array to a low-dimensional array and then back to the input space. This process, when stacked, creates an architecture that scales linearly in compute/memory with input size, similar to the Perceiver's cross-attention module. However, it lacks the Perceiver's latent array advantage, which fully separates the cost of the latent Transformer from the input size. A fully-ISAB model scales as O(LMN), unlike the Perceiver, which scales as O(MN + LN 2).",
  "summary_type": "reports"
}{
  "input": "top-1 accuracy, surpassing the human-\ndesigned hybrid CNN-transformer, BoTNet50, by 2.2%\nwhile being 1.19× faster in terms of compute time; when\nequipped with SE and SiLU activation, BossNet-T0 fur-\nther achieves 80.8% top-1 accuracy, surpassing the NAS\nsearched EfﬁcientNet-B1 by 1.7% while being 1.14× faster.\nSecondly, our searched model demonstrates absolute su-\nperiority over manually and randomly selected models from\nsearch space HyTra. In particular, BossNet-T0 achieves\nup to 6.0% improveme",
  "output": "BossNet-T0 surpasses the human-designed hybrid CNN-transformer, BoTNet50, by 2.2% in top-1 accuracy and is 1.19x faster in compute time. With SE and SiLU activation, BossNet-T0 achieves 80.8% top-1 accuracy, surpassing the NAS searched EfficientNet-B1 by 1.7% and is 1.14x faster. The model demonstrates superiority over manually and randomly selected models from search space HyTra, achieving up to 6.0% improvement.",
  "summary_type": "reports"
}{
  "input": "ConViT: Improving Vision Transformers\nwith Soft Convolutional Inductive Biases\nSt´ephane d’Ascoli 1 2 Hugo Touvron 2 Matthew L. Leavitt 2 Ari S. Morcos 2 Giulio Biroli 1 2 Levent Sagun 2\nAbstract\nConvolutional architectures have proven ex-\ntremely successful for vision tasks. Their hard\ninductive biases enable sample-efﬁcient learning,\nbut come at the cost of a potentially lower perfor-\nmance ceiling. Vision Transformers (ViTs) rely\non more ﬂexible self-attention layers, and have\nrecently outper",
  "output": "The text discusses ConViT, a method aimed at improving Vision Transformers (ViTs) with soft convolutional inductive biases. While convolutional architectures have been successful for vision tasks due to their sample-efficient learning, they may have a lower performance ceiling. ViTs, which use more flexible self-attention layers, have recently shown superior performance.",
  "summary_type": "reports"
}{
  "input": "e show Batch Prioritized Routing versus vanilla routing.\n0.01\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ncapacity C\n10\n15\n20\n25\n30\n35\nImageNet/1shot\n0.01\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ncapacity C\n15\n20\n25\n30\n35\n40\n45\n50\nImageNet/5shot\n0.01\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ncapacity C\n20\n25\n30\n35\n40\n45\n50\nImageNet/10shot\n0.01\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ncapacity C\n5\n10\n15\n20\n25\n30\n35\nJFT validation prec@1\nDense VIT-S/32\nV-MoE-S/32 (BPR)\nV-MoE-S/32 (Vanilla)\nFigure 19: Infe",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "uence length n and dimension d. Each block is deﬁned as:\nZ = σ(XU),\n˜Z = s(Z),\nY = ˜ZV\n(1)\nwhere σ is an activation function such as GeLU [24]. U and V deﬁne linear projections along the\nchannel dimension—the same as those in the FFNs of Transformers (e.g., their shapes are 768× 3072\nand 3072× 768 for BERTbase). Shortcuts, normalizations and biases are omitted for brevity.\nA key ingredient in the aforementioned formulation is s(·), a layer which captures spatial interactions\n(see below). When s",
  "output": "The text describes a block defined as Z = σ(XU), ˜Z = s(Z), Y = ˜ZV, where σ is an activation function like GeLU. U and V are linear projections along the channel dimension, similar to the FFNs of Transformers. The shapes of U and V are 768×3072 and 3072×768 for BERTbase. Shortcuts, normalizations, and biases are not included in this description. The function s(·) is a layer that captures spatial interactions.",
  "summary_type": "reports"
}{
  "input": "]\n22\n5.2\n-\n81.5\nTNT-S [15]\n23.8\n5.2\n-\n81.3\nCoaT Mini [17]\n10\n6.8\n-\n80.8\nCoaT-Lite Small [17]\n20\n4.0\n-\n81.9\nPVT-Small [8]\n24.5\n3.8\n820\n79.8\nCPVT-Small-GAP [9]\n23\n4.6\n817\n81.5\nTwins-PCPVT-S (ours)\n24.1\n3.8\n815\n81.2 (+1.3)\nSwin-T [4]\n29\n4.5\n766\n81.3\nSwin-T + CPVT†\n28\n4.4\n766\n81.2\nTwins-SVT-S (ours)\n24\n2.9\n1059\n81.7 (+1.8)\nT2T-ViT-19 [27]\n39.2\n8.9\n-\n81.9\nPVT-Medium [8]\n44.2\n6.7\n526\n81.2\nTwins-PCPVT-B(ours)\n43.8\n6.7\n525\n82.7 (+0.8)\nSwin-S [4]\n50\n8.7\n444\n83.0\nTwins-SVT-B (ours)\n56\n8.6\n469\n83.2 (+1.3)",
  "output": "The text presents a comparison of different models with their respective parameters. The models include TNT-S, CoaT Mini, CoaT-Lite Small, PVT-Small, CPVT-Small-GAP, Twins-PCPVT-S, Swin-T, Swin-T + CPVT†, Twins-SVT-S, T2T-ViT-19, PVT-Medium, Twins-PCPVT-B, Swin-S, and Twins-SVT-B. Each model is evaluated based on four metrics, possibly related to performance and complexity, though the specific metrics are not defined in the provided text.",
  "summary_type": "reports"
}{
  "input": "context,” in ECCV, 2014.\n[50]\nW. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back,\nP. Natsev, et al., “The kinetics human action video dataset,” ArXiv:1705.06950, 2017.\n[51]\nK. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers: Surpassing human-level performance\non imagenet classiﬁcation,” in ICCV, 2015.\n[52]\nG. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, “Improving neural\nnetworks by preventing",
  "output": "The text references several academic papers. The first, presented at ECCV 2014, is not named. The second, \"The kinetics human action video dataset,\" was published on ArXiv in 2017 by W. Kay and others. The third, \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,\" was presented at ICCV in 2015 by K. He and others. The fourth, by G. E. Hinton and others, discusses improving neural networks, but the title and publication details are not provided.",
  "summary_type": "reports"
}{
  "input": "CNN-transformers [62] at different scales.\n5\n\n--------------------------------------------------\nMethod\nMAdds\nSteptime\nTop-1 (%)\nTop-5 (%)\nResNet50 [25]\n4.1B\n100ms\n77.7\n93.9\nViT-B/32 [20]\n-\n68ms\n73.4\n-\nViT-B/16 [20]\n17.6B\n158ms\n77.9\n-\nBoT50 [62]\n4.0B\n120ms\n78.3\n94.2\nR50-T\nConv-Only\n4.1B\n104ms\n78.2\n94.2\nViT-T/32\nAtt-Only\n2.9B\n92ms\n74.5\n91.7\nViT-T/16\nAtt-Only\n3.2B\n96ms\n76.5\n93.0\nBoT50-T\nHybrid\n3.9B\n103ms\n79.5\n94.8\nRandom-T Hybrid\n3.7B\n84ms\n76.7\n93.1\nBossNet-T0 w/o SE\n3.4B\n101ms\n80.5\n95.0\nSENet50 [",
  "output": "The text presents a comparison of different CNN-transformers, including ResNet50, ViT-B/32, ViT-B/16, BoT50, R50-T, ViT-T/32, ViT-T/16, BoT50-T, Random-T Hybrid, and BossNet-T0 w/o SE. The comparison is based on MAdds, Steptime, Top-1 (%), and Top-5 (%) metrics. The highest Top-1 (%) is achieved by BossNet-T0 w/o SE at 80.5%, while the lowest Steptime is recorded by ViT-B/32 at 68ms. The text ends abruptly without providing information on SENet50.",
  "summary_type": "reports"
}{
  "input": ".\narXiv preprint\narXiv:2010.11929, 2020.\n[8] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-\nculty of training deep feedforward neural networks. In Pro-\nceedings of the thirteenth international conference on artiﬁ-\ncial intelligence and statistics, pages 249–256. JMLR Work-\nshop and Conference Proceedings, 2010.\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nr",
  "output": "The text refers to two academic papers: one by Xavier Glorot and Yoshua Bengio titled \"Understanding the difficulty of training deep feedforward neural networks\", presented at the thirteenth international conference on artificial intelligence and statistics, and another by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun titled \"Deep residual learning for image recognition\", presented at the IEEE conference on computer vision and pattern recognition.",
  "summary_type": "reports"
}{
  "input": "ince the origi-\nnal methods do not converge: we have re-introduced the Layer-normalization η and\nwarmup. We have adapted the drop rate dr for all the methods, including the baseline.\nThe column α = ε reports the performance when initializing the scalar with the same\nvalue as for LayerScale. †: failed before the end of the training.\ndepth\nbaseline\nscalar α weighting\nLayerScale\ndr = 0.05\nadjust [dr]\nRezero\nT-Fixup\nFixup\nα = ε\n12\n79.9\n79.9 [0.05]\n78.3\n79.4\n80.7\n80.4\n80.5\n18\n80.1\n80.7 [0.10]\n80.1\n81",
  "output": "The original methods were modified by reintroducing Layer-normalization η and warmup, and adjusting the drop rate (dr) for all methods, including the baseline. The performance when initializing the scalar with the same value as for LayerScale is reported in the column α = ε. The table shows the results of these changes on different methods at depths of 12 and 18.",
  "summary_type": "reports"
}{
  "input": "feature of experts with the predicted\ngate-values of Router. These methods aim at conditionally selecting a speciﬁc layer or block.\nDifferently, our method can take more advantage of conditional computation as the selection of whole\nnetwork making every parameter input-dependent. The recently proposed Dynamic Convolution\nmethods [11, 12, 23] share the same idea and achieve remarkable performance with low FLOPs but\nhigh latency. It is because these methods need to load many basic models or exper",
  "output": "The text discusses a method that enhances conditional computation by making the selection of the entire network and making every parameter input-dependent. This method is similar to the recently proposed Dynamic Convolution methods, which have achieved impressive performance with low FLOPs but high latency due to the need to load many basic models.",
  "summary_type": "reports"
}{
  "input": "Dynamic Routing module fails to abstract many prime\ncapsules into a small amount of latent capsules. On the\ncontrast, our centorid transformer can aggregate informa-\ntion no matter the output length is short or long.\nFurther, our centroid attention block has a much smaller\nparameter size as well as a larger max batch size per GPU.\nTjos means we can process more data in the same time\nand lead a much faster training speed comparing with the\ncapsule network using Dynamic Routing. This roughly\nleads",
  "output": "The Dynamic Routing module struggles to condense numerous prime capsules into fewer latent capsules. In contrast, the centroid transformer can aggregate information regardless of the output length. The centroid attention block has a smaller parameter size and a larger max batch size per GPU, allowing for faster data processing and training speed compared to the capsule network using Dynamic Routing.",
  "summary_type": "reports"
}{
  "input": "stands for knowledge distillation [16].\nMethods\nTop-1\n(%)\nFLOPs\n(G)\nBatch Time\n(ms)\nResNet-50∗\n78.7\n4.1\n34.2\nDeiT-S∗\n80.1\n4.6\n36.9\nRegNetY-4GF∗\n80.0\n4.0\n40.2\nEfﬁcientNet-B3 [33]\n81.6\n1.8\n48.3\nVisformer-S (ours)\n82.2\n4.9\n36.7\nTable 8. Comparison of inference efﬁciency among Visformer-S\nand other models. A batch size of 32 is used for testing. Besides\nEfﬁcientNet-B3, other models are trained using the elite setting.\n4.2. Training with limited data\nLast but not least, we evaluate the performance o",
  "output": "The text provides a comparison of inference efficiency among different models including ResNet-50, DeiT-S, RegNetY-4GF, EfficientNet-B3, and Visformer-S. The metrics used for comparison are Top-1 (%), FLOPs (G), and Batch Time (ms). Visformer-S, the model proposed by the authors, has the highest Top-1 score of 82.2%, uses 4.9 FLOPs, and has a batch time of 36.7 ms. The batch size used for testing is 32. All models except EfficientNet-B3 are trained using the elite setting.",
  "summary_type": "reports"
}{
  "input": "e dotted red line is at the target accuracy of 75.9%,\nand the boxes show the min, max, and quartiles of the distribution of accuracies over the 50 training\nruns.\nwhose median over 50 seeds continues to beat the target 75.9% accuracy (noted by the dotted red\nline) is “BN & LR tuning”, with the rest having between 0.1%-0.3% drops in median accuracy.\nD\nHyperparameter tuning\nD.1\nNesterov momentum training speed on ResNet-50\nWe considered two conﬁgurations of Nesterov hyperparameters: Conﬁguration A,",
  "output": "The text discusses the results of 50 training runs, aiming for a target accuracy of 75.9%. The distribution of accuracies is represented by boxes showing the min, max, and quartiles. The method \"BN & LR tuning\" consistently exceeds the target accuracy, while other methods show a drop in median accuracy between 0.1%-0.3%. The text also mentions the consideration of two configurations of Nesterov hyperparameters in the context of hyperparameter tuning and Nesterov momentum training speed on ResNet-50.",
  "summary_type": "reports"
}{
  "input": "ig. 12 compares the mIoU-MAdds trade-off obtained by\nNAT and the original BiSeNet [75] on the Cityscapes dataset.\nEmpirically, we observe that NAT based backbones consistently\noutperform the original BiSeNets, which are based on ResNets. To\nrealize the full potential of the searched NATNets, we further ﬁne-\ntune the obtained models for 4K iterations. As shown in Table 5,\nthe resulting NAT model yields comparable performance against\nstate-of-the-art methods, including PSPNet [76], DeepLabv3 [77],",
  "output": "Figure 12 compares the mIoU-MAdds trade-off obtained by NAT and the original BiSeNet on the Cityscapes dataset. Empirical observations show that NAT based backbones consistently outperform the original BiSeNets, which are based on ResNets. To maximize the potential of the searched NATNets, the obtained models are further fine-tuned for 4K iterations. As shown in Table 5, the resulting NAT model yields comparable performance against state-of-the-art methods, including PSPNet and DeepLabv3.",
  "summary_type": "reports"
}{
  "input": "raining + ImageNet-1K ﬁnetuning\nset up. “PT-RA” denotes applying RandAugment during 21K pre-training and “E150” means 150\nepochs of pre-training, which is longer than the standard 90 epochs.\nModels\nEval Size\n#Params\n#FLOPs\nTop-1 Accuracy\nConv Only\nENetV2-S\n3842\n24M\n8.8B\n85.0\nENetV2-M\n4802\n55M\n24B\n86.1\nENetV2-L\n4802\n121M\n53B\n86.8\nViT-Stem TFM Only\nViT-B/16\n3842\n87M\n55.4B\n84.6\nViT-L/16\n3842\n304M\n190.7B\n85.3\nMulti-Stage TFM Only\nHaloNet-H4\n3842\n85M\n-\n85.6\nHaloNet-H4\n5122\n85M\n-\n85.8\nSwin-B\n3842\n88M",
  "output": "The text discusses the setup for finetuning ImageNet-1K with RandAugment during 21K pre-training, denoted as \"PT-RA\", and 150 epochs of pre-training, referred to as \"E150\". This is longer than the standard 90 epochs. The models evaluated include ENetV2-S, ENetV2-M, ENetV2-L, ViT-B/16, ViT-L/16, HaloNet-H4, and Swin-B. The parameters, FLOPs, and top-1 accuracy for each model are provided.",
  "summary_type": "reports"
}{
  "input": "perts in the last layers.\nWe would like to remark that each image is subject to a large number of routing decisions through\nits patches. Concretely, Figure 32 shows how most images use –on aggregate by pooling over all\ntheir patches– most of the experts in every layer. This motivated our efforts to try to save compute by\ndiscarding, or not processing, patches that are not useful for the ﬁnal classiﬁcation. We cover this in\ndetail in Section 4.\n35\n\n------------------------------------------------",
  "output": "The text discusses how each image undergoes numerous routing decisions through its patches, as illustrated in Figure 32. This process uses most of the experts in every layer. To save computational resources, efforts were made to discard or avoid processing patches that are not beneficial for the final classification. This topic is discussed in detail in Section 4.",
  "summary_type": "reports"
}{
  "input": "wing.\nminimize\nΠ0,Π1∈{0,1}n×n −∥(1 −z∗) ⊙Π⊺\n0s(x0)∥1\n−∥(z∗⊙Π⊺\n1s(x1)∥1\n+ ξ\nX\nk=0,1\n⟨Πk, C⟩\nsubject to Πk1n = 1n, Π⊺\nk1n = 1n\nfor k = 0, 1.\nNote the problem is completely separable as two inde-\npendent optimization problems of each Πk. Let s(x1)i\n\n--------------------------------------------------\nPuzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup\nAlgorithm 1 Masked Transport\nInput: mask z∗, cost C′, large value v\nInitialize C(0) = C′, t = 0\nrepeat\ntarget = argmin(C(t), dim =",
  "output": "The text discusses a mathematical optimization problem involving two independent variables Π0 and Π1. The problem is completely separable into two independent optimization problems for each Πk. The text also mentions an algorithm called \"Masked Transport\", which takes three inputs: a mask z∗, a cost C′, and a large value v. The algorithm involves an iterative process where the target is determined by minimizing C(t).",
  "summary_type": "reports"
}{
  "input": "ve loss. Models are all trained without RandAugment.\nMethod\nTop-1 Accuracy\nCE\n48.4\nmulti-task (CE+Re-weighting)\n49.0\nmulti-task (CE+Blance Softmax)\n48.6\nPaCo\n51.0\nE. Implementation details for Table 1\nWe train models with cross-entropy, parametric contrastive loss 400 epochs without RandAugment respectively. For\nsupervised contrastive loss, following the original paper, we ﬁrstly train the model 400 epochs. Then we ﬁx the backbone\nand train a linear classiﬁer 400 epochs.\nF. Ablation Study\nRe-wei",
  "output": "The text discusses the accuracy of different models trained without RandAugment. The models are trained with cross-entropy, parametric contrastive loss for 400 epochs. For supervised contrastive loss, the model is first trained for 400 epochs, then the backbone is fixed and a linear classifier is trained for another 400 epochs. The top-1 accuracy for the models are: CE (48.4), multi-task (CE+Re-weighting) (49.0), multi-task (CE+Blance Softmax) (48.6), and PaCo (51.0).",
  "summary_type": "reports"
}{
  "input": "by vote. However, it requires\nn forward passes, leading to a signiﬁcant runtime cost. MIMO [5] draws inspiration from model\nsparsity [6] and tries to ensemble several subnetworks within one regular network. It only needs one\nsingle forward pass of the regular network but is incompatible with compact models. Conditional\ncomputation methods [7, 8, 9, 10] alleviates this problem via delegation scheme, i.e. assigning one\nor several, rather than all, models conditionally to make the prediction. Some",
  "output": "The text discusses different computational models. Voting requires multiple forward passes, which can be time-consuming. MIMO, inspired by model sparsity, ensembles several subnetworks within one regular network, requiring only one forward pass, but is not compatible with compact models. Conditional computation methods alleviate this issue by using a delegation scheme, assigning one or several models conditionally to make the prediction.",
  "summary_type": "reports"
}{
  "input": "a 1×1\nconv lacks the spatial resolution for performing downsampling, we maintain a max pooling on the\nprojection shortcut in the ﬁrst block of each following stages. Our ﬁnal network result is 22.12 top-1\nerror requiring only 24.85 million parameters and 3.88 GFLOPs. The conclusion is that regardless of\nthe settings of PyConv, using our formulation, it consistently provides better results than the baseline.\nFig. 7 shows the training and validation curves for comparing our networks, PyConvResNet",
  "output": "The text discusses a network that uses a 1x1 convolution, which lacks the spatial resolution for performing downsampling, so max pooling is maintained on the projection shortcut in the first block of each following stages. The final network result has a 22.12 top-1 error, requiring only 24.85 million parameters and 3.88 GFLOPs. Regardless of the PyConv settings, using this formulation consistently provides better results than the baseline. The training and validation curves for comparing these networks are shown in Fig. 7.",
  "summary_type": "reports"
}{
  "input": "n that only\nchanges the shape speciﬁcation of tensors but not the or-\nder of data in memory, which is cost-free. The input is\nﬁrst ﬂattened into N vectors of length CHW, which is\nV(in) = RS(M(in), (N, CHW)), multiplied by the kernel\nW(OHW, CHW), then the output V(out)(N, OHW) is re-\nshaped back into M(out)(N, O, H, W). For the better read-\n\n--------------------------------------------------\nability, we omit the RS if there is no ambiguity,\nM(out) = MMUL(M(in), W) .\n(3)\nSuch an FC cannot take adv",
  "output": "The text discusses a process that alters the shape specification of tensors without changing the data order in memory. The input is flattened into N vectors of length CHW, multiplied by the kernel W(OHW, CHW), and then the output is reshaped back into M(out)(N, O, H, W). For clarity, the RS is omitted if there's no ambiguity, resulting in M(out) = MMUL(M(in), W). This Fully Connected (FC) process cannot take advantage.",
  "summary_type": "reports"
}{
  "input": "mkv). Unlike linear attention mechanisms, lambda\nlayers have quadratic time complexity with respect to the input length (in the global context case)\nbecause they consider position-based interactions.\n22\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\n2) Lambda layers do not necessarily attempt to approximate an attention kernel.\nWhile ap-\nproximations of the attention kernel are theoretically motivated, we argue that they may be unnec-\nessarily re",
  "output": "Lambda layers, unlike linear attention mechanisms, have quadratic time complexity in relation to input length due to their consideration of position-based interactions. Lambda layers do not necessarily aim to approximate an attention kernel, despite theoretical motivations for such approximations.",
  "summary_type": "reports"
}{
  "input": "en. Vision transformers are robust learners. arXiv preprint arXiv:2105.07581,\n2021.\n[34] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, and\nMatthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference. arXiv preprint\narXiv:2104.01136, 2021.\n[35] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv preprint arXiv:2103.00112, 2021.\n[36] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu,",
  "output": "The text mentions several academic papers discussing various aspects of vision transformers. These include \"Vision transformers are robust learners\" (arXiv:2105.07581, 2021), \"Levit: a vision transformer in convnet’s clothing for faster inference\" (arXiv:2104.01136, 2021), and \"Transformer in transformer\" (arXiv:2103.00112, 2021). The authors of these papers include Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze, Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang, Li Yuan, Yunpeng Chen, Tao Wang, and Weihao Yu.",
  "summary_type": "reports"
}{
  "input": "iki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jon Shlens. Stand-alone self-\nattention in vision models. In NeurIPS, pages 68–80, 2019.\n2\n[30] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll´ar. Designing network design\nspaces, 2020. 6, 7\n[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael S. Bernstein, Alexander C. Berg,\nand Fei-Fei Li.",
  "output": "The text mentions two studies. The first one is \"Stand-alone self-attention in vision models\" by Iki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens, presented at NeurIPS in 2019. The second study is \"Designing network design spaces\" by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll´ar, published in 2020. The text also lists several other researchers, presumably authors of other works, including Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li.",
  "summary_type": "reports"
}