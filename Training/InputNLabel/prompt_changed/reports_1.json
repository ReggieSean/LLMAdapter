{
  "input": "n select a subset of tokens from the\noriginal sequence, while the emphasis of our work is\nsummarize the information into several centroids. This\nleads to completely distinct structure design. (3) Their\nscoring function is human-designed. In contrast, we start\nfrom clustering algorithm, and derives a novel connection\nbetween gradient-based clustering and attention.\nCapsule Networks\nSimilar to our method, capsule net-\nworks (Hinton et al., 2011) are also based on the idea\nof “building clustering a",
  "output": "The text discusses a method of information summarization that focuses on creating several centroids from the original sequence, differing from other methods that select a subset of tokens. This approach results in a unique structure design. The scoring function is derived from a clustering algorithm, establishing a new link between gradient-based clustering and attention. This method is similar to capsule networks (Hinton et al., 2011), which are also based on the concept of clustering.",
  "summary_type": "reports"
}{
  "input": "ko & Komodakis, 2016). We set m = 10\nfor WideResNet and m = 4 for the other models. We use\nthe public code2 and keep their hyper-parameters.\nImplementation Details\nFor CIFAR-10 and CIFAR-\n100, we use the standard data processing pipeline (mirror+\ncrop) and train the model with 200 epochs. All the results\nreported in this section are averaged over ﬁve runs.\nWe train the models for 200 epochs on the training\nset with 256 examples per mini-batch, and evaluate\nthe trained models on the test set.\nThe",
  "output": "WideResNet 모델에는 m = 10을, 다른 모델에는 m = 4를 설정합니다. 공개 코드를 사용하고 하이퍼파라미터를 유지합니다. CIFAR-10 및 CIFAR-100에 대해 표준 데이터 처리 파이프라인(미러+크롭)을 사용하고 모델을 200 에포크 동안 학습시킵니다. 이 섹션에서 보고된 모든 결과는 5회 실행 평균입니다. 훈련 세트에서 모델을 200 에포크 동안 학습시키고, 미니 배치당 256개의 예제로 테스트 세트에서 훈련된 모델을 평가합니다.",
  "summary_type": "reports"
}{
  "input": "geNet\nclassiﬁcation with a top-1 accuracy of 85.8%.\nKeywords: AutoML, AutoAugment, Knowledge Distillation\n1\nIntroduction\nAutomated machine learning (AutoML) has been attracting increasing atten-\ntions in recent years. In standard image classiﬁcation tasks, there are mainly\ntwo categories of AutoML techniques, namely, neural architecture search (NAS)\nand hyper-parameter optimization (HPO), both of which focus on the possibility\nof using automatically learned strategies to replace human expertise.",
  "output": "AutoML, or Automated Machine Learning, is gaining popularity. It mainly includes two techniques: Neural Architecture Search (NAS) and Hyper-parameter Optimization (HPO). These techniques aim to use automatically learned strategies to replace human expertise in standard image classification tasks. The classification has achieved a top-1 accuracy of 85.8%.",
  "summary_type": "reports"
}{
  "input": "on ImageNet-21k. Our hyper-parameters are\nmostly adopted from Touvron et al. [56, 57].\nHyper-parameter settings. In the case of supervised learning, we train our network with the Lamb\noptimizer [64] with a learning rate of 5 × 10−3 and weight decay 0.2. We initialize the LayerScale\nparameters as a function of the depth by following CaiT [57]. The rest of the hyper-parameters follow\nthe default setting used in DeiT [56]. For the knowledge distillation paradigm, we use the same\nRegNety-16GF [48]",
  "output": "ImageNet-21k üzerinde çalışıyoruz. Hiper-parametrelerimizi çoğunlukla Touvron ve ark. [56, 57] çalışmasından alıyoruz. Hiper-parametre ayarları. Denetimli öğrenme durumunda, ağı Lamb optimizatörü [64] ile 5 × 10−3 öğrenme hızı ve 0.2 ağırlık çürümesi ile eğitiyoruz. LayerScale parametrelerini CaiT [57] takip ederek derinliğin bir fonksiyonu olarak başlatıyoruz. Diğer hiper-parametreler DeiT [56] kullanılan varsayılan ayarı takip eder. Bilgi damıtma paradigması için, aynı RegNety-16GF [48] kullanıyoruz.",
  "summary_type": "reports"
}{
  "input": "ur T2T-ViT backbone. Speciﬁcally, it has a\nsmall channel number and a hidden dimension d but more\nlayers b. For tokens with ﬁxed length Tf from the last layer\nof T2T module, we concatenate a class token to it and then\nadd Sinusoidal Position Embedding (PE) to it, the same as\nViT to do classiﬁcation:\nTf0 = [tcls; Tf] + E,\nE ∈R(l+1)×d\nTfi = MLP(MSA(Tfi−1)),\ni = 1...b\ny = fc(LN(Tfb))\n(5)\n\n--------------------------------------------------\nFixed Tokens\nImage\n224 x 224\n7\n7\nTransformer  layer\n+ PE\ncls",
  "output": "T2T-ViT backbone memiliki jumlah saluran kecil dan dimensi tersembunyi d tetapi lebih banyak lapisan b. Untuk token dengan panjang tetap Tf dari lapisan terakhir modul T2T, kita menggabungkan token kelas ke dalamnya dan kemudian menambahkan Sinusoidal Position Embedding (PE) ke dalamnya, sama seperti ViT melakukan klasifikasi. Tf0 = [tcls; Tf] + E, E ∈R(l+1)×d Tfi = MLP(MSA(Tfi−1)), i = 1...b y = fc(LN(Tfb)) (5)",
  "summary_type": "reports"
}{
  "input": "ilar to Linformer, but uses a convolution layer with ker-\nnel size R and stride R to project the key-value pairs, hence\nresulting in nl/R2 compressed key-value pairs. Therefore,\nThe memory complexity of SRA is O(n2\nl /R2), which is\nstill quadratic w.r.t. nl but with a much smaller constant\n1/R2. When transferring the ImageNet-pretrained SRA-\nmodels to high-resolution tasks, SRA still suffers from the\nquartic computation/memory blow-up w.r.t. the feature map\nresolution. Pyramid Vision Transformer",
  "output": "SRA, similar to Linformer, uses a convolution layer with kernel size R and stride R to project key-value pairs, resulting in nl/R2 compressed pairs. Its memory complexity is O(n2l /R2), still quadratic to nl but with a smaller constant 1/R2. However, when transferring the ImageNet-pretrained SRA models to high-resolution tasks, SRA still experiences a quartic computation/memory blow-up concerning the feature map resolution.",
  "summary_type": "reports"
}{
  "input": "out-\nput complexity. So, in deep layers, concentrated interaction\nmight be enough. There are signiﬁcant differences between\nthe vision and the language domain, and we believe that the\nattention of PiT is suitable for image recognition backbone.\n3.4. Architecture design\nThe architectures proposed in ViT paper [9] aimed at\ndatasets larger than ImageNet. These architectures (ViT-\nLarge, ViT-Huge) have an extremely large scale than gen-\neral ImageNet networks, so it is not easy to compare them\nwith",
  "output": "Deep layers may only require concentrated interaction due to output complexity. There are notable differences between the vision and language domain. The attention of PiT is deemed suitable for image recognition backbone. The architectures proposed in the ViT paper are designed for datasets larger than ImageNet. These architectures (ViT-Large, ViT-Huge) are significantly larger than general ImageNet networks, making comparisons challenging.",
  "summary_type": "reports"
}{
  "input": "properties of\nconvolution into the Transformer backbone [25, 24, 23, 22, 21, 43, 44].\nWhile our work also belongs to this category, we show that our relative attention instantiation is a\nnatural mixture of depthwise convolution and content-based attention with minimum additional cost.\nMore importantly, starting from the perspectives of generalization and model capacity, we take a\nsystematic approach to the vertical layout design and show how and why different network stages\nprefer different typ",
  "output": "Our work falls under the category of integrating convolution properties into the Transformer backbone. We demonstrate that our relative attention model is a blend of depthwise convolution and content-based attention, with minimal extra cost. From the viewpoints of generalization and model capacity, we adopt a systematic approach to the vertical layout design, explaining how and why different network stages prefer various types.",
  "summary_type": "reports"
}{
  "input": "rXiv\npreprint arXiv:1409.1556, 2014.\n[31] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani.\nBottle-\nneck transformers for visual recognition.\narXiv preprint\narXiv:2101.11605, 2021.\n[32] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy,\nand Cordelia Schmid. Videobert: A joint model for video\nand language representation learning. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 7464–7473, 2019.\n[33] Christian Szeged",
  "output": "Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani published a preprint on \"Bottleneck transformers for visual recognition\" on arXiv (arXiv:2101.11605) in 2021. In 2019, Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid presented \"Videobert: A joint model for video and language representation learning\" at the IEEE/CVF International Conference on Computer Vision.",
  "summary_type": "reports"
}{
  "input": ". arXiv preprint arXiv:2101.11605, 2021.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\nand Salakhutdinov, R. Dropout: a simple way to prevent\n\n--------------------------------------------------\nHigh-Performance Normalizer-Free ResNets\nneural networks from overﬁtting. The Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\nSrivastava, R. K., Greff, K., and Schmidhuber, J. Highway\nnetworks. arXiv preprint arXiv:1505.00387, 2015.\nSummers, C. and Dinneen, M. J. Four things ever",
  "output": "Srivastava, Hinton, Krizhevsky, Sutskever, ve Salakhutdinov, aşırı uyumun önlenmesi için basit bir yol olan Dropout'u tanımlarlar (Makine Öğrenme Araştırmaları Dergisi, 2014). Srivastava, Greff ve Schmidhuber, Highway ağları üzerine bir ön baskı yayınladılar (arXiv:1505.00387, 2015). Summers ve Dinneen, dört önemli konuyu tartışıyorlar (arXiv:2101.11605, 2021). Bu bilgiler, Normalizer-Free ResNets'in yüksek performansı hakkında bilgi sağlar.",
  "summary_type": "reports"
}{
  "input": "image-text pairs as positive and all other random\nimage-text pairs that can be formed in a training batch as\nnegative.\nWe minimize the sum of two losses: one for image-to-text\nclassiﬁcation\nLi2t = −1\nN\nN\nX\ni\nlog\nexp(x⊤\ni yi/σ)\nPN\nj=1 exp(x⊤\ni yj/σ)\n(1)\nand the other for text-to-image classiﬁcation\nLt2i = −1\nN\nN\nX\ni\nlog\nexp(y⊤\ni xi/σ)\nPN\nj=1 exp(y⊤\ni xj/σ)\n(2)\nHere, xi and yj are the normalized embedding of image in\nthe i-th pair and that of text in the j-th pair, respectively. N\nis the batch si",
  "output": "The text discusses a method of image-text classification. It involves minimizing the sum of two losses: image-to-text and text-to-image classification. The losses are calculated using normalized embeddings of images and texts in pairs. The pairs are formed in a training batch, with image-text pairs as positive and all other random image-text pairs as negative.",
  "summary_type": "reports"
}{
  "input": "stributed computing. They were proved both efﬁcient and effective in speeding up the\nnetworks. As an extreme case, depthwise convolutions [12, 36] use the number of groups that is\n2\n\n--------------------------------------------------\nequal to the input or output channels, which is followed by point-wise convolutions to aggregate the\ninformation across different channels. Here, the proposed spatially separable self-attention shares\nsome similarities with them.\nPositional Encodings. Most vision tr",
  "output": "Distributed computing has been shown to effectively speed up networks. Depthwise convolutions, an extreme case, use a number of groups equal to the input or output channels, followed by point-wise convolutions to aggregate information across different channels. The proposed spatially separable self-attention shares similarities with them. Positional Encodings are also mentioned in the context of vision.",
  "summary_type": "reports"
}{
  "input": "sparsity-based efﬁcient at-\ntentions that are developed for image generation. The sec-\nond is the memory-based mechanism, including Compres-\nsive Transformers [32] and Set Transformer [20]. These\nmodels use some extra global tokens as static memory and\nallow all the other tokens to attend only to those global\ntokens. The third is the low-rank based mechanism. For\nexample the Linformer [46] projects the input key-value\npairs into a smaller chunk, and performs cross-attention be-\ntween the querie",
  "output": "Three mechanisms are developed for image generation: sparsity-based efficient attentions, memory-based mechanisms like Compressive Transformers and Set Transformer that use extra global tokens as static memory, and low-rank based mechanisms like Linformer that projects input key-value pairs into a smaller chunk for cross-attention.",
  "summary_type": "reports"
}{
  "input": "nd 194M respectively. Compared with OFA [22],\nour method reduces the FLOPs from 230M to 100M and from 595M to 194M respectively, with\nbetter top-1 accuracy. Compared with EfﬁcientNet-B1 with noisy student training [35], our method\nalso reduces the FLOPs by 3.6x while improving the accuracy by 0.5%. Though dynamic networks\nlike GFNet [37], CondConv [12] and BasisNet [7] are more efﬁcient than traditional networks, our\nmethod still has signiﬁcantly higher accuracy with smaller FLOPs. Compared with",
  "output": "Our method significantly reduces FLOPs from 230M to 100M and from 595M to 194M, improving top-1 accuracy compared to OFA. It also outperforms EfficientNet-B1 with noisy student training, reducing FLOPs by 3.6x and enhancing accuracy by 0.5%. Despite the efficiency of dynamic networks like GFNet, CondConv, and BasisNet, our method offers higher accuracy with fewer FLOPs.",
  "summary_type": "reports"
}{
  "input": "similar principles than\nthe one used by Yan et al. [28] but imposing a constrain that\nensures balanced clusters. Concretely, given a set of classes\nYc = {y1, y2, . . . yc} we deﬁne Yk as a partition of Yc into\nk subsets. The grouping starts by constructing a distance\nmatrix Dc, based on the confusion matrix from a pre-trained\nmodel. Given a normalized confusion matrix F with the di-\nagonal set to zero, the distance matrix Dc is constructed by\nsubtracting 1 from it and then making it symmetric b",
  "output": "Yan et al. [28] kullanılan prensiplere benzer bir yöntem uygulanmıştır, ancak dengeli kümelemeyi garantileyen bir kısıtlama getirilmiştir. Yc = {y1, y2, . . . yc} sınıflar kümesi verildiğinde, Yk'yi Yc'nin k alt kümesine bölünmüş olarak tanımlarız. Gruplama, önceden eğitilmiş bir modelden karışıklık matrisine dayalı bir mesafe matrisi Dc oluşturarak başlar. Diyagonalı sıfıra ayarlanmış bir normalize karışıklık matrisi F verildiğinde, mesafe matrisi Dc, ondan 1 çıkarılarak ve sonra simetrik hale getirilerek oluşturulur.",
  "summary_type": "reports"
}{
  "input": "ng [69] with the\ninput tokens. We generate them in 64 dimensions from the 2d patch coordinates and then\nlinearly project to the transformer working dimension d. This choice is orthogonal to the use\nof learned positional encoding, as in ViT [22]. However, it is more ﬂexible since there is no\nneed to interpolate or ﬁne-tune the network when changing the image size.\nModel conﬁgurations.\nIn Table 1 we list different variants of our model which we use in\nour experiments, with different choices for mo",
  "output": "The text discusses a model that generates 64-dimensional inputs from 2D patch coordinates, projected to the transformer working dimension. This approach is more flexible than learned positional encoding, as used in ViT, because it doesn't require interpolation or fine-tuning when changing the image size. The text also mentions different model variants used in experiments.",
  "summary_type": "reports"
}{
  "input": ". We compare BEIT with vision Transformers\ntrained by random initialization, supervised pre-training, and previous self-supervised learning\nmethods. All the compared models are base-size, except iGPT has 1.36B parameters. Pre-training is\nconducted on ImageNet for the comparison purpose, except ViT-JFT300M is pretrained on Google’s\nin-house 300M images.\nCompared with the models trained by random initialization, we ﬁnd that pre-trained BEIT signif-\nicantly improves performance on both datasets. No",
  "output": "BEIT is compared with vision Transformers trained by random initialization, supervised pre-training, and previous self-supervised learning methods. All models are base-size, except iGPT with 1.36B parameters. Pre-training is conducted on ImageNet, except ViT-JFT300M, which is pretrained on Google’s in-house 300M images. BEIT significantly improves performance on both datasets compared to models trained by random initialization.",
  "summary_type": "reports"
}{
  "input": "n Table D.4 one can see there is a large beneﬁt in employing a patch based method\nalong with our XCiT transformers: XCiT-VLAD performs signiﬁcantly better than the CLS\ntoken, likely thanks to the higher dimensionality. This is further magniﬁed with AMSK,\nwhere we obtain results approaching the absolute state of the art on Holidays, despite a sub-\noptimal training setting for image retrieval. This is interesting since our method has not been\nﬁne-tuned for retrieval tasks and we have not been adap",
  "output": "Table D.4 zeigt, dass es einen großen Vorteil hat, eine patchbasierte Methode zusammen mit unseren XCiT-Transformatoren zu verwenden: XCiT-VLAD ist deutlich besser als das CLS-Token, wahrscheinlich dank der höheren Dimensionalität. Dies wird mit AMSK noch verstärkt, wo wir Ergebnisse erzielen, die sich dem absoluten Stand der Technik bei Holidays nähern, trotz einer suboptimalen Trainingseinstellung für die Bildsuche. Dies ist interessant, da unsere Methode nicht für Suchaufgaben optimiert wurde und wir uns nicht angepasst haben.",
  "summary_type": "reports"
}{
  "input": "JFT-300M [54] can achieve better\naccuracies without the need for regularization. To under-\nstand the transfer properties of HaloNet models, we scale up\nHaloNet-H4 by increasing the base width to 128 and evalu-\nate the transfer protocol from [29], pretraining on the public\nImageNet-21k dataset, and ﬁnetuning on ImageNet. Follow-\ning our observation in Table 4, we train a hybrid version of\nthis model with convolutions in the ﬁrst two stages. Note\nthat our hybrids can be seen as using a series of c",
  "output": "JFT-300M [54] can achieve better accuracies without regularization. HaloNet models' transfer properties are understood by scaling up HaloNet-H4, increasing the base width to 128, and evaluating the transfer protocol from [29], pretraining on the public ImageNet-21k dataset, and finetuning on ImageNet. A hybrid version of this model is trained with convolutions in the first two stages. The hybrids can be seen as using a series of c.",
  "summary_type": "reports"
}{
  "input": "oral axis, similar as done in previous work [39,89,58],\nfollowed by application of frame-wise RoIAlign [43] and\ntemporal global average pooling. The RoI features are then\nmax-pooled and fed to a per-class, sigmoid classiﬁer for\nprediction.\nTraining. We initialize the network weights from the Ki-\nnetics models and adopt synchronized SGD training on 64\nGPUs. We use 8 clips per GPU as the mini-batch size and a\nhalf-period cosine schedule of learning rate decaying. The\nbase learning rate is set as 0",
  "output": "El eje oral, similar a lo hecho en trabajos anteriores [39,89,58], se sigue de la aplicación de RoIAlign en cada cuadro [43] y el agrupamiento promedio global temporal. Las características de RoI se agrupan al máximo y se alimentan a un clasificador sigmoidal por clase para la predicción. Entrenamiento. Inicializamos los pesos de la red a partir de los modelos de Kinetics y adoptamos un entrenamiento SGD sincronizado en 64 GPUs. Usamos 8 clips por GPU como tamaño de mini-lote y un horario de tasa de aprendizaje en decadencia de medio período. La tasa de aprendizaje base se establece en 0.",
  "summary_type": "reports"
}{
  "input": "ated in multiple\nworks [24, 39] and a head width of 64 is recommended for visual tasks [10, 35]. We adopt the head\nwidth of 64 in outer transformer block in our model. The number of heads in inner transformer block\n7\n\n--------------------------------------------------\nis another hyper-parameter for investigation. We evaluate the effect of #heads in inner transformer\nblock (Table 7). We can see that a proper number of heads (e.g., 2 or 4) achieve the best performance.\nTable 7: Effect of #heads in",
  "output": "In verschiedenen Arbeiten wird eine Kopfbreite von 64 für visuelle Aufgaben empfohlen. Diese Kopfbreite wird im äußeren Transformatorblock unseres Modells verwendet. Die Anzahl der Köpfe im inneren Transformatorblock ist ein weiterer Hyperparameter zur Untersuchung. Die Auswertung zeigt, dass eine angemessene Anzahl von Köpfen (z.B. 2 oder 4) die beste Leistung erzielt.",
  "summary_type": "reports"
}{
  "input": "tandard training hyperparameters (see Table 7). We then\ncompare the outcome to two other recently proposed efﬁcient\nmodels, MobileNetV3 [22] and FBNetV2 [32]. The results are\nsummarized in Table 8, where we observe that the NAT searched\n\n--------------------------------------------------\n15\nFig. 17: Comparing the performance of adapting supernet, adapting subnet and additional ﬁne-tuning under a bi-objective search setup on four datasets. Details\nare provided in Section 5.5.\nmodel, NAT-M1, is 0.",
  "output": "The text discusses a comparison of the outcome of standard training hyperparameters with two other efficient models, MobileNetV3 and FBNetV2. The results are summarized in a table. It also mentions the performance of adapting supernet, adapting subnet, and additional fine-tuning under a bi-objective search setup on four datasets. The NAT-M1 model is also mentioned.",
  "summary_type": "reports"
}{
  "input": "ral Information Processing Systems,\nvolume 32. Curran Associates, Inc., 2019.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao,\nSonghao Piao, Ming Zhou, and Hsiao-Wuen Hon. UniLMv2: Pseudo-masked language models\nfor uniﬁed language model pre-training. In Proceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of\nMachine Learning Research, pages 642–652. PMLR, 2020. URL http",
  "output": "Hangbo Bao et al. presented UniLMv2, a pseudo-masked language model for unified language model pre-training, at the 37th International Conference on Machine Learning (ICML 2020), a virtual event. The details are published in the Proceedings of Machine Learning Research, volume 119, pages 642-652.",
  "summary_type": "reports"
}{
  "input": "ses signiﬁcantly. Right: The beneﬁ-\ncial effect of locality is particularly strong in the early epochs.\nof the convolutional initialization ([a], +3.1), and is un-\nhelpful otherwise. These mild improvements due to gating\nand convolutional initialization (likely due to performance\nsaturation above 80% top-1) become much clearer in the\nlow data regime. Here, GPSA alone brings +6.8, with an\nextra +2.3 coming from gating, +2.8 from convolution ini-\ntialization and +5.1 with the two together, illustr",
  "output": "La localidad tiene un efecto beneficioso especialmente fuerte en las primeras épocas. La inicialización convolucional ([a], +3.1) es útil, pero no en otros casos. Estas leves mejoras debido a la compuerta y la inicialización convolucional (probablemente debido a la saturación de rendimiento por encima del 80% top-1) se vuelven mucho más claras en el régimen de datos bajos. Aquí, GPSA solo aporta +6.8, con un extra de +2.3 proveniente de la compuerta, +2.8 de la inicialización convolucional y +5.1 con los dos juntos.",
  "summary_type": "reports"
}{
  "input": "images\nare randomly resized and cropped to 512 × 512.\nResults All experimental results on ADE20K are presented\nin Figure 3. Similar to the results of object detection, it\nshows that visual representations learned by ConvMLP can\nbe transferred to pixel-level prediction task like semantic\nsegmentation. More details of the results can be found in\nAppendix.\n4.7. Visualization\nWe\nvisualize\nfeature\nmaps\nof\nResNet50,\nMLP-\nMixer-B/16, Pure-MLP Baseline and ConvMLP-M under\n(1024, 1024) input size (MLP-M",
  "output": "Bilder werden zufällig auf 512 × 512 verkleinert und zugeschnitten. Alle experimentellen Ergebnisse auf ADE20K sind in Abbildung 3 dargestellt. Ähnlich wie bei den Ergebnissen der Objekterkennung zeigt sich, dass visuelle Darstellungen, die von ConvMLP gelernt wurden, auf pixelgenaue Vorhersageaufgaben wie semantische Segmentierung übertragen werden können. Weitere Details zu den Ergebnissen finden Sie im Anhang. Wir visualisieren Feature-Maps von ResNet50, MLP-Mixer-B/16, Pure-MLP-Baseline und ConvMLP-M unter einer Eingangsgröße von (1024, 1024) (MLP-M).",
  "summary_type": "reports"
}{
  "input": "sover operator on four datasets; ImageNet [1], CIFAR-10 [9],\nOxford Flowers102 [19], and Stanford Cars [15]. The hyperpa-\nrameters that we compare are:\n1)\nw/ crx: crossover probability of 0.9; mutation probability\nof 0.1; mutation index ηm of 3.\n2)\nw/o crx: crossover probability of 0.0; mutation probability\nof 0.2; mutation index ηm of 3.\nWe double the mutation probability when crossover is not used to\ncompensate for the reduced exploration ability of the search. On\neach dataset, we run each set",
  "output": "The text discusses a comparison of hyperparameters on four datasets: ImageNet, CIFAR-10, Oxford Flowers102, and Stanford Cars. Two scenarios are considered: with crossover (crx) having a probability of 0.9 and mutation probability of 0.1, and without crossover (w/o crx) having a mutation probability of 0.2. The mutation index ηm is 3 in both cases. The mutation probability is doubled when crossover is not used to compensate for the reduced exploration ability of the search.",
  "summary_type": "reports"
}{
  "input": "etV3 [13], and MixNet [42]. In terms of accuracy and\nefﬁciency, MUXNets are on par with current state-of-the-art\nmodels, i.e. MobileNetV3 and MixNet.\nIn terms of latency, the performance of MUXNet models\nis mixed since they, (i) use non-standard primitives that do\nnot have readily available efﬁcient low-level implementa-\ntions, and (ii) are not explicitly optimized for latency. Com-\npared to methods that use optimized convolutional prim-\nitives but do not directly optimize for latency (Efﬁcient-",
  "output": "MUXNets are comparable to state-of-the-art models like MobileNetV3 and MixNet in terms of accuracy and efficiency. However, their latency performance is mixed due to the use of non-standard primitives lacking efficient low-level implementations and not being explicitly optimized for latency. This is in contrast to methods using optimized convolutional primitives but not directly optimizing for latency.",
  "summary_type": "reports"
}{
  "input": "olutional\nToken Embedding. Conv. Proj.: Convolutional Projection. Hi and Di is the number of heads and embedding feature\ndimension in the ith MHSA module. Ri is the feature dimension expansion ratio in the ith MLP layer.\nstrate that the fusion of local neighboring information plays\nan important role on the performance.\n4. Experiments\nIn this section, we evaluate the CvT model on large-scale\nimage classiﬁcation datasets and transfer to various down-\nstream datasets. In addition, we perform throug",
  "output": "Convolutional Token Embedding, Conv. Proj., fusioniert lokale Nachbarinformationen, was eine wichtige Rolle für die Leistung spielt. Hi und Di sind die Anzahl der Köpfe und die Einbettungsmerkmalsdimension im ith MHSA-Modul. Ri ist das Verhältnis der Merkmalsdimensionserweiterung im ith MLP-Layer. Der CvT-Modell wird auf groß angelegten Bildklassifikationsdatensätzen bewertet und auf verschiedene Downstream-Datensätze übertragen.",
  "summary_type": "reports"
}{
  "input": "86.56\nReﬁned-ViT-448\n1.000\n85.94\n1.130\n85.98\n4.5\nApplied to NLP tasks\nWe also evaluate the performance of Reﬁner-ViT models for natural language processing tasks on the\nGLUE benchmark, to investigate whether Reﬁner also improves other transformer-based models. We\nuse the BERT-small [11] as the baseline model and replace the self-attention module with reﬁner,\nusing the same pre-training dataset and recipes. From the results in Tab. 6, Reﬁner boosts the model\nperformance across all the tasks sign",
  "output": "Reﬁner-ViT-Modelle wurden auch für Aufgaben der natürlichen Sprachverarbeitung auf dem GLUE-Benchmark getestet, um zu untersuchen, ob Reﬁner auch andere Transformer-basierte Modelle verbessert. Als Basismodell wurde BERT-small verwendet und das Self-Attention-Modul durch Reﬁner ersetzt, wobei der gleiche Vor-Trainingsdatensatz und Rezepte verwendet wurden. Die Ergebnisse zeigen, dass Reﬁner die Modellleistung bei allen Aufgaben steigert.",
  "summary_type": "reports"
}{
  "input": "magnitude gradients\nmake the optimization problem simpler by allowing for larger learning rates. In Section\n3.3 of the paper, it is shown that in the case of a single positive and negative, the contrastive\nloss is equivalent to a triplet loss with margin ∝τ. Therefore, in these cases, a larger\ntemperature makes the optimization easier, and classes more separated.\n2. Hard positives/negatives: On the other hand, as shown in Sec 7, the supervised contrastive\nloss has structure that cause hard posit",
  "output": "Los gradientes de magnitud simplifican el problema de optimización al permitir tasas de aprendizaje más grandes. En la Sección 3.3 del documento, se muestra que en el caso de un solo positivo y negativo, la pérdida contrastiva es equivalente a una pérdida de trípode con margen ∝τ. Por lo tanto, en estos casos, una temperatura más grande facilita la optimización y separa más las clases. Por otro lado, como se muestra en la Sección 7, la pérdida contrastiva supervisada tiene una estructura que causa positivos/negativos difíciles.",
  "summary_type": "reports"
}{
  "input": "k 2. We suppose pl, ph ∈(0, 0.71) and pl < ph. To\nachieve the optimal value, when Psup=p, the supervised con-\ntrastive loss value Lsupcon decreases as shown in Eq. (7).\nHere Psup increases from pl to ph, Lsupcon decreases to\na much smaller loss value to achieve the optimal solution,\nwhich implies the need to make two different class samples\nLsupcon = −\nX\nz+∈P (i)\nlog\nexp(z+ · G(xi))\nP\nzk∈A(i) exp(zk · G(xi))\n= −\nX\nz+∈P (i)\nlog\nexp(z+·G(xi))\nExpSum\nP\nzk∈A(i) exp(zk·G(xi))\nExpSum\n= −K∗log 0.035\n1",
  "output": "Text: Suponemos que pl, ph ∈(0, 0.71) y pl < ph. Para alcanzar el valor óptimo, cuando Psup=p, el valor de la pérdida contrastiva supervisada Lsupcon disminuye como se muestra en la Ec. (7). Aquí, Psup aumenta de pl a ph, Lsupcon disminuye a un valor de pérdida mucho menor para alcanzar la solución óptima, lo que implica la necesidad de hacer dos muestras de clases diferentes Lsupcon = -K*log 0.035.",
  "summary_type": "reports"
}{
  "input": "Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.\nImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.\nLavanya Sharan, Ruth Rosenholtz, and Edward H. Adelson. Accuracy and speed of material cate-\ngorization in real-world images. Journal of Vision, 14(10), 2014.\nBaifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, and Jingdong Wang. Informative\ndropout f",
  "output": "Russakovsky et al. published \"ImageNet Large Scale Visual Recognition Challenge\" in IJCV, 2015, contributing to visual recognition research. Sharan, Rosenholtz, and Adelson studied the accuracy and speed of material categorization in real-world images, published in Journal of Vision, 2014. Shi, Zhang, Dai, Zhu, Mu, and Wang worked on a project titled \"Informative dropout f\".",
  "summary_type": "reports"
}{
  "input": "nd\nCrossViT-15† incur 30-50% more FLOPs and parameters\nthan the baselines. However, their accuracy is considerably\nimproved by ∼2.5-5%. On the other hand, CrossViT-18†\nreduces the FLOPs and parameters almost by half compared\nto DeiT-B while still being 1.0% more accurate.\nComparisons with SOTA Transformers. We further com-\npare our proposed approach with some very recent concur-\nrent works on vision transformers. They all improve the\noriginal ViT [11] with respect to efﬁciency, accuracy or\nboth.",
  "output": "CrossViT-15† models have 30-50% more FLOPs and parameters than baselines, but their accuracy improves by ∼2.5-5%. CrossViT-18†, on the other hand, halves the FLOPs and parameters compared to DeiT-B, while increasing accuracy by 1.0%. These models, along with other recent vision transformers, enhance the original ViT [11] in terms of efficiency, accuracy, or both.",
  "summary_type": "reports"
}{
  "input": "m\nImageNet (we include additional results in the Appendix).\nEach attention map shows the output of the QKT opera-\ntion at each of the model’s 512 latent indices and each input\npixel. We visualize attention maps before the softmax, as the\n\n--------------------------------------------------\nPerceiver: General Perception with Iterative Attention\nModel / Inputs\nAudio\nVideo\nA+V\nBenchmark (Gemmeke et al., 2017)\n31.4\n-\n-\nAttention (Kong et al., 2018)\n32.7\n-\n-\nMulti-level Attention (Yu et al., 2018)\n36.",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "correct prediction,\nand the ImageNet model fails. For example in Fig. 7(e), we observe that the network trained on\nImageNet alone is not focusing on the whole bird and is only focusing on the body to make the\ndecision; whereas the one trained with Anisotropic ImageNet is focusing on complete bird to make\na decision.\nWe include more saliency maps on Sketch-ImageNet, and cases where ImageNet trained models are\ncorrect and our model fails in the supplementary material. We show more analysis about",
  "output": "The network trained on ImageNet focuses only on the bird's body for decision-making, while the one trained with Anisotropic ImageNet considers the entire bird. More saliency maps on Sketch-ImageNet and instances where ImageNet trained models succeed and our model fails are included in the supplementary material. Further analysis is also presented.",
  "summary_type": "reports"
}{
  "input": "erleaving local and global attention can produce impressive\nresults, yet it comes with higher throughputs. Both transformer models set a new state of the art in\nimage classiﬁcation, objection detection and semantic/instance segmentation.\n9\n\n--------------------------------------------------\nReferences\n[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, an",
  "output": "Die Kombination von lokaler und globaler Aufmerksamkeit kann beeindruckende Ergebnisse erzielen, führt jedoch zu höheren Durchsätzen. Beide Transformator-Modelle setzen einen neuen Standard in der Bildklassifikation, Objekterkennung und semantischen/instanzbasierten Segmentierung.",
  "summary_type": "reports"
}{
  "input": "3], etc. CNNs are\nbased on locality in that convolutional ﬁlters only perceive\na local region of the input image, i.e. the receptive ﬁeld. By\nstacking multiple layers, the effective receptive ﬁelds of a\ndeep neural network can be enlarged progressively. This\ndesign enables the network to learn a hierarchy of deep fea-\nFigure 1: Comparison between LocalViT and the baseline\ntransformers. The transformers enhanced by the proposed\nlocality mechanism outperform their baselines.\ntures, which is essent",
  "output": "CNNs se basan en la localidad ya que los filtros convolucionales solo perciben una región local de la imagen de entrada, es decir, el campo receptivo. Al apilar múltiples capas, los campos receptivos efectivos de una red neuronal profunda pueden ampliarse progresivamente. Este diseño permite que la red aprenda una jerarquía de características profundas. Los transformadores mejorados por el mecanismo de localidad propuesto superan a sus bases.",
  "summary_type": "reports"
}{
  "input": "since the success of\nAlexNet in 2012 (Krizhevsky et al., 2017), epitomizes this\ntrend. Inductive biases are hard-coded into the architectural\nstructure of CNNs in the form of two strong constraints\non the weights: locality and weight sharing. By encourag-\ning translation equivariance (without pooling layers) and\ntranslation invariance (with pooling layers) (Scherer et al.,\n2010; Schmidhuber, 2015; Goodfellow et al., 2016), the\nconvolutional inductive bias makes models more sample-\nefﬁcient and",
  "output": "Since AlexNet's success in 2012, inductive biases have been hard-coded into the structure of CNNs, with two strong constraints on weights: locality and weight sharing. This encourages translation equivariance (without pooling layers) and translation invariance (with pooling layers), making models more sample-efficient.",
  "summary_type": "reports"
}{
  "input": "mer for visual tasks.\nTo utilize the transformer architectures for conducting visual tasks, a number of researchers have\nexplored for representing the sequence information from different data. For example, Wang et al. ex-\nplore self-attention mechanism in non-local networks [41] for capturing long-range dependencies\nin video and image recognition. Carion et al. present DETR [3], which treats object detection\nas a direct set prediction problem and solve it using a transformer encoder-decoder arch",
  "output": "Forskere har brukt transformer-arkitekturer for visuelle oppgaver ved å representere sekvensinformasjon fra forskjellige data. Wang et al. brukte selvoppmerksomhetsmekanisme i ikke-lokale nettverk for å fange langtrekkende avhengigheter i video- og bildegjenkjenning. Carion et al. presenterte DETR, som behandler objektdeteksjon som et direkte sett prediksjonsproblem og løser det ved hjelp av en transformer encoder-decoder arkitektur.",
  "summary_type": "reports"
}{
  "input": "w-parameter, low-memory models without af-\nfecting accuracy. In Figure 13, we test RegNetX with theses\nconstraints and observe that the constrained version is supe-\nrior across all ﬂop regimes. We use this version in §5, and\nfurther limit depth to 12 ≤d ≤28 (see also Appendix D).\nAlternate design choices. Modern mobile networks often\nemploy the inverted bottleneck (b < 1) proposed in [25]\nalong with depthwise conv [1] (g = 1). In Figure 14 (left),\nwe observe that the inverted bottleneck degrades",
  "output": "RegNetX-Modelle mit niedrigem Speicherbedarf und w-Parameter beeinträchtigen die Genauigkeit nicht. Bei Tests mit diesen Einschränkungen erweist sich die eingeschränkte Version in allen FLOP-Regimen als überlegen. Die Tiefe wird weiter auf 12 ≤d ≤28 begrenzt. Moderne mobile Netzwerke nutzen oft den in [25] vorgeschlagenen umgekehrten Flaschenhals (b < 1) zusammen mit der Tiefenkonvolution [1] (g = 1). Es wird beobachtet, dass der umgekehrte Flaschenhals die Leistung verschlechtert.",
  "summary_type": "reports"
}{
  "input": "ation\nresults. The following describes the details of the experiments, results and analysis.\nProgressive sparsiﬁcation.\nTo verify the effectiveness of the progressive sparsiﬁcation strategy,\nwe test different sparsiﬁcation methods that result in similar overall complexity. Here we provide\nmore detailed results and more analysis. We ﬁnd that progressive sparsiﬁcation is much better than\nsingle-shot sparsiﬁcation. Increasing the number of stages will lead to better performance. Since\nfurther incre",
  "output": "The text discusses the effectiveness of progressive sparsification strategy in experiments. It suggests that progressive sparsification is superior to single-shot sparsification, and that increasing the number of stages improves performance.",
  "summary_type": "reports"
}{
  "input": "of both convolutions and self-attention\ncompared to pure attention models on ImageNet-1K.\n5. Conclusion\nThe design of vision backbone architectures that use\nself-attention is an exciting topic.\nWe hope that our\nwork helps in improving the understanding of architec-\nture design in this space. Incorporating self-attention for\nother computer vision tasks such as keypoint detection [9]\nand 3D shape prediction [23]; studying self-attention ar-\nchitectures for self-supervised learning in computer vi-",
  "output": "The text discusses the design of vision backbone architectures using self-attention, a promising area in computer vision. The authors hope their work will enhance understanding of architecture design in this field. They suggest incorporating self-attention for other tasks like keypoint detection and 3D shape prediction, and exploring self-attention architectures for self-supervised learning in computer vision.",
  "summary_type": "reports"
}{
  "input": "rap.\nCompressive transform-\ners for long-range sequence modelling.\narXiv preprint\narXiv:1911.05507, 2019. 2\n\n--------------------------------------------------\n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learn-\ning with a uniﬁed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019. 5\n[34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards rea",
  "output": "Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu explored the limits of transfer learning with a unified text-to-text transformer in their preprint on arXiv (1910.10683) in 2019. Ren, He, Girshick, and Sun worked on Faster R-CNN, aiming for real-time object detection.",
  "summary_type": "reports"
}{
  "input": "input feature into stripes of equal width. This stripe width is an important\nparameter of the cross-shaped window because it allows us to achieve strong modelling capability\nwhile limiting the computation cost. Speciﬁcally, we adjust the stripe width according to the depth of\nthe network: small widths for shallow layers and larger widths for deep layers. A larger stripe width\nencourages a stronger connection between long-range elements and achieves better network capacity\nwith a small increase",
  "output": "La anchura de la franja es un parámetro importante de la ventana en forma de cruz, ya que permite lograr una fuerte capacidad de modelado limitando el costo computacional. Ajustamos la anchura de la franja según la profundidad de la red: anchuras pequeñas para capas superficiales y anchuras más grandes para capas profundas. Una mayor anchura de la franja fomenta una conexión más fuerte entre elementos de largo alcance y logra una mejor capacidad de red con un pequeño aumento.",
  "summary_type": "reports"
}{
  "input": "tails).\n5\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\nCIFAR-10\nCIFAR-100\nModel\nAugmentation\nSAM\nSGD\nSAM\nSGD\nWRN-28-10 (200 epochs)\nBasic\n2.7±0.1\n3.5±0.1\n16.5±0.2\n18.8±0.2\nWRN-28-10 (200 epochs)\nCutout\n2.3±0.1\n2.6±0.1\n14.9±0.2\n16.9±0.1\nWRN-28-10 (200 epochs)\nAA\n2.1±<0.1\n2.3±0.1\n13.6±0.2\n15.8±0.2\nWRN-28-10 (1800 epochs)\nBasic\n2.4±0.1\n3.5±0.1\n16.3±0.2\n19.1±0.1\nWRN-28-10 (1800 epochs)\nCutout\n2.1±0.1\n2.7±0.1\n14.0±0.1\n17.4±0.1\nWRN-28-10 (1800 epochs",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "sically\napplied to all our models described in Section 3.\nWe\nbrieﬂy describe the default hyperparameters and training\ntechniques as follows.\nPreprocessing In the training phase, a rectangular region\nis randomly cropped using a randomly sampled aspect ratio\n3https://github.com/tensorﬂow/models\nfrom 3/4 to 4/3, and the fraction of cropped area over whole\nimage is randomly chosen from 5% to 100%. Then, the\ncropped region is resized to 224 × 224 and ﬂipped horizon-\ntally with a random probability of",
  "output": "In the training phase, a rectangular region is randomly cropped from the image using a randomly sampled aspect ratio from 3/4 to 4/3. The fraction of the cropped area over the entire image is randomly chosen from 5% to 100%. The cropped region is then resized to 224 × 224 and may be flipped horizontally with a random probability.",
  "summary_type": "reports"
}{
  "input": "e but decreases\nthe elite performance. To study the reasons, we replace the\nself-attention blocks with bottleneck blocks in each stage\nseparately for Net5, by which we can estimate the impor-\ntance of self-attention in different stages. The results are\nshown in Table 3. The replacement of self-attention in all\nthree stages reduces both the base performance and the elite\nperformance. There is a trend that self-attentions in lower\nresolutions play more important roles than those in higher\nresoluti",
  "output": "The study replaces self-attention blocks with bottleneck blocks in each stage of Net5 to assess self-attention's importance at different stages. The results indicate that replacing self-attention in all three stages lowers both base and elite performance. Self-attentions at lower resolutions seem to play more significant roles than those at higher resolutions.",
  "summary_type": "reports"
}{
  "input": "elf-attention in existing architectures. Similar ob-\nservations have been made in natural language processing. Notably, Synthesizer [54] shows that\ndot-product self-attention can be replaced by a feedforward network, with competitive performance\non sentence representation benchmarks. As opposed to our work, Synthesizer does use data dependent\nweights, but in contrast to transformers the weights are determined from the queries only.\n5\nConclusion\nIn this paper we have shown that a simple residual",
  "output": "Synthesizer, a model in natural language processing, demonstrates that dot-product self-attention can be substituted with a feedforward network, yielding competitive results on sentence representation benchmarks. Unlike our work, Synthesizer uses data-dependent weights, but unlike transformers, these weights are determined solely from the queries.",
  "summary_type": "reports"
}{
  "input": "onal space spanned by a larger number of attention\nmaps. As such, the number of attention heads (used for computing the attention maps) are implicitly\nincreased without reducing the embedding dimension per head, enabling the model to enjoy both\nbeneﬁts from more SA heads and high embedding dimension.\nAdditionally, we argue that ignoring the local relationship among the tokens is another main cause\nof the above mentioned over-smoothing issue of global SA. The locality (local receptive ﬁelds)\nand",
  "output": "The text discusses the benefits of increasing the number of attention heads in a model without reducing the embedding dimension per head. This allows the model to benefit from more self-attention (SA) heads and a high embedding dimension. The text also suggests that ignoring the local relationship among tokens can cause over-smoothing issues in global SA.",
  "summary_type": "reports"
}{
  "input": "educed FLOPs, we\nmay construct wider or deeper HVT-S, with 11 heads or 48\nblocks, then the overall FLOPs would be around 4.51G and\n4.33G, respectively. Moreover, we may consider a longer\nsequence by setting a smaller patch size or using a larger\nresolution. For example, with a patch size of 8 and an im-\nage resolution of 192×192, the FLOPs for HVT-S is around\n4.35G. Alternatively, enlarging the image resolution into\n384×384 will lead to 4.48G FLOPs. In all of the above\nmentioned cases, the compu",
  "output": "Reduciendo los FLOPs, podemos construir HVT-S más anchos o profundos, con 11 cabezas o 48 bloques, lo que daría como resultado unos FLOPs totales de alrededor de 4.51G y 4.33G, respectivamente. Además, podríamos considerar una secuencia más larga estableciendo un tamaño de parche más pequeño o utilizando una resolución más grande. Por ejemplo, con un tamaño de parche de 8 y una resolución de imagen de 192×192, los FLOPs para HVT-S son alrededor de 4.35G. Alternativamente, aumentar la resolución de la imagen a 384×384 resultará en 4.48G FLOPs.",
  "summary_type": "reports"
}{
  "input": "e\n83.0\n83.5\n84.0\n84.5\n85.0\n85.5\nImagenet Top-1 Accuracy (%)\nEffNetV2\nEffNet(baseline)\nEffNet(reprod)\nNFNet\nBoTNet\nFigure 3. ImageNet accuracy and training step time on TPUv3 –\nLower step time is better; all models are trained with ﬁxed image\nsize without progressive learning.\nTraining Speed Comparison: Figure 3 compares the train-\ning step time for our new EfﬁcientNetV2, where all models\nare trained with ﬁxed image size without progressive learn-\ning. For EfﬁcientNet (Tan & Le, 2019a), we show t",
  "output": "Figure 3 muestra una comparación de la velocidad de entrenamiento para nuestro nuevo EfficientNetV2, donde todos los modelos se entrenan con un tamaño de imagen fijo sin aprendizaje progresivo. Para EfficientNet (Tan & Le, 2019a), mostramos t.",
  "summary_type": "reports"
}{
  "input": "lnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao\nLiu, Ekin D Cubuk, and Quoc V Le. Rethinking pre-training\nand self-training. In Advances in Neural Information Process-\ning Systems, 2020. 1, 8\n[90] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\nLe. Learning transferable architectures for scalable image\nrecognition. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 2018. 7\n12\n\n--------------------------------------------------\nA. Derivation of the Teacher’s",
  "output": "Inaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D Cubuk, ve Quoc V Le, ön eğitim ve kendini eğitme konularını yeniden düşünmeyi öneriyorlar. Bu, 2020'deki Sinir Bilgi İşlem Sistemlerindeki İlerlemelerde sunulmuştur. Barret Zoph, Vijay Vasudevan, Jonathon Shlens ve Quoc V Le, ölçeklenebilir görüntü tanıma için aktarılabilir mimariler öğrenmeyi önermişlerdir. Bu, 2018'deki IEEE bilgisayar görüşü ve desen tanıma konferansında sunulmuştur.",
  "summary_type": "reports"
}{
  "input": "Xiv preprint arXiv:1711.05101, 2017. 3\n[24] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll´ar. Designing network design\nspaces.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 10428–\n10436, 2020. 4\n[25] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\n\n--------------------------------------------------\nAditya Khosla, Michael Bernstein, et al.\nImagenet l",
  "output": "Xiv preprint arXiv:1711.05101, 2017. 3\n[24] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, ve Piotr Doll´ar. Ağ tasarım alanlarını tasarlama. IEEE/CVF Bilgisayar Görüşü ve Desen Tanıma Konferansı Bildirilerinde, sayfalar 10428–10436, 2020. 4\n[25] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, vb. Imagenet l",
  "summary_type": "reports"
}{
  "input": "and -101 [25]. Note that MDEQ\nhas only one layer of residual blocks followed by multi-resolution fusion. Therefore, to match the\ncapacity of standard explicit models, we need to increase the feature dimensionality within MDEQ.\nThis is accomplished mainly by adjusting the width of the convolutional ﬁlter within the residual\nblock (see Figure 2).\nTable 2 shows the accuracy of two MDEQs (of different sizes) in comparison to well-known reference\nmodels in computer vision. MDEQs are remarkably compe",
  "output": "MDEQ (Multi-resolution Deep Equilibrium Model) has a single layer of residual blocks followed by multi-resolution fusion. To match the capacity of standard models, the feature dimensionality within MDEQ is increased, mainly by adjusting the convolutional filter width within the residual block. Table 2 compares the accuracy of two different sized MDEQs with well-known reference models in computer vision, showing MDEQs' notable competitiveness.",
  "summary_type": "reports"
}{
  "input": "(JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7\n22\n\n--------------------------------------------------",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "Training BEIT: Masked Image Modeling\nWe propose a masked image modeling (MIM) task to pretrain BEIT. We randomly mask some\npercentage of image patches, and then predict the visual tokens that are corresponding to the masked\npatches.\nFigure 1 shows the overview of our method. As presented in Section 2.1, given an input image\nx, we split it into N image patches ({xp\ni }N\ni=1), and tokenize it to N visual tokens ({zi}N\ni=1). We\nrandomly mask approximately 40% image patches, where the masked positio",
  "output": "Se propone una tarea de modelado de imágenes enmascaradas (MIM) para preentrenar BEIT. Se enmascaran aleatoriamente algunos porcentajes de parches de imagen, y luego se predicen los tokens visuales que corresponden a los parches enmascarados. Dada una imagen de entrada, se divide en N parches de imagen y se tokeniza en N tokens visuales. Se enmascaran aleatoriamente aproximadamente el 40% de los parches de imagen.",
  "summary_type": "reports"
}{
  "input": ", green\nborder indicates humans are not contained.\n13\n\n--------------------------------------------------\nReferences\n[1] Yikang Zhang, Jian Zhang, and Zhao Zhong. Autobss: An efﬁcient algorithm for block stacking\nstyle search. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,\nAdvances in Neural Information Processing Systems, volume 33, pages 10259–10270. Curran\nAssociates, Inc., 2020.\n[2] Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions o",
  "output": "<Text: A green border indicates that humans are not contained. Yikang Zhang, Jian Zhang, and Zhao Zhong developed an efficient algorithm for block stacking style search, as detailed in the Advances in Neural Information Processing Systems. Lars Kai Hansen and Peter Salamon have also contributed to the field with their work on neural network ensembles.>",
  "summary_type": "reports"
}{
  "input": "d and self-supervised settings. We also distinguish our work from other data\naugmentation strategies like Auto-Augment (Cubuk et al., 2018) which uses Reinforcement Learn-\ning to automatically search for improved data augmentation policies and introduces Patch Gaussian\nAugmentation, which allows the network to interpolate between robustness and accuracy (Lopes\net al., 2019). The motivation behind our proposed approach is to suppress the reliance of CNNs on\nlow-level cues and encourage CNNs to le",
  "output": "Our work differs from other data augmentation strategies like Auto-Augment (Cubuk et al., 2018), which uses Reinforcement Learning to search for improved data augmentation policies, and introduces Patch Gaussian Augmentation, enhancing network's balance between robustness and accuracy (Lopes et al., 2019). Our approach aims to reduce CNNs' dependence on low-level cues and promote their learning ability.",
  "summary_type": "reports"
}{
  "input": "d as performing 1D DFT on the two dimensions alternatively. Similar to 1D\nDFT, 2D DFT of real input x[m, n] satisﬁed the conjugate symmetry property X[M −u, N −v] =\nX∗[u, v]. The FFT algorithms can also be applied to 2D DFT to improve computational efﬁciency.\n3.2\nGlobal Filter Networks\nOverall architecture. Recent advances in vision transformers [10, 43] demonstrate that models\nbased on self-attention can achieve competitive performance even without the inductive biases\nassociated with the convo",
  "output": "2D DFT, similar to 1D DFT, satisfies the conjugate symmetry property X[M −u, N −v] = X∗[u, v] and can utilize FFT algorithms for improved computational efficiency. Recent advances in vision transformers show that models based on self-attention can achieve competitive performance without the inductive biases associated with convolution.",
  "summary_type": "reports"
}{
  "input": "in an attention-free network whose top-1 accuracy on Imagenet-\nval is 79.2%, which is comparable to a ResNet-50 trained with a modern training strategy. This\nnetwork has served as our baseline for subsequent ablations. Note that, at this stage, we still include\nLayerScale, a class embedding (in the class-MLP stage) and positional encodings.\nDistillation.\nThe same model trained with distillation inspired by Touvron et al. [56] achieves\n81.5%. The distillation variant we choose corresponds to the",
  "output": "In an attention-free network, the top-1 accuracy on Imagenet-val is 79.2%, comparable to a ResNet-50 trained with a modern strategy. This network, which includes LayerScale, a class embedding, and positional encodings, serves as a baseline for further ablations. When trained with distillation inspired by Touvron et al., the same model achieves 81.5%.",
  "summary_type": "reports"
}{
  "input": "n, the\ncropped region is resized to 224 × 224 and ﬂipped horizon-\ntally with a random probability of 0.5 followed by the RGB\nchannel normalization. During validation, shorter dimen-\nsion of each image is resized to 256 pixels while the aspect\nratio is maintained. Next, the image is center-cropped to\n224 × 224, and the RGB channels are normalized.\nHyperparameter We use 1,024 batch size for training\nwhich is close to the maximum size that can be received\nby a single machine with 8 P40 GPUs. Stocha",
  "output": "During image processing, the cropped region is resized to 224 × 224, possibly flipped horizontally, and RGB channels are normalized. For validation, images are resized to 256 pixels (maintaining aspect ratio), center-cropped to 224 × 224, and RGB channels are normalized. The training uses a batch size of 1,024, near the maximum size manageable by a single machine with 8 P40 GPUs.",
  "summary_type": "reports"
}{
  "input": "urrent neural\nnetworks. arXiv preprint arXiv:1308.0850, 2013. 5\n[16] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017. 9, 14\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 1, 5, 6, 14\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In Eu",
  "output": "Kaiming He, Georgia Gkioxari, Piotr Dollár, ve Ross Girshick, Mask r-cnn'yi ICCV'de 2017'de sundular. Kaiming He, Xiangyu Zhang, Shaoqing Ren ve Jian Sun, 2016'da IEEE bilgisayar görüşü ve desen tanıma konferansında derin kalıntı öğrenmesi için görüntü tanıma üzerine çalıştılar. Aynı ekip, derin kalıntı ağlarında kimlik eşlemeleri üzerine çalıştı.",
  "summary_type": "reports"
}{
  "input": "Hengel.\nWider or deeper: Revisiting the resnet model for visual\nrecognition. arXiv preprint arXiv:1611.10080, 2016. 8\n[45] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. In ICCV, 2021. 2\n[46] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. In BMVC, 2016. 8\n[47] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,\nXin Jiang, and Qun Liu.\nTernarybe",
  "output": "Hengel reevaluated the ResNet model for visual recognition in 2016. Li Yuan and others developed Tokens-to-token ViT, training vision transformers from scratch on ImageNet, presented at ICCV in 2021. Sergey Zagoruyko and Nikos Komodakis introduced wide residual networks at BMVC in 2016. Wei Zhang, Lu Hou, and others worked on a project named Ternarybe.",
  "summary_type": "reports"
}{
  "input": "d Nick Barnes. Deep texture and structure aware ﬁltering network for\nimage smoothing, 2018. 4\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of\naircraft. Technical report, 2013. 2, 5\nIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representa-\ntions, 2019. 3\nT. Nathan Mundhenk, Daniel Ho, and Barry Y. Chen. Improvements to context based self-supervised\nlearning. 2018 IEEE/CVF Conference on Computer Vision and Pat",
  "output": "Nick Barnes, 2018, a développé un réseau de filtrage conscient de la texture et de la structure pour le lissage des images. S. Maji et al., 2013, ont réalisé une classification visuelle fine des avions. Ishan Misra et Laurens van der Maaten, 2019, ont étudié l'apprentissage auto-supervisé de représentations invariantes de prétexte. T. Nathan Mundhenk et al., ont apporté des améliorations à l'apprentissage auto-supervisé basé sur le contexte lors de la Conférence IEEE/CVF 2018 sur la vision par ordinateur.",
  "summary_type": "reports"
}{
  "input": "ls uses UDA in training the teacher. Both of these base-\nlines use the same experimental protocols and hence ensure\na fair comparison. We follow [48]’s train/eval/test splitting,\nand we use the same amount of resources to tune hyper-\nparameters for our baselines as well as for Meta Pseudo\nLabels. More details are in Appendix C.\nAdditional baselines.\nIn addition to these two baselines,\nwe also include a range of other semi-supervised baselines\nin two categories: Label Propagation and Self-Supervi",
  "output": "ls utiliza UDA en la formación del profesor. Ambas líneas base utilizan los mismos protocolos experimentales, asegurando una comparación justa. Seguimos la división de entrenamiento/evaluación/prueba de [48], y utilizamos la misma cantidad de recursos para ajustar los hiperparámetros de nuestras líneas base y de Meta Pseudo Labels. Más detalles en el Apéndice C. Líneas base adicionales. Además de estas dos líneas base, también incluimos una serie de otras líneas base semi-supervisadas en dos categorías: Propagación de Etiquetas y Auto-Supervisión.",
  "summary_type": "reports"
}{
  "input": "eleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30(4):838–855, 1992.\n[37] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and J. Shlens. Stand-alone\nself-attention in vision models. In NeurIPS, 2019.\n[38] M. Sandler, J. Baccash, A. Zhmoginov, and Howard. Non-discriminative data or weak model?\nOn the relative importance of data and model resolution. In ICCV Workshop on Real-World\nRecognition from Low-Quality Images and Videos, 2019.\n[39",
  "output": "Stochastic approximation acceleration by averaging was discussed in the SIAM Journal on Control and Optimization in 1992. In 2019, at NeurIPS, Ramachandran et al. presented on stand-alone self-attention in vision models. Also in 2019, Sandler et al. explored the relative importance of data and model resolution in non-discriminative data or weak models at the ICCV Workshop on Real-World Recognition from Low-Quality Images and Videos.",
  "summary_type": "reports"
}{
  "input": "D relative encoding into\nhorizontal and vertical directions, such that each direction\ncan by modeled by a 1D encoding. The method formula-\ntion is given as follows\neij =\n(xiWQ)(xjWK+ concat(pK\nδ˜x, pK\nδ˜y))T\n√dz\n,\n(12)\nwhere δ˜x = ˜xi −˜xj and δ˜y = ˜yi −˜yj denote the relative\nposition offsets on x-axis and y-axis of the image coordi-\nnate respectively, pK\nδ˜x and pK\nδ˜y are learnable vectors with\nlength 1\n2dz, the concat operation concatenates the two en-\ncodings to form a ﬁnal relative encodi",
  "output": "The text describes a method for relative encoding into horizontal and vertical directions. This is modeled by a 1D encoding. The method uses a formula that includes position offsets on the x-axis and y-axis of the image coordinate. It also involves learnable vectors with a specific length and a concatenation operation to form the final relative encoding.",
  "summary_type": "reports"
}{
  "input": "age which generated the anchor image.\n• Views of a sample image different from that which generated the anchor image but having\nthe same label as that of the anchor.\nThe loss for self-supervised contrastive learning (Eq. 1 in the paper) is a special case of SupCon\nwhen P(i) is restricted to contain only a view of the same source image as that of the anchor (i.e.,\nthe ﬁrst category above). In this case, P(i) = j(i), where j(i) is the index of view, and Eq. 25\nreadily takes on the self-supervised",
  "output": "The text discusses self-supervised contrastive learning. It mentions two types of images: the anchor image and a sample image with the same label as the anchor. The loss for self-supervised contrastive learning is a special case of SupCon when P(i) is restricted to contain only a view of the same source image as that of the anchor. In this case, P(i) equals j(i), where j(i) is the view index.",
  "summary_type": "reports"
}{
  "input": "se-\nmantic segmentation examples in Figure 14.\nC.2. Generalization and Robustness\nTo further evaluate the generalization performance of our pro-\nposed models, we compare on a recently proposed benchmark\ndataset, ImageNetV2 [32], complementary to the original Im-\nageNet 2012.\nWe use the MatchedFrequency version of the\nImageNet-V2.\nFigure 15a reports the top-5 accuracy compari-\n\n--------------------------------------------------\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n2\n4\n6\n\u001b\n10\n0.5\n1\n1.5\n2\nLHaYH­RXW\u0003UaWLR\nGURXS\u0003I",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "images, ViT-B/16-SAM outperforms ViT-B/16-AUG by 7.6%. Apart from the improved validation\naccuracy, we also observe that both SAM and strong augmentations increase the training error (see\nFigure 2 (Middle) and Table 4), indicating their regularization effects. However, they have distinct\ntraining dynamics as the loss curve for ViT-B/16-AUG is much nosier than ViT-B/16-SAM.\nTable 4:\nComparison between ViT-B/16-\nSAM and ViT-B/16-AUG. We average the\nresults for 1,000 random noises when calcu-\nlati",
  "output": "ViT-B/16-SAM supera a ViT-B/16-AUG en un 7,6%. Además de la mejora en la precisión de validación, observamos que tanto SAM como las fuertes aumentaciones incrementan el error de entrenamiento, indicando sus efectos de regularización. Sin embargo, tienen dinámicas de entrenamiento distintas ya que la curva de pérdida para ViT-B/16-AUG es mucho más ruidosa que ViT-B/16-SAM. Comparamos entre ViT-B/16-SAM y ViT-B/16-AUG promediando los resultados para 1.000 ruidos aleatorios al calcular.",
  "summary_type": "reports"
}{
  "input": "hape\n(3, 224, 224) into (256, 14, 14) with 184 MFLOPs. For\ncomparison, the ﬁrst 10 layers of a ResNet-18 perform\nthe same dimensionality reduction with 1042 MFLOPs.\nNo classiﬁcation token.\nTo use the BCHW tensor for-\nmat, we remove the classiﬁcation token. Similar to convo-\nlutional networks, we replace it by average pooling on the\nlast activation map, which produces an embedding used\nin the classiﬁer. For distillation during training, we train\nseparate heads for the classiﬁcation and distillati",
  "output": "Tekst ini menjelaskan proses transformasi tensor dari (3, 224, 224) menjadi (256, 14, 14) dengan 184 MFLOPs, dibandingkan dengan 10 lapisan pertama ResNet-18 yang melakukan reduksi dimensi yang sama dengan 1042 MFLOPs. Tidak ada token klasifikasi. Untuk menggunakan format tensor BCHW, kami menghapus token klasifikasi dan menggantinya dengan pooling rata-rata pada peta aktivasi terakhir, yang menghasilkan embedding yang digunakan dalam klasifikasi. Selama pelatihan, kami melatih kepala terpisah untuk klasifikasi dan distilasi.",
  "summary_type": "reports"
}{
  "input": "d shed light on the difﬁculties of comparing optimizers for\nneural network training more generally.\n1\nIntroduction\nIn recent years, hardware systems employing GPUs and TPUs have enabled neural network training\nprograms to process dramatically more data in parallel than ever before. The most popular way to\nexploit these systems is to increase the batch size in the optimization algorithm (i.e. the number\nof training examples processed per training step). On many workloads, modern systems can scale",
  "output": "In recent years, hardware systems using GPUs and TPUs have allowed neural network training programs to process significantly more data in parallel. The most common method to leverage these systems is to increase the batch size in the optimization algorithm, which is the number of training examples processed per training step. Modern systems can scale on many workloads.",
  "summary_type": "reports"
}{
  "input": "osed method achieves better performance than supervised pre-\ntraining, although BEIT does not require manual annotations for pre-training. Moreover, we employ\nintermediate ﬁne-tuning for BEIT on ImageNet, i.e., we ﬁrst ﬁne-tune pretrained BEIT on ImageNet,\nand then ﬁne-tune the model on ADE20K. The results indicate that intermediate ﬁne-tuning further\nimproves BEIT on semantic segmentation.\n3.3\nAblation Studies\nWe conduct ablation studies to analyze the contributions of each component in BEIT. T",
  "output": "The BEIT method outperforms supervised pre-training without needing manual annotations. An intermediate fine-tuning process on ImageNet further enhances BEIT's performance on semantic segmentation. Ablation studies are conducted to understand the contributions of each BEIT component.",
  "summary_type": "reports"
}{
  "input": "ith softmax normalization.\nThe size and number of convolutional ﬁlters, and the num-\nber of fully connected neurons were determined via meta-\nparameter search. See Section C of the supplementary ma-\nterial for further details about said parameters.\nWe use a single SSAL branch (i.e. an auxiliary classiﬁer)\nwith either 2, 4, 10 or 20 groups, and a grouping criterion\nthat either splits or joins visually similar classes following\nthe computation outlined in Section 3.1. The model pre-\ndiction is don",
  "output": "The model uses a single SSAL branch with 2, 4, 10, or 20 groups, and a grouping criterion that either splits or joins visually similar classes. The size and number of convolutional filters, and the number of fully connected neurons were determined through a meta-parameter search. Further details about these parameters can be found in Section C of the supplementary material.",
  "summary_type": "reports"
}{
  "input": "es multiply-add cumulation and Params\nmeans the number of parameters in the model.\n4\nRELATED WORKS\nFast Transformers\nA line of recent works have been\ndeveloped to approximate self-attention layers to improve\nover the O(N 2) complexity. These methods keep the ba-\nsic design of self-attention, mapping N inputs to N out-\nputs, and is hence different in purpose with our method,\nwhich compresses N inputs to a smaller number M of out-\nputs. Among these works, Reformer (Kitaev et al., 2019)\nuses Locall",
  "output": "Recientes trabajos se han desarrollado para aproximar capas de auto-atención y mejorar la complejidad O(N 2). Estos métodos mantienen el diseño básico de auto-atención, mapeando N entradas a N salidas, y por lo tanto difieren en propósito con nuestro método, que comprime N entradas a un número menor M de salidas. Entre estos trabajos, Reformer (Kitaev et al., 2019) utiliza Locall.",
  "summary_type": "reports"
}{
  "input": "Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\nexperiments.\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\nparameters – weight decay, dropout, and label smoothing. Figure 3 shows the results after ﬁne-\ntuning to ImageNe",
  "output": "Transformer performs well when pre-trained on a large JFT-300M dataset. The importance of dataset size is examined by pre-training ViT models on increasing dataset sizes: ImageNet, ImageNet-21k, and JFT-300M. To enhance performance on smaller datasets, three basic regularization parameters are optimized: weight decay, dropout, and label smoothing. The results are shown after fine-tuning to ImageNet.",
  "summary_type": "reports"
}{
  "input": "-------------------\nTable 2: Evaluation on ImageNet classiﬁcation\nwith top-1 and top-5 accuracies reported. MDEQs\nwere trained for 100 epochs.\nModel Size\ntop1 Acc.\ntop5 Acc.\nAlexNet [31]\n238M\n57.0%\n80.3%\nResNet-18 [25]\n13M\n70.2%\n89.9%\nResNet-34 [25]\n21M\n74.8%\n91.1%\nInception-V2 [28]\n12M\n74.8%\n92.2%\nResNet-50 [25]\n26M\n75.1%\n92.5%\nHRNet-W18-C [55]\n21M\n76.8%\n93.4%\nSingle-stream DEQ + global pool [5]\n18M\n72.9%\n91.0%\nMDEQ-small (ours) [Implicit]\n18M\n75.5%\n92.7%\nResNet-101 [25]\n52M\n77.1%\n93.5%\nW-ResNe",
  "output": "Table 2: ImageNet classification evaluation with top-1 and top-5 accuracies. Models were trained for 100 epochs. AlexNet has 238M size, 57.0% top1 accuracy, 80.3% top5 accuracy. ResNet-18 has 13M size, 70.2% top1, 89.9% top5. ResNet-34 has 21M size, 74.8% top1, 91.1% top5. Inception-V2 has 12M size, 74.8% top1, 92.2% top5. ResNet-50 has 26M size, 75.1% top1, 92.5% top5. HRNet-W18-C has 21M size, 76.8% top1, 93.4% top5. Single-stream DEQ + global pool has 18M size, 72.9% top1, 91.0% top5. MDEQ-small has 18M size, 75.5% top1, 92.7% top5. ResNet-101 has 52M size, 77.1% top1, 93.5% top5.",
  "summary_type": "reports"
}{
  "input": "8,2,3,192\n1,2,6,384\nSmall 1-10-1\n1,8,3,192\n10,2,6,384\n1,2,12,768\nSmall 2-9-1\n2,8,3,192\n9,2,6,384\n1,2,12,768\nSmall 1-9-2\n1,8,3,192\n9,2,6,384\n2,2,12,768\nSmall 2-8-2\n2,8,3,192\n8,2,6,384\n2,2,12,768\nSmall 1-1-9-1 1,4,3,96\n1,2,3,192\n9,2,6,384\n1,2,12,768\nSmall 1-2-8-1 1,4,3,96\n2,2,3,192\n8,2,6,384\n1,2,12,768\nTable 9. Model architecture for multi-scale stacked ViTs. Archi-\ntecture parameters for each E-ViT module E-ViT(a × n/p ; h, d):\nnumber of attention blocks n, input patch size p, number of heads\nh",
  "output": "The text provides the model architecture for multi-scale stacked Vision Transformers (ViTs). It details the architecture parameters for each E-ViT module, which include the number of attention blocks (n), input patch size (p), and number of heads (h). The data provided seems to be specific configurations of these parameters.",
  "summary_type": "reports"
}{
  "input": "e-excitation networks and beyond,” in ICCV\nWorkshops, 2019, pp. 1971–1980.\n[64]\nA. Radford, L. Metz, and S. Chintala, “Unsupervised represen-\ntation learning with deep convolutional generative adversarial\nnetworks,” in ICLR (Poster), 2016.\n[65]\nX. Mao, Q. Li, H. Xie, R. Y. K. Lau, Z. Wang, and S. P. Smolley,\n“Least squares generative adversarial networks,” in ICCV, 2017,\npp. 2813–2821.\n[66]\nX. Wei, B. Gong, Z. Liu, W. Lu, and L. Wang, “Improving the\nimproved training of wasserstein gans: A consi",
  "output": "Text:\n\"e-excitation networks and beyond,\" pada ICCV Workshops, 2019, hal. 1971–1980.\n[64]\nA. Radford, L. Metz, dan S. Chintala, \"Pembelajaran representasi tanpa pengawasan dengan jaringan generatif adversarial konvolusional dalam,\" pada ICLR (Poster), 2016.\n[65]\nX. Mao, Q. Li, H. Xie, R. Y. K. Lau, Z. Wang, dan S. P. Smolley, \"Jaringan generatif adversarial kuadrat terkecil,\" pada ICCV, 2017, hal. 2813–2821.\n[66]\nX. Wei, B. Gong, Z. Liu, W. Lu, dan L. Wang, \"Meningkatkan pelatihan yang ditingkatkan dari wasserstein gans: Sebuah konsi",
  "summary_type": "reports"
}{
  "input": "s the resolution of each image patch, and N = HW/P 2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\nded",
  "output": "In image processing, each image patch's resolution is defined, and N = HW/P 2 is the number of patches, serving as the input sequence length for the Transformer. The Transformer uses a constant latent vector size D across all layers, and patches are flattened and mapped to D dimensions with a trainable linear projection. This projection output is referred to as the patch embeddings. Similar to BERT’s [class] token, a learnable embedding is prepended to the sequence of embedded patches.",
  "summary_type": "reports"
}{
  "input": "(CNNs) can learn powerful visual features that have resulted\nin signiﬁcant improvements on many computer vision tasks such as semantic segmentation (Shel-\nhamer et al., 2017), object recognition (Krizhevsky et al., 2012), and object detection (Ren et al.,\n2015). However, CNNs often fail to generalize well across datasets under domain-shift due to var-\nied lighting, sensor resolution, spectral-response etc. One of the reasons for this poor generalization\nis CNNs’ over reliance on low-level cues",
  "output": "Convolutional Neural Networks (CNNs) have significantly improved many computer vision tasks like semantic segmentation, object recognition, and object detection. However, they often struggle to generalize across datasets under domain-shift due to factors like varied lighting and sensor resolution. This poor generalization is partly due to CNNs' over-reliance on low-level cues.",
  "summary_type": "reports"
}{
  "input": "ccuracy of Swin Transformer using different methods\nfor self-attention computation on three benchmarks.\nsentation and has linear computational complexity with re-\nspect to input image size. Swin Transformer achieves the\nstate-of-the-art performance on COCO object detection and\nADE20K semantic segmentation, signiﬁcantly surpassing\nprevious best methods. We hope that Swin Transformer’s\nstrong performance on various vision problems will encour-\nage uniﬁed modeling of vision and language signals.\nAs",
  "output": "Swin Transformer, with linear computational complexity relative to input image size, achieves state-of-the-art performance on COCO object detection and ADE20K semantic segmentation, significantly outperforming previous methods. Its strong performance on various vision problems may encourage unified modeling of vision and language signals.",
  "summary_type": "reports"
}{
  "input": "arison, where a 1D FFT on feature dimension and a 2D FFT on spatial\ndimensions are used to mix tokens. As shown in Table 6, our method outperforms all baseline\nmethods except DeiT-S that has 64% higher FLOPs.\nRobustness & generalization ability. Inspired by the [32], we further conduct experiments to\nevaluate the robustness and the generalization ability of the GFNet. For robustness, we consider\nImageNet-A, ImageNet-C, FGSM and PGD. ImageNet-A [16] (IN-A) is a challenging dataset that\ncontains n",
  "output": "GFNet's method surpasses all baseline methods except DeiT-S, which has 64% higher FLOPs. Inspired by [32], experiments were conducted to evaluate GFNet's robustness and generalization ability. For robustness, ImageNet-A, ImageNet-C, FGSM, and PGD were considered. ImageNet-A (IN-A) is a challenging dataset.",
  "summary_type": "reports"
}{
  "input": "en observe\nthat the gradient descent update yields a generalization\nof self-attention layer and use it to motivate the design of\nour new module.\nUsing our new modules, we build centroid transformers,\nin which we insert our centroid attention modules between\ntypical self-attention modules. We apply centroid trans-\nformers on several challenging scenarios, ranging from\nnatural language processing to computer vision. On ab-\n1\narXiv:2102.08606v2  [cs.LG]  8 Mar 2021\n\n--------------------------------",
  "output": "The text discusses the development of a new module inspired by the generalization of the self-attention layer through gradient descent update. This new module is used to construct centroid transformers, which are applied in various challenging scenarios, including natural language processing and computer vision. The centroid attention modules are inserted between typical self-attention modules in these transformers.",
  "summary_type": "reports"
}{
  "input": "l., 2016; Zhang et al., 2020; Bello, 2021).\nWide ResNets (Zagoruyko & Komodakis, 2016) and Mo-\nbileNets (Howard et al., 2017) instead scale the width. In-\ncreasing image resolutions has also been a reliable source\nof progress. Thus as training budgets have grown, so have\nthe image resolutions: EfﬁcientNet uses 600 image reso-\nlutions (Tan & Le, 2019) and both ResNeSt (Zhang et al.,\n2020) and TResNet (Ridnik et al., 2020) use 448 image\nresolutions for their largest model. In an attempt to sys-\nte",
  "output": "Wide ResNets (Zagoruyko & Komodakis, 2016) y MobileNets (Howard et al., 2017) escalan el ancho. El aumento de las resoluciones de imagen ha sido una fuente confiable de progreso. Conforme han crecido los presupuestos de entrenamiento, también lo han hecho las resoluciones de imagen: EfficientNet utiliza 600 resoluciones de imagen (Tan & Le, 2019) y tanto ResNeSt (Zhang et al., 2020) como TResNet (Ridnik et al., 2020) utilizan 448 resoluciones de imagen para su modelo más grande.",
  "summary_type": "reports"
}{
  "input": "better since more discriminative\nparts’ features will be extracted; then, attention regions pro-\nvide accurate location of object, which ensures our model\nto look at the object closer and further improve the per-\nformance.\nComprehensive experiments in common ﬁne-\ngrained visual classiﬁcation datasets also prove the effec-\ntiveness of WS-DAN.\nDCL is also a data augmentation method for ﬁne-grained\nrecognition, it enhances the difﬁculty of ﬁne-grained recog-\nnition and exercises the classiﬁcation m",
  "output": "WS-DAN and DCL are effective methods for fine-grained visual classification. WS-DAN extracts more discriminative features and provides accurate object location, improving model performance. DCL is a data augmentation method that enhances the difficulty of fine-grained recognition, thereby improving classification.",
  "summary_type": "reports"
}{
  "input": "f MUXNets on three tasks; im-\nage classiﬁcation, object detection, and transfer learning.\n5.1. Hyperparameter Search Details\nSearch Space: To compensate for the extra hyperparam-\neters introduced by spatial and channel multiplexing, we\nconstrain the commonly adopted layer-wise search space [1,\n40, 13] to a stage-wise search space, where layers within\nthe same stage share the same hyperparameters. MUXNets\nconsist of four stages, where each stage begins with a reduc-\ntion block and is followed by",
  "output": "MUXNets se aplica en tres tareas: clasificación de imágenes, detección de objetos y aprendizaje de transferencia. Para compensar los hiperparámetros adicionales introducidos por la multiplexación espacial y de canal, limitamos el espacio de búsqueda por capas a un espacio de búsqueda por etapas, donde las capas dentro de la misma etapa comparten los mismos hiperparámetros. Los MUXNets constan de cuatro etapas, cada una comienza con un bloque de reducción.",
  "summary_type": "reports"
}{
  "input": "e\nhave considered the Cait-S24 model for faster iterations. This network consists of 24-layer with a\nworking dimension of 384. All our experiments below were carried out with images in resolution\n224×224 and N = 16 × 16 patches. Trained with regular supervision, Cait-S24 attains 82.7% top-1\nacc. on Imagenet.\nSA →MLP.\nThe self-attention can be seen a weight generator for a linear transformation on the\nvalues. Therefore, our ﬁrst design modiﬁcation was to get rid of the self-attention by replacing",
  "output": "Se ha considerado el modelo Cait-S24 para iteraciones más rápidas. Esta red consta de 24 capas con una dimensión de trabajo de 384. Todos nuestros experimentos se realizaron con imágenes en resolución 224×224 y parches N = 16 × 16. Entrenado con supervisión regular, Cait-S24 alcanza el 82.7% de precisión top-1 en Imagenet. La autoatención puede verse como un generador de peso para una transformación lineal en los valores. Por lo tanto, nuestra primera modificación de diseño fue eliminar la autoatención al reemplazarla.",
  "summary_type": "reports"
}{
  "input": "reported by the original UDA paper [76]. In their work, UDA [76] use a much\nlarger batch size for their UDA objective. In our implementation of UDA, we keep these batch sizes the same. This leads to a\nmuch easier implementation of data parallelism in our framework, TensorFlow [1] running on TPU big pods. To compensate\nfor the difference, we train all UDA baselines for much longer than the UDA paper [76]. During the training process, we also\nmask out the supervised examples with high conﬁdence. E",
  "output": "The UDA paper [76] uses a larger batch size for their UDA objective. In our implementation, we maintain the same batch sizes, simplifying data parallelism in our TensorFlow [1] framework on TPU big pods. To compensate, we train all UDA baselines longer than in the UDA paper [76]. We also mask out high-confidence supervised examples during training.",
  "summary_type": "reports"
}{
  "input": "5\n77.5\n80\n82.5\n85\n87.5\n90\n92.5\n95\n97.5\n100\nCCT⋆\nProxyless-G\nCCT\nViT-Lite\nMobileNetV2\nResNet164\nResNet1001\nResNet18\n# Parameters (M)\nTop-1 validation accuracy\nFigure 4: CIFAR-10 accuracy vs model size (sizes < 12M).\nCCT⋆was trained longer with extra augmentations.)\nWe also present ImageNet training results in Table 3. We\nshould note that ResNet152 [7] used 10-crop testing, and\nViT has more than 3 times as many parameters than our\nvariants. We used the timm package [40] to train our models\non Imag",
  "output": "Figure 4 zeigt die Genauigkeit von CIFAR-10 im Vergleich zur Modellgröße (Größen < 12M). CCT⋆ wurde länger mit zusätzlichen Augmentierungen trainiert. Die ImageNet-Trainingsergebnisse sind in Tabelle 3 dargestellt. Es sollte beachtet werden, dass ResNet152 [7] einen 10-Crop-Test verwendet hat und ViT mehr als das Dreifache an Parametern als unsere Varianten hat. Wir haben das Timm-Paket [40] verwendet, um unsere Modelle auf Imag zu trainieren.",
  "summary_type": "reports"
}{
  "input": "URL http://arxiv.org/abs/1805.\n07836.\n13\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\nA\nAPPENDIX\nA.1\nPAC BAYESIAN GENERALIZATION BOUND\nBelow, we state a generalization bound based on sharpness.\nTheorem 2. For any ρ > 0 and any distribution D, with probability 1 −δ over the choice of the\ntraining set S ∼D,\nLD(w) ≤max\n∥ϵ∥2≤ρ LS(w + ϵ) +\nv\nu\nu\nu\nu\nt\nk log\n \n1 + ∥w∥2\n2\nρ2\n\u0012\n1 +\nq\nlog(n)\nk\n\u00132!\n+ 4 log n\nδ + ˜O(1)\nn −1\n(4)\nwhere n = |S|, k is the num",
  "output": "Veröffentlicht als Konferenzpapier bei ICLR 2021, wird ein Verallgemeinerungsbegrenzung basierend auf Schärfe angegeben. Theorem 2 besagt, dass für jede ρ > 0 und jede Verteilung D, mit einer Wahrscheinlichkeit von 1 - δ über die Auswahl des Trainingssets S ∼D, LD(w) ≤max ∥ϵ∥2≤ρ LS(w + ϵ) + √k log (1 + ∥w∥2/ρ2) (1 + √log(n)/k)²! + 4 log n/δ + ˜O(1)/n -1 gilt, wobei n = |S| und k die Anzahl ist.",
  "summary_type": "reports"
}{
  "input": ".8M\n311M\n1372.5\n94.17\n77.01\nSwin-B [35]\n86.7M\n497M\n868.3\n94.55\n78.45\nNesT-T\n6.2M\n187M\n1616.9\n96.04\n78.69\nNesT-S\n23.4M\n411M\n627.9\n96.97\n81.70\nNesT-B\n90.1M\n984M\n189.8\n97.20\n82.56\n3.2\nVisual Interpretability via Tree Traversal\nDifferently from existing methods, the nested hierarchy with independent block process in NesT\nresembles a decision tree that each block is encouraged to learn non-overlapping features and be\nselected by the block aggregation. This unique behavior motivates us to explore a ne",
  "output": "Text:\n.8M\n311M\n1372.5\n94.17\n77.01\nSwin-B [35]\n86.7M\n497M\n868.3\n94.55\n78.45\nNesT-T\n6.2M\n187M\n1616.9\n96.04\n78.69\nNesT-S\n23.4M\n411M\n627.9\n96.97\n81.70\nNesT-B\n90.1M\n984M\n189.8\n97.20\n82.56\n3.2\nVisual Interpretability via Tree Traversal\nA diferencia de los métodos existentes, la jerarquía anidada con el proceso de bloque independiente en NesT se asemeja a un árbol de decisiones en el que se anima a cada bloque a aprender características no superpuestas y a ser seleccionado por la agregación de bloques. Este comportamiento único nos motiva a explorar un ne",
  "summary_type": "reports"
}{
  "input": "ientNet. We trained for 100 epochs, using RMSProp (Tieleman and Hinton 2012) as implemented in the timm\npackage8 with a learning rate of 0.0016, a weight decay of 1e −5 and a momentum of 0.9. The learning rate is decayed by a\nfactor of 0.9875 with every epoch. As a warmup, the learning rate is linearly scaled from 0 to 0.0016 over the ﬁrst 3 epochs.\nFollowing (Wightman 2019), we evaluate with an exponential moving average of the weights of the model, with a decay of\n0.99985. We use the random er",
  "output": "Se entrenó durante 100 épocas, utilizando RMSProp (Tieleman y Hinton 2012) como se implementa en el paquete timm con una tasa de aprendizaje de 0.0016, una decadencia de peso de 1e -5 y un impulso de 0.9. La tasa de aprendizaje se reduce en un factor de 0.9875 con cada época. Como calentamiento, la tasa de aprendizaje se escala linealmente de 0 a 0.0016 durante las primeras 3 épocas. Siguiendo a (Wightman 2019), evaluamos con un promedio móvil exponencial de los pesos del modelo, con una decadencia de 0.99985. Usamos el aleatorio er.",
  "summary_type": "reports"
}{
  "input": "periments with Reﬁner, we shared the attention maps for 1\nadjacent transformer block.\nC\nDataset description for NLP experiments\nGLUE benchmark [52] is a collection of nine natural language understanding tasks where the labels\nof the testing set is hidden and the researchers need to submit their predictions to the evaluation\nserver3 to obtain results on testing sets. In this work, we only present results of single-task setting for\nfair comparison. The nine tasks included in GLUE benchmark are des",
  "output": "GLUE benchmark [52] es una colección de nueve tareas de comprensión del lenguaje natural donde las etiquetas del conjunto de prueba están ocultas y los investigadores deben enviar sus predicciones al servidor de evaluación para obtener resultados en los conjuntos de prueba. En este trabajo, solo presentamos resultados de la configuración de una sola tarea para una comparación justa. Las nueve tareas incluidas en el benchmark GLUE están des",
  "summary_type": "reports"
}{
  "input": "1,000 classes of ImageNet. This selection results in\n12.8 million images. We also make sure that none of the\n12.8 million images that we use overlaps with the ILSVRC\n2012 validation set of ImageNet. This procedure of ﬁltering\nextra unlabeled data has been used by UDA [76] and Noisy\nStudent [77].\nImplementation details.\nWe implement Meta Pseudo La-\nbels the same as in Section 3.2 but we use a larger batch size\nand more training steps, as the datasets are much larger for\nthis experiment. Speciﬁcal",
  "output": "ImageNet consta de 1,000 clases y 12.8 millones de imágenes, asegurándose de que ninguna de ellas se superponga con el conjunto de validación ILSVRC 2012 de ImageNet. Este procedimiento de filtrado de datos sin etiquetar ha sido utilizado por UDA [76] y Noisy Student [77]. Los detalles de implementación de Meta Pseudo Labels son los mismos que en la Sección 3.2, pero se utiliza un tamaño de lote más grande y más pasos de entrenamiento, ya que los conjuntos de datos son mucho más grandes para este experimento.",
  "summary_type": "reports"
}{
  "input": "sults for image\nclassiﬁcation on ImageNet reported in this paper, we did not use any external dataset for pre-training.\nViP vs. CNNs. Table 2 compares ViP family with some of the state-of-the-art CNN models on\nImageNet. The RegNet is also better tuned using training tricks in [59]. ViP is both cost-efﬁcient\nand parameter-efﬁcient compared to these state-of-the-art models. For example, ViP-M can achieve\na competitive 83.3% with only 49.6M parameters and 8.0G FLOPS. The counterpart BOTNet-T5\nneeds",
  "output": "In diesem Papier berichten wir über die Ergebnisse der Bildklassifizierung auf ImageNet ohne Verwendung eines externen Datensatzes für das Vortraining. Tabelle 2 vergleicht die ViP-Familie mit einigen der modernsten CNN-Modellen auf ImageNet. Das RegNet ist ebenfalls besser abgestimmt mit Trainingstricks in [59]. ViP ist sowohl kosteneffizient als auch parameter-effizient im Vergleich zu diesen modernsten Modellen. Zum Beispiel kann ViP-M mit nur 49,6M Parametern und 8,0G FLOPS eine wettbewerbsfähige 83,3% erreichen. Der entsprechende BOTNet-T5 benötigt.",
  "summary_type": "reports"
}{
  "input": "ang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nProceedings of the European conference on computer vision\n(ECCV), pages 801–818, 2018. 7\n[12] Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen\nLin, and Han Hu. Reppoints v2: Veriﬁcation meets regres-\nsion for object detection. In NeurIPS, 2020. 6, 7, 9\n[13] Cheng Chi, Fangyun Wei, and Han Hu.\nRelationnet++:\nBridging visual re",
  "output": "Chen等人在2018年的ECCV会议上提出了一种基于Encoder-Decoder和Atrous Separable Convolution的语义图像分割方法。Chen等人在2020年的NeurIPS会议上提出了Reppoints v2，这是一种将验证与回归相结合的目标检测方法。Chi等人提出了Relationnet++，旨在弥合视觉关系的方法。",
  "summary_type": "reports"
}{
  "input": "r-parameter S ∈(0,1). We will process a fraction\nS of the patches, and directly skip the remaining 1 −S fraction. As before, we rank the N patches\n25\n\n--------------------------------------------------\naccording to some scoring function s(⋅). Then, we directly discard the bottom (1 −S)% of the\npatches, and proceed like in Algorithm 2 over the selected M = SN patches. Algorithm 3 formally\ndescribes the idea. Going back to our previous example with two patches, if we set S = 0.5 there, we\nwill dis",
  "output": "R-parametre S ∈(0,1) kullanılıyor. Yamanın bir kısmı S oranında işlenirken, kalan 1 - S oranındaki kısmı doğrudan atlanır. Daha önce olduğu gibi, N yamayı bir skorlama fonksiyonu s(⋅) ile sıralarız. Ardından, yamaların alttaki (1 - S)% kısmını doğrudan atar ve seçilen M = SN yamaları üzerinde Algoritma 2 gibi ilerleriz. Algoritma 3 bu fikri resmi olarak tanımlar. İki yamalı önceki örneğimize dönersek, orada S = 0.5 olarak ayarlarsak, yamaların yarısını işleriz.",
  "summary_type": "reports"
}{
  "input": "c assumptions about grid structure are to the perfor-\nmance of the benchmark methods, we evaluate all methods\non permuted ImageNet. To generate permutations, we use a\nsingle, shared permutation pattern for all images. The per-\nmutation is performed after position features are generated.\nWe make this choice because it still allows each network\nto infer the spatial relationship between points (using the\nposition encoding), but prevents the network from using an\n\n-----------------------------------",
  "output": "The text discusses an evaluation of benchmark methods on permuted ImageNet. Permutations are generated using a single, shared pattern for all images, performed after position features are generated. This allows each network to infer the spatial relationship between points using the position encoding, but prevents the network from using an unspecified element.",
  "summary_type": "reports"
}{
  "input": "Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 1, 2, 3, 8, 9\n[61] Ross\nWightman.\nPytorch\nimage\nmodels.\nhttps://github.com/rwightman/\npytorch-image-models, 2019. 10\n[62] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging\nthe gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2",
  "output": "Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin published \"Attention is all you need\" in NeurIPS, 2017. Ross Wightman developed Pytorch image models in 2019 (https://github.com/rwightman/pytorch-image-models). Yonghui Wu and others contributed to Google's neural machine translation system, aiming to bridge the gap between human and machine translation, as detailed in their arXiv preprint (arXiv:1609.08144).",
  "summary_type": "reports"
}{
  "input": "rate the probability\nscores of correct class. After this we calculated the mean of correct probability scores on both the\nmodels. As we can see from Table 10 that Anistropic ImageNet has larger mean which means that\nAnistropic ImageNet has better conﬁdence as compared to Standard ImageNet. We also calculate\nthe entropy of output probability distribution from both the models. We can see from Table 10\nAnistropic ImageNet has lower entropy scores as compared to Standard ImageNet.\n[t!]\n15\nSALIENCY M",
  "output": "The Anistropic ImageNet model has a higher mean of correct probability scores and lower entropy scores compared to the Standard ImageNet model, indicating better confidence and output probability distribution.",
  "summary_type": "reports"
}{
  "input": "er of epochs on large-scale datasets (e.g. ∼40\nepochs on 300M images), consistent with our experimen-\ntal ﬁndings that scaling the width is preferable in shorter\nepoch regimes. In particular, Kolesnikov et al. (2019) train\na ResNet-152 with 4x ﬁlter multiplier while Brock et al.\n(2021) scales the width with ∼1.5x ﬁlter multiplier.\n6.2. Strategy #2 - Slow Image Resolution Scaling\nIn Figure 2, we also observe that larger image resolutions\nyield diminishing returns. We therefore propose to increase",
  "output": "Large-scale datasets benefit from more epochs (e.g., ~40 epochs on 300M images), with width scaling being preferable in shorter epoch regimes. Kolesnikov et al. (2019) trained a ResNet-152 with a 4x filter multiplier, while Brock et al. (2021) used a ~1.5x filter multiplier. Larger image resolutions yield diminishing returns, suggesting a strategy of slow image resolution scaling.",
  "summary_type": "reports"
}{
  "input": "384\n84.6\n88.9\nReﬁned-ViT-M\n55M\n13.5B\n224\n224\n84.6\n88.9\nReﬁned-ViT-M↑384\n55M\n49.2B\n224\n384\n85.6\n89.3\nReﬁned-ViT-L\n81M\n19.1B\n224\n224\n84.9\n89.1\nReﬁned-ViT-L↑384\n81M\n69.1B\n224\n384\n85.7\n89.7\nReﬁned-ViT-L↑448\n81M\n98.0B\n224\n448\n85.9\n90.1\ngood results. It is worthy future studies on the effects of such subtraction attention (with negative\nattention weights) for feature aggregation. Second, the local distributed attention indeed imposes a\nlocality inductive bias which is proven to be effective in enhanc",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "0.03\n83.5\n72.7\n88.3\n3B\n20K\n10K\n0.03\n67.5\n54.5\n74.9\n3B\n30K\n10K\n0.03\n72.3\n60.0\n79.4\n3B\n60K\n10K\n0.03\n76.7\n64.8\n83.3\n3B\n120K\n50K\n0.03\n79.1\n67.9\n85.4\n3B\n400K\n50K\n0.03\n81.9\n71.1\n87.3\n3B\n1.2M\n50K\n0.01\n82.8\n72.7\n87.9\n3B\n2M\n50K\n0.03\n83.2\n73.2\n88.1\n3B\n4M\n50K\n0.01\n84.0\n73.8\n88.5\n27\n\n--------------------------------------------------\nTable 18: Tabular representation of the ﬁnetune results (%) for model ViT-Ti/16 on ImageNet,\nImageNet V2 test set and ImageNet ReaL test set.\nData Size\nSteps\nCooldown\nLR\nImage",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "tNet-4\n3842\n275M\n189.5B\n87.9\n+ PT-RA\n3842\n275M\n189.5B\n88.3\n+ PT-RA-E150\n3842\n275M\n189.5B\n88.4\nCoAtNet-2\n5122\n75M\n96.7B\n87.3\nCoAtNet-3\n5122\n168M\n203.1B\n87.9\nCoAtNet-4\n5122\n275M\n360.9B\n88.1\n+ PT-RA\n5122\n275M\n360.9B\n88.4\n+ PT-RA-E150\n5122\n275M\n360.9B\n88.56\n18\n\n--------------------------------------------------",
  "output": "tNet-4 and CoAtNet-2, -3, -4 are models with varying parameters and performance. tNet-4 has 3842 units, 275M parameters, 189.5B operations, and 87.9-88.4 performance score. CoAtNet-2 has 5122 units, 75M parameters, 96.7B operations, and 87.3 performance score. CoAtNet-3 has 168M parameters, 203.1B operations, and 87.9 performance score. CoAtNet-4 has 275M parameters, 360.9B operations, and 88.1-88.56 performance score. PT-RA and PT-RA-E150 are additional features improving performance.",
  "summary_type": "reports"
}{
  "input": "ng efﬁciency is thus boosted since the training space\nhas been greedily shrunk from all paths to those potentially-\ngood ones. Moreover, we further adopt an exploration and\nexploitation policy by introducing an empirical candidate\npath pool. Our proposed method GreedyNAS is easy-to-\nfollow, and experimental results on ImageNet dataset indi-\ncate that it can achieve better Top-1 accuracy under same\nsearch space and FLOPs or latency level, but with only\n∼60% of supernet training cost. By searching",
  "output": "GreedyNAS, metoda propusă, este ușor de urmărit și rezultatele experimentale pe setul de date ImageNet indică faptul că poate obține o acuratețe Top-1 mai bună sub același spațiu de căutare și nivel FLOPs sau latență, dar cu doar aproximativ 60% din costul de instruire a supernetei.",
  "summary_type": "reports"
}{
  "input": "eneﬁcial once features contain semantic information, i.e. after having been processed by\na few operations, in which case using global contexts in the early layers would be wasteful. In\nthe Appendix 5.3, we study hybrid designs that use standard convolutions to capture local contexts\nand lambda layers to capture global contexts. We ﬁnd that such convolution-lambda hybrids have\nincreased representational power at a negligible decrease in throughput compared to their purely\nconvolutional counterpar",
  "output": "Las características son beneficiosas una vez que contienen información semántica, es decir, después de haber sido procesadas por algunas operaciones, en cuyo caso usar contextos globales en las primeras capas sería un desperdicio. En el Apéndice 5.3, estudiamos diseños híbridos que utilizan convoluciones estándar para capturar contextos locales y capas lambda para capturar contextos globales. Encontramos que estos híbridos de convolución-lambda tienen un mayor poder representativo con una disminución insignificante en el rendimiento en comparación con sus contrapartes puramente convolucionales.",
  "summary_type": "reports"
}{
  "input": "knowledge they did not use it with CutMix or Mixup as we propose to do.\n4\n\n--------------------------------------------------\nIn our experiments, even when using BCE, setting all mixed concepts with a target to\n1 (or 1 −ε) is more effective than considering a distribution of concepts that sum to 1.\nConceptually we believe it is more aligned with what Mixup and CutMix are actually\ndoing: it is likely that a human could recognize each of two mixed concepts.\nData-Augmentation.\nWe adopt the followi",
  "output": "In their experiments, the authors found that using Binary Cross-Entropy (BCE) and setting all mixed concepts with a target to 1 (or 1 - ε) is more effective than considering a distribution of concepts that sum to 1. They believe this approach aligns better with what Mixup and CutMix are doing, as a human could likely recognize each of two mixed concepts. They also mention adopting data augmentation.",
  "summary_type": "reports"
}{
  "input": "ple and classic TwoM-\noon dataset [7]. The 2D nature of the TwoMoon dataset\nallows us to visualize how Meta Pseudo Labels behaves\ncompared to Supervised Learning and Pseudo Labels.\nDataset.\nFor this experiment, we generate our own version\nof the TwoMoon dataset. In our version, there are 2,000 ex-\namples forming two clusters each with 1,000 examples. Only\n6 examples are labeled, 3 examples for each cluster, while the\nremaining examples are unlabeled. Semi-supervised learn-\ning algorithms are ask",
  "output": "In an experiment, a custom version of the TwoMoon dataset is generated, comprising 2,000 examples divided into two clusters of 1,000 examples each. Only 6 examples are labeled, with 3 examples for each cluster, while the rest are unlabeled. This setup is used to test semi-supervised learning algorithms.",
  "summary_type": "reports"
}{
  "input": "eep convolutional neural net-\nworks. In NIPS, 2012. 2\n[28] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, 86(11):2278–2324, 1998. 4\n[29] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyramid\nnetworks for object detection. In CVPR, 2017. 6\n[30] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Dollar. Focal loss for dense object detection. In",
  "output": "Y. Lecun et al. applied gradient-based learning to document recognition in 1998. Tsung-Yi Lin et al. developed Feature Pyramid Networks for object detection, presented at CVPR in 2017. They also proposed a method for dense object detection called Focal Loss.",
  "summary_type": "reports"
}{
  "input": "for each image\nis L ∈R18×18×1000. During training, the target labels for the tokens are generated by applying\nRoIAlign [18] on the corresponding score map. In practice, we only store the top-5 score maps for\neach position in half-precision to save space as storing the entire score maps for all the images results\nin 2TB storage. In our experiment, we only need 10GB of storage to store all the score maps.\n5\n\n--------------------------------------------------\nTable 1: Performance of the proposed L",
  "output": "Durante el entrenamiento, las etiquetas objetivo para los tokens se generan aplicando RoIAlign en el mapa de puntuación correspondiente. En la práctica, solo almacenamos los 5 mapas de puntuación principales para cada posición en media precisión para ahorrar espacio, ya que almacenar todos los mapas de puntuación para todas las imágenes resulta en 2TB de almacenamiento. En nuestro experimento, solo necesitamos 10GB de almacenamiento para almacenar todos los mapas de puntuación.",
  "summary_type": "reports"
}{
  "input": "tersection over Union) is better.\nBackbone\nModel Size\nmIoU\nResNet-18-A [39]\nResNet-18\n3.8M\n55.4\nResNet-18-B [39]\nResNet-18\n15.24M\n69.1\nMobileNetV2Plus [46]\nMobileNetV2\n8.3M\n74.5\nGSCNN [53]\nResNet-50\n-\n73.0\nHRNetV2-W18-Small-v2* [55]\nHRNet\n4.0M\n76.0\nMDEQ-small (ours) [Implicit]\nMDEQ\n7.8M\n75.1\nU-Net++ [64]\nResNet-101\n59.5M\n75.5\nDilated-ResNet [60]\nD-ResNet-101\n52.1M\n75.7\nPSPNet [62]\nD-ResNet-101\n65.9M\n78.4\nDeepLabv3 [9]\nD-ResNet-101\n58.0M\n78.5\nPSANet [63]\nResNet-101\n-\n78.6\nHRNetV2-W48* [55]\nHRNet",
  "output": "Text: \nResNet-18-A, ResNet-18-B, MobileNetV2Plus, GSCNN, HRNetV2-W18-Small-v2, MDEQ-small, U-Net++, Dilated-ResNet, PSPNet, DeepLabv3, PSANet, and HRNetV2-W48 are models with varying sizes and mIoU scores. ResNet-18-A has 3.8M size and 55.4 mIoU, ResNet-18-B has 15.24M size and 69.1 mIoU. MobileNetV2Plus has 8.3M size and 74.5 mIoU. GSCNN and PSANet's sizes are unknown, but they have 73.0 and 78.6 mIoU respectively. HRNetV2-W18-Small-v2 has 4.0M size and 76.0 mIoU. MDEQ-small has 7.8M size and 75.1 mIoU. U-Net++ has 59.5M size and 75.5 mIoU. Dilated-ResNet, PSPNet, and DeepLabv3 have sizes of 52.1M, 65.9M, 58.0M and mIoU scores of 75.7, 78.4, 78.5 respectively.",
  "summary_type": "reports"
}{
  "input": "alysis (face detection, face alignment,\nface recognition and face generation). During\nhis PhD studies, he has organised the Menpo\n2D Challenge (CVPR 2017), the Menpo 3D Challenge (ICCV 2017)\nand Lightweight Face Recognition Challenge (ICCV 2019). He also\nwon many academic challenges, such as ILSVRC Object Detection\nand Tracking 2017, Activity-Net Untrimmed Video Classiﬁcation 2017,\niQIYI Celebrity Video Identiﬁcation Challenge 2018, Disguised Face\nRecognition Challenge 2019. He is a reviewer in",
  "output": "Durante sus estudios de doctorado, organizó el Menpo 2D Challenge (CVPR 2017), el Menpo 3D Challenge (ICCV 2017) y el Lightweight Face Recognition Challenge (ICCV 2019). También ganó muchos desafíos académicos, como ILSVRC Object Detection and Tracking 2017, Activity-Net Untrimmed Video Classification 2017, iQIYI Celebrity Video Identification Challenge 2018, Disguised Face Recognition Challenge 2019. Es revisor en <CENSORED>.",
  "summary_type": "reports"
}{
  "input": ".\nThis block uses our implementation of the baselines. The Perceiver\nis competitive with standard baselines on ImageNet without relying\non domain-speciﬁc architectural assumptions.\nnecessary details from the input signal. To hedge against\nthis effect, the Perceiver may be structured with multiple\ncross-attend layers, which allow the latent array to iter-\natively extract information from the input image as it is\nneeded. This allows us to tune the model to balance ex-\npensive, but informative cros",
  "output": "This text discusses the Perceiver, a model that competes with standard baselines on ImageNet without domain-specific architectural assumptions. The Perceiver can be structured with multiple cross-attend layers, allowing the latent array to iteratively extract information from the input image as needed. This structure enables model tuning to balance expensive, but informative cross-attend layers.",
  "summary_type": "reports"
}{
  "input": "lution 224 unless mentioned otherwise. We follow the setup of [14].\nThe only differences are: (1) We exclude lr = 0.001 from the grid search and instead include\nlr = 0.06 for CIFAR-10, CIFAR-100, Flowers, and Pets. (2) We perform a grid search over\nlr ∈{0.003, 0.01, 0.03} for VTAB-1k. (3) We try two different ways of pre-processing during\nevaluation: (i) “resize-crop”: ﬁrst resize the image to 256 × 256 pixels and then take a 224 × 224\npixel sized central crop. (ii) “resmall-crop”: ﬁrst resize t",
  "output": "Se sigue la configuración de [14] con dos diferencias: (1) Excluimos lr = 0.001 de la búsqueda en cuadrícula e incluimos lr = 0.06 para CIFAR-10, CIFAR-100, Flowers y Pets. (2) Realizamos una búsqueda en cuadrícula sobre lr ∈{0.003, 0.01, 0.03} para VTAB-1k. (3) Probamos dos formas diferentes de preprocesamiento durante la evaluación: (i) \"resize-crop\": primero redimensionamos la imagen a 256 × 256 píxeles y luego tomamos un recorte central de 224 × 224 píxeles. (ii) \"resmall-crop\": primero redimensionamos t",
  "summary_type": "reports"
}{
  "input": "ﬁers. We democratise the use\nof Transformers by providing a framework that allows for\nthese SOTA results on small datasets and minimal hard-\nware. Our method is ﬂexible in size, and the smallest of\nour variants can be easily loaded on even a minimal GPU,\nor even a CPU. While the trend of research has been bigger\nand bigger transformers, we show that there is still much\nresearch to be done to make efﬁcient networks that work on\nsmall datasets and less efﬁcient hardware. This kind of re-\nsearch is",
  "output": "We provide a framework that democratizes the use of Transformers, enabling state-of-the-art results on small datasets and minimal hardware. Our method is flexible in size, with the smallest variants capable of running on minimal GPUs or even CPUs. Despite the trend towards larger transformers, we demonstrate the need for continued research into efficient networks for small datasets and less efficient hardware.",
  "summary_type": "reports"
}{
  "input": "onal CNNs and the\nconcurrently proposed all-MLP models, e.g., MLP-Mixer\n[25], ResMLP [26], gMLP [19], AS-MLP [18], etc. In this\npaper, we use RepMLP in ResNet for most of our experi-\nments because this work was ﬁnished before the publicity\nof all the above-mentioned all-MLP models. The applica-\ntion of RepMLP on the all-MLP models is scheduled as our\nfuture work.\nIn order to use RepMLP in ResNet, we follow the bottle-\nneck [14] design principle of ResNet-50 to reduce the chan-\nnels by 4× via 1×1",
  "output": "Este documento se centra en el uso de RepMLP en ResNet para la mayoría de sus experimentos, completados antes de la publicidad de todos los modelos MLP mencionados. Se planea aplicar RepMLP a todos los modelos MLP en trabajos futuros. Para usar RepMLP en ResNet, siguen el principio de diseño de cuello de botella de ResNet-50 para reducir los canales en 4 veces a través de 1×1.",
  "summary_type": "reports"
}{
  "input": "nition 3. (Data local smoothness for the multi labels)\nφi,j(zi, zj) := zizjφb′\ni,j(1, 1) + zi(1 −zj)φb′\ni,j(1, 0) + (1 −\nzi)zjφb′\ni,j(0, 1) + (1 −zi)(1 −zj)φb′\ni,j(0, 0), ∀zi, zj ∈L.\nProposition 2. With φi,j deﬁned as Deﬁnition 3, ei,j satis-\nﬁes pairwise submodularity.\nProof. We can represent φi,j as follows:\nφi,j(zi, zj)\n= f(zi, zj)φb′\ni,j(1, 0) + φb′\ni,j(0, 1) −φb′\ni,j(0, 0) −φb′\ni,j(1, 1)\n2\n+ zi\nφb′\ni,j(1, 0) + φb′\ni,j(1, 1) −φb′\ni,j(0, 0) −φb′\ni,j(0, 1)\n2\n+ zj\nφb′\ni,j(0, 1) + φb′\ni,j(1, 1)",
  "output": "Deﬁnition 3 stellt die lokale Glätte der Daten für die Multi-Labels dar, wobei φi,j(zi, zj) definiert ist als eine Funktion von zi und zj. Proposition 2 besagt, dass mit φi,j wie in Definition 3 definiert, ei,j die paarweise Submodularität erfüllt. Der Beweis stellt φi,j als Funktion von zi und zj dar, die verschiedene Kombinationen von φb′ i,j beinhaltet.",
  "summary_type": "reports"
}{
  "input": "riginal image is clean and there is no doubt to use the ground-truth label, while the\naugmented counterparts look more like other classes which the annotation is not aware\nof. This phenomenon is called augment ambiguity. Right: We leverage the idea of\nknowledge distillation to provide softened signals to avoid ambiguity.\ncareful control (e.g., in an expanded search space or with an increased distortion\nmagnitude), these methods are not guaranteed to perform well – as we shall see\nin Section 3.2,",
  "output": "The text discusses the issue of augment ambiguity, where augmented images resemble other classes unknown to the annotation. To mitigate this, the concept of knowledge distillation is used to provide softened signals. However, without careful control, such as in an expanded search space or with increased distortion magnitude, these methods may not perform effectively. Further details will be discussed in Section 3.2.",
  "summary_type": "reports"
}{
  "input": "ing, most convolutional neural net-\nworks crop a subimage with a given ratio, typically extracting a 224×224 center\ncrop from a 256×256 resized image, leading to the typical ratio of 0.875. Wight-\nman et al. [68] notice that setting this crop ratio to 1.0 for transformer models\nhas a positive impact: the distilled DeiT-B↑384 reaches a top1-accuracy on\nImagenet1k-val of 85.42% in this setting, which is a gain of +0.2% compared to\nthe accuracy of 85.2% reported by Touvron et al. [63].\nOur measurem",
  "output": "La mayoría de las redes neuronales convolucionales recortan una subimagen con una proporción dada, extrayendo típicamente un recorte central de 224×224 de una imagen redimensionada de 256×256, lo que lleva a la proporción típica de 0.875. Wightman et al. [68] observan que establecer esta proporción de recorte a 1.0 para los modelos transformadores tiene un impacto positivo: el DeiT-B↑384 destilado alcanza una precisión top1 en Imagenet1k-val del 85.42% en esta configuración, lo que supone una ganancia de +0.2% en comparación con la precisión del 85.2% informada por Touvron et al. [63].",
  "summary_type": "reports"
}{
  "input": "his gives rise to the question if a network can dy-\nnamically and adaptively fuse the received features in\na contextual scale-aware way.\nMotivated by the above observations, we present the at-\ntentional feature fusion (AFF) module, trying to answer the\nquestion of how a uniﬁed approach for all kinds of feature\nfusion scenarios should be and address the problems of con-\ntextual aggregation and initial integration. The AFF frame-\nwork generalizes the attention-based feature fusion from the\nsame-la",
  "output": "Este texto presenta el módulo de fusión de características de atención (AFF), que busca responder cómo debería ser un enfoque unificado para todo tipo de escenarios de fusión de características. El marco AFF generaliza la fusión de características basada en la atención, abordando los problemas de agregación contextual e integración inicial.",
  "summary_type": "reports"
}{
  "input": "s are conducted to utilize the transformer archi-\ntecture to the backbone network for computer vision tasks.\nNon-local network [37] adds a few self-attention layers to\nCNN backbone, and it shows that the self-attention mecha-\nnism can be used in CNN. [28] replaced 3 × 3 convolution\nof ResNet to local self-attention layer. [36] used an atten-\ntion layer for each spatial axis. [2] enables self-attention of\nthe entire spatial map by reducing the computation of the\nattention mechanism. Most of these",
  "output": "Researches are being conducted to apply the transformer architecture to the backbone network for computer vision tasks. Non-local network adds self-attention layers to CNN backbone, demonstrating that the self-attention mechanism can be used in CNN. Some replaced ResNet's 3x3 convolution with a local self-attention layer, others used an attention layer for each spatial axis, and some enabled self-attention of the entire spatial map by reducing the computation of the attention mechanism.",
  "summary_type": "reports"
}{
  "input": "atial re-\nduction layer in a convolutional architecture. ResNet50, one\nof the most widely used networks in ImageNet, is used for\narchitecture and is trained over 100 epochs without complex\ntraining techniques. For ResNet with ViT style dimension,\nwe use the stem layer of ViT to reduce the feature to 14×14\nspatial dimensions while reducing the spatial information\nloss in the stem layer. We also remove the spatial reduction\nlayers of ResNet to maintain the initial feature dimensions\nfor all layers",
  "output": "ResNet50, a popular network in ImageNet, is trained over 100 epochs without complex techniques. It uses the ViT style dimension stem layer to reduce the feature to 14x14 spatial dimensions, minimizing spatial information loss. The spatial reduction layers of ResNet are removed to maintain the initial feature dimensions for all layers.",
  "summary_type": "reports"
}{
  "input": "ugmentation can be dif-\nferent between the stages of training teachers and distilla-\n6\n\n--------------------------------------------------\n\"# = %. '\n\"# = %. %'\n\"# = %. %%'\n\"# = %. '\n\"# = %. %' \"# = %. %%'\nFigure 6. A comparison of training from random initialized parameters, inferior parameters and superior parameters. Left are the training\naccuracy curves and middle are validation accuracy curves. The results indicate that a good initialization is crucial for the ﬁnal performance.\nLeft is the c",
  "output": "Figure 6 compares training from random initialized parameters, inferior parameters, and superior parameters. It presents training accuracy curves and validation accuracy curves. The results suggest that a good initialization is crucial for the final performance.",
  "summary_type": "reports"
}{
  "input": "centroid transformers is signiﬁcantly re-\nduced against their full-length counterparts. Experiments\nconducted on text summarization, 3D vision and image\nprocessing demonstrates centroid transformers can yield\nsimilar / better performance over the original transformers\nwith high efﬁciency.\nREFERENCES\nAljalbout, E., Golkov, V., Siddiqui, Y., Strobel, M., and\nCremers, D. Clustering with deep learning: Taxonomy\nand new methods. arXiv preprint arXiv:1801.07648,\n2018.\nBeltagy, I., Peters, M. E., and",
  "output": "Centroid transformers have been shown to be more efficient than their full-length counterparts. They have been tested in text summarization, 3D vision, and image processing, demonstrating similar or better performance than the original transformers.",
  "summary_type": "reports"
}{
  "input": "se building block for multi-head context aggregation. A CONTAINER block contains both\nstatic afﬁnity as well as dynamic afﬁnity based aggregation, which are combined using learnable\nmixing coefﬁcients. This enables the CONTAINER block to process long range information while\nstill exploiting the inductive bias of the local convolution operation. CONTAINER blocks are easy to\nimplement, can easily be substituted into many present day neural architectures and lead to highly\nperformant networks whils",
  "output": "CONTAINER block, sebuah blok bangunan untuk agregasi konteks multi-kepala, mengandung agregasi berdasarkan afinitas statis dan dinamis yang digabungkan menggunakan koefisien pencampuran yang dapat dipelajari. Ini memungkinkan blok CONTAINER untuk memproses informasi jangka panjang sambil tetap memanfaatkan bias induktif operasi konvolusi lokal. Blok CONTAINER mudah diimplementasikan, dapat dengan mudah digantikan ke dalam banyak arsitektur neural saat ini dan menghasilkan jaringan yang sangat berkinerja.",
  "summary_type": "reports"
}{
  "input": ")\nCPVT-S\n(80.5%)\nRPE methods on DeiT-S\n0\n20\n40\nExtra Rate (%)\n(Ours)\n(Ours)\n(Ours)\n39\n18\n38\n12\n17\n18\n29\n11\n37\n10\n39\n 5\n20\n10\n27\n14\nextra inference-time rate (%)\nextra memory rate (%)\nFigure 10: The extra cost brought by RPEs. The reference\nmodel is DeiT-S [22] without RPE, taking 1,096 images/s\nand 8,930 Mb memory.\nuct method (deﬁned in Sec. 3.2 in the main manuscript) to\nadapt 1D encoding methods for 2D images. We replace the\npiecewise function g(x) with the clip function h(x), which\nis matched",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "and thus increase the representation power of contextual\naggregation compared with a single-head version. Note that only spatial information is propagated\nduring contextual aggregation using the afﬁnity matrices; cross-channel information exchange does\nnot occur within the afﬁnity matrix multiplication, and that there is no non-linear activation function.\n3.2\nThe Transformer, Depthwise Convolution and MLP-Mixer\nTransformer [56], depthwise convolution [30] and the recently proposed MLP-Mixer [52",
  "output": "The contextual aggregation's representation power is enhanced compared to a single-head version, with only spatial information propagated using affinity matrices. Cross-channel information exchange doesn't occur within the affinity matrix multiplication, and there's no non-linear activation function. The Transformer, Depthwise Convolution, and the recently proposed MLP-Mixer are also mentioned.",
  "summary_type": "reports"
}{
  "input": "datasets either through few-shot or ﬁne-tuning accuracy.\nFine-tuning accuracies capture the performance of each model after ﬁne-tuning it on the respective\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\nthat maps the (frozen) representation of a subset of training images to {−1, 1}K target vectors. This\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\nﬁne-tuning performance, we sometimes use linear",
  "output": "Fine-tuning accuracies reflect the performance of a model after it's fine-tuned on a specific dataset. Few-shot accuracies are achieved by solving a regularized least-squares regression problem, mapping the representation of a subset of training images to {-1, 1}K target vectors. This method allows for the exact solution to be found in closed form. The primary focus is on fine-tuning performance, but linear use is occasionally employed.",
  "summary_type": "reports"
}{
  "input": "es, we ﬁrst train a separate\nhigh-quality image embedding model following (Wang et al.,\n2014) with a large-scale labeled dataset as in (Juan et al.,\n2020), and then generate 4K clusters via k-means based on\nall training images of the embedding model. For each query\nimage (from the ALIGN dataset) and index image (from\ntest sets of downstream tasks), we ﬁnd their top-10 nearest\nclusters based on the embedding distance. Each image is\nthen assigned to\n\u000010\n3\n\u0001\nbuckets (all possible combinations of\n3",
  "output": "Primero, entrenamos un modelo de incrustación de imágenes de alta calidad siguiendo a (Wang et al., 2014) con un conjunto de datos etiquetado a gran escala como en (Juan et al., 2020), y luego generamos 4K clusters mediante k-means basado en todas las imágenes de entrenamiento del modelo de incrustación. Para cada imagen de consulta (del conjunto de datos ALIGN) e imagen de índice (de los conjuntos de prueba de las tareas posteriores), encontramos sus 10 clusters más cercanos basados en la distancia de incrustación. Luego, cada imagen se asigna a 10^3 cubos (todas las combinaciones posibles de 3).",
  "summary_type": "reports"
}{
  "input": "binary matrix with one-hot row vectors, then get Wm×n via smoothing and normalizing. For\nexample, Aj,k = 1 means the delegator assigns the j-th sample to the k-th expert, while Wj,k should\nbe larger. Given that the main concern of this paper is not the optimization of network architectures,\nwe can suppose they have similar accuracy/FLOPs trade-off, thus make an assumption about the\nexperts:\nNo Superiority Assumption\nThe overall accuracy/FLOPs trade-off for each expert is same, only\nthe one spec",
  "output": "In una matrice binaria con vettori di riga one-hot, ottenere Wm×n attraverso la levigatura e la normalizzazione. Ad esempio, Aj,k = 1 significa che il delegato assegna il campione j-th all'esperto k-th, mentre Wj,k dovrebbe essere più grande. Dato che la preoccupazione principale di questo documento non è l'ottimizzazione delle architetture di rete, possiamo supporre che abbiano un compromesso simile tra precisione/FLOPs, quindi fare un'ipotesi sugli esperti: Nessuna Assunzione di Superiorità. Il compromesso generale tra precisione/FLOPs per ogni esperto è lo stesso, solo quello specifico.",
  "summary_type": "reports"
}{
  "input": "pace. For our implementation in Table 2 for\nM = 12 blocks, ((N+1)V1V2V3V4)M ≈1.3×1030, which\nis about 1012 times the search space of DARTS [22] and\n1018 times the search space of SPOS [13]. The main-stream\nfast Neural Architecture Search methods, such as differen-\ntial [22] and one-shot [13] method, can not work well on\nthis huge search space. For DARTS, the parameters of all\ncandidate networks are trained for each iteration, leading\nto an unacceptable memory requirements. One-shot NAS\nmethod do",
  "output": "The text discusses the implementation of a system with a search space of approximately 1.3x10^30, which is significantly larger than the search spaces of DARTS and SPOS. Traditional fast Neural Architecture Search methods, such as differential and one-shot, struggle with this large search space. DARTS, in particular, has high memory requirements due to the training of all candidate networks for each iteration.",
  "summary_type": "reports"
}{
  "input": "crease the number of\nparameters in the model body, We increase the base width\nrw to 2.0 (Making the base width 128, twice the normal\nwidth), and we also change rb from 3.0 to the default 4.0.\nWe remove the ﬁnal extra 1×1 convolution, so that the label\nembeddings have a large number of ﬁlters to account for the\nlarger number of labels. Finally, we increase the number of\nlayers in the second stage from 3 to 4. For the hybrid model,\nwe use convolutions in the ﬁrst two stages.\nFor pretraining on 256",
  "output": "Model parametrelerinin sayısını artırmak için, temel genişliği 2.0'a (normal genişliğin iki katı olan 128'e) çıkarıyoruz ve rb'yi 3.0'dan varsayılan 4.0'a değiştiriyoruz. Etiket gömülülerinin daha fazla sayıda etiketi hesaba katması için son ekstra 1×1 evrişimini kaldırıyoruz. Son olarak, ikinci aşamadaki katman sayısını 3'ten 4'e çıkarıyoruz. Hibrit model için, ilk iki aşamada evrişimler kullanıyoruz. 256 üzerinde önceden eğitim için.",
  "summary_type": "reports"
}{
  "input": "onference on Computer Vision\nand Pattern Recognition, pages 9308–9316, 2019. 8, 12\n[77] Yi Zhu, Karan Sapra, Fitsum A. Reda, Kevin J. Shih, Shawn\nNewsam, Andrew Tao, and Bryan Catanzaro. Improving Se-\nmantic Segmentation via Video Propagation and Label Re-\nlaxation. 2019. 8\n[78] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\nLe. Learning transferable architectures for scalable image\nrecognition.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 86",
  "output": "Yi Zhu, Karan Sapra, Fitsum A. Reda, Kevin J. Shih, Shawn Newsam, Andrew Tao, and Bryan Catanzaro published a paper in 2019 on improving semantic segmentation via video propagation and label relaxation. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le also published a paper on learning transferable architectures for scalable image recognition. Both papers were presented at the Conference on Computer Vision and Pattern Recognition.",
  "summary_type": "reports"
}{
  "input": "gure 7: Saliency maps on three different set of images. The text on the left of the row indicates\nwhether Anisotropic model or ImageNet model was used. The ﬁrst two rows show the saliency maps\nwhere both model gave correct predictions. We can see from saliency maps that the Anisotropic\nmodel has more diffused saliency maps. The second two rows show the saliency maps where\nAnisotropic model gave correct predictions and ImageNet model gave wrong predictions. The fail-\nure of ImageNet model might b",
  "output": "Figure 7 presents saliency maps on three different image sets, using Anisotropic and ImageNet models. The first two rows show correct predictions from both models, with the Anisotropic model displaying more diffused saliency maps. The next two rows show correct predictions from the Anisotropic model and incorrect ones from the ImageNet model.",
  "summary_type": "reports"
}{
  "input": "strategy. However, given a transformer model with ﬁxed embedding\ndimension, directly increasing the number of heads will reduce the number of embeddings allocated\nto each head, making the computed attention map less comprehensive and accurate as shown in\nTab. 9 in [49]. To address this dilemma, we explore attention expansion that linearly projects the\nmulti-head attention maps to a higher-dimensional space spanned by a larger number of attention\nmaps. As such, the number of attention heads (use",
  "output": "Strategi. Namun, dengan model transformer dengan dimensi embedding tetap, peningkatan langsung jumlah kepala akan mengurangi jumlah embedding yang dialokasikan untuk setiap kepala, membuat peta perhatian yang dihitung kurang komprehensif dan akurat seperti yang ditunjukkan di Tab. 9 di [49]. Untuk mengatasi dilema ini, kami menjelajahi ekspansi perhatian yang memproyeksikan peta perhatian multi-kepala ke ruang dimensi yang lebih tinggi yang dijajaki oleh sejumlah peta perhatian yang lebih besar. Dengan demikian, jumlah kepala perhatian (gunakan",
  "summary_type": "reports"
}{
  "input": "., 2017) has emerged as a popular approach to do\nso, but the costly memory requirement of self-attention hinders its application to long sequences\nand multidimensional data such as images2. Linear attention mechanisms (Katharopoulos et al.,\n2020; Choromanski et al., 2020) offer a scalable remedy for high memory usage but fail to model\ninternal data structure, such as relative distances between pixels or edge relations between nodes in\na graph.\nThis work addresses both issues. We propose lambda l",
  "output": "Self-attention, despite its popularity, has limitations due to its high memory requirements, making it difficult to apply to long sequences and multidimensional data like images. Linear attention mechanisms provide a solution to the memory issue but fail to model internal data structures. This work proposes a solution to both problems.",
  "summary_type": "reports"
}{
  "input": "erNorm →X.\nWe have tried different activations on top of the aforementioned\nMLP-based baseline, and kept GeLU for its accuracy and to be consistent with the transformer choice.\nActivation\ntop-1 acc.\nGeLU (baseline)\n79.2%\nSILU\n78.7%\nHard Swish\n78.8%\nReLU\n79.1%\nI\n\n--------------------------------------------------\nAblation on the size of the communication MLP.\nFor the MLP that replaced the class-attention,\nwe have explored different sizes of the latent layer, by adjusting the expansion factor e in",
  "output": "Verschiedene Aktivierungen wurden auf der oben genannten MLP-basierten Basislinie getestet und GeLU wurde aufgrund seiner Genauigkeit und zur Konsistenz mit der Transformer-Auswahl beibehalten. Die Top-1-Genauigkeit für GeLU lag bei 79,2%, für SILU bei 78,7%, für Hard Swish bei 78,8% und für ReLU bei 79,1%. Für das MLP, das die Klassen-Aufmerksamkeit ersetzte, wurden verschiedene Größen der latenten Schicht untersucht, indem der Expansionsfaktor e angepasst wurde.",
  "summary_type": "reports"
}{
  "input": "r of the polynomial. To\nthat end, we cast polynomial parameters estimation as a coupled\ntensor factorization [11] that jointly factorizes all the polynomial\nparameters tensors. We introduce three joint decompositions with\nshared factors and exhibit the resulting hierarchical structures (i.e.,\narchitectures of neural networks).\nIn our preliminary works [12], [13], [14], we introduced\nthe concept of higher-order expansions for both generative and\ndiscriminative networks. In this work, our improvem",
  "output": "The text discusses casting polynomial parameters estimation as a coupled tensor factorization, introducing three joint decompositions with shared factors. It also mentions the creation of hierarchical structures or neural network architectures. Previous works introduced the concept of higher-order expansions for both generative and discriminative networks.",
  "summary_type": "reports"
}{
  "input": "image’s spatial dimension by 4×, 2×, and 2×, respectively. The feature\nmaps generated by the last RC are of a size of [H/16, W/16, D], which are then ﬂattened into visual\ntokens and fed into the following NCs.\n3.4\nNormal cell\nAs shown in the bottom right part of Figure 2, NCs share a similar structure with the reduction cell\nexcept for the absence of the PRM. Due to the relatively small ( 1\n16×) spatial size of feature maps after\nRCs, it is unnecessary to use PRM in NCs. Given f3 from the third",
  "output": "The feature maps generated by the last Reduction Cell (RC) are of size [H/16, W/16, D], which are then transformed into visual tokens for the following Normal Cells (NCs). NCs have a similar structure to the RC, but do not include the Pooling Reduction Module (PRM). Due to the small spatial size of feature maps after RCs (1/16×), the use of PRM in NCs is unnecessary.",
  "summary_type": "reports"
}{
  "input": "), the query chunk (dark\nbrown) attends to itself and the i’th neighbor chunk.\nFigure 10. The random-shifting strategy does not harm the model\nperformance (Left), an accelerates the Vision Longformer train-\ning signiﬁcantly (Right).\nWhen zooming in, the performance\nof “random-shift and switch at 75%” is slightly better than the\n“Sliding-chunk attention with 8 neighbor chunks”.\nthis, we deﬁne 10 modes of the sliding-chunk local atten-\ntion:\n• 0 (default): attend to itself and all 8 neighbor chunk",
  "output": "The text discusses a random-shifting strategy that doesn't harm model performance and accelerates Vision Longformer training. It mentions that \"random-shift and switch at 75%\" performs slightly better than \"Sliding-chunk attention with 8 neighbor chunks\". It also refers to 10 modes of the sliding-chunk local attention, with the default mode being to attend to itself and all 8 neighbor chunks.",
  "summary_type": "reports"
}{
  "input": "of N ≤1024, which makes\nour latent Transformer comparable in input size to models\nin wide-spread use in language. The latent array itself is\ninitialized using a learned position encoding (Gehring et al.,\n2017) (see Appendix Sec. C for details).\nIterative cross-attention & weight sharing. The size of\nthe latent array allows us to directly model pixels and to\nbuild deeper Transformers, but the severity of the bottle-\nneck may restrict the network’s ability to capture all of the\n1We ignore the con",
  "output": "The latent Transformer is comparable in input size to widely-used language models, with N ≤1024. It's initialized using a learned position encoding (Gehring et al., 2017). The size of the latent array enables direct pixel modeling and deeper Transformers construction. However, the bottleneck's severity might limit the network's ability to capture all details.",
  "summary_type": "reports"
}{
  "input": "ter Vision and\nPattern Recognition, pages 248–255, 2009. 8\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv\npreprint arXiv:1810.04805, 2018. 22\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\n23\n\n--------------------------------------------------\nHeigold, Sylvain Gelly, et al. An",
  "output": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova developed BERT, a method for pre-training deep bidirectional transformers for language understanding, as detailed in their 2018 preprint on arXiv. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and others also contributed to this field.",
  "summary_type": "reports"
}{
  "input": "04.\n20\n\n--------------------------------------------------\n[32] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for ﬁne-grained categorization. In\n4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia,\n2013.\n[33] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. Advances",
  "output": "J. Krause, M. Stark, J. Deng, and L. Fei-Fei discussed 3D object representations for fine-grained categorization at the 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13) in Sydney, Australia, 2013. A. Krizhevsky, G. Hinton, and others explored learning multiple layers of features from tiny images in 2009. A. Krizhevsky, I. Sutskever, and G. E. Hinton worked on ImageNet classification with deep convolutional neural networks.",
  "summary_type": "reports"
}{
  "input": "hese MLP-like models can not be used in other downstream\ntasks, e.g., object detection and semantic segmentation due\nto the two challenges:\n(1) Such models are composed of blocks with the same\narchitectures, resulting in features with a single scale in low\nresolution. Thus, the non-hierarchical architectures make\nthe model infeasible to provide pyramid feature representa-\ntions.\n(2) They can not deal with ﬂexible input scales. They\nutilize the Spatial FC (Figure 1b), which is connected to\nall sp",
  "output": "MLP-like models face challenges in tasks like object detection and semantic segmentation due to two main issues. First, their composition of identical architecture blocks results in single-scale, low-resolution features, making it impossible to provide pyramid feature representations. Second, they struggle with flexible input scales, as they use the Spatial FC, which is connected to all spatial positions.",
  "summary_type": "reports"
}{
  "input": "themes have emerged. The ﬁrst studies\nthe origin of the beneﬁts of batch normalization during train-\ning (Balduzzi et al., 2017; Santurkar et al., 2018; Bjorck\net al., 2018; Luo et al., 2018; Yang et al., 2019; Jacot et al.,\n2019; De & Smith, 2020), while the second seeks to train\ndeep ResNets to competitive accuracies without normaliza-\ntion layers (Hanin & Rolnick, 2018; Zhang et al., 2019a; De\n& Smith, 2020; Shao et al., 2020; Brock et al., 2021).\nA key theme in many of these works is that it",
  "output": "Two main themes have emerged in recent studies. The first explores the origin of the benefits of batch normalization during training. The second aims to train deep ResNets to competitive accuracies without normalization layers. These themes are key in many of these works.",
  "summary_type": "reports"
}{
  "input": "alable and efﬁcient\nthan existing one-shot approaches since the supernet training now\ncan focus on the promising task-speciﬁc subnets recommended by\nthe search algorithm, instead of on all subnets globally.\nTo visualize the difference between the existing approach\nof disentangling supernet training from architecture search, and\nour approach that use architecture search to guide the supernet\ntraining, let us consider the following problem of minimizing a\ntwo-variable Rosenbrock function [89]:\nmin",
  "output": "The text discusses an improved approach to supernet training. Unlike existing methods that focus on all subnets globally, this new method concentrates on promising task-specific subnets recommended by the search algorithm. This approach uses architecture search to guide supernet training, contrasting with the traditional method of separating supernet training from architecture search. The text then introduces a problem involving the minimization of a two-variable Rosenbrock function.",
  "summary_type": "reports"
}{
  "input": "E\nwith comparable model size with T2T-ViT-14, the ghost op-\neration can reduce the number of parameters and MACs of\nmodels, so the T2T-ViT-Ghost only has 80% parameters and\nof T2T-ViT-14.\nFor fair comparisons, the above variants of T2T-ViT\nare designed with comparable size with T2T-ViT-14 and\nResNet50 except for T2T-ViT-Ghost. It is noted that our\ndesign of each transferring is not the only choice, and we\nwish the transfers can motivate the model designs of Trans-\nformers in vision tasks.\nA.2. D",
  "output": "The ghost operation can reduce the parameters and MACs of models, making T2T-ViT-Ghost have only 80% parameters of T2T-ViT-14. All T2T-ViT variants are designed comparably to T2T-ViT-14 and ResNet50, except for T2T-ViT-Ghost. The design of each transfer is not the only choice, and it is hoped that the transfers can inspire the model designs of Transformers in vision tasks.",
  "summary_type": "reports"
}{
  "input": "es the convnet’s large third stage. Note that in\nthis experiment, the training process is similar to DeiT:\n300 epochs, we measure the top-1 validation accuracy on\nImageNet, and the speed as the number of images that one\nGPU can process per second.\nOne interesting observation that we show Figure 3 is\nthat the convergence of grafted models during training\nseems to be similar to a convnet during the early epochs\nand then switch to a convergence rate similar to DeiT-S. A\nhypothesis is that the convo",
  "output": "El proceso de entrenamiento es similar a DeiT: 300 épocas, se mide la precisión de validación top-1 en ImageNet y la velocidad como el número de imágenes que una GPU puede procesar por segundo. Una observación interesante es que la convergencia de los modelos injertados durante el entrenamiento parece ser similar a una convnet durante las primeras épocas y luego cambia a una tasa de convergencia similar a DeiT-S.",
  "summary_type": "reports"
}{
  "input": "nstance, 8 or 4 bits, the set of possible values\n*These authors contributed equally.\nCopyright 2021 to the authors. Preprint version.\nCorrespondence to defossez at fb.com.\nis no longer a good approximation of R, hence preventing the\nuse of ﬁrst-order optimization methods. Speciﬁcally, uniform\nquantization requires using the round function, which has\nzero gradients wherever it is differentiable.\nQuantization can be done as a post-processing step to\nregular training. However, errors accumulate in",
  "output": "Quantization, which can be done as a post-processing step to regular training, requires using the round function. However, this method is not a good approximation of R, preventing the use of first-order optimization methods. Specifically, uniform quantization has zero gradients wherever it is differentiable, leading to accumulated errors.",
  "summary_type": "reports"
}{
  "input": "ccuracy in Tab. 8. The ac-\ncuracies of the CNN branch, the transformer branch, and\nthe Conformer-S respectively reach 83.3%, 83.1%, and\n83.4%. In contrast, the ensemble model (DeiT-S+ResNet-\n2Refer to Appendix for detailed attention-based sampling.\n0°\n60°\n120°\n180°\n240°\n300°\nConformer-S\nDeiT-S\nResNet-101\nDeiT-B\n50\n55\n60\n65\n70\n75\n80\n85\n112\n160\n224\n336\n448\nTop-1  Accuracy(%)\nImage Resolution\nConformer-S\nDeiT-S\nResNet-50\nResNet-152\n(a)\n(b)\nFigure 5: Generalization capability. (a) Comparison of ro-",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "observations\nso that they are easily described by its external memory (or\nschemata), while during accommodation, PAWS updates its\nexternal memory to account for the new observations.\nThe use of a supervised support set has some practical\nadvantages as well, since it enables the model to learn efﬁ-\nciently. However, it remains an interesting question to see if\none can learn competitive representations in this framework\nusing only instance supervision and more ﬂexible memory\nrepresentations. We p",
  "output": "The text discusses the PAWS model's ability to update its external memory to accommodate new observations. It also mentions the practical advantages of using a supervised support set for efficient learning. However, it raises a question about the possibility of learning competitive representations using only instance supervision and more flexible memory representations.",
  "summary_type": "reports"
}{
  "input": "Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistill-\ning the knowledge in a neural network.\narX",
  "output": "Dehghani et al. propose transformers for image recognition at scale in their 2020 preprint on arXiv. Kaiming He and his team discuss deep residual learning for image recognition in the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean focus on distilling the knowledge in a neural network.",
  "summary_type": "reports"
}{
  "input": "in a window size. After all, there\nare four components in this ”local attention + global mem-\nory” mechanism, namely global-to-global, local-to-global,\nglobal-to-local, and local-to-local, as illustrated in Figure 2\n(Left). In Equation (2), a Multi-head Self-Attention (MSA)\nblock with the Vision Longformer attention mechanism is\ndenoted as MSAViL, i.e., a = ViL in Equation (2).\nRelative positional bias for Vision Longformer. Follow-\ning [33, 2, 26], we add a relative positional bias B to each\nhe",
  "output": "The \"local attention + global memory\" mechanism consists of four components: global-to-global, local-to-global, global-to-local, and local-to-local. A Multi-head Self-Attention (MSA) block with the Vision Longformer attention mechanism is denoted as MSAViL. A relative positional bias B is added to each Vision Longformer, following certain references.",
  "summary_type": "reports"
}{
  "input": "] dataset as labeled data and the JFT-300M\ndataset [26, 60] as unlabeled data.\nWe train a pair of\nEfﬁcientNet-L2 networks, one as a teacher and one as a\nstudent, using Meta Pseudo Labels. The resulting student\nnetwork achieves the top-1 accuracy of 90.2% on the Im-\nageNet ILSVRC 2012 validation set [56], which is 1.6%\nbetter than the previous record of 88.6% [16]. This student\nmodel also generalizes to the ImageNet-ReaL test set [6], as\nsummarized in Table 1. Small scale semi-supervised learn-\ni",
  "output": "Ein Paar von EfficientNet-L2-Netzwerken wurde mit Meta Pseudo Labels trainiert, wobei eines als Lehrer und das andere als Schüler fungierte. Das resultierende Schülernetzwerk erreichte eine Top-1-Genauigkeit von 90,2% im ImageNet ILSVRC 2012 Validierungsset, was 1,6% besser ist als der vorherige Rekord von 88,6%. Dieses Schülermodell generalisiert auch auf das ImageNet-ReaL Testset, wie in Tabelle 1 zusammengefasst.",
  "summary_type": "reports"
}{
  "input": "nd Machine Intelligence. 6\nNakkiran, P.; Kaplun, G.; Bansal, Y.; Yang, T.; Barak, B.;\nand Sutskever, I. 2020. Deep Double Descent: Where Bigger\nModels and More Data Hurt. In International Conference on\nLearning Representations. 1\nPark, D. S.; Chan, W.; Zhang, Y.; Chiu, C.-C.; Zoph, B.;\nCubuk, E. D.; and Le, Q. V. 2019. SpecAugment: A Simple\nData Augmentation Method for Automatic Speech Recogni-\ntion. In Interspeech. 2, 3\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Kill",
  "output": "Nakkiran, Kaplun, Bansal, Yang, Barak, and Sutskever (2020) discussed the phenomenon of Deep Double Descent, where larger models and more data can be detrimental, at the International Conference on Learning Representations. Park, Chan, Zhang, Chiu, Zoph, Cubuk, and Le (2019) introduced SpecAugment, a simple data augmentation method for automatic speech recognition, at Interspeech. Paszke, Gross, Massa, Lerer, Bradbury, Chanan, and Kill's work is not detailed.",
  "summary_type": "reports"
}{
  "input": "s one of the earliest convolutional networks in com-\nputer vision. AlexNet uses three max-pooling layers. In the\nmax-pooling layer, the spatial size of the feature is reduced\nby half, and the channel size is increased by the convolu-\ntion after the max-pooling. VGGnet [30] uses 5 spatial res-\nolutions using 5 max-pooling. In the pooling layer, the spa-\ntial size is reduced by half and the channel size is doubled.\nGoogLeNet [31] also used the pooling layer. ResNet [13]\nperformed spatial size redu",
  "output": "AlexNet, one of the earliest convolutional networks in computer vision, uses three max-pooling layers to reduce spatial size by half and increase channel size. VGGnet uses five max-pooling layers to achieve similar results. GoogLeNet also employs pooling layers, while ResNet performs spatial size reduction.",
  "summary_type": "reports"
}{
  "input": "the\nInternational Conference on Learning Representations.\nFan, A.; Stock, P.; Graham, B.; Grave, E.; Gribonval, R.;\nJegou, H.; and Joulin, A. 2020. Training with Quantization\nNoise for Extreme Model Compression. In ICLR 2021.\nGong, R.; Liu, X.; Jiang, S.; Li, T.; Hu, P.; Lin, J.; Yu, F.;\nand Yan, J. 2019. Differentiable soft quantization: Bridging\nfull-precision and low-bit neural networks. In Proceedings\nof the IEEE International Conference on Computer Vision.\nHe, K.; Zhang, X.; Ren, S.; and Su",
  "output": "Fan, Stock, Graham, Grave, Gribonval, Jegou, and Joulin (2020) trained with quantization noise for extreme model compression, presented at ICLR 2021. Gong, Liu, Jiang, Li, Hu, Lin, Yu, and Yan (2019) discussed differentiable soft quantization, bridging full-precision and low-bit neural networks at the IEEE International Conference on Computer Vision.",
  "summary_type": "reports"
}{
  "input": "information ﬂow than dep-sep+1 × 1 conv. This\nis more apparent in small models which have less channels,\nso 1 × 1 conv cannot effectively mix channel information.\nChannel Multiplexing: To make models more efﬁcient,\nmethods such as scaling down the number of channels by\na factor (named width multiplier), or scaling down the in-\nput resolution have been proposed. Here we investigate the\nimpact of channel multiplexing as an alternative to reduce\nmodel complexity. To be consistent with the main exp",
  "output": "Channel Multiplexing: For more efficient models, methods like reducing the number of channels by a factor (called width multiplier), or lowering the input resolution have been suggested. Channel multiplexing is explored as an alternative to decrease model complexity.",
  "summary_type": "reports"
}{
  "input": "nd Song Han. Proxylessnas: Direct\nneural architecture search on target task and hardware. In In-\nternational Conference on Learning Representations, 2018.\n7\n[39] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted\nresiduals and linear bottlenecks.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4510–4520, 2018. 8\n[40] Ross\nWightman.\nPytorch\nimage\nmod-\nels.\nhttps://github.com/rwightman/\npytorch-image-",
  "output": "\"Proxylessnas\" ist eine Methode zur direkten Suche nach neuronalen Architekturen auf Ziel-Aufgaben und Hardware, vorgestellt auf der Internationalen Konferenz für Lernrepräsentationen 2018. \"Mobilenetv2\" ist ein Modell mit invertierten Resten und linearen Engpässen, präsentiert auf der IEEE-Konferenz für Computer Vision und Mustererkennung 2018. Ross Wightman hat Pytorch-Bildmodelle auf GitHub veröffentlicht.",
  "summary_type": "reports"
}{
  "input": "tzer et al., 2011) and\nFashion-MNIST datasets (Xiao et al., 2017). Once again, SAM enables a simple WideResNet to\nachieve accuracy at or above the state-of-the-art for these datasets: 0.99% error for SVHN, and\n3.59% for Fashion-MNIST. Details are available in appendix B.1.\nTo assess SAM’s performance at larger scale, we apply it to ResNets (He et al., 2015) of different\ndepths (50, 101, 152) trained on ImageNet (Deng et al., 2009). In this setting, following prior work\n(He et al., 2015; Szegedy",
  "output": "SAM permite que un simple WideResNet alcance una precisión en o por encima del estado del arte para los conjuntos de datos SVHN y Fashion-MNIST, con un error del 0.99% y 3.59% respectivamente. Para evaluar el rendimiento de SAM a mayor escala, se aplica a ResNets de diferentes profundidades (50, 101, 152) entrenados en ImageNet. Más detalles están disponibles en el apéndice B.1.",
  "summary_type": "reports"
}{
  "input": "Cosine LR Decay\n79.3\n+0.3\n+ Increase training epochs\n78.8 †\n-0.5\n+ EMA of weights\n79.1\n+0.3\n+ Label Smoothing\n80.4\n+1.3\n+ Stochastic Depth\n80.6\n+0.2\n+ RandAugment\n81.0\n+0.4\n+ Dropout on FC\n80.7 ‡\n-0.3\n+ Decrease weight decay\n82.2\n+1.5\n+ Squeeze-and-Excitation\n82.9\n+0.7\n+ ResNet-D\n83.4\n+0.5\nTable 1. Additive study of the ResNet-RS training recipe. The\ncolors refer to Training Methods , Regularization Methods\nand Architecture Improvements .\nThe baseline ResNet-200\nwas trained for the standard 90",
  "output": "ResNet-RS eğitim tarifinin eklemeli çalışması incelendi. Eğitim dönemlerini artırmak, ağırlıkların EMA'sı, Etiket Düzleştirme, Stokastik Derinlik, RandAugment, FC'de Dropout, ağırlık çürümesini azaltma, Sıkıştırma ve Uyarılma ve ResNet-D'nin eklenmesi, performansı artırdı. Ancak, FC'de Dropout performansı azalttı. Başlangıç ResNet-200, standart 90 için eğitildi.",
  "summary_type": "reports"
}{
  "input": "jecture that this maybe due to the use of\nshifted windows in Swin, which might not work well with CPE.\nArchitecture settings\nWe report the detailed settings of Twins-PCPVT in Table 9 (in supplemen-\ntary), which are similar to PVT [8]. Therefore, Twins-PCPVT has similar FLOPs and number of\nparameters to [8].\n3.2\nTwins-SVT\nVision transformers suffer severely from the heavy computational complexity in dense prediction\ntasks due to high-resolution inputs. Given an input of H × W resolution, the comp",
  "output": "Twins-PCPVT architecture settings are similar to PVT, hence they have similar FLOPs and parameters. Vision transformers, like Twins-SVT, face high computational complexity in dense prediction tasks due to high-resolution inputs. The complexity increases with the resolution of the input.",
  "summary_type": "reports"
}{
  "input": "d task cascade for instance\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4974–\n4983, 2019. 6, 9\n[10] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, et al. Mmdetection: Open mmlab detection tool-\nbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\n6, 9\n[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder wit",
  "output": "Text: \n在IEEE/CVF计算机视觉和模式识别会议的论文中，介绍了实例分割的任务级联。Kai Chen，Jiaqi Wang，Jiangmiao Pang，Yuhang Cao，Yu Xiong，Xiaoxiao Li，Shuyang Sun，Wansen Feng，Ziwei Liu，Jiarui Xu等人发布了Mmdetection：开放的mmlab检测工具箱和基准。Liang-Chieh Chen，Yukun Zhu，George Papandreou，Florian Schroff，和Hartwig Adam正在进行编码器-解码器的研究。",
  "summary_type": "reports"
}{
  "input": "lbox and bench-\nmark.\nhttps://github.com/open-mmlab/\nmmsegmentation, 2020. 8, 10\n[17] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe.\nRandaugment:\nPractical automated data augmenta-\ntion with a reduced search space.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702–703, 2020. 9\n[18] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE Inter",
  "output": "Ekin D Cubuk, Barret Zoph, Jonathon Shlens, ve Quoc V Le, \"Randaugment: Practical automated data augmentation with a reduced search space\" adlı çalışmalarında veri artırmanın otomatikleştirilmiş bir yöntemini sunmuşlardır. Bu, IEEE/CVF Bilgisayar Görüşü ve Desen Tanıma Atölyeleri'nde sunulmuştur (2020). Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu ve Yichen Wei ise \"Deformable convolutional networks\" adlı çalışmalarında deformable convolutional ağlarını tanıtmışlardır. Bu da IEEE International'da sunulmuştur.",
  "summary_type": "reports"
}{
  "input": "Therefore, we only need to perform\ninterpolation to shift from one resolution to another.\n5\n\n--------------------------------------------------\nTable 2: Detailed conﬁgurations of different variants of GFNet. For hierarchical models, we provide\nthe number of channels and blocks in 4 stages. The FLOPs are calculated with 224 × 224 input.\nModel\n#Blocks\n#Channels\nParams (M)\nFLOPs (G)\nGFNet-Ti\n12\n256\n7\n1.3\nGFNet-XS\n12\n384\n16\n2.9\nGFNet-S\n19\n384\n25\n4.5\nGFNet-B\n19\n512\n43\n7.9\nGFNet-H-Ti\n[3, 3, 10, 3]\n[64",
  "output": "Para cambiar de una resolución a otra, solo necesitamos realizar interpolación. La Tabla 2 proporciona configuraciones detalladas de diferentes variantes de GFNet. Para los modelos jerárquicos, se proporciona el número de canales y bloques en 4 etapas. Los FLOPs se calculan con una entrada de 224 × 224. Los modelos varían desde GFNet-Ti con 12 bloques y 256 canales hasta GFNet-B con 19 bloques y 512 canales. Los parámetros y FLOPs también varían entre los modelos.",
  "summary_type": "reports"
}{
  "input": "Net-50 + DropBlock (kp=0.9) [10] + label smoothing (0.1)\n224\n25.6M\n78.35\n94.15\nResNet-50 + MEAL [42]\n224\n25.6M\n78.21\n94.01\nResNet-50 + Ours (MEAL V2)\n224\n25.6M\n80.67\n95.09\nResNet-50 + FixRes [49]\n384\n25.6M\n79.0\n94.6\nResNet-50 + FixRes (*) [49]\n384\n25.6M\n79.1\n94.6\nResNet-50 + Ours (MEAL V2)\n380\n25.6M\n81.72\n95.81\nResNet-50 + FixRes [49] + CutMix\n320\n25.6M\n79.7\n94.9\nResNet-50 + FixRes [49] + CutMix (*)\n320\n25.6M\n79.8\n94.9\nResNet-50 + Ours (MEAL V2) + CutMix\n224\n25.6M\n80.98\n95.35\nTable 4. Comparison",
  "output": "Table 4 compares different configurations of ResNet-50. Net-50 + DropBlock has an accuracy of 78.35 and precision of 94.15. ResNet-50 + MEAL has slightly lower accuracy and precision. ResNet-50 + MEAL V2 improves accuracy to 80.67 and precision to 95.09. ResNet-50 + FixRes and its variant have accuracy around 79.0 and precision of 94.6. ResNet-50 + MEAL V2 with different resolution has the highest accuracy (81.72) and precision (95.81). ResNet-50 + FixRes + CutMix and its variant have accuracy around 79.7 and precision of 94.9. ResNet-50 + MEAL V2 + CutMix has accuracy of 80.98 and precision of 95.35.",
  "summary_type": "reports"
}{
  "input": "softplus provides a smooth approximation to the ReLU, but has slope\n1 −\n1\n1+exp(βz) →1\n2 around z = 0 (where β controls the curvature).\nConvolution and Convergence to Equilibrium.\nWhereas the original DEQ model focused pri-\nmarily on self-attention transformations [54], where all hidden units communicate globally, MDEQ\nmodels face additional challenges due to the nature of typical vision models. Speciﬁcally, our MDEQ\nmodels employ convolutions with small receptive ﬁelds (e.g., the two 3 × 3 conv",
  "output": "Softplus bietet eine glatte Annäherung an die ReLU, hat jedoch eine Steigung von 1 - 1/(1+exp(βz)) →1/2 um z = 0 (wo β die Krümmung kontrolliert). Konvolution und Konvergenz zum Gleichgewicht. Während das ursprüngliche DEQ-Modell hauptsächlich auf Selbstaufmerksamkeitstransformationen [54] fokussierte, bei denen alle versteckten Einheiten global kommunizieren, stellen MDEQ-Modelle aufgrund der Natur typischer Vision-Modelle zusätzliche Herausforderungen. Insbesondere verwenden unsere MDEQ-Modelle Konvolutionen mit kleinen rezeptiven Feldern (z.B. die beiden 3 × 3 Konv).",
  "summary_type": "reports"
}{
  "input": "els to initialize the backbone models and jointly train the\nwhole model for 30 epochs. We set the learning rate of the prediction module to batch size\n1024\n× 0.001 and\nuse 0.01× learning rate for the backbone model. We ﬁx the weights of the backbone models in the\nﬁrst 5 epochs. All of our models are trained on a single machine with 8 GPUs. Other training setups\nand details can be found in the supplementary material.\n6\n\n--------------------------------------------------\n4\n6\n8\n10\n12\nGFLOPs\n80\n81\n8",
  "output": "Se inicializan los modelos de base y se entrena conjuntamente todo el modelo durante 30 épocas. Se establece la tasa de aprendizaje del módulo de predicción a tamaño de lote 1024 × 0.001 y se utiliza una tasa de aprendizaje de 0.01× para el modelo de base. Los pesos de los modelos de base se fijan en las primeras 5 épocas. Todos nuestros modelos se entrenan en una sola máquina con 8 GPUs. Otros detalles y configuraciones de entrenamiento se pueden encontrar en el material suplementario.",
  "summary_type": "reports"
}{
  "input": "ith positional or content attention masked off at test time.\n\n--------------------------------------------------",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "of probing other promising paths that\nare yet to be sampled and get trained, which can be easily\nfulﬁlled by uniform sampling from A. For achieve a bal-\nanced trade-off of exploration and exploitation, we adopt a\ntypical ϵ-sampling policy, i.e., implementing uniform sam-\npling both from A and pool P (line 4 of Algorithm 1),\na ∼(1 −ϵ) · U(A) + ϵ · U(P),\n(8)\nwhere ϵ ∈[0, 1] indicates the probability of sampling from\nthe pool P. Note that candidate pool runs through the train-\ning process of supern",
  "output": "Para explorar caminhos promissores ainda não amostrados e treinados, é possível realizar uma amostragem uniforme de A. Para equilibrar a exploração e a explotação, adotamos uma política de amostragem ϵ típica, ou seja, implementando amostragem uniforme tanto de A quanto do pool P (linha 4 do Algoritmo 1), a ∼(1 −ϵ) · U(A) + ϵ · U(P), onde ϵ ∈[0, 1] indica a probabilidade de amostragem do pool P. Observe que o pool de candidatos percorre o processo de treinamento do supern.",
  "summary_type": "reports"
}{
  "input": "Y-8GF [40]⋆\n39M\n2242\n591.6\n81.7\n87.4\n70.8\nRegNetY-16GF [40]⋆\n84M\n2242\n334.7\n82.9\n88.1\n72.4\nEfﬁcientNet-B0 [48]\n5M\n2242\n2694.3\n77.1\n83.5\n64.3\nEfﬁcientNet-B1 [48]\n8M\n2402\n1662.5\n79.1\n84.9\n66.9\nEfﬁcientNet-B2 [48]\n9M\n2602\n1255.7\n80.1\n85.9\n68.8\nEfﬁcientNet-B3 [48]\n12M\n3002\n732.1\n81.6\n86.8\n70.6\nEfﬁcientNet-B4 [48]\n19M\n3802\n349.4\n82.9\n88.0\n72.3\nEfﬁcientNet-B5 [48]\n30M\n4562\n169.1\n83.6\n88.3\n73.6\nEfﬁcientNet-B6 [48]\n43M\n5282\n96.9\n84.0\n88.8\n73.9\nEfﬁcientNet-B7 [48]\n66M\n6002\n55.1\n84.3\nEfﬁcientNet-B5 RA [12",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "is still under the original transformers. These smaller mod-\nels are constructed by compressing the original model along\nwith depth so all transformer blocks share the same knowl-\nedge. Such structure induces the unavoidable loss of model\ncapacity.\nIn this paper, we present a parameter deployment frame-\nwork that deploys trainable parameters more effectively: go-\ning wider instead of deeper. We then implement it on the\ntransformer and named it as WideNet. Specially, we ﬁrst em-\nploys parameter",
  "output": "El texto presenta un marco de implementación de parámetros que despliega parámetros entrenables de manera más efectiva: yendo más ancho en lugar de más profundo. Luego se implementa en el transformador y se le nombra como WideNet. Específicamente, primero emplea parámetros.",
  "summary_type": "reports"
}{
  "input": "tion to enhance the\nperformance of ViT on the ImageNet benchmark and obtain an about 81.8% ImageNet top-1 accuracy,\nwhich is comparable to that of the state-of-the-art convolutional networks. Chen et al. further treat the\nimage processing tasks (e.g., denosing and super-resolution) as a series of translations and develop\nthe IPT model for handling multiple low-level computer vision problems [4]. Nowadays, transformer\narchitectures have been used in a growing number of computer vision tasks [11]",
  "output": "ViT's performance on the ImageNet benchmark has been enhanced to achieve approximately 81.8% top-1 accuracy, comparable to state-of-the-art convolutional networks. Chen et al. developed the IPT model to handle multiple low-level computer vision problems, treating image processing tasks as a series of translations. Currently, transformer architectures are increasingly used in various computer vision tasks.",
  "summary_type": "reports"
}{
  "input": "m ran-\ndom crop (with additional random horizontal ﬂip) in training\nand central crop in evaluation. For BERT we use wordpiece\nsequence of maximum 64 tokens since the input texts are\nno longer than 20 unigrams. The softmax temperature vari-\nable is initialized as 1.0 (this temperature variable is shared\nbetween image-to-text loss and text-to-image loss) and we\nuse 0.1 as label smoothing parameter in the softmax losses.\nWe use LAMB optimizer (You et al., 2020)1 with weight\ndecay ratio 1e-5. The le",
  "output": "Training involves random crop and additional random horizontal flip, while evaluation uses central crop. BERT employs a wordpiece sequence of maximum 64 tokens, as input texts don't exceed 20 unigrams. The softmax temperature variable is initialized at 1.0, shared between image-to-text and text-to-image loss, with 0.1 used as label smoothing parameter in the softmax losses. The LAMB optimizer is used with a weight decay ratio of 1e-5.",
  "summary_type": "reports"
}{
  "input": "experimental results on “CNN to ViT” in\nTab. 6, we can ﬁnd both SE (ViT-SE) and Deep-Narrow\nstructure (ViT-DN) beneﬁt the ViT but the most effective\nstructure is deep-narrow structure, which decreases model\nsize and MACs nearly 2x and brings 0.9% improvement on\nthe baseline model ViT-S/16.\nWe further apply these structures from CNN to our T2T-\nViT, and conduct experiments on ImageNet under the same\ntraining scheme.\nWe take ResNet50 as the baseline for\nCNN, ViT-S/16 for ViT, and T2T-ViT-14 for T2",
  "output": "Text:\nLos resultados experimentales en \"CNN a ViT\" en la Tab. 6, muestran que tanto SE (ViT-SE) como la estructura Deep-Narrow (ViT-DN) benefician a ViT, siendo la más efectiva la estructura Deep-Narrow, que reduce casi a la mitad el tamaño del modelo y MACs, y aporta una mejora del 0,9% en el modelo base ViT-S/16. Aplicamos estas estructuras de CNN a nuestro T2T-ViT, y realizamos experimentos en ImageNet bajo el mismo esquema de entrenamiento. Tomamos ResNet50 como la base para CNN, ViT-S/16 para ViT, y T2T-ViT-14 para T2.",
  "summary_type": "reports"
}{
  "input": "to a much larger dataset with higher-resolution images:\nImageNet [16]. As with CIFAR-10 classiﬁcation, we add a shallow classiﬁcation layer after the\nMDEQ module to fuse the equilibrium outputs from different scales, and train on a combined loss.\nWe benchmark both a small MDEQ model and a large MDEQ to provide appropriate comparisons\nwith a number of reference models, such as ResNet-18, -34, -50, and -101 [25]. Note that MDEQ\nhas only one layer of residual blocks followed by multi-resolution fu",
  "output": "MDEQ models are trained on ImageNet, a larger dataset with high-resolution images. A shallow classification layer is added after the MDEQ module to fuse equilibrium outputs from different scales. Both small and large MDEQ models are benchmarked for comparison with reference models like ResNet-18, -34, -50, and -101. MDEQ only has one layer of residual blocks followed by multi-resolution fusion.",
  "summary_type": "reports"
}{
  "input": "ayer 4\nLayer 7\nLayer 10\n(b) DeiT\nLayer 1\n( ) = 0.01\nHead 1\n( ) = 0.52\nHead 2\n( ) = 0.01\nHead 3\n( ) = 0.00\nHead 4\n( ) = 0.04\nHead 5\n( ) = 0.92\nHead 6\n( ) = 0.07\nHead 7\n( ) = 0.01\nHead 8\n( ) = 0.07\nHead 9\nLayer 4\n( ) = 0.00\n( ) = 0.82\n( ) = 0.03\n( ) = 0.82\n( ) = 0.14\n( ) = 0.84\n( ) = 0.75\n( ) = 0.00\n( ) = 0.34\nLayer 7\n( ) = 0.15\n( ) = 0.00\n( ) = 0.59\n( ) = 0.00\n( ) = 0.04\n( ) = 0.02\n( ) = 0.04\n( ) = 0.00\n( ) = 0.00\nLayer 10\n( ) = 0.00\n( ) = 0.00\n( ) = 0.00\n( ) = 0.00\n( ) = 0.00\n( ) = 0.00\n( ) = 0.",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "-------------------------------------------\nthan existing models. In addition, on three commonly used\ndownstream single-label classiﬁcation datasets it reaches\nnew state-of-the-art accuracies. We also show that TResNet\ngeneralizes well to other computer vision tasks, reaching\ntop scores on multi-label classiﬁcation and object detection\ndatasets.\nReferences\n[1] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-\nmawat, Geoffrey Irving, Mic",
  "output": "TResNet supera los modelos existentes y alcanza nuevas precisiones de vanguardia en tres conjuntos de datos de clasificación de etiqueta única comúnmente utilizados. Además, se demuestra que TResNet se generaliza bien a otras tareas de visión por computadora, alcanzando las mejores puntuaciones en conjuntos de datos de clasificación de múltiples etiquetas y detección de objetos.",
  "summary_type": "reports"
}{
  "input": "he\nhead decreases the spatial resolution of the feature maps from 38×38 to 1×1. All the output feature\nmaps from the PyConvs in the head are used for detections.\nFor each of the six output feature maps selected for detection {S3F M, HF M1, HF M2, HF M3,\nHF M4, HF M5} the framework performs the detection using a coresponding number of default boxes\n(anchor boxes) {4, 6, 6, 6, 4, 4} for each spatial location. For instance, for (S3F M) output feature\nmaps with the spatial resolution 38×38, using th",
  "output": "The head reduces the spatial resolution of feature maps from 38×38 to 1×1. All output feature maps from the PyConvs in the head are used for detections. Six output feature maps {S3F M, HF M1, HF M2, HF M3, HF M4, HF M5} are selected for detection, using a corresponding number of default boxes (anchor boxes) {4, 6, 6, 6, 4, 4} for each spatial location. For example, for (S3F M) output feature maps with the spatial resolution 38×38.",
  "summary_type": "reports"
}{
  "input": "cknowledgment\nThis work was supported in part by the National Key Research and Development Program of China\nunder Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under\nGrant 62125603, Grant 61822603, Grant U1813218, Grant U1713214, in part by Beijing Academy\nof Artiﬁcial Intelligence (BAAI), and in part by a grant from the Institute for Guo Qiang, Tsinghua\nUniversity.\n10\n\n--------------------------------------------------\nReferences\n[1] Gregory A Baxes. Digital",
  "output": "本研究部分得到中国国家重点研发计划（Grant 2017YFA0700802）、国家自然科学基金（Grant 62125603、Grant 61822603、Grant U1813218、Grant U1713214）、北京人工智能学院（BAAI）以及清华大学郭强研究所的资助。",
  "summary_type": "reports"
}{
  "input": "rove.\nTraining efﬁciency has gained signiﬁcant interests recently.\nFor instance, NFNets (Brock et al., 2021) aim to improve\ntraining efﬁciency by removing the expensive batch nor-\nmalization; Several recent works (Srinivas et al., 2021) fo-\ncus on improving training speed by adding attention layers\ninto convolutional networks (ConvNets); Vision Transform-\ners (Dosovitskiy et al., 2021) improves training efﬁciency\non large-scale datasets by using Transformer blocks. How-\never, these methods often",
  "output": "La eficiencia en el entrenamiento ha despertado un interés significativo recientemente. Por ejemplo, NFNets (Brock et al., 2021) busca mejorar la eficiencia del entrenamiento eliminando la costosa normalización por lotes; varios trabajos recientes (Srinivas et al., 2021) se centran en mejorar la velocidad de entrenamiento añadiendo capas de atención a las redes convolucionales (ConvNets); Vision Transformers (Dosovitskiy et al., 2021) mejora la eficiencia del entrenamiento en conjuntos de datos a gran escala utilizando bloques Transformer. Sin embargo, estos métodos a menudo.",
  "summary_type": "reports"
}{
  "input": "net-\nworks (CNNs) to the ViT architecture (i.e. shift, scale,\nand distortion invariance) while maintaining the merits of\nTransformers (i.e. dynamic attention, global context, and\nbetter generalization). We validate CvT by conducting ex-\ntensive experiments, showing that this approach achieves\nstate-of-the-art performance over other Vision Transform-\ners and ResNets on ImageNet-1k, with fewer parame-\nters and lower FLOPs.\nIn addition, performance gains\nare maintained when pretrained on larger da",
  "output": "CvT integra las ventajas de las redes neuronales convolucionales (CNN) a la arquitectura ViT, manteniendo los méritos de los Transformers. Los experimentos validan que CvT logra un rendimiento superior a otros Vision Transformers y ResNets en ImageNet-1k, con menos parámetros y FLOPs más bajos. Además, se mantienen las ganancias de rendimiento cuando se preentrena en conjuntos de datos más grandes.",
  "summary_type": "reports"
}{
  "input": "subsumption in description logics. In IJCAI (1), 1995. 3\n[18] Cynthia Rudin. Stop explaining black box machine learning\nmodels for high stakes decisions and use interpretable mod-\nels instead. Nature Machine Intelligence, 2019. 1, 8\n[19] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nPar-\nallel distributed processing: Explorations in the microstruc-\nture of cognition, vol. 1. In David E. Rumelhart, James L.\nMcClelland, and CORPORATE PDP Research Group, ed-\nitors, Parallel Distributed Process",
  "output": "Cynthia Rudin, in Nature Machine Intelligence 2019, advocates for the use of interpretable models over black box machine learning models for high stakes decisions. D.E. Rumelhart, G.E. Hinton, and R.J. Williams explore the microstructure of cognition in their work \"Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1\".",
  "summary_type": "reports"
}{
  "input": "60%\nLinear 5-shot ImageNet accuracy\nToken (ICLR'21)\nToken (linear)\nToken + head-wd\nGAP + head-wd\nMAP + head-wd\nFigure 4: Left and middle: The dependence of 5-shot ImageNet accuracy and upstream performance\ndepends on the weight decay strength. Normally, a single weight decay value is applied to all weights\n(corresponds to the diagonal on the heatmaps). We show that by using weight decay values for the\n“head” and the rest of the weights one signiﬁcantly improves few-shot transfer performance. Ri",
  "output": "La precisión de ImageNet de 5 disparos lineales depende de la fuerza de la decadencia del peso. Normalmente, se aplica un solo valor de decadencia de peso a todos los pesos. Demostramos que al usar valores de decadencia de peso para la \"cabeza\" y el resto de los pesos, se mejora significativamente el rendimiento de transferencia de pocos disparos.",
  "summary_type": "reports"
}{
  "input": "omputational cost and\nthe accuracy for 14 × 14 feature map in DeiT-S [22].\nComponent-wise analysis.\nWe perform a component-\nwise analysis to study the effects of different position en-\nModel\n#Param.\nInput\nMACs\nTop-1\n(M)\nAcc (%)\nConvnets\nResNet-50 [10]\n25M\n2242\n4121\n79.0\nRegNetY-4.0GF [15]\n21M\n2242\n4012\n79.4\nEfﬁcientNet-B1 [21]\n8M\n2402\n712\n79.1\nEfﬁcientNet-B5 [21]\n30M\n4562\n10392\n83.6\nTransformers\nViT-B/16 [6]\n86M\n3842\n55630\n77.9\nViT-L/16 [6]\n307M\n3842\n191452\n76.5\nDeiT-Ti [22]\n5M\n2242\n1261\n72.2\nCP",
  "output": "Text:\nSe realiza un análisis de componentes para estudiar los efectos de las diferentes posiciones en el mapa de características 14x14 en DeiT-S [22]. Modelos: ResNet-50 [10] con 25M parámetros, 2242 MACs, 79.0% precisión; RegNetY-4.0GF [15] con 21M parámetros, 2242 MACs, 79.4% precisión; EfficientNet-B1 [21] con 8M parámetros, 2402 MACs, 79.1% precisión; EfficientNet-B5 [21] con 30M parámetros, 4562 MACs, 83.6% precisión; ViT-B/16 [6] con 86M parámetros, 3842 MACs, 77.9% precisión; ViT-L/16 [6] con 307M parámetros, 3842 MACs, 76.5% precisión; DeiT-Ti [22] con 5M parámetros, 2242 MACs, 72.2% precisión.",
  "summary_type": "reports"
}{
  "input": "--------------------\nModel\nCIFAR-10\nCIFAR-100\nFashion-MNIST\nMNIST\n# Params\nMACs\nConvolutional Networks (Designed for ImageNet)\nResNet18\n90.27%\n63.41%\n93.51%\n99.18%\n11.18 M\n0.04 G\nResNet34\n90.51%\n64.52%\n93.47%\n99.24%\n21.29 M\n0.08 G\nResNet50\n90.60%\n61.68%\n93.17%\n99.18%\n23.53 M\n0.08 G\nMobileNetV2/0.5\n84.78%\n56.32%\n93.49%\n99.08%\n0.70 M\n< 0.01 G\nMobileNetV2/1.0\n89.07%\n63.69%\n93.62%\n99.28%\n2.24 M\n0.01 G\nMobileNetV2/1.25\n90.60%\n65.24%\n93.83%\n99.25%\n3.47 M\n0.01 G\nMobileNetV2/2.0\n91.02%\n67.44%\n94.07%\n99.",
  "output": "ResNet18, ResNet34, ResNet50, and different versions of MobileNetV2 are convolutional networks designed for ImageNet. Their performance varies on CIFAR-10, CIFAR-100, Fashion-MNIST, and MNIST models. ResNet18 has 11.18M parameters and 0.04G MACs, ResNet34 has 21.29M parameters and 0.08G MACs, and ResNet50 has 23.53M parameters and 0.08G MACs. MobileNetV2 versions have fewer parameters and MACs, ranging from 0.70M to 3.47M parameters and less than 0.01G MACs. Their performance on the models ranges from 84.78% to 91.02%.",
  "summary_type": "reports"
}{
  "input": "the training settings (e.g., learning rate, training epochs, etc.) of [10].\n3.2\nImage classiﬁcation datasets\nStem\nImage\nObject class\nTokenLearner\nTransformer\n...\nClassification head\n8 tokens\n...\nTransformer\n1024 tokens\n8 tokens\nTransformer\n8 tokens\n8 tokens\n8 tokens\nStem\nImage\nObject class\n...\nClassification head\n1024 tokens\nTokenLearner\nTokenFuser\nTransformer\nRepeat\n1024 tokens\n(a)\n(b)\nTransformer\nFigure 3: Our models following the ViT archi-\ntecture. (a) with TokenLearner and (b) with\nboth Tok",
  "output": "Text: \nDie Trainingseinstellungen (z.B. Lernrate, Trainingsepochen usw.) von [10].\n3.2\nBildklassifizierungsdatensätze\nStamm\nBild\nObjektklasse\nTokenLearner\nTransformator\n...\nKlassifikationskopf\n8 Token\n...\nTransformator\n1024 Token\n8 Token\nTransformator\n8 Token\n8 Token\n8 Token\nStamm\nBild\nObjektklasse\n...\nKlassifikationskopf\n1024 Token\nTokenLearner\nTokenFuser\nTransformator\nWiederholen\n1024 Token\n(a)\n(b)\nTransformator\nAbbildung 3: Unsere Modelle folgen der ViT-Architektur. (a) mit TokenLearner und (b) mit beiden Tok.",
  "summary_type": "reports"
}{
  "input": "ize on the upper layers of the pyramid. In the ﬁrst stage of the\nnetwork, our PyConv with four layers contains kernel sizes of: 7×9×9, 5×7×7, 3×5×5 and 3×3×3\n(the temporal dimension comes ﬁrst).\nFor video classiﬁcation, we perform the experiments on Kinetics-400 [50], which is a large-scale\nvideo recognition dataset that contains ∼246k training videos and 20k validation videos, with 400\naction classes. Similar to image recognition, use the SGD optimizer with a standard momentum of\n0.9 and weight",
  "output": "The text discusses a network with a PyConv of four layers with specified kernel sizes. It mentions experiments performed on Kinetics-400, a large-scale video recognition dataset with approximately 246k training videos and 20k validation videos across 400 action classes. The SGD optimizer is used with a standard momentum of 0.9 for video classification.",
  "summary_type": "reports"
}{
  "input": "s are\nconsistent with [22].\nTable A.1: ImageNet Top-1 accuracy of efﬁcient self-attention variants (after 300 epochs of training).\nModel\nComplexity\nTop-1\nDeiT-S [65]\nO(N 2)\n79.9\nSRA (Average Pool) [71]\nO(N 2/R2)\n73.5\nSRA (Convolutional) [71]\nO(N 2/R2)\n74.0\nLinformer (k=√n) [70]\nO(kN)\n75.7\nEfﬁcient Transformer [56]\nO(N)\n76.3\nAxial [30]\nO(N\n√\nN)\n78.4\nI\n\n--------------------------------------------------\nA.3\nTraining and testing with varying resolution\nAs discussed in the main manuscript, for sever",
  "output": "Table A.1 zeigt die Top-1-Genauigkeit von ImageNet für verschiedene effiziente Self-Attention-Modelle nach 300 Trainingszyklen. DeiT-S [65] erreicht eine Genauigkeit von 79,9 bei einer Komplexität von O(N 2). SRA (Average Pool) [71] und SRA (Convolutional) [71] erreichen Genauigkeiten von 73,5 bzw. 74,0 bei einer Komplexität von O(N 2/R2). Linformer [70] erreicht eine Genauigkeit von 75,7 bei einer Komplexität von O(kN). Efficient Transformer [56] erreicht eine Genauigkeit von 76,3 bei einer Komplexität von O(N). Axial [30] erreicht eine Genauigkeit von 78,4 bei einer Komplexität von O(N √N).",
  "summary_type": "reports"
}{
  "input": "de\n2242 × 3\n3 × 3 conv\n32\n1\n2\n1122 × 32\nMB1 K3\n16\n1\n1\n1122 × 16\nChoice Block\n32\n4\n2\n562 × 32\nChoice Block\n40\n4\n2\n282 × 40\nChoice Block\n80\n4\n2\n142 × 80\nChoice Block\n96\n4\n1\n142 × 96\nChoice Block\n192\n4\n2\n72 × 192\nChoice Block\n320\n1\n1\n72 × 320\n1 × 1 conv\n1280\n1\n1\n72 × 1280\nglobal avgpool\n-\n1\n-\n1280\nFC\n1000\n1\n-\n11\n\n--------------------------------------------------\nTable 6: Operation choices for each MobileNetV2-based Choice Block in Table 5, where ID means for an identity mapping.\nblock type\nexpansi",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "ach visual token.\n2.2\nVision transformers with learned IB\nViT [19] is the pioneering work that applies a pure transformer to vision tasks and achieves promising\nresults. However, since ViT lacks intrinsic inductive bias in modeling local visual structures, it indeed\nlearns the IB from amounts of data implicitly. Following works along this direction are to simplify\nthe model structures with fewer intrinsic IBs and directly learn them from large scale data [46, 70,\n71, 21, 18], which have achieved",
  "output": "Vision Transformers (ViT) are a groundbreaking application of pure transformers to visual tasks, yielding impressive results. However, ViT lacks an intrinsic inductive bias for modeling local visual structures, instead learning the inductive bias implicitly from large amounts of data. Subsequent works aim to simplify model structures with fewer intrinsic inductive biases, learning them directly from large-scale data.",
  "summary_type": "reports"
}{
  "input": "24\n82.5\n-\n87.2\nConViT-S+ [15]\n48.0\n20.0\n224\n82.2\n-\n-\nConViT-B [15]\n86.0\n34.0\n224\n82.4\n-\n-\nConViT-B+ [15]\n152.0\n60.0\n224\n82.5\n-\n-\nPiT-B [26]\n73.8\n25.0\n224\n82.0\n-\n-\nTNT-B [22]\n65.6\n28.2\n224\n82.8\n96.3\n-\nT2T-ViT-19 [88]\n39.2\n8.9\n224\n81.9\n95.7\n86.9\nViTAE-B-Stage\n48.5\n13.8\n224\n83.6\n96.4\n87.9\n12\n\n--------------------------------------------------\nTable 6: Model details of ViTAE variants.\nModel\nReduction Cell\nNormal Cell\nNC\nParams Macs\ndilation\ncells heads embed cells Arrangement\n(M)\n(G)\nViTAE-T\n[1, 2,",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "ons. In Conference on\nComputer Vision and Pattern Recognition, 2015. 22\n[62] Mingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convo-\nlutional neural networks. arXiv preprint arXiv:1905.11946, 2019. 1, 14, 15, 21\n[63] Hugo Touvron, M. Cord, M. Douze, F. Massa, Alexandre Sablayrolles, and H.\nJ´egou. Training data-efﬁcient image transformers & distillation through attention.\narXiv preprint arXiv:2012.12877, 2020. 3, 7, 8, 12, 13, 14, 15, 16, 17, 18, 21, III\n[64] Hugo Touvron,",
  "output": "Mingxing Tan ve Quoc V. Le, \"Efficientnet: Convolutional neural networks için model ölçeklendirmeyi yeniden düşünme\" üzerine bir çalışma yapmışlardır (arXiv ön baskı:1905.11946, 2019). Hugo Touvron ve diğerleri ise \"Eğitim veri-etkin görüntü dönüştürücüler & dikkat aracılığıyla damıtma\" konusunda bir araştırma gerçekleştirmiştir (arXiv ön baskı:2012.12877, 2020).",
  "summary_type": "reports"
}{
  "input": "onstructs\nsuch a hierarchy with transformers. ViP divides visual representations into two\nlevels, the part level and the whole level. Information of each part represents a\ncombination of several independent vectors within the whole. To model the repre-\nsentations of the two levels, we ﬁrst encode the information from the whole into\npart vectors through an attention mechanism, then decode the global information\nwithin the part vectors back into the whole representation. By iteratively parsing the",
  "output": "ViP construye una jerarquía con transformadores, dividiendo las representaciones visuales en dos niveles: el nivel de parte y el nivel de todo. Cada parte representa una combinación de varios vectores independientes dentro del todo. Para modelar las representaciones de los dos niveles, primero codificamos la información del todo en vectores de parte a través de un mecanismo de atención, luego decodificamos la información global dentro de los vectores de parte de vuelta a la representación del todo. Este proceso se realiza de manera iterativa.",
  "summary_type": "reports"
}{
  "input": "en Ma, and Shilei Wen. Multi-label classiﬁcation with\nlabel graph superimposing. ArXiv, abs/1911.09243, 2019.\n[41] Ross\nWightman.\npytorch-image-models,\n2019.\nhttps://github.com/rwightman/pytorch-image-models.\n[42] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines\nfor human pose estimation and tracking. In Proceedings of\nthe European conference on computer vision (ECCV), pages\n466–481, 2018.\n[43] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and\nKaiming He. Aggregated residual trans",
  "output": "Ma和Shilei Wen在2019年的ArXiv上发表了一篇关于多标签分类和标签图叠加的论文。Ross Wightman在2019年发布了pytorch-image-models。Bin Xiao, Haiping Wu和Yichen Wei在2018年的欧洲计算机视觉会议（ECCV）上发表了关于人体姿态估计和跟踪的简单基线。Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu和Kaiming He研究了残差聚合。",
  "summary_type": "reports"
}{
  "input": "hyper-parameters:\nsize, stride, share_t, and share_c. size indicates\nhow many tokens does a pattern affects; stride indicates\nthe number of tokens to be skipped by the pattern; share_t\nis a binary deciding whether all the tokens covered by size\nare set to zero using the same noise mask or independent\nnoise masks; and share_c is a binary deciding whether\na the dropout pattern shared along the channel dimension\nC. Once the values of size, stride, share_t, and\nshare_c are decided, at each training",
  "output": "Hyperparameters include size, stride, share_t, and share_c. Size refers to the number of tokens a pattern affects. Stride is the number of tokens skipped by the pattern. Share_t is a binary determining if all tokens covered by size use the same or independent noise masks. Share_c is a binary deciding if the dropout pattern is shared along the channel dimension C. These values are determined at each training.",
  "summary_type": "reports"
}{
  "input": "Table 4).\nIn Figure 7,\nwe show the running time (includ-\ning forward and backward) and memory usage of our\n“SCw/Handgrad” implementation of conv-like local atten-\ntion (sliding chunk attention without padding mode) with\ndifferent window sizes. We can see that the speed is not\nsensitive to the window size for small window sizes (≤17)\nand the memory usage monotonically increases.\nFinally, both the “unfold/nn.F” and the “cuda kernel”\nimplementations support dilated conv-like attention. The\ncustomi",
  "output": "Figure 7 illustrates the running time and memory usage of the \"SCw/Handgrad\" implementation of conv-like local attention with varying window sizes. The speed remains unaffected for small window sizes (≤17), while memory usage consistently increases. Both the \"unfold/nn.F\" and the \"cuda kernel\" implementations support dilated conv-like attention.",
  "summary_type": "reports"
}{
  "input": "------------------------\nﬁnd gMLPs can achieve appealing results on challenging tasks such as SQuAD without self-attention,\nand can signiﬁcantly outperform Transformers in certain cases. We also ﬁnd the inductive bias in\nTransformer’s multi-head self-attention useful on downstream tasks that require cross-sentence align-\nment. However in those cases, making gMLP substantially larger closes the gap with Transformers.\nMore practically, blending a small single-head self-attention into gMLP allows f",
  "output": "gMLPs can achieve good results on challenging tasks like SQuAD without self-attention, and can sometimes outperform Transformers. The inductive bias in Transformer’s multi-head self-attention is useful for tasks requiring cross-sentence alignment. However, making gMLP larger can close the gap with Transformers. Blending a small single-head self-attention into gMLP can be practical.",
  "summary_type": "reports"
}{
  "input": "hes. For CIFAR-10 and CIFAR-100, we ﬁne-tune the models for 64 epochs, and for\nﬁne-grained datasets, we ﬁne-tune the models for 300 epochs. Table 9 compares the transfer learning\nresults of TNT to those of ViT, DeiT and other convolutional networks. We ﬁnd that TNT outperforms\nDeiT in most datasets with less parameters, which shows the superiority of modeling pixel-level\nrelations to get better feature representation.\nTable 9: Results on downstream image classiﬁcation tasks with ImageNet pre-tra",
  "output": "For CIFAR-10 and CIFAR-100, models are fine-tuned for 64 epochs, and for fine-grained datasets, they are fine-tuned for 300 epochs. The transfer learning results of TNT are compared to ViT, DeiT, and other convolutional networks. TNT outperforms DeiT in most datasets with fewer parameters, demonstrating the advantage of modeling pixel-level relations for better feature representation.",
  "summary_type": "reports"
}{
  "input": "rformer\n71.12\n73.09\n78.81\n78.72\n\u0013\nPar-Global\n75.32\n75.4\n81.6\n81.46\n–\nPar-Linformer\n75.56\n75.33\n81.66\n81.79\n\u0017\nPar-SRA/32\n75.2\n75.26\n81.62\n81.61\n–\nPar-Performer\n75.34\n75.93\n81.72\n81.72\n–\nTable 3. Overall comparison of different attention mechanisms on\nImageNet classiﬁcation top-1 accuracy (%), with input size 224.\nTiny-4stage / 4 means that the model has a comparable size with\nDeiT-Tiny, has 4 stages and uses patch size 4x4 in the initial pixel\nspace. “1,2,8,1” are the numbers of attention blocks",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "pansion (shrink) rate for the SE is always 0.25.\nStages\nSize\nCoAtNet-0\nCoAtNet-1\nCoAtNet-2\nCoAtNet-3\nCoAtNet-4\nS0-Conv\n1/2\nL=2 D=64\nL=2\nD=64\nL=2\nD=128\nL=2\nD=192\nL=2\nD=192\nS1-MbConv\n1/4\nL=2 D=96\nL=2\nD=96\nL=2\nD=128\nL=2\nD=192\nL=2\nD=192\nS2-MBConv\n1/8\nL=3 D=192\nL=6\nD=192\nL=6\nD=256\nL=6\nD=384\nL=12 D=384\nS3-TFMRel\n1/16\nL=5 D=384\nL=14 D=384\nL=14 D=512\nL=14 D=768\nL=28 D=768\nS4-TFMRel\n1/32\nL=2 D=768\nL=2\nD=768\nL=2\nD=1024\nL=2\nD=1536\nL=2\nD=1536\nresolutions for 30 epochs and obtain the corresponding evaluation",
  "output": "The text provides information about different stages and sizes of CoAtNet models. Each model has different layers (L) and dimensions (D). For instance, CoAtNet-0 has L=2, D=64 at S0-Conv stage, while CoAtNet-4 has L=2, D=1536 at S4-TFMRel stage. The shrink rate for the SE is consistently 0.25. The models are evaluated over 30 epochs.",
  "summary_type": "reports"
}{
  "input": "800 epochs)\nthat must happen sequentially (not in parallel) with the student updates.\nPAWS-NN refers to performing nearest-neighbour classiﬁcation directly us-\ning the PAWS-pretrained representations, with the labeled training samples\nas support, while PAWS refers to ﬁne-tuning a classiﬁer using the available\nlabeled data after PAWS-pretraining.\n6\nMain Results\nIn this section we analyze the features learned by PAWS on\nImageNet [41]. The standard procedure for evaluating semi-\nsupervised methods",
  "output": "PAWS-NN refers to performing nearest-neighbour classification using PAWS-pretrained representations, with labeled training samples as support. PAWS refers to fine-tuning a classifier using available labeled data after PAWS-pretraining. The features learned by PAWS on ImageNet are analyzed in this section, following the standard procedure for evaluating semi-supervised methods.",
  "summary_type": "reports"
}{
  "input": "e visualization results are provided in Figure 4. Compared with the baseline T2T-ViT,\nour ViTAE covers the single or multiple targets in the images more precisely and attends less to the\nbackground. Moreover, ViTAE can better handle the scale variance issue as shown in Figure 4(b).\nNamely, it can precisely cover the birds no matter they are in small, middle, or large size. Such\nobservations demonstrate that introducing the intrinsic IBs of locality and scale-invariance from\nconvolutions to trans",
  "output": "ViTAE, in comparison to the baseline T2T-ViT, more accurately covers single or multiple targets in images and pays less attention to the background. It can also better manage the scale variance issue, accurately covering birds regardless of their size. This shows that introducing the intrinsic IBs of locality and scale-invariance from convolutions to trans improves performance.",
  "summary_type": "reports"
}{
  "input": "of modules are abstracted in our framework, and such modules (classes and functions)\nin user’s installed torchvision are registered. In this example, ’resnet34’ function4 is\nused to instantiate an object of type ResNet by using a dictionary of keyword arguments\n(**params). i.e. num classess = 1000 and pretrained = True are given as arguments\nof ’resnet34’ function. For image classiﬁcation models implemented in torchvision or\nthose users add to the registry in our framework, users can easily try",
  "output": "In our framework, modules are abstracted and registered from the user's installed torchvision. The 'resnet34' function is used to create a ResNet object using a dictionary of keyword arguments (**params), such as num classes = 1000 and pretrained = True. This allows users to easily experiment with image classification models implemented in torchvision or those added to our framework's registry.",
  "summary_type": "reports"
}{
  "input": "ageNet\n77.91\n73.38\n76.53\n71.16\n-\nImageNet ReaL\n83.57\n79.56\n82.19\n77.83\n-\nOxford Flowers-102\n89.49\n85.43\n89.66\n86.36\n-\nOxford-IIIT-Pets\n93.81\n92.04\n93.64\n91.35\n-\nImageNet-21k\nCIFAR-10\n98.95\n98.79\n99.16\n99.13\n99.27\nCIFAR-100\n91.67\n91.97\n93.44\n93.04\n93.82\nImageNet\n83.97\n81.28\n85.15\n80.99\n85.13\nImageNet ReaL\n88.35\n86.63\n88.40\n85.65\n88.70\nOxford Flowers-102\n99.38\n99.11\n99.61\n99.19\n99.51\nOxford-IIIT-Pets\n94.43\n93.02\n94.73\n93.09\n94.82\nJFT-300M\nCIFAR-10\n99.00\n98.61\n99.38\n99.19\n99.50\nCIFAR-100\n91.87\n90.4",
  "output": "The text appears to be a set of numerical data related to different image datasets such as ImageNet, Oxford Flowers-102, Oxford-IIIT-Pets, CIFAR-10, CIFAR-100, and JFT-300M. The numbers might represent performance metrics like accuracy or precision of a model trained on these datasets. However, without context, it's impossible to provide a meaningful summary. <NULL>",
  "summary_type": "reports"
}{
  "input": "]. The learning rate\nis initialized to be 0.001 and decayed to zero within 300 epochs following the cosine strategy. We use\na linear warm-up in the ﬁrst ﬁve epochs and the same regularization setting as in [2]. Note that we\ndo not utilize extra tricks in [26, 28] to make fair comparisons although it may further improve the\n5\n\n--------------------------------------------------\nperformance of our method. We use increasing stochastic depth [38] augmentation of 0.2, 0.3, 0.5 for\nsmall, base and larg",
  "output": "La tasa de aprendizaje se inicializa en 0.001 y se reduce a cero en 300 épocas siguiendo la estrategia coseno. Utilizamos un calentamiento lineal en las primeras cinco épocas y la misma configuración de regularización que en [2]. No utilizamos trucos adicionales en [26, 28] para hacer comparaciones justas aunque podría mejorar aún más el rendimiento de nuestro método. Utilizamos una creciente profundidad estocástica [38] de aumento de 0.2, 0.3, 0.5 para pequeño, base y grande.",
  "summary_type": "reports"
}{
  "input": "--------------------------------------\ntorchdistill: A Modular, Conﬁguration-Driven Framework for Knowledge Distillation\n15\nTable 6: Validation mAP of bottleneck-injected R-CNN models for split computing\n(student) trained on COCO 2017 dataset by GHND with original Faster/Mask R-CNN\nmodels (teacher). Reproduced results match those reported in the original work [25].\nBackbone: ResNet-50 and FPN\nmAP\n# Epochs Training time\nBBox\nMask\nFaster R-CNN w/ Bottleneck\n0.359\nN/A\n20\n24hr 13min\nMask R-CNN w/ Bo",
  "output": "Torchdistill ist ein modulares, konfigurationsgesteuertes Framework für Knowledge Distillation. Es wurde verwendet, um R-CNN-Modelle auf dem COCO 2017-Datensatz zu trainieren. Die validierten mAP-Ergebnisse der Modelle entsprechen denen, die in der Originalarbeit berichtet wurden. Das Training der Faster R-CNN- und Mask R-CNN-Modelle dauerte 20 Epochen und 24 Stunden 13 Minuten.",
  "summary_type": "reports"
}{
  "input": "ethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105–6114, 2019.\n1, 2, 6\n[48] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nCon-\ntrastive representation distillation. In ICLR, 2020. 7\n[49] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv´e\nJ´egou. Fixing the train-test resolution discrepancy. In Ad-\nvances in Neural Information Processing Systems, pages\n8252–8262, 2019. 1, 6\n[50] C. Wah, S. Branson, P. Welinder, P. Peron",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "involved, the candidate pool can be maintained by a\nmin-heap structure in real time.\nAs a result, we can conservatively implement local\nsearch by sampling from the candidate pool since it consists\nof a smaller number (but promising) of paths. However, this\ngreedy exploitation brings in the risks of losing path diver-\nsity for the training. In this way, we also favor a global\nsearch with the hope of probing other promising paths that\nare yet to be sampled and get trained, which can be easily\nful",
  "output": "El pool de candidatos puede mantenerse mediante una estructura de min-heap en tiempo real. Podemos implementar una búsqueda local muestreando desde el pool de candidatos, ya que consta de un número menor (pero prometedor) de rutas. Sin embargo, esta explotación codiciosa conlleva el riesgo de perder la diversidad de rutas para el entrenamiento. De esta manera, también favorecemos una búsqueda global con la esperanza de sondear otras rutas prometedoras que aún no se han muestreado y se entrenan fácilmente.",
  "summary_type": "reports"
}{
  "input": "ructure of\nthe network, consisting of a stem, multiple stages and a tail (see\nFig. 2a). The stem and tail are common to all networks and not\nsearched. Each stage in turn comprises of multiple layers, and\neach layer itself is an inverted residual bottleneck structure [56].\n-Network: We search for the input image resolution and the width\nmultiplier (a factor that scales the # of output channels of each\nlayer uniformly [57]). Following previous work [27], [28], [31],\nwe segment the CNN architecture",
  "output": "The network structure includes a stem, multiple stages, and a tail. Each stage consists of multiple layers, and each layer is an inverted residual bottleneck structure. The network searches for the input image resolution and the width multiplier, a factor that scales the number of output channels of each layer uniformly. The CNN architecture is segmented as per previous work.",
  "summary_type": "reports"
}{
  "input": "e on computer vision, pages 2961–2969, 2017.\n[24] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual\nrecognition. IEEE transactions on pattern analysis and machine intelligence, 37(9):1904–1916, 2015.\n[25] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[26] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J.",
  "output": "K. He, X. Zhang, S. Ren, and J. Sun have made significant contributions to computer vision and image recognition. In 2015, they developed a method for spatial pyramid pooling in deep convolutional networks, published in IEEE Transactions on Pattern Analysis and Machine Intelligence. The following year, they presented deep residual learning for image recognition at the IEEE Conference on Computer Vision and Pattern Recognition.",
  "summary_type": "reports"
}{
  "input": "s, RVT has surpassed most of CNN architectures with fewer\nparameters and FLOPs. RVT-Ti∗achieves 79.2% Top-1 accuracy on ImageNet-1K validation set,\nwhich is competitive with currently popular ResNet and RegNet series, but only has 1.3G FLOPs and\n10.9M parameters (around 60% lower than CNNs). With the same computation cost, RVT-S∗obtains\n81.9% test accuracy, 2.9% higher than ResNet-50. This result is closed to EfﬁcientNet-B4, however\nEfﬁcientNet-B4 requires larger 380×380 input size and has much",
  "output": "RVT ha superado a la mayoría de las arquitecturas de CNN con menos parámetros y FLOPs. RVT-Ti∗ logra un 79.2% de precisión Top-1 en el conjunto de validación ImageNet-1K, competitivo con las series ResNet y RegNet actuales, pero solo tiene 1.3G FLOPs y 10.9M parámetros (alrededor del 60% menos que las CNNs). Con el mismo costo de cálculo, RVT-S∗ obtiene un 81.9% de precisión en la prueba, 2.9% más que ResNet-50. Este resultado está cerca de EfﬁcientNet-B4, sin embargo, EfﬁcientNet-B4 requiere un tamaño de entrada más grande de 380×380.",
  "summary_type": "reports"
}{
  "input": "), 1.1% worse on Stylized-ImageNet (16.3% vs. 17.4%),\nand 7.0% worse on defending against FGSM (20.4% vs. 27.4%). Moreover, due to model ensem-\nble, this strategy is 2× expensive at the inference stage. These evidences clearly demonstrate the\neffectiveness and efﬁciency of the proposed shape-texture debiased training.\nDoes our method help models to learn debiased shape-texture representations? Here we take\na close look at whether our method indeed prevents models from being biased toward shape o",
  "output": "The proposed shape-texture debiased training method shows effectiveness and efficiency, despite being 1.1% worse on Stylized-ImageNet and 7.0% worse on defending against FGSM. However, it is 2x more expensive at the inference stage. The method's ability to help models learn debiased shape-texture representations is being examined.",
  "summary_type": "reports"
}{
  "input": "OCO\nval2017, 2.6 AP higher than the one with Swin-T [21],\n5.7 AP higher than the one with ResNet50 [12]. We hope\nthese improved baselines will provide a reference for future\nresearch in vision Transformer.\n1PVTv2 has 6 different size variants, from B0 to B5 according to the\nparameter number.\narXiv:2106.13797v4  [cs.CV]  17 Jul 2021\n\n--------------------------------------------------\n2. Related Work\nTransformer Backbones. ViT [7] treats each image as a\nsequence of tokens (patches) with ﬁxed lengt",
  "output": "OCO val2017 supera Swin-T [21] por 2.6 AP y ResNet50 [12] por 5.7 AP. Esperamos que estas mejoras sirvan de referencia para futuras investigaciones en vision Transformer. PVTv2 tiene 6 variantes de tamaño diferentes, de B0 a B5 según el número de parámetros. Publicado en arXiv:2106.13797v4 [cs.CV] el 17 de julio de 2021. En trabajos relacionados, ViT [7] trata cada imagen como una secuencia de tokens (parches) de longitud fija.",
  "summary_type": "reports"
}{
  "input": "gure 13, left), the naive implementation is extremely slow\non TPUs (Figure 13, right).\nD.7\nATTENTION DISTANCE\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed\nthe average distance spanned by attention weights at different layers (Figure 11). This “attention\ndistance” is analogous to receptive ﬁeld size in CNNs. Average attention distance is highly variable\nacross heads in lower layers, with some heads attending to much of the image, while others at",
  "output": "The naive implementation of ViT is slow on TPUs. The study analyzed how ViT uses self-attention to integrate information across the image by examining the average attention distance at different layers. This concept is similar to receptive field size in CNNs. The average attention distance varies greatly across heads in lower layers, with some heads attending to a large portion of the image, while others do not.",
  "summary_type": "reports"
}{
  "input": "hlights the hares and\na little bit of the person (near the face) but predicts the in-\ncorrect label of car wheel. Lastly, ConvMLP also pays at-\ntention to the hares but incorrectly classiﬁes the images as\nhare. This presents an interesting phenomena when analyz-\ning results. We have a clue that MLP-Mixer and ResMLP\nmight be overﬁtting the data. While ConvMLP makes the\nmisclassiﬁcation, we note that it is at least paying attention\nto the part of the image that closely corresponds to the label\ntha",
  "output": "The text discusses an analysis of image classification models, specifically MLP-Mixer, ResMLP, and ConvMLP. It suggests that MLP-Mixer and ResMLP may be overfitting the data, while ConvMLP, despite misclassifying images, is noted for focusing on the part of the image that closely corresponds to the label.",
  "summary_type": "reports"
}{
  "input": "sion transformers. arXiv preprint\narXiv:2104.14294, 2021.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In Hal Daumé III and Aarti Singh, editors, Proceedings of\nthe 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine\nLearning Research, pages 1691–1703. PMLR, 13–18 Jul 2020a. URL http://proceedings.\nmlr.press/v119/chen20s.html.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and",
  "output": "Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, ve Ilya Sutskever, 37. Uluslararası Makine Öğrenmesi Konferansı'nda piksellerden jeneratif önceden eğitim hakkında bir sunum yaptılar. Bu çalışma, Makine Öğrenmesi Araştırmaları'nın 119. cildinde, 1691-1703 sayfalarında yayınlandı. Bu konferans 13-18 Temmuz 2020'de PMLR tarafından düzenlendi.",
  "summary_type": "reports"
}{
  "input": "] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So\nKweon.\nBAM: bottleneck attention module.\nIn British\nMachine Vision Conference (BMVC) 2018, Newcastle, UK,\npages 1–14, 2018.\n[29] Shaoqing Ren, Kaiming He, Ross B. Girshick, Xiangyu\nZhang, and Jian Sun. Object detection networks on convolu-\ntional feature maps. IEEE Trans. Pattern Anal. Mach. Intell.,\n39(7):1476–1481, 2017.\n[30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentati",
  "output": "Jongchan Park, Sanghyun Woo, Joon-Young Lee, ve In So Kweon, 2018'de \"BAM: bottleneck attention module\" adlı bir çalışma yayınladılar. Shaoqing Ren, Kaiming He, Ross B. Girshick, Xiangyu Zhang ve Jian Sun, 2017'de evrişimli özellik haritaları üzerinde nesne algılama ağları üzerine bir çalışma yaptılar. Olaf Ronneberger, Philipp Fischer ve Thomas Brox, biyomedikal görüntü segmentasyonu için evrişimli ağlar üzerine U-net adlı bir çalışma yaptılar.",
  "summary_type": "reports"
}{
  "input": "rmed on the NVIDIA 2080Ti GPUs. We\nimplement RVT in three sizes named by RVT-Ti, RVT-S, RVT-B respectively. All of them adopt the\nbest settings investigated in section 2. For RVT∗, we add PAAS on multiple transformer blocks. The\npatch-wise augmentation uses the combination of base augmentation introduced in section 6.4. Other\ntraining hyperparameters are same with DeiT [4].\nEvaluation Benchmarks We adopt the ImageNet-1K [45] dataset for training and standard perfor-\nmance evaluation. No other la",
  "output": "RVT se implementa en tres tamaños: RVT-Ti, RVT-S, RVT-B, todos utilizando los mejores ajustes investigados en la sección 2. Para RVT∗, agregamos PAAS en varios bloques transformadores. La ampliación por parches utiliza la combinación de la ampliación base introducida en la sección 6.4. Los demás hiperparámetros de entrenamiento son los mismos que DeiT [4]. Adoptamos el conjunto de datos ImageNet-1K [45] para el entrenamiento y la evaluación estándar del rendimiento.",
  "summary_type": "reports"
}{
  "input": "omputer\nVision, 2016.\nKuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin,\nI., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M.,\nKolesnikov, A., Duerig, T., and Ferrari, V. The open\nimages dataset v4: Uniﬁed image classiﬁcation, object\ndetection, and visual relationship detection at scale. In-\nternational Journal of Computer Vision, 2020.\nLi, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning\nvisual n-grams from web data. In Proceedings of IEEE\nInternational Conference on Comput",
  "output": "Kuznetsova, Rom, Alldrin, Uijlings, Krasin, Pont-Tuset, Kamali, Popov, Malloci, Kolesnikov, Duerig, ve Ferrari, 2020'de \"Open Images Dataset v4\" adlı bir çalışma yayınladılar. Bu çalışma, birleşik görüntü sınıflandırması, nesne algılama ve ölçeklendirilmiş görsel ilişki algılama üzerine odaklanmaktadır. Ayrıca, Li, Jabri, Joulin ve van der Maaten, web verilerinden görsel n-gramları öğrenme üzerine bir çalışma yapmışlardır. Bu çalışma, IEEE Uluslararası Bilgisayar Konferansı'nda sunulmuştur.",
  "summary_type": "reports"
}{
  "input": "Conference on Ma-\nchine Learning (ICML), Lille, France, pages 448–456, 2015.\n[19] Alex Krizhevsky. Learning multiple layers of features from\ntiny images. Technical report, University of Toronto, 2009.\n[20] Hanchao Li, Pengfei Xiong, Jie An, and Lingxue Wang.\nPyramid attention network for semantic segmentation.\nIn\nBritish Machine Vision Conference (BMVC) 2018, Newcas-\ntle, UK, pages 1–13, 2018.\n[21] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Se-\nlective kernel networks. In 2019 IEEE Confer",
  "output": "Text:\nKonferenz über Maschinenlernen (ICML), Lille, Frankreich, Seiten 448–456, 2015. [19] Alex Krizhevsky. Erlernen mehrerer Schichten von Merkmalen aus winzigen Bildern. Technischer Bericht, Universität von Toronto, 2009. [20] Hanchao Li, Pengfei Xiong, Jie An und Lingxue Wang. Pyramiden-Aufmerksamkeitsnetzwerk für semantische Segmentierung. In der British Machine Vision Conference (BMVC) 2018, Newcastle, UK, Seiten 1–13, 2018. [21] Xiang Li, Wenhai Wang, Xiaolin Hu und Jian Yang. Selektive Kernel-Netzwerke. In der 2019 IEEE-Konferenz.",
  "summary_type": "reports"
}{
  "input": "adk =\n\u001aH-Attentionk(X)\nk = 1, . . . , K/2\nV-Attentionk(X)\nk = K/2 + 1, . . . , K\nWhere W O ∈RC×C is the commonly used projection matrix that projects the self-attention results\ninto the target output dimension (set as C by default). As described above, one key insight in\nour self-attention mechanism design is splitting the multi-heads into different groups and applying\ndifferent self-attention operations accordingly. In other words, the attention area of each token within\none Transformer block i",
  "output": "The text describes a self-attention mechanism design in which multi-heads are split into different groups, each applying different self-attention operations. This is done using a projection matrix W O ∈RC×C, which projects self-attention results into the target output dimension. The attention area of each token within one Transformer block is determined by this process.",
  "summary_type": "reports"
}{
  "input": "ization, vol. 8, no. 3, p. 631–657, Mar.\n1998. [Online]. Available: https://doi.org/10.1137/S1052623496307510\n[62] B. Wu, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda,\nY. Jia, and K. Keutzer, “Fbnet: Hardware-aware efﬁcient convnet design\nvia differentiable neural architecture search,” in CVPR, 2019.\n[63] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment: Prac-\ntical automated data augmentation with a reduced search space,” arXiv\npreprint arXiv:1909.13719, 2019.\n[64] J.",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "NETX-200MF\n0.2\n2.7\n2.2\n1024\n10\n2.8\n31.1±0.09\nREGNETX-400MF\n0.4\n5.2\n3.1\n1024\n15\n3.9\n27.3±0.15\nREGNETX-600MF\n0.6\n6.2\n4.0\n1024\n17\n4.4\n25.9±0.03\nREGNETX-800MF\n0.8\n7.3\n5.1\n1024\n21\n5.7\n24.8±0.09\nREGNETX-1.6GF\n1.6\n9.2\n7.9\n1024\n33\n8.7\n23.0±0.13\nREGNETX-3.2GF\n3.2\n15.3\n11.4\n512\n57\n14.3\n21.7±0.08\nREGNETX-4.0GF\n4.0\n22.1\n12.2\n512\n69\n17.1\n21.4±0.19\nREGNETX-6.4GF\n6.5\n26.2\n16.4\n512\n92\n23.5\n20.8±0.07\nREGNETX-8.0GF\n8.0\n39.6\n14.1\n512\n94\n22.6\n20.7±0.07\nREGNETX-12GF\n12.1\n46.1\n21.4\n512\n137\n32.9\n20.3±0.04\nREGNETX-16GF",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "0.90\n0.95\n1.00\nTrue Positive Rate\nProdpoly-ResNet50\nResNet50\n(a) ROC for IJB-B\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nFalse Positive Rate\n0.85\n0.88\n0.91\n0.94\n0.97\n1.00\nTrue Positive Rate\nProdpoly-ResNet50\nResNet50\n(b) ROC for IJB-C\nFig. 11: ROC curves of ResNet50 and Prodpoly-ResNet50 under 1:1\nveriﬁcation protocol on the IJB-B and IJB-C dataset.\nResults on MegaFace. The MegaFace dataset [85] includes 1M\nimages of 690K different individuals as the gallery set and 100K\nphotos of 530 unique individuals fr",
  "output": "The text presents ROC curves of ResNet50 and Prodpoly-ResNet50 under 1:1 verification protocol on the IJB-B and IJB-C dataset. It also mentions the use of the MegaFace dataset, which includes 1M images of 690K different individuals as the gallery set and 100K photos of 530 unique individuals.",
  "summary_type": "reports"
}{
  "input": ": Deep residual learning for image recognition. In: Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770–778\n(2016)\n12. Heo, B., Lee, M., Yun, S., Choi, J.Y.: Knowledge transfer via distillation of activation bound-\naries formed by hidden neurons. In: Proceedings of the AAAI Conference on Artiﬁcial In-\ntelligence. vol. 33, pp. 3779–3787 (2019)\n13. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In: Deep\nLearning and Representat",
  "output": "He, K. et al. (2016) explored deep residual learning for image recognition at the IEEE Conference on Computer Vision and Pattern Recognition. Heo, B. et al. (2019) discussed knowledge transfer via distillation of activation boundaries formed by hidden neurons at the AAAI Conference on Artificial Intelligence. Hinton, G. et al. presented on distilling knowledge in a neural network in the context of deep learning.",
  "summary_type": "reports"
}{
  "input": "1), ∆2 =\n(−1, −1), ∆3 = (1, 1), ∆4 = (1, −1).\n• The locality strength αh > 0 determines how focused\nthe attention is around its center ∆h (it can also by un-\nderstood as the “temperature” of the softmax in Eq. 1).\nWhen αh is large, the attention is focused only on the\npatch(es) located at ∆h, as in Fig. 3(d); when αh is\nsmall, the attention is spread out into a larger area, as\nin Fig. 3(c).\nThus, the PSA layer can achieve a strictly convolutional\nattention map by setting the centers of attentio",
  "output": "La fuerza de localidad αh > 0 determina cuán enfocada está la atención alrededor de su centro ∆h (también puede entenderse como la \"temperatura\" del softmax en la Ecuación 1). Cuando αh es grande, la atención se centra solo en los parches ubicados en ∆h; cuando αh es pequeño, la atención se dispersa en un área más grande. Por lo tanto, la capa PSA puede lograr un mapa de atención estrictamente convolucional estableciendo los centros de atención.",
  "summary_type": "reports"
}{
  "input": "rn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig\nAdam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2018 dataset.\narXiv preprint arXiv:1707.06642, 2018. 15\n[31] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig\nAdam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2019 dataset.\narXiv preprint arXiv:1707.06642, 2019. 15\n[32] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. arXiv preprint\narXiv:1709.01507, 2017. 21\n[3",
  "output": "Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig Adam, Pietro Perona, and Serge J. Belongie published the iNaturalist Challenge datasets for 2018 and 2019, available on arXiv preprint arXiv:1707.06642. Jie Hu, Li Shen, and Gang Sun also published a work on Squeeze-and-excitation networks on arXiv preprint arXiv:1709.01507 in 2017.",
  "summary_type": "reports"
}{
  "input": "is\nrelated to a speciﬁc family of data augmentation, called Mixup (Zhang et al., 2018), which blends\npairs of images and their labels in a convex manner, either at pixel-level (Zhang et al., 2018; Yun\net al., 2019) or feature-level (Verma et al., 2019; Li et al., 2021). Our method can be interpreted\nas a special instantiation of Mixup which blends pairs of images at the abstraction level—images’\ntexture information and shape information are mixed. Our method successfully guides CNNs to\nlearn be",
  "output": "The text discusses a specific data augmentation family called Mixup, which blends pairs of images and their labels either at pixel-level or feature-level. The method described is a special instantiation of Mixup that blends pairs of images at the abstraction level, mixing texture and shape information. This method effectively guides Convolutional Neural Networks (CNNs) to learn.",
  "summary_type": "reports"
}{
  "input": "ub.com/facebookresearch/detectron2, 2019. 6\n[38] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR:\ndeformable transformers for end-to-end object detection. CoRR, abs/2010.04159, 2020. 2, 3, 5\n13\n\n--------------------------------------------------",
  "output": "Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, ve Jifeng Dai, deformable DETR adlı bir çalışma gerçekleştirdi: uçtan uca nesne algılama için deformable transformerlar. Bu çalışma, CoRR'da, abs/2010.04159 numarasıyla 2020'de yayınlandı.",
  "summary_type": "reports"
}{
  "input": "es the similarity\nto a mini-batch of labeled support samples, and outputs a soft class distri-\nbution. Positive views are created using data-augmentations of the anchor\nview. Since the trivial collapse of all representations to a single vector would\nlead to high-entropy predictions by the similarity classiﬁer, sharpening the\ntarget pseudo-labels is sufﬁcient to eliminate all trivial solutions.\n3\nMethodology\nWe consider a large dataset of unlabeled images D\n=\n(xi)i∈[1,N] and a small support datas",
  "output": "El método considera un gran conjunto de datos de imágenes sin etiquetar D = (xi)i∈[1,N] y un pequeño conjunto de datos de soporte. Se crean vistas positivas mediante aumentos de datos de la vista ancla. Dado que el colapso trivial de todas las representaciones en un solo vector conduciría a predicciones de alta entropía por el clasificador de similitud, afilar las etiquetas pseudo-objetivo es suficiente para eliminar todas las soluciones triviales.",
  "summary_type": "reports"
}{
  "input": "of token\nembedding, type of projection, and Transformer structure in\nthe backbone, between the above representative concurrent\nworks and ours.\nIntroducing Self-attentions to CNNs.\nSelf-attention\nmechanisms have been widely applied to CNNs in vision\ntasks. Among these works, the non-local networks [35] are\ndesigned for capturing long range dependencies via global\nattention. The local relation networks [17] adapts its weight\naggregation based on the compositional relations (similar-\nity) between",
  "output": "The text discusses the application of self-attention mechanisms to Convolutional Neural Networks (CNNs) in vision tasks. Non-local networks are used to capture long-range dependencies through global attention. Local relation networks adapt their weight aggregation based on compositional relations.",
  "summary_type": "reports"
}{
  "input": "ptor is predeﬁned (subset of the input elements), which\ndoes not allow the method to scale to high-dimensional data with\ncomplex correlations.\nShin et al. [36] introduce the pi-sigma network, which is\na neural network with a single hidden layer. Multiple afﬁne\ntransformations of the data are learned; a product unit multiplies\nall the features to obtain the output. Improvements in the pi-sigma\nnetwork include regularization for training in [37] or using multiple\nproduct units to obtain the output",
  "output": "O ptor é pré-definido (subconjunto dos elementos de entrada), o que não permite que o método escale para dados de alta dimensão com correlações complexas. Shin et al. [36] introduzem a rede pi-sigma, que é uma rede neural com uma única camada oculta. Múltiplas transformações afins dos dados são aprendidas; uma unidade de produto multiplica todos os recursos para obter a saída. Melhorias na rede pi-sigma incluem regularização para treinamento em [37] ou usando várias unidades de produto para obter a saída.",
  "summary_type": "reports"
}{
  "input": "ter on TPUs (2.1x - 3.3x faster\non GPUs) than the popular EfﬁcientNets on the speed-\naccuracy Pareto curve. In a large-scale semi-supervised\nlearning setup, ResNet-RS obtains a 4.7x training speed-\nup on TPUs (5.5x on GPUs) over EfﬁcientNet-B5 when\nco-trained on ImageNet and an additional 130M pseudo-\nlabeled images.\nFinally, we conclude with a suite of experiments test-\ning the generality of the improved training and scaling\nstrategies.\nWe ﬁrst design a faster version of Efﬁcient-\nNet using our",
  "output": "ResNet-RS is faster than EfficientNets on TPUs (2.1x - 3.3x) and GPUs (4.7x - 5.5x) on the speed-accuracy Pareto curve. In a large-scale semi-supervised learning setup, it achieves a significant training speed-up when co-trained on ImageNet and an additional 130M pseudo-labeled images. A suite of experiments is concluded to test the generality of the improved training and scaling strategies. A faster version of EfficientNet is designed using these strategies.",
  "summary_type": "reports"
}{
  "input": "e ratio of similar attention maps in M after\nthe 17th block is larger than 90% . This indicates that the\nlearned attention maps afterwards are similar and the trans-\nformer block may degenerate to an MLP. As a result, fur-\nther stacking such degenerated MHSA may introduce the\nmodel rank degeneration issue (i.e., the rank of the model\nparameter tensor from multiplying the layer-wise parame-\nters together will decrease) and limits the model learning\ncapacity. This is also validated by our analysis",
  "output": "After the 17th block in M, over 90% of attention maps are similar, suggesting the transformer block may become an MLP. This could lead to model rank degeneration (a decrease in the rank of the model parameter tensor from layer-wise parameter multiplication), limiting the model's learning capacity. This is confirmed by our analysis.",
  "summary_type": "reports"
}{
  "input": "st architectures against adversarial attacks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 631–640, 2020.\n[21] Minjing Dong, Yanxi Li, Yunhe Wang, and Chang Xu. Adversarially robust neural architectures. arXiv\npreprint arXiv:2009.00902, 2020.\n[22] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards\ndeep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n[23] Yingwei",
  "output": "The text refers to various studies on adversarial attacks on neural architectures. One study, presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition in 2020, discusses strategies against these attacks. Another paper by Minjing Dong et al. explores robust neural architectures against adversarial attacks. A third preprint by Aleksander Madry et al. aims to develop deep learning models resistant to such attacks.",
  "summary_type": "reports"
}{
  "input": "the training of very deep\nnetworks. In Feature Pyramid Networks (FPN) [23] and\nU-Net [30], low-level features and high-level features are\nfused via long skip connections to obtain high-resolution\nand semantically strong features, which are vital for seman-\ntic segmentation and object detection. However, despite its\nprevalence in modern networks, most works on feature fu-\nsion focus on constructing sophisticated pathways to com-\nbine features in different kernels, groups, or layers. The\nfeature f",
  "output": "Feature Pyramid Networks (FPN) ve U-Net, düşük seviye özellikler ve yüksek seviye özelliklerin uzun atlamalı bağlantılar aracılığıyla birleştirilerek yüksek çözünürlüklü ve semantik olarak güçlü özellikler elde eder. Bu özellikler, semantik segmentasyon ve nesne algılama için hayati öneme sahiptir. Ancak, modern ağlarda yaygın olmasına rağmen, özellik birleştirme üzerine yapılan çalışmalar genellikle farklı çekirdekler, gruplar veya katmanlarda özellikleri birleştirmek için karmaşık yollar oluşturmaya odaklanır.",
  "summary_type": "reports"
}{
  "input": "dford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding with unsupervised learning. Technical Report, 2018.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. Technical Report, 2019.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. In NeurIPS, 2019.\nChen Sun, Abhinav Shrivastava, Sau",
  "output": "Alec Radford, Karthik Narasimhan, Tim Salimans, ve Ilya Sutskever, dil anlama yeteneğini denetimsiz öğrenme ile geliştirmeyi tartışmışlardır (Teknik Rapor, 2018). Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei ve Ilya Sutskever, dil modellerinin denetimsiz çoklu görev öğrenenler olduğunu belirtmişlerdir (Teknik Rapor, 2019). Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya ve Jon Shlens, görüş modellerinde bağımsız dikkati incelemişlerdir (NeurIPS, 2019).",
  "summary_type": "reports"
}{
  "input": "through the\nnetwork. This built-in property allows dropping the position\nembedding from the network without hurting performance,\nas evidenced by our experiments (Section 4.4), simplifying\ndesign for vision tasks with variable input resolution.\nRelations to Concurrent Work:\nRecently, two more re-\nlated concurrent works also propose to improve ViT by in-\ncorporating elements of CNNs to Transformers. Tokens-\nto-Token ViT [41] implements a progressive tokenization,\nand then uses a Transformer-based",
  "output": "The text discusses a network property that enables the removal of position embedding without affecting performance, simplifying design for variable input resolution vision tasks. It also mentions concurrent works that aim to enhance ViT by integrating CNN elements into Transformers. One such work, Tokens-to-Token ViT, employs progressive tokenization followed by a Transformer-based approach.",
  "summary_type": "reports"
}{
  "input": "g experimental results and few\nempirical theoretical results already. We expect in the following\nyears new works that improve our results and extend our formula-\ntion. To that end, we summarize below several fundamental topics\nthat are open for interested practitioners.\nThe generalization of the Π´nets is a crucial topic. In our\nevaluation without activation functions, we noticed that polynomi-\nals might be prone to overﬁtting (e.g., in the classiﬁcation setting\nwithout activation functions in t",
  "output": "Se han obtenido resultados experimentales y teóricos empíricos. En los próximos años, esperamos nuevos trabajos que mejoren nuestros resultados y amplíen nuestra formulación. Resumimos varios temas fundamentales abiertos para los profesionales interesados. La generalización de las Π´nets es un tema crucial. En nuestra evaluación sin funciones de activación, notamos que los polinomios podrían ser propensos al sobreajuste (por ejemplo, en la configuración de clasificación sin funciones de activación).",
  "summary_type": "reports"
}{
  "input": "ck, the transformer block, and the Feature\nCoupling Unit (FCU). (c) Thumbnail of Conformer.\nDespite of the progress, existing solutions that introduce\nglobal cues to CNNs have obvious disadvantages. For the\nﬁrst solution, larger receptive ﬁelds require more intensive\npooling operations, which implies lower spatial resolution.\nFor the second solution, if convolutional operations are not\nproperly fused with attention mechanisms, local feature\ndetails could deteriorate.\nVisual Transformers. As a pi",
  "output": "Existing solutions introducing global cues to CNNs have drawbacks. Larger receptive fields need more intensive pooling operations, implying lower spatial resolution. If convolutional operations aren't properly fused with attention mechanisms, local feature details could deteriorate.",
  "summary_type": "reports"
}{
  "input": "peciﬁcally, our MDEQ\nmodels employ convolutions with small receptive ﬁelds (e.g., the two 3 × 3 convolutional ﬁlters\nin fθ’s residual block) on potentially very large images: for instance, we eventually evaluate our\nsemantic segmentation model on megapixel-scale images. In consequence, we typically need a\nhigher number of root-ﬁnding iterations to converge to an exact equilibrium. While this does pose a\nchallenge, we ﬁnd that using the aforementioned strategies of 1) multiscale simultaneous up-",
  "output": "Los modelos MDEQ utilizan convoluciones con campos receptivos pequeños (por ejemplo, los dos filtros convolucionales 3 × 3 en el bloque residual de fθ) en imágenes potencialmente muy grandes: por ejemplo, evaluamos nuestro modelo de segmentación semántica en imágenes a escala de megapíxeles. Como resultado, generalmente necesitamos un mayor número de iteraciones de búsqueda de raíces para converger a un equilibrio exacto. A pesar de este desafío, encontramos que el uso de las estrategias mencionadas de 1) actualización simultánea a múltiples escalas es útil.",
  "summary_type": "reports"
}{
  "input": "e ﬁnd that position-based inter-\nactions are crucial to reach high accuracies, while content-based interactions only bring marginal\nimprovements over position-based interactions15.\nContent\nPosition\nParams (M)\nFLOPS (B)\ntop-1\n✓\n×\n14.9\n5.0\n68.8\n×\n✓\n14.9\n11.9\n78.1\n✓\n✓\n14.9\n12.0\n78.4\nTable 10: Contributions of content and positional interactions. As expected, positional interac-\ntions are crucial to perform well on the image classiﬁcation task.\n15This observation is challenged by concurrent work (Do",
  "output": "The table shows the contributions of content and positional interactions in image classification tasks. Position-based interactions are crucial for high accuracy, while content-based interactions only provide marginal improvements. The top-1 accuracy is 68.8% with only position interactions, 78.1% with only content interactions, and 78.4% with both. The observation is challenged by concurrent work.",
  "summary_type": "reports"
}{
  "input": "he compositional features directly from the network. Ideal modeling of the\nvisual representation should be able to model the part-whole hierarchy as humans do so that we can\nleverage representations of all levels directly from one backbone model.\nBuilding up a framework that includes different levels of representations in the part-whole hierarchy\nis difﬁcult for conventional neural networks as it requires neurons to dynamically respond to the\ninput, while neural networks with ﬁxed weights cannot",
  "output": "La modelización ideal de la representación visual debería poder modelar la jerarquía de parte-todo como lo hacen los humanos para que podamos aprovechar las representaciones de todos los niveles directamente desde un modelo de columna vertebral. Construir un marco que incluya diferentes niveles de representaciones en la jerarquía de parte-todo es difícil para las redes neuronales convencionales, ya que requiere que las neuronas respondan dinámicamente a la entrada, mientras que las redes neuronales con pesos fijos no pueden.",
  "summary_type": "reports"
}{
  "input": "low-level fea-\nture map, and Y is the high-level semantic feature map\nin a feature pyramid.\nBased on the multi-scale channel attention module M, At-\ntentional Feature Fusion (AFF) can be expressed as\nZ = M(X ⊎Y) ⊗X + (1 −M(X ⊎Y)) ⊗Y,\n(4)\nwhere Z ∈RC×H×W is the fused feature, and ⊎denotes\nthe initial feature integration. In this subsection, for the\nsake of simplicity, we choose the element-wise summation\nas initial integration. The AFF is illustrated in Fig. 2(a),\nwhere the dashed line denotes 1",
  "output": "Text:\nIn einem Merkmalspyramidenmodell ist X die niedrigstufige Merkmalskarte und Y die hochstufige semantische Merkmalskarte. Basierend auf dem Multi-Scale-Kanal-Aufmerksamkeitsmodul M kann die Aufmerksamkeitsmerkmalsfusion (AFF) als Z = M(X ⊎Y) ⊗X + (1 −M(X ⊎Y)) ⊗Y ausgedrückt werden, wobei Z das fusionierte Merkmal ist und ⊎ die initiale Merkmalsintegration bezeichnet. In diesem Unterabschnitt wählen wir aus Gründen der Einfachheit die elementweise Summation als initiale Integration. Die AFF ist in Abb. 2(a) dargestellt, wobei die gestrichelte Linie 1 darstellt.",
  "summary_type": "reports"
}{
  "input": "and Quoc V. Le.\nEfﬁcientnet: Rethinking\nmodel scaling for convolutional neural networks. In ICML,\npages 6105–6114. PMLR, 2019. 11\n[41] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 3, 4,\n6, 7, 8, 11\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nl",
  "output": "Quoc V. Le đã thảo luận về việc tái cấu trúc quy mô mô hình cho mạng nơ-ron tích chập trong ICML, trang 6105-6114, PMLR, 2019. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé Jégou đã huấn luyện các biến đổi hình ảnh hiệu quả về dữ liệu và chưng cất thông qua sự chú ý, arXiv preprint arXiv:2012.12877, 2020.",
  "summary_type": "reports"
}{
  "input": "es encountered in a realistic\nsetting, such as the ﬁne-grained similarity between classes,\nsigniﬁcant class imbalance, and domain mismatch between\nthe labeled and unlabeled data. This challenge is part of the\nFGVC7 workshop in CVPR 2020.\nThe data set is divided into the following three parts, the\nﬁrst is labeled data, a total of 5959 pictures, the second is\nunlabeled data whose categories belong to the labeled data,\na total of 26640 pictures, and the third is unlabeled data but\nits category is o",
  "output": "El texto describe un desafío que implica similitud entre clases, desequilibrio de clases y desajuste de dominio entre los datos etiquetados y no etiquetados. Este desafío es parte del taller FGVC7 en CVPR 2020. El conjunto de datos se divide en tres partes: la primera son datos etiquetados con un total de 5959 imágenes, la segunda son datos no etiquetados cuyas categorías pertenecen a los datos etiquetados, con un total de 26640 imágenes, y la tercera son datos no etiquetados pero su categoría es o.",
  "summary_type": "reports"
}{
  "input": "cks. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 1911–1920, 2019. 2, 4\n[10] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,\nGuiguang Ding, and Jian Sun. Repvgg: Making vgg-style\nconvnets great again.\narXiv preprint arXiv:2101.03697,\n2021. 2, 4\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 wor",
  "output": "Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, dan Jian Sun telah menerbitkan sebuah prapercetakan di arXiv berjudul \"RepVGG: Making VGG-style convnets great again\" pada tahun 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, dan lainnya juga telah berkontribusi dalam penelitian terkait pengolahan citra.",
  "summary_type": "reports"
}{
  "input": "ision Transformer [47] uses this SRA\nto build multi-scale vision transformer backbones, with dif-\nferent spatial reduction ratios (R1 = 8, R2 = 4, R3 =\n2, R4 = 1) for each stage. With this PVT’s setting, the\nkey and value feature maps at all stages are essentially with\nresolution H\n32 × W\n32 . This choice is able to scale up to image\nresolution 600×1000, but the memory usage is much larger\nthan ResNet counterparts for 800 × 1333.\nIn this paper, we benchmarked the performance of\nSRA/32 with SR ra",
  "output": "Vision Transformer utiliza SRA para construir backbones de transformador de visión a múltiples escalas, con diferentes ratios de reducción espacial (R1 = 8, R2 = 4, R3 = 2, R4 = 1) para cada etapa. Con esta configuración de PVT, los mapas de características clave y valor en todas las etapas tienen esencialmente una resolución H/32 x W/32. Esta elección puede escalar hasta una resolución de imagen de 600x1000, pero el uso de memoria es mucho mayor que los equivalentes de ResNet para 800 x 1333. En este documento, evaluamos el rendimiento de SRA/32 con SR ra.",
  "summary_type": "reports"
}{
  "input": "isual data, resulting in exces-\nsively sharp loss landscapes and poor generalization, as shown in the previous section. We hypothe-\nsize that smoothing the loss landscapes at convergence can signiﬁcantly improve the generalization\nability of those convolution-free architectures, leading us to the recently proposed sharpness-aware\nminimizer (SAM) (Foret et al., 2021) that explicitly avoids sharp minima.\n4.1\nSAM: OVERVIEW\nIntuitively, SAM (Foret et al., 2021) seeks to ﬁnd the parameter w whose ent",
  "output": "Visual data can lead to excessively sharp loss landscapes and poor generalization. The hypothesis is that smoothing these loss landscapes can improve the generalization ability of convolution-free architectures. This leads to the sharpness-aware minimizer (SAM) (Foret et al., 2021), which aims to avoid sharp minima.",
  "summary_type": "reports"
}{
  "input": "sts the\nresults. These networks serve as teachers when we use our distillation strategy.\n12\n\n--------------------------------------------------\n⚗↑\n⚗\nFigure 3: Distillation on ImageNet [42] with DeiT-B: performance as a func-\ntion of the number of training epochs. We provide the performance without\ndistillation (horizontal dotted line) as it saturates after 400 epochs.\n5.4\nTransfer learning: Performance on downstream tasks\nAlthough DeiT perform very well on ImageNet it is important to evaluate th",
  "output": "Distillation on ImageNet with DeiT-B is evaluated by the number of training epochs. Performance without distillation saturates after 400 epochs. DeiT's performance on ImageNet is good, but it's also important to assess its performance on downstream tasks.",
  "summary_type": "reports"
}{
  "input": "is needed. Placing the SE layers after the reduction\nphase of the Bottleneck layer further reduces the computa-\ntional cost. The complete blocks design, with SE layers and\nInplace-ABN, is presented in Figure 3.\n2.2. Code Optimizations\nIn this section we will describe code optimizations we\ndid to enhance the GPU throughput and reduce the mem-\nory footprint of TResNet models. While code optimizations\nare sometimes overlooked and seen as ’implementation de-\ntails’, we claim that they are crucial fo",
  "output": "SE layers are placed after the reduction phase of the Bottleneck layer to reduce computational cost. The complete blocks design includes SE layers and Inplace-ABN. Code optimizations were made to enhance the GPU throughput and reduce the memory footprint of TResNet models. These optimizations are crucial.",
  "summary_type": "reports"
}{
  "input": "to match this result with a standard\noptimizer. In Section 3, contradicting the claims in You et al. [2019], we show that Adam obtains\nbetter BERT pre-training results than LAMB at the largest batch sizes, resulting in better downstream\nperformance metrics after ﬁne-tuning.\nIn addition, we establish a new state-of-the-art for BERT pretraining speed, reaching an F1 score of\n90.46 in 7,818 steps using Adam at batch size 65,536 (we report training speed in steps because our\nfocus is algorithmic efﬁ",
  "output": "Contrariamente a lo afirmado por You et al. [2019], se demuestra que Adam obtiene mejores resultados en la pre-entrenamiento de BERT que LAMB en los tamaños de lote más grandes, lo que resulta en mejores métricas de rendimiento después del ajuste fino. Además, se establece un nuevo récord en velocidad de pre-entrenamiento de BERT, alcanzando una puntuación F1 de 90.46 en 7,818 pasos usando Adam con un tamaño de lote de 65,536.",
  "summary_type": "reports"
}{
  "input": ")\nx = x.mean(dim=1).reshape(B,-1) #average pooling\nreturn self.linear_classifier(x)\nD\nAdditional Ablations\nTraining recipe.\nDeiT [56] proposes a training strategy which allows for data-efﬁcient vision\ntransformers on ImageNet only. In Table D.1 we ablate each component of the DeiT training to go\nback to the initial ResNet50 training. As to be expected, the training used in the ResNet-50 paper [23]\ndegrades the performance.\nTraining schedule.\nTable D.2 compares the performance of ResMLP-S36 accor",
  "output": "DeiT [56] предлагает стратегию обучения, которая позволяет эффективно использовать данные для визуальных трансформеров только на ImageNet. В таблице D.1 мы аблагируем каждый компонент обучения DeiT, чтобы вернуться к начальному обучению ResNet50. Как и ожидалось, обучение, используемое в статье ResNet-50 [23], снижает производительность. В таблице D.2 сравнивается производительность ResMLP-S36.",
  "summary_type": "reports"
}{
  "input": "number of classes. In In-\ndian Conference on Computer Vision, Graphics and Image\nProcessing, Dec 2008. 6\n[23] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nC. V. Jawahar. Cats and dogs. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2012. 6\n[24] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In International Conference on Machine\nLearning, pages 4055–4064. PMLR, 2018. 2\n[25] Alec Radford,",
  "output": "Text:\nAnzahl der Klassen. In der indischen Konferenz für Computer Vision, Grafik und Bildverarbeitung, Dezember 2008. 6\n[23] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman und C. V. Jawahar. Katzen und Hunde. In der IEEE-Konferenz für Computer Vision und Mustererkennung, 2012. 6\n[24] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku und Dustin Tran. Bildtransformator. In der Internationalen Konferenz für maschinelles Lernen, Seiten 4055–4064. PMLR, 2018. 2\n[25] Alec Radford,",
  "summary_type": "reports"
}{
  "input": "ientNet-B5 [69]\n30\n98.1\n91.1\n-\n-\n98.5\n-\nViT-B/16 [19]\n86.5\n98.1\n87.1\n-\n-\n89.5\n93.8\nViT-L/16 [19]\n304.3\n97.9\n86.4\n-\n-\n89.7\n93.6\nDeiT-B [72]\n86.6\n99.1\n90.8\n77.7\n92.1\n98.4\n-\nT2T-ViT-14 [88]\n21.5\n98.3\n88.4\n-\n-\n-\n-\nViTAE-T\n4.8\n97.3\n86.0\n73.3\n89.5\n97.5\n92.6\nViTAE-S\n23.6\n98.8\n90.8\n76.0\n91.4\n97.8\n94.2\nWe further investigate the generalization of the proposed ViTAE models on downstream tasks by ﬁne-\ntuning them on the training sets of several ﬁne-grained classiﬁcation tasks, including Flowers [49],\nCars",
  "output": "The text presents a comparison of various models including ientNet-B5, ViT-B/16, ViT-L/16, DeiT-B, T2T-ViT-14, ViTAE-T, and ViTAE-S, based on different parameters. The ViTAE models' generalization is further investigated on downstream tasks by fine-tuning them on the training sets of several fine-grained classification tasks, including Flowers and Cars.",
  "summary_type": "reports"
}{
  "input": "10.41% in classiﬁcation which suggests that we\nare indeed able to learn texture agnostic feature representations.\nOther Datasets - Aircraft, Birds, Dogs, and Cars. We further evaluate our method on image clas-\nsiﬁcation task on four different ﬁne-grained classiﬁcation datasets. We also observe improvement\non image classiﬁcation across ﬁve datasets in Table 2. These results suggest that in case of domain\nshift, higher level semantics are more important and capturing them helps in better transfer",
  "output": "Our method has shown a 10.41% improvement in classification, indicating our ability to learn texture agnostic feature representations. We've also seen improvement in image classification across four fine-grained datasets: Aircraft, Birds, Dogs, and Cars. The results suggest that higher level semantics become more important in case of domain shift, aiding in better transfer.",
  "summary_type": "reports"
}{
  "input": "ingle scale and multi-scale testing (we use\nscales from 0.5 to 1.75 with step 0.25) in the right columns of Table 2. Both with multi-scale\ntesting, Twins-SVT-S outperforms Swin-T by 1.3% mIoU. Moreover, Twins-SVT-L achieves new\nstate of the art result 50.2% mIoU under comparable FLOPs and outperforms Swin-B by 0.5% mIoU.\nTwins-PCPVT also achieves comparable performance to Swin [4].\n4.3\nObject Detection and Segmentation on COCO\nWe evaluate the performance of our method using two representative fr",
  "output": "En las pruebas de escala única y múltiple (usamos escalas de 0.5 a 1.75 con paso 0.25), Twins-SVT-S supera a Swin-T por 1.3% mIoU. Además, Twins-SVT-L logra un nuevo estado del arte con un resultado del 50.2% mIoU bajo FLOPs comparables y supera a Swin-B por 0.5% mIoU. Twins-PCPVT también logra un rendimiento comparable al de Swin [4]. Evaluamos el rendimiento de nuestro método utilizando dos representantes fr.",
  "summary_type": "reports"
}{
  "input": "images of 690K different individuals as the gallery set and 100K\nphotos of 530 unique individuals from FaceScrub [87] as the\nprobe set. On MegaFace, there are two testing protocols (e.g.,\nidentiﬁcation and veriﬁcation). Table 12 show the identiﬁcation\nand veriﬁcation results on MegaFace dataset. In particular, the\nproposed Prodpoly-ResNet50 achieve 0.50% improvement at the\nRank-1@1e6 identiﬁcation rate and 0.31% improvement at the\nveriﬁcation TPR@FAR=1e-6 rate over the baseline ResNet50. In\nFigu",
  "output": "The MegaFace dataset includes images of 690K unique individuals and 100K photos of 530 unique individuals from FaceScrub. It has two testing protocols: identification and verification. The proposed Prodpoly-ResNet50 shows a 0.50% improvement at the Rank-1@1e6 identification rate and a 0.31% improvement at the verification TPR@FAR=1e-6 rate over the baseline ResNet50.",
  "summary_type": "reports"
}{
  "input": "------------------------------------------------\nXue, F.; Sun, A.; Zhang, H.; and Chng, E. S. 2020a. An\nEmbarrassingly Simple Model for Dialogue Relation Ex-\ntraction. arXiv preprint arXiv:2012.13873.\nXue, F.; Sun, A.; Zhang, H.; and Chng, E. S. 2020b. GDP-\nNet: Reﬁning Latent Multi-View Graph for Relation Extrac-\ntion. arXiv preprint arXiv:2012.06780.\nYang, A.; Lin, J.; Men, R.; Zhou, C.; Jiang, L.; Jia, X.;\nWang, A.; Zhang, J.; Wang, J.; Li, Y.; et al. 2021.\nEx-\nploring Sparse Expert Models an",
  "output": "Xue, F., Sun, A., Zhang, H., & Chng, E. S. (2020) have published two preprints on arXiv. The first discusses an embarrassingly simple model for dialogue relation extraction (arXiv:2012.13873). The second introduces GDP-Net, a method for refining latent multi-view graphs for relation extraction (arXiv:2012.06780). In 2021, Yang, A., Lin, J., Men, R., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Wang, J., Li, Y., et al. began exploring sparse expert models.",
  "summary_type": "reports"
}{
  "input": "2)\nViT-24B [7]\n11\n36.3\n79.4\nDeepViT-24B\n0\n36.3\n80.1 (+0.7)\nViT-32B [7]\n16\n48.1\n79.3\nDeepViT-32B\n0\n48.1\n80.9 (+1.6)\nComparison to adding temperature in self-attention:\nThe most intuitive way to mitigate the over-smoothing phe-\nnomenon is to sharpen the distribution of the elements in\nthe attention map of MHSA. We could achieve this by as-\nsigning a temperature τ to the Softmax layer of MHSA:\nAttention(Q, K, V ) = Softmax\n\u0012QK⊤\nτ\n√\nd\n\u0013\nV.\n(4)\nAs the attention collapse is observed to be severe on de",
  "output": "ViT-24B and ViT-32B are compared with their DeepViT counterparts. DeepViT-24B shows a 0.7 increase in performance, while DeepViT-32B shows a 1.6 increase. The text also discusses mitigating over-smoothing in self-attention by assigning a temperature τ to the Softmax layer of MHSA, which could sharpen the distribution of elements in the attention map.",
  "summary_type": "reports"
}{
  "input": "respect to the unquantized weights and number of bits used.\n(ii) We provide a differentiable model size estimate, so that\ngiven a single penalty level λ, DIFFQ optimizes the number\nof bits per weight or group of weights to achieve a given\ntrade-off between model size and accuracy.\narXiv:2104.09987v2  [stat.ML]  19 Oct 2021\n\n--------------------------------------------------\n(iii) We provide extensive experimental validation using vari-\nous models (ConvNets and Transformers) and domains (im-\nage",
  "output": "(ii) Proporcionamos una estimación diferenciable del tamaño del modelo, de modo que, dado un único nivel de penalización λ, DIFFQ optimiza el número de bits por peso o grupo de pesos para lograr un equilibrio dado entre el tamaño del modelo y la precisión. (iii) Proporcionamos una validación experimental extensa utilizando varios modelos (ConvNets y Transformadores) y dominios (imagen).",
  "summary_type": "reports"
}{
  "input": "convolution,\nillustrated\nin\nFig. 2(a), contains a single type of kernel:\nwith a single spatial size K1\n2 (in the case of\nsquare kernels, e.g., height×width: 3×3 = 32,\nK1 = 3) and the depth equal to the number\nof input feature maps FMi.\nThe result of\napplying a number of FMo kernels (all hav-\ning the same spatial resolution and the same\ndepth) over FMi input feature maps is a num-\nber of FMo output feature maps (with spa-\ntial height H and width W). Thus, the num-\nber of parameters and FLOPs (ﬂo",
  "output": "Convolution, seperti yang diilustrasikan dalam Gambar 2(a), mengandung satu jenis kernel: dengan ukuran spasial tunggal K1^2 (dalam kasus kernel persegi, misalnya, tinggi x lebar: 3x3 = 32, K1 = 3) dan kedalaman sama dengan jumlah peta fitur input FMi. Hasil dari penerapan sejumlah kernel FMo (semua memiliki resolusi spasial yang sama dan kedalaman yang sama) pada peta fitur input FMi adalah sejumlah peta fitur output FMo (dengan tinggi spasial H dan lebar W). Jadi, jumlah parameter dan FLOPs.",
  "summary_type": "reports"
}{
  "input": "e(Error)\nAnistropic Model(Error)\n0.2\n0.05\n29.61\n28.75\n0.2\n0.1\n28.31\n27.84\n0.4\n0.05\n31.92\n30.5\n0.4\n0.1\n32.59\n31.48\n0.6\n0.05\n39.04\n38.52\n0.6\n0.1\n36.75\n34.98\n0.8\n0.05\n54.23\n52.6\n0.8\n0.1\n44.31\n43\n1.0\n0.05\n75.05\n71.21\n1.0\n0.1\n51.19\n45.51\nfollow the details of (Ren et al., 2015) to train a model; 10 epochs with an initial learning rate of\n0.001.\nFor semantic segmentation, we report our results on FCN(Shelhamer et al., 2017) using our pre-\ntrained model as backbone. We train the FCN model for 30 epochs",
  "output": "The text provides data on an anisotropic model, with varying parameters and results. It also mentions training a model following Ren et al., 2015's details, with 10 epochs and an initial learning rate of 0.001. For semantic segmentation, results are reported on FCN (Shelhamer et al., 2017), using the pre-trained model as a backbone, trained for 30 epochs.",
  "summary_type": "reports"
}{
  "input": "hat we\nhave not extensively tuned the RandAugment magnitudes.\nC.4. Detection and instance segmentation hyperpa-\nrameters\nWe use Mask-RCNN [16] for all detection and instance\nsegmentation experiments. We pretrain the backbone on\nImageNet, mostly reusing the same hyperparameters as in\nSection C.3. Backbones are pretrained for 350 epochs using\nan image size of 512, which was chosen to be closer to the\n14\n\n--------------------------------------------------\nHaloNet\nModel\nb\nh\nrv\nrb\nTotal\nLayers\nl3\ns\nd",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "sed on the standard residual bottleneck\nblock with group convolution [31]. (a) Each X block consists of a\n1×1 conv, a 3×3 group conv, and a ﬁnal 1×1 conv, where the 1×1\nconvs alter the channel width. BatchNorm [12] and ReLU follow\neach conv. The block has 3 parameters: the width wi, bottleneck\nratio bi, and group width gi. (b) The stride-two (s = 2) version.\nThe AnyNetX design space has 16 degrees of freedom as\neach network consists of 4 stages and each stage i has 4 pa-\nrameters: the number of",
  "output": "El bloque estándar residual de cuello de botella se basa en la convolución de grupo [31]. Cada bloque X consta de una convolución 1x1, una convolución de grupo 3x3 y una convolución final 1x1, donde las convoluciones 1x1 alteran el ancho del canal. BatchNorm [12] y ReLU siguen cada convolución. El bloque tiene 3 parámetros: el ancho wi, la proporción de cuello de botella bi y el ancho de grupo gi. El espacio de diseño AnyNetX tiene 16 grados de libertad ya que cada red consta de 4 etapas y cada etapa i tiene 4 parámetros.",
  "summary_type": "reports"
}{
  "input": "t image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 1, 3, 6, 7\n[52] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv´e J´egou. Going deeper with im-\nage transformers. arXiv preprint arXiv:2103.17239, 2021.\n3\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, page",
  "output": "Text:\nt image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020. 1, 3, 6, 7\n[52] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021. 3\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, page",
  "summary_type": "reports"
}{
  "input": "attention operator.\nWe decouple the usage of K, V and Q pooling, with\nQ pooling being used in the ﬁrst layer of each stage and\nK, V pooling being employed in all other layers. Since the\nsequence length of key and value tensors need to be identical\nto allow attention weight calculation, the pooling stride used\non K and value V tensors needs to be identical. In our\ndefault setting, we constrain all pooling parameters (k; p; s)\nto be identical i.e. ΘK ≡ΘV within a stage, but vary s\nadaptively w.r.t",
  "output": "Operatör dikkat edin. K, V ve Q havuz kullanımını ayırıyoruz, Q havuzunu her aşamanın ilk katmanında kullanırken, K ve V havuzunu diğer tüm katmanlarda kullanıyoruz. Anahtar ve değer tensorlarının dizi uzunluğunun, dikkat ağırlığı hesaplamasına izin vermek için aynı olması gerektiğinden, K ve değer V tensorlarında kullanılan havuzlama adımının aynı olması gerekiyor. Varsayılan ayarımızda, tüm havuzlama parametrelerini (k; p; s) aynı tutarız, yani bir aşama içinde ΘK ≡ΘV, ancak s'yi adaptif olarak değiştiririz.",
  "summary_type": "reports"
}{
  "input": "khorn attention.\nIn International\nConference on Machine Learning, pages 9438–9447. PMLR,\n2020. 2\n[41] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian\nRuder, and Donald Metzler. Long range arena: A benchmark\nfor efﬁcient transformers. arXiv preprint arXiv:2011.04006,\n2020. 2\n[42] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler.\nEfﬁcient transformers:\nA survey.\narXiv preprint\narXiv:2009.06732, 2020. 2\n[43] Hugo Touvron, Matt",
  "output": "Text:\nKhorn attention представлен на Международной конференции по машинному обучению, страницы 9438-9447. PMLR, 2020. 2 \n[41] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder и Donald Metzler представили \"Long range arena: A benchmark for efficient transformers\", arXiv preprint arXiv:2011.04006, 2020. 2 \n[42] Yi Tay, Mostafa Dehghani, Dara Bahri и Donald Metzler провели обзор \"Efficient transformers\", arXiv preprint arXiv:2009.06732, 2020. 2 \n[43] Hugo Touvron, Matt",
  "summary_type": "reports"
}{
  "input": "we follow [35]. Therefore, different hierarchy has a gradually increased\nnumber of head, hidden dimensions, and number of repeated MSANesT layers. Table A1 speciﬁes\ndetails.\nData Augmentation. We apply the commonly used data augmentation and regularization techniques\nas [49]. Repeated augmentation [25] in DeiT is not used. In addition, for ImageNet models, we\nalso add color jittering similar to [8, 9] which seems to reduce dependency on local texture cues and\nslightly improves generalization (∼0",
  "output": "Se sigue [35], por lo que diferentes jerarquías tienen un número gradualmente creciente de cabezas, dimensiones ocultas y número de capas MSANesT repetidas. La Tabla A1 especifica los detalles. Se aplican técnicas comunes de aumento de datos y regularización como en [49]. No se utiliza el aumento repetido [25] en DeiT. Además, para los modelos de ImageNet, también añadimos una alteración de color similar a [8, 9] que parece reducir la dependencia de las señales de textura local y mejora ligeramente la generalización (∼0).",
  "summary_type": "reports"
}{
  "input": "eption-resnet-v2\nResNeXt-101\nAmoebaNet-A\nFig. 1. Improvement brought by FixRes (in bold) to several\npopular architectures from the literature. Our FixEfﬁcientNet\n(orange curve) surpasses all EfﬁcientNet models, including\nthe models trained with Noisy student (red curve) and adver-\nsarial examples (blue curve). The sws models are from [2].\nTables 1 and 2 report results on larger models.\nwhich jointly optimizes the choice of resolutions and scales at\ntraining and test time, while keeping the same",
  "output": "FixRes (in bold) improves several popular architectures. FixEfficientNet (orange curve) surpasses all EfficientNet models, including those trained with Noisy student (red curve) and adversarial examples (blue curve). It optimizes the choice of resolutions and scales at training and test time, maintaining the same parameters.",
  "summary_type": "reports"
}{
  "input": "0\n300M\n1.2M\n50K\n0.03\n79.0\n67.9\n85.1\n300M\n2M\n50K\n0.03\n79.6\n67.8\n85.6\n300M\n4M\n50K\n0.03\n79.9\n68.5\n85.8\n1B\n20K\n10K\n0.03\n59.0\n46.2\n66.2\n1B\n30K\n10K\n0.03\n64.0\n51.4\n71.4\n1B\n60K\n10K\n0.03\n70.5\n57.7\n77.7\n1B\n120K\n50K\n0.03\n73.6\n60.8\n80.7\n1B\n400K\n50K\n0.03\n77.6\n65.7\n84.0\n1B\n1.2M\n50K\n0.03\n79.5\n68.0\n85.5\n1B\n2M\n50K\n0.03\n79.7\n68.2\n85.5\n1B\n4M\n50K\n0.03\n80.2\n68.1\n85.9\n3B\n20K\n10K\n0.03\n59.3\n47.3\n66.4\n3B\n30K\n10K\n0.03\n64.3\n51.5\n71.6\n3B\n60K\n10K\n0.03\n70.2\n57.2\n77.6\n3B\n120K\n50K\n0.03\n73.5\n61.3\n80.7\n3B\n400K\n50K\n0.03\n77.6\n65.7",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "ing network tweaks and regu-\nlarizations is also demonstrated in mobile-oriented mod-\nels. Based on this, we reduced the reduction ratio r of the\nSE block from 16 to 2 to maximize synergy between net-\nwork tweaks and regularization. By doing so, we could im-\nprove MobileNet-V1’s top-1 accuracy by 1% with minimal\nthroughput loss. However, unlike ResNet, the top-1 accu-\nracy of SE-MobileNet-V1 decreased when DropBlock was\napplied. As the network capacity of MobileNet-V1 back-\nbone is smaller than",
  "output": "The reduction ratio r of the SE block was decreased from 16 to 2 to enhance synergy between network tweaks and regularization, improving MobileNet-V1's top-1 accuracy by 1% with minimal throughput loss. However, applying DropBlock decreased the top-1 accuracy of SE-MobileNet-V1, as the network capacity of MobileNet-V1 backbone is smaller.",
  "summary_type": "reports"
}{
  "input": "neural networks from overﬁtting. The journal of machine\nlearning research, 15(1):1929–1958, 2014.\nVerma, V., Lamb, A., Beckham, C., Najaﬁ, A., Mitliagkas,\nI., Courville, A., Lopez-Paz, D., and Bengio, Y. Manifold\nmixup: Better representations by interpolating hidden\nstates. ICML, 2019.\nVillani, C. Optimal transport: old and new, volume 338.\nSpringer Science & Business Media, 2008.\nWang, L., Lu, H., Ruan, X., and Yang, M.-H. Deep networks\nfor saliency detection via local estimation and global\nse",
  "output": "Neural networks are used to prevent overfitting in machine learning research. Verma et al. introduced Manifold mixup for better representations by interpolating hidden states in 2019. Villani's work on optimal transport is significant. Wang et al. developed deep networks for saliency detection using local estimation and global search.",
  "summary_type": "reports"
}{
  "input": "Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint\narXiv:1906.05849, 2019. 2, 3, 4, 5, 19\n[49] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola.\nWhat makes for good views for contrastive learning. arXiv preprint arXiv:2005.10243, 2019.\n6, 7, 8, 21, 22\n[50] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural networks for machine",
  "output": "Yonglong Tian, Dilip Krishnan, ve Phillip Isola, \"Contrastive Multiview Coding\" üzerine bir ön baskı yayınladılar (arXiv:1906.05849, 2019). Ayrıca, Tian, Chen Sun, Ben Poole, Krishnan, Cordelia Schmid ve Isola, \"Contrastive Learning için İyi Görünümler Nelerdir\" konusunda bir ön baskı yayınladılar (arXiv:2005.10243, 2019). Tijmen Tieleman ve Geoffrey Hinton, gradyanı son zamanlardaki büyüklüğünün çalışan ortalamasıyla bölen \"Lecture 6.5-rmsprop\" üzerine bir ders verdiler (COURSERA: Neural networks for machine).",
  "summary_type": "reports"
}{
  "input": ": Supervised Learn-\ning, Pseudo Labels, and Meta Pseudo Labels. We observe\nthat Meta Pseudo Labels has a much higher success rate of\nﬁnding the correct classiﬁer than Supervised Learning and\nPseudo Labels. Figure 2 presents a typical outcome of our\nexperiment, where the red and green regions correspond to\nthe classiﬁers’ decisions. As can be seen from the ﬁgure, Su-\npervised Learning ﬁnds a bad classiﬁer which classiﬁes the\nlabeled instances correctly but fails to take advantage of the\nclusterin",
  "output": "Meta Pseudo Labels have a higher success rate in finding the correct classifier compared to Supervised Learning and Pseudo Labels. Supervised Learning, while correctly classifying labeled instances, fails to utilize clustering.",
  "summary_type": "reports"
}{
  "input": "ages smaller than 224). All models use a label smoothing of 0.1 and a weight decay of 4e-5. These values were set\nbased on the preliminary experiments across various model scales on the ImageNet minival-set.\n16\n\n--------------------------------------------------\nRevisiting ResNets: Improved Training and Scaling Strategies\nFilter Scaling\nDropout Rate\n0.25\n0.0\n0.5\n0.1\n1.0\n0.25\n1.5\n0.6\n2.0\n0.75\nTable 12. Dropout values for ﬁlter scaling. Filter scaling refers to the ﬁlter scaling multiplier based o",
  "output": "All models use a label smoothing of 0.1 and a weight decay of 4e-5, determined from preliminary experiments on the ImageNet minival-set. The document also revisits ResNets, discussing improved training and scaling strategies. It presents a table of dropout values for filter scaling, with filter scaling referring to the filter scaling multiplier.",
  "summary_type": "reports"
}{
  "input": ".2 ± 0.07\n78.3 / 94.3\n76.3 / 92.8\n−\n−\n−\nStochastic Depth (Huang et al. 2016)\n96.2 ± 0.07\n77.5 / 93.7\n76.8 / 93.1\n80.2 / 95.0\n82.5 / 96.2\n84.1 / 96.9\nAutoDropout\n96.8 ± 0.09\n78.7 / 94.3\n77.5 / 93.8\n80.9 / 95.6\n83.1 / 96.5\n84.7 / 97.1\nTable 1: Performance of AutoDropout and the baselines on supervised image classiﬁcation (higher is better). This is a control experiment and\nall models are implemented by us.\nMethods\nCIFAR-10\nImageNet\n(WRN-28-10)\n(ResNet-50)\nStochastic Depth (2016)\n96.2 ± 0.07†\n77.5",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "ness\nTest error\nEstimated sharpness\n1\n2.77±0.03\n0.17±0.03\n16.72±0.08\n0.82±0.05\n2\n2.76±0.03\n0.82±0.03\n16.59±0.08\n1.83±0.05\n3\n2.73±0.04\n1.49±0.05\n16.62±0.09\n2.36±0.03\n5\n2.77±0.03\n2.26±0.05\n16.60±0.06\n2.82±0.04\nTable 11: Test error rate and estimated sharpness (maxϵ L(w +ϵ)−L(w)) at the end of the training.\n19\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\n10 6\n10 5\n10 4\n10 3\n10 2\n10 1\n100\n101\n102\n103\n0.034\n0.036\n0.038\n0.040\n0.042\n0.044\n0.046\n0.048",
  "output": "The text presents a table showing test error rate and estimated sharpness at the end of training. The data includes four sets of values (1, 2, 3, 5) with varying degrees of test error and sharpness. The rest of the text contains numerical values without clear context or meaning.",
  "summary_type": "reports"
}{
  "input": "error rates on ImageNet on ResNet-50\nfollowing the training protocol in (Yun et al., 2019) (300 epochs).\n6.2. Robustness Against Corruption\nHendrycks et al. (2020) proposed AugMix which performs\nInput mixup between clean and augmented images to im-\nprove robustness against corrupted datasets as well as the\ngeneralization performance. AugMix uses Jensen-Shannon\ndivergence (JSD) between network outputs of a clean image\nand two AugMix images as a consistency loss. However,\ncomputing the JSD term r",
  "output": "Following the training protocol in Yun et al., 2019, error rates on ImageNet on ResNet-50 were evaluated. Hendrycks et al., 2020 proposed AugMix, a method that improves robustness against corrupted datasets and generalization performance by performing input mixup between clean and augmented images. AugMix uses Jensen-Shannon divergence (JSD) between network outputs of a clean image and two AugMix images as a consistency loss.",
  "summary_type": "reports"
}{
  "input": "size is doubled.\nGoogLeNet [31] also used the pooling layer. ResNet [13]\nperformed spatial size reduction using the convolution layer\nof stride 2 instead of max pooling. It is an improvement\nin the spatial reduction method. The convolution layer of\nstride 2 is also used as a pooling method in recent archi-\ntectures (EfﬁcietNet [32], MobileNet [29, 19]). Pyramid-\nNet [11] pointed out that the channel increase occurs only\nin the pooling layer and proposed a method to gradually\nincrease the channel",
  "output": "GoogLeNet and ResNet utilise pooling layers for spatial size reduction, with ResNet using a stride 2 convolution layer instead of max pooling. This method is also used in recent architectures like EfficientNet and MobileNet. PyramidNet, however, notes that channel increase only occurs in the pooling layer and suggests a gradual channel increase method.",
  "summary_type": "reports"
}{
  "input": "MoEx (Li et al., 2021)), the SoTA on ImageNet-C (Deep-\nAugment + AugMix (Hendrycks et al., 2020)), and the SoTA on Stylized-ImageNet (SIN (Geirhos\net al., 2019)). Interestingly, we note the improvements of all these SoTAs are not consistent across\ndifferent benchmarks. For example, as shown in Table 3, SIN signiﬁcantly improves the results on\nStylized-ImageNet, but at the cost of huge performance drop on ImageNet (-16.2%) and ImageNet-\nC (-2.3%). Our shape-texture debiased training stands as the",
  "output": "MoEx (Li et al., 2021), SoTA on ImageNet-C (Deep-Augment + AugMix (Hendrycks et al., 2020)), and SoTA on Stylized-ImageNet (SIN (Geirhos et al., 2019)) 的改进在不同的基准测试中并不一致。例如，如表3所示，SIN显著提高了Stylized-ImageNet的结果，但以ImageNet (-16.2%) 和 ImageNet-C (-2.3%) 的巨大性能下降为代价。我们的形状-纹理去偏训练保持稳定。",
  "summary_type": "reports"
}{
  "input": "gns to verify these curiosities.\nFrom the results, it is important to ﬁnd out that: 1) when performing these spatial operations, it is\nalways important to apply on the holistic image plane versus the block plane although both reasonably\ncan introduce spatial priors. 2) small kernel convolution is sufﬁcient and has to be applied ahead of\npooling; 3) Max pooling is far better than other options, such as stride-2 sub-sampling and average\npooling; 4) sub-sampling the query sequence length (similar t",
  "output": "From the findings, it's crucial to note: 1) Spatial operations should be applied on the holistic image plane rather than the block plane. 2) Small kernel convolution, applied before pooling, is sufficient. 3) Max pooling outperforms other options like stride-2 sub-sampling and average pooling. 4) Sub-sampling the query sequence length is also important.",
  "summary_type": "reports"
}{
  "input": "e in dealing with image corruptions. Because ViTs are good at long-range\ndependencies modeling, which makes it easier to learn the shape feature than CNNs. Such a shape\nfeature is more robust and less likely to be destroyed by image corruptions.\nOut-of-distribution Robustness. We test the model generalization on out-of-distribution data by\nreporting the accuracy on ImageNet-R and ImageNet-Sketch in Table 5. The performance gap is\nnot obvious on ImageNet-R, as the task is hard for most compared m",
  "output": "ViTs are effective in handling image corruptions due to their proficiency in long-range dependencies modeling, enabling them to learn shape features more efficiently than CNNs. These shape features are robust and less susceptible to image corruption. The model's generalization on out-of-distribution data was tested, with accuracy reported on ImageNet-R and ImageNet-Sketch. The performance gap on ImageNet-R was not significant, indicating the task's difficulty.",
  "summary_type": "reports"
}{
  "input": "1 outperform all prior work using\nResNet-101.\n6.3. Semantic Segmentation\nIn transfer learning for semantic segmentation, we use\nthe GluonCV [19] implementation of DeepLabV3 [9] as a\nbaseline approach. Here a dilated network strategy [8, 65]\nis applied to the backbone network, resulting in a stride-\n8 model. Synchronized Batch Normalization [68] is used\nduring training, along with a polynomial-like learning rate\nschedule (with initial learning rate = 0.1).\nFor evalua-\ntion, the network prediction",
  "output": "In transfer learning for semantic segmentation, the GluonCV implementation of DeepLabV3 is used as a baseline approach. A dilated network strategy is applied to the backbone network, resulting in a stride-8 model. Synchronized Batch Normalization is used during training, along with a polynomial-like learning rate schedule with an initial learning rate of 0.1. The network prediction is used for evaluation.",
  "summary_type": "reports"
}{
  "input": "A. Vedaldi, A. Zisserman, and C. Jawahar, “Cats and dogs,”\nin CVPR, 2012.\n[19] M. Nilsback and A. Zisserman, “Automated ﬂower classiﬁcation over a\nlarge number of classes,” in 2008 Sixth Indian Conference on Computer\nVision, Graphics Image Processing, 2008.\n[20] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,\nU. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic\nurban scene understanding,” in CVPR, 2016.\n[21] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. H",
  "output": "A. Vedaldi, A. Zisserman, C. Jawahar, \"Cats and dogs,\" CVPR, 2012. M. Nilsback, A. Zisserman, \"Automated flower classification over a large number of classes,\" 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing, 2008. M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, B. Schiele, \"The cityscapes dataset for semantic urban scene understanding,\" CVPR, 2016. C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. H.",
  "summary_type": "reports"
}{
  "input": "patch and feature dimensions\nof the image tokens. The architecture is identical to that of\nViT [4] with the attention layer replaced by a feed-forward\nlayer.\nthe model a global receptive ﬁeld. This layer may be seen\nas a data-dependent linear layer, and when applied on im-\nage patches it resembles (but is not exactly equivalent to)\na convolution. Indeed, a signiﬁcant amount of recent work\nhas gone into improving the efﬁciency and efﬁcacy of the\nattention layer.\nIn this short report, we conduct",
  "output": "The architecture of the model is similar to ViT, but the attention layer is replaced by a feed-forward layer. This layer, which may be seen as a data-dependent linear layer, gives the model a global receptive field. When applied on image patches, it resembles a convolution. Recent work has focused on improving the efficiency and efficacy of the attention layer.",
  "summary_type": "reports"
}{
  "input": "ko, W. Wang, T. Weyand, M. Andreetto, and H. Adam.\nMobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint\narXiv:1704.04861, 2017.\n[29] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708,\n2017.\n[30] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate",
  "output": "W. Wang, T. Weyand, M. Andreetto, H. Adam은 \"Mobilenets: 효율적인 컨볼루션 신경망을 이용한 모바일 비전 애플리케이션\"에 대해 논문을 발표했습니다. G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger는 \"밀집 연결 컨볼루션 네트워크\"에 대한 논문을 IEEE 컴퓨터 비전 및 패턴 인식 회의에서 발표했습니다. S. Ioffe와 C. Szegedy는 \"배치 정규화: 내부 공변량을 줄이는 깊은 네트워크 훈련 가속화\"에 대한 논문을 발표했습니다.",
  "summary_type": "reports"
}{
  "input": "on-preserving) skip-\nconnections that operate on the un-normalized signal.\n3.3. Network instantiation details\nTable 3 shows concrete instantiations of the base mod-\nels for Vision Transformers [25] and our Multiscale Vision\nTransformers. ViT-Base [25] (Table 3b) initially projects\nthe input to patches of shape 1×16×16 with dimension\nD = 768, followed by stacking N = 12 transformer\nblocks. With an 8×224×224 input the resolution is ﬁxed to\n768×8×14×14 throughout all layers. The sequence length\n(sp",
  "output": "The text discusses the implementation details of Vision Transformers (ViT) and Multiscale Vision Transformers. ViT-Base initially projects the input to patches of shape 1×16×16 with dimension D = 768, followed by stacking N = 12 transformer blocks. With an 8×224×224 input, the resolution is fixed to 768×8×14×14 throughout all layers.",
  "summary_type": "reports"
}{
  "input": "omated data augmentation with a reduced search space. arXiv preprint arXiv:1909.13719,\n2019. 3, 5, 18, 20\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Conference on Computer Vision and Pattern Recognition, pages\n248–255, 2009. 3\n[10] Piotr Doll´ar, Mannat Singh, and Ross B. Girshick. Fast and accurate model scaling. In\nConference on Computer Vision and Pattern Recognition, 2021. 3, 6\n[11] Alexey Dosovitskiy, Lu",
  "output": "Automated data augmentation with a reduced search space is discussed in arXiv preprint arXiv:1909.13719, 2019. Jia Deng et al. presented ImageNet, a large-scale hierarchical image database, at the Conference on Computer Vision and Pattern Recognition in 2009. Piotr Dollár et al. discussed fast and accurate model scaling at the same conference in 2021.",
  "summary_type": "reports"
}{
  "input": "d recipes. From the results in Tab. 6, Reﬁner boosts the model\nperformance across all the tasks signiﬁcantly and increases the average score by 1%, demonstrating\nReﬁner is well generalizable to transformer-based NLP models to improve their attentions and ﬁnal\nperformance.\nTable 6: Comparison of BERT-small w/o and w/ reﬁner on the GLUE development set.\nModel\nParams\nMNLI\nQNLI\nQQP\nRTE\nSST\nMRPC\nCoLA\nSTS-B\nAvg.\nBERT-small [11]\n14M\n75.8\n83.7\n86.8\n57.4\n88.4\n83.8\n41.6\n83.6\n75.1\n+ Reﬁner\n14M\n78.1\n86.4\n88",
  "output": "Text: \nReﬁner potencia el rendimiento del modelo en todas las tareas y aumenta la puntuación media en un 1%, demostrando que Reﬁner es generalizable a los modelos de NLP basados en transformadores para mejorar sus atenciones y rendimiento final. La Tabla 6 compara BERT-small sin y con reﬁner en el conjunto de desarrollo de GLUE. BERT-small [11] obtuvo un promedio de 75.1, mientras que con Reﬁner, el rendimiento mejoró significativamente.",
  "summary_type": "reports"
}{
  "input": "e datasets, meta-parameters,\nbaselines and performance experiments to support and\nquantify the beneﬁts of SSAL models.\n4.1. Datasets\nWe conduct experiments on three different image classi-\nﬁcation datasets with varying degrees of complexity:\nCIFAR100 [12]: extension of CIFAR10 where 60 000\ncolor images of size 32x32 belong to 100 different classes\nof ﬁne-grained objects or animals. The training and test set\ncontain 50 000 and 10 000 images respectively.\nTinyImagenet [16]: 110 000 color images of",
  "output": "Experiments are conducted on three image classification datasets of varying complexity: CIFAR100 and TinyImagenet. CIFAR100 is an extension of CIFAR10, containing 60,000 color images of size 32x32, divided into 100 different classes of fine-grained objects or animals. The training and test set contain 50,000 and 10,000 images respectively. TinyImagenet comprises 110,000 color images.",
  "summary_type": "reports"
}{
  "input": "the case with the attention operation which produces per-example attention\nmaps. In practice, the hyperparameter |k| is set to a small value (such as |k|=16) and we can process\nlarge batches of large inputs in cases where attention cannot (see Table 4). Additionally, position\nembeddings can be shared across lambda layers to keep their Θ(knm) memory footprint constant -\nwhereas the memory footprint of attention maps scales with the number of layers5.\nMulti-query lambda layers reduce time and spa",
  "output": "The attention operation creates per-example attention maps. The hyperparameter |k| is typically set to a small value (e.g., |k|=16) allowing processing of large batches of large inputs where attention can't. Position embeddings can be shared across lambda layers to maintain a constant Θ(knm) memory footprint, unlike attention maps whose memory footprint scales with the number of layers. Multi-query lambda layers reduce time and space.",
  "summary_type": "reports"
}{
  "input": "ral networks, triggering the surge of deep learning in vi-\nsion. In this work, we rethink the inherent principles of\nstandard convolution for vision tasks, speciﬁcally spatial-\nagnostic and channel-speciﬁc. Instead, we present a novel\natomic operation for deep neural networks by inverting\nthe aforementioned design principles of convolution, coined\nas involution. We additionally demystify the recent pop-\nular self-attention operator and subsume it into our invo-\nlution family as an over-complicat",
  "output": "In this work, the standard principles of convolution for vision tasks are reevaluated, introducing a new atomic operation for deep neural networks called involution. This new concept inverts the traditional design principles of convolution. The popular self-attention operator is also analyzed and included in the involution family, suggesting it may be overly complicated.",
  "summary_type": "reports"
}{
  "input": "e aggregation of\nlocal descriptors, like in VLAD [36]. Most of the modern methods now rely on convolutional\nTable D.2: Evaluation on transfer learning.\nArchitecture\nCIFAR10\nCIFAR100\nFlowers102\nCars\niNat18\niNat19\nEfﬁcientNet-B7 [58]\n98.9\n91.7\n98.8\n94.7\n_\n_\nViT-B/16 [22]\n98.1\n87.1\n89.5\n_\n_\n_\nViT-L/16 [22]\n97.9\n86.4\n89.7\n_\n_\n_\nDeit-B/16 [65] Υ\n99.1\n91.3\n98.8\n92.9\n73.7\n78.4\nXCiT-S24/16 Υ\n99.1\n91.2\n97.4\n92.8\n68.8\n76.1\nXCiT-M24/16 Υ\n99.1\n91.4\n98.2\n93.4\n72.6\n78.1\nXCiT-L24/16 Υ\n99.1\n91.3\n98.3\n93.7\n75.6",
  "output": "The table presents a comparison of different architectures' performance on various datasets. EfficientNet-B7 shows high performance on CIFAR10, CIFAR100, Flowers102, and Cars datasets. ViT-B/16 and ViT-L/16 also perform well but lack results for some datasets. Deit-B/16 and XCiT variants (S24/16, M24/16, L24/16) demonstrate consistent performance across all datasets, with Deit-B/16 and XCiT-L24/16 showing particularly strong results on the iNat18 and iNat19 datasets.",
  "summary_type": "reports"
}{
  "input": "g, Mason Liu, and Weilong Yang. Beyond Synthetic Noise: Deep Learning on\nControlled Noisy Labels. arXiv e-prints, art. arXiv:1911.09781, November 2019.\nYiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic\ngeneralization measures and where to ﬁnd them. arXiv preprint arXiv:1912.02178, 2019.\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv e-prints,\nart. arXiv:1412.6980, December 2014.\nAlexander Kolesnikov, Lucas Beyer, Xiaohu",
  "output": "Mason Liu, Weilong Yang, Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, Samy Bengio, Diederik P. Kingma, Jimmy Ba, Alexander Kolesnikov, Lucas Beyer, Xiaohu等人在arXiv上发表了多篇论文。这些论文涵盖了深度学习在控制噪声标签上的应用、优秀的泛化度量方法的寻找，以及Adam：一种随机优化方法等主题。",
  "summary_type": "reports"
}{
  "input": "ge problems, attaining 90.2% top-1 accu-\nracy on ImageNet, which is 1.6% better than the previous\nstate-of-the-art [16]. The consistent gains conﬁrm the beneﬁt\nof the student’s feedback to the teacher.\nAcknowledgements\nThe authors wish to thank Rohan Anil, Frank Chen, Wang\nTao for their help with many technical issues in running\nour experiments. We also thank David Berthelot, Nicholas\nCarlini, Sylvain Gelly, Geoff Hinton, Mohammad Norouzi,\nand Colin Raffel for their comments on earlier drafts of",
  "output": "The text discusses an improvement in image recognition problems, achieving 90.2% top-1 accuracy on ImageNet, which is 1.6% better than the previous best. This confirms the benefit of student feedback to the teacher. The authors thank several individuals for their technical help and comments on earlier drafts.",
  "summary_type": "reports"
}{
  "input": "ave almost no effect on FLOPs. In\nthis way, CeiT-T outperforms DeiT-T by a large margin of\n4.2% for the Top-1 accuracy. And CeiT-S obtains higher re-\nsults than that of DeiT-S and DeiT-B by 2.1% and 0.2% re-\nspectively. We also compare CeiT with concurrent work of\nT2T-ViT and PVT. CeiT-S achieves slightly higher accuracy\nthan T2T-ViT-19 with much fewer FLOPs and parameters.\nFor PVT models, CeiT-T obtains higher accuracy of 1.3%\nthan PVT-T with fewer FLOPs and parameters. CeiT-S also\noutperforms",
  "output": "CeiT-T supera a DeiT-T con un amplio margen del 4,2% en la precisión Top-1. CeiT-S obtiene resultados superiores a los de DeiT-S y DeiT-B en un 2,1% y 0,2% respectivamente. CeiT-S logra una precisión ligeramente superior a T2T-ViT-19 con muchos menos FLOPs y parámetros. Para los modelos PVT, CeiT-T obtiene una precisión superior del 1,3% que PVT-T con menos FLOPs y parámetros. CeiT-S también supera.",
  "summary_type": "reports"
}{
  "input": "r, 2020a.\nZoph, B., Ghiasi, G., Lin, T.-Y., Cui, Y., Liu, H., Cubuk,\nE. D., and Le, Q. V. Rethinking pre-training and self-\ntraining. arXiv preprint arXiv:2006.06882, 2020b.\n13\n\n--------------------------------------------------\nRevisiting ResNets: Improved Training and Scaling Strategies\nA. Author Contributions\nIB, BZ: led the research, designed and ran the scaling experiments, designed and experimented with the training strate-\ngies. JS, TL, EC, AS, WF, XD: advised the research, proposed exper",
  "output": "Zoph, B., Ghiasi, G., Lin, T.-Y., Cui, Y., Liu, H., Cubuk, E. D., ve Le, Q. V. Ön eğitimi ve kendi kendine eğitimi yeniden düşünme. arXiv ön baskı arXiv:2006.06882, 2020b.\n--------------------------------------------------\nResNets'i Yeniden Ziyaret Etme: İyileştirilmiş Eğitim ve Ölçeklendirme Stratejileri\nYazar Katkıları\nIB, BZ: araştırmayı yönetti, ölçeklendirme deneylerini tasarladı ve yürüttü, eğitim stratejileri tasarladı ve denedi. JS, TL, EC, AS, WF, XD: araştırmayı yönlendirdi, deney önerdi.",
  "summary_type": "reports"
}