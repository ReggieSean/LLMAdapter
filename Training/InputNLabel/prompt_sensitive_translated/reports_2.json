{
  "input": "P\nFigure 3: ImageNet acc. vs model complexity.\nTable 6: Comparisons among the GFNet and other\nvariants based on the transformer-like architecture\non ImageNet. We show that GFNet outperforms the\nResMLP [42], FNet [25] and models with local depth-\nwise convolutions. We also report the number of pa-\nrameters and theoretical complexity in FLOPs.\nModel\nAcc\nParam\nFLOPs\n(%)\n(M)\n(G)\nDeiT-S [43]\n79.8\n22\n4.6\nLocal Conv (3 × 3)\n77.7\n15\n2.8\nLocal Conv (5 × 5)\n78.1\n15\n2.9\nLocal Conv (7 × 7)\n78.2\n15\n2.9\nResML",
  "output": "The text compares the performance of the GFNet model with other transformer-like architecture models on ImageNet. It shows that GFNet outperforms the ResMLP, FNet, and models with local depth-wise convolutions. The comparison includes the number of parameters and theoretical complexity in FLOPs. For instance, DeiT-S has an accuracy of 79.8%, 22M parameters, and 4.6G FLOPs. Local Convolution models with different kernel sizes (3x3, 5x5, 7x7) have accuracies ranging from 77.7% to 78.2%, 15M parameters, and FLOPs between 2.8G to 2.9G.",
  "summary_type": "reports"
}{
  "input": "ages 448–456. PMLR, 2015. 2\n[25] Md Amirul Islam, Matthew Kowal, Sen Jia, Konstantinos G\nDerpanis, and Neil DB Bruce. Position, padding and predic-\ntions: A deeper look at position information in cnns. arXiv\npreprint arXiv:2101.12322, 2021. 5\n[26] Anil K. Jain and Farshid Farrokhnia.\nUnsupervised tex-\nture segmentation using gabor ﬁlters.\nPattern Recognit.,\n24(12):1167–1186, 1991. 3\n[27] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:\nTwo transformers can make one strong gan. arXiv prepr",
  "output": "The text mentions several academic works. Md Amirul Islam et al. (2021) discuss the role of position information in Convolutional Neural Networks (CNNs). Anil K. Jain and Farshid Farrokhnia (1991) present an unsupervised texture segmentation method using Gabor filters. Yifan Jiang et al. propose TransGAN, a Generative Adversarial Network (GAN) model that utilizes two transformers.",
  "summary_type": "reports"
}{
  "input": "ed\nposition-aware attention scaling and patch-wise data augmentation, RVT-Ti∗can further improve\n0.8% on RVT-Ti with little additional computation cost. For other scales of the model, RVT-S∗and\nRVT-B∗also achieve a good promotion compared with DeiT. Although the improvement becomes\nsmaller with the increase of model capacity, we think the advance of our model is still obvious as it\nstrengthen the model ability in various views such as robustness and out-of-domain generalization.\n6.3\nRobustness E",
  "output": "RVT-Ti* improves on RVT-Ti by 0.8% through position-aware attention scaling and patch-wise data augmentation, with minimal extra computation cost. RVT-S* and RVT-B* also show good improvement compared to DeiT. Despite the decrease in improvement with increased model capacity, the model's advancement is still evident in enhancing robustness and out-of-domain generalization.",
  "summary_type": "reports"
}{
  "input": "have *Partial* version because its window size is set as 15 (comparable with\nthe ViT(DeiT)/16 feature map size 14), and its attention mechanism in the last two stages is equivalent to full attention.\n\n--------------------------------------------------\ntion!\n2. Using a customized CUDA kernel,\ndenoted as\n“cuda kernel”. We make use of the TVM, like what\nhas done in Longformer [3], to write a customized\nCUDA kernel for Vision Longformer. As shown in\nFigure 5, the “cuda kernel” (green line) achieves",
  "output": "The text discusses a partial version with a window size of 15, comparable to the ViT(DeiT)/16 feature map size 14. It uses an attention mechanism in the last two stages equivalent to full attention. A customized CUDA kernel, called \"cuda kernel\", is used in conjunction with TVM, similar to the Longformer. This customized CUDA kernel is used for Vision Longformer, achieving significant results as shown in Figure 5.",
  "summary_type": "reports"
}{
  "input": "74.74\n80.76\n80.99\n81.19\n80.98\nPartial Linformer\n75.64\n75.82\n75.56\n75.33\n81.66\n81.63\n81.66\n81.79\nSRA/64 [47]\n68.71\n68.84\n69.08\n68.78\n75.9\n76.18\n76.35\n76.37\nSRA/32 [47]\n73.16\n73.46\n73.22\n73.2\n79.82\n79.8\n79.96\n79.9\nPartial SRA/32\n75.17\n75.8\n75.2\n75.26\n81.63\n81.59\n81.62\n81.61\nGlobal\n70.93\n71.62\n71.52\n72.00\n79.04\n79.08\n79.17\n78.97\nPartial Global\n75.55\n75.61\n75.32\n75.4\n81.39\n81.42\n81.6\n81.45\nPerformer\n71.28\n71.87\n71.12\n73.09\n78.17\n78.58\n78.81\n78.72\nPartial Performer\n75.65\n75.74\n75.34\n75.93\n81.59\n81.86",
  "output": "The text appears to be a series of numerical data, possibly representing scores or measurements related to different categories such as \"Partial Linformer\", \"SRA/64\", \"SRA/32\", \"Partial SRA/32\", \"Global\", \"Partial Global\", \"Performer\", and \"Partial Performer\". However, without context, it's impossible to provide a meaningful summary of this data. Therefore, the return is <NULL>.",
  "summary_type": "reports"
}{
  "input": "different model sizes, namely, TNT-Ti, TNT-S and TNT-B. They consist of 6.1M,\n23.8M and 65.6M parameters respectively. The corresponding FLOPs for processing a 224×224\nimage are 1.4B, 5.2B and 14.1B respectively.\nTable 1: Variants of our TNT architecture. ‘Ti’ means tiny, ‘S’ means small, and ‘B’ means base.\nThe FLOPs are calculated for images at resolution 224×224.\nModel\nDepth\nInner transformer\nOuter transformer\nParams\nFLOPs\ndim c\n#heads\nMLP r\ndim d\n#heads\nMLP r\n(M)\n(B)\nTNT-Ti\n12\n12\n2\n4\n192\n3\n4",
  "output": "The text discusses three models: TNT-Ti, TNT-S, and TNT-B, with 6.1M, 23.8M, and 65.6M parameters respectively. The FLOPs (Floating Point Operations Per Second) for processing a 224×224 image are 1.4B, 5.2B, and 14.1B respectively. 'Ti' stands for tiny, 'S' for small, and 'B' for base. The models are variants of the TNT architecture.",
  "summary_type": "reports"
}{
  "input": "79.35% top-1, and trained on Imagenet-1k with a 8xV100 GPU machine.\n14\n\n--------------------------------------------------\ntop-1 accuracy\nAblation on ↓\nPre-training\nFine-tuning\nRand-Augment\nAutoAug\nMixup\nCutMix\nErasing\nStoch. Depth\nRepeated Aug.\nDropout\nExp. Moving Avg.\npre-trained 2242\nﬁne-tuned 3842\nnone: DeiT-B\nadamw\nadamw\n\u0013\n\u0017\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017\n81.8 ±0.2\n83.1 ±0.1\noptimizer\nSGD\nadamw\n\u0013\n\u0017\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017\n74.5\n77.3\nadamw\nSGD\n\u0013\n\u0017\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017\n81.8\n83.1\ndata\naugmentation\nadamw\nadamw\n\u0017\n\u0017\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017",
  "output": "The text discusses a model trained on Imagenet-1k with an 8xV100 GPU machine, achieving a top-1 accuracy of 79.35%. It mentions various techniques like Pre-training, Fine-tuning, Rand-Augment, AutoAug, Mixup, CutMix, Erasing, Stochastic Depth, Repeated Augmentation, Dropout, and Exponential Moving Average. It also refers to pre-training and fine-tuning scores, and different optimizers like SGD and AdamW. However, the text contains several uninterpretable symbols and lacks clear context, making a comprehensive summary challenging.",
  "summary_type": "reports"
}{
  "input": "depthwise conv1d(values, embeddings)\n# Transpose from shape [b, n, v, k] to shape [b, n, k, v]\nposition lambdas = transpose(position lambdas, [0, 1, 3, 2])\nreturn position lambdas\ndef lambda layer(queries, keys, embeddings, values, impl=’einsum’):\n\"\"\"Multi−query lambda layer.\"\"\"\ncontent lambda = einsum(softmax(keys), values, ’bmk,bmv−>bkv’)\nposition lambdas = compute position lambdas(embeddings, values, impl=impl)\ncontent output = einsum(queries, content lambda, ’bhnk,bkv−>bnhv’)\nposition output",
  "output": "The text describes a depthwise convolution operation on 'values' and 'embeddings'. It then transposes 'position lambdas' from shape [b, n, v, k] to [b, n, k, v]. A lambda layer function is defined, which calculates 'content lambda' and 'position lambdas' using einsum and softmax operations. The 'content output' and 'position output' are then computed.",
  "summary_type": "reports"
}{
  "input": "make ConvMLP scalable, we extend\nConvMLP model by scaling both the depth and width of\nboth convolution and Conv-MLP stages. It achieves com-\npetitive performances on ImageNet-1k with fewer param-\neters compared to recent MLP-based models. On object\ndetection and semantic segmentation, we conduct experi-\nments on MS COCO and ADE20K benchmarks. It shows\nthat using ConvMLP as a backbone achieves better trade-off\nbetween performance and model size.\nIn conclusion, our contributions are as follows:\n•",
  "output": "The ConvMLP model is made scalable by increasing both the depth and width of the convolution and Conv-MLP stages. It performs competitively on ImageNet-1k with fewer parameters than recent MLP-based models. Experiments on MS COCO and ADE20K benchmarks show that using ConvMLP as a backbone provides a better balance between performance and model size.",
  "summary_type": "reports"
}{
  "input": "[30]) and/or additional data used for training (as in [29], [31]), however, this\nrequires signiﬁcantly more computational and data resources, which are not easily available.\n8\n\n--------------------------------------------------\nTable 4: Validation error rates comparison results of PyConvResNet on ImageNet with different\ntraining settings, for network depth 50 and 101 (†on the already trained model with 224×224 crop, just perform the test on 320×320).\nNetwork\ntest crop: 224×224\ntest crop: 320×32",
  "output": "The text discusses the use of additional data for training, which requires substantial computational and data resources. It also mentions a comparison of validation error rates for PyConvResNet on ImageNet under different training settings. The network depth is 50 and 101, and tests are performed on 224x224 and 320x320 crops on a pre-trained model.",
  "summary_type": "reports"
}{
  "input": "e 2: Masked Image Modeling\n\u0001\n(3)\nwhere the second term is our BEIT pre-training objective.\n2.5\nPre-Training Setup\nThe network architecture of BEIT follows that of ViT-Base (Dosovitskiy et al., 2020) for a fair\ncomparison. We use a 12-layer Transformer with 768 hidden size, and 12 attention heads. The\nintermediate size of feed-forward networks is 3072. We employ the default 16 × 16 input patch size.\nWe directly borrow the image tokenizer trained by Ramesh et al. (2021). The vocabulary size of\nvis",
  "output": "The BEIT pre-training setup uses the ViT-Base network architecture for fair comparison. It employs a 12-layer Transformer with 768 hidden size and 12 attention heads. The intermediate size of feed-forward networks is 3072, and the default 16x16 input patch size is used. The image tokenizer trained by Ramesh et al. (2021) is directly borrowed, with an unspecified vocabulary size.",
  "summary_type": "reports"
}{
  "input": "ImageNet data (13 examples per class), as commonly\nused in self-supervised and semi-supervised learn-\ning [42]. For reference, Figure 3 shows two state-\nof-the-art self-supervised learning models, SimCLR\nv2 [6] and BYOL [12], using 1% of ImageNet data.\nNote, however, that these approaches a quite differ-\nent: ViT-G/14 uses large source of weakly-supervised\ndata, and is pre-trained only once and transferred\nto different tasks. Meanwhile, the self-supervised\nlearning models use unlabeled but in-d",
  "output": "The text discusses the use of ImageNet data in self-supervised and semi-supervised learning. It mentions two state-of-the-art self-supervised learning models, SimCLR v2 and BYOL, which use 1% of ImageNet data. It also refers to ViT-G/14, which uses a large source of weakly-supervised data and is pre-trained only once before being transferred to different tasks.",
  "summary_type": "reports"
}{
  "input": "ality, we can add terms in region A whose coefﬁcient is zero. By design,\nnon-local region is also within the receptive ﬁeld of the boundary pixels of the local region. Thus, we\ncan transform Eq. 4 into below formulation:\nX\nxi∈(Ω\\A)\nX\nxk∈˚\nA\nαiβk ⟨xk, xi⟩=\nX\nxi∈Ω\nX\nxk∈˚\nA\nαiβk ⟨xk, xi⟩.\n(5)\nAccording to the Markovian property of the image, we can assume that for xk ∈A, the interaction\nbetween xi (far away from xk) and xk is weak. Thus, the Eq. 5 can be further simpliﬁed:\nX\nxi∈Ω\nX\nxk∈˚\nA\nαiβk ⟨xk,",
  "output": "The text discusses transforming an equation in image processing. It suggests that terms in region A with zero coefficients can be added. The non-local region is within the receptive field of the boundary pixels of the local region. The equation is then transformed according to this principle. It also mentions the Markovian property of the image, assuming weak interaction between xi (far away from xk) and xk. This allows further simplification of the equation.",
  "summary_type": "reports"
}{
  "input": "s. Here, sw is the stripe\nwidth and can be adjusted to balance the learning capacity and computation complexity. Formally,\nsuppose the projected queries, keys and values of the kth head all have dimension dk, then the output\nof the horizontal stripes self-attention for kth head is deﬁned as:\nX = [X1, X2, . . . , XM], where Xi ∈R(sw×W )×C and M = H/sw\nY i\nk = Attention(XiW Q\nk , XiW K\nk , XiW V\nk ), where i = 1, . . . , M\nH-Attentionk(X) = [Y 1\nk , Y 2\nk , . . . , Y M\nk ]\n(1)\nWhere W Q\nk ∈RC×dk,",
  "output": "The text discusses a method to balance learning capacity and computation complexity by adjusting the stripe width (sw). The output of the horizontal stripes self-attention for the kth head is defined using projected queries, keys, and values, all having dimension dk. The output is represented as X, with elements Xi, and M = H/sw. The attention function is applied to each Xi, resulting in Yik. The final output is H-Attentionk(X), which is an array of all Yik.",
  "summary_type": "reports"
}{
  "input": "4.8\n62.6\n68.4\n90.2\n92.1\n63.6\n70.3\n30M\n1.2M\n64.2\n67.4\n63.9\n69.3\n91.0\n92.2\n66.4\n73.9\n30M\n2M\n64.4\n67.5\n64.0\n69.0\n91.6\n92.2\n68.4\n73.7\n300M\n20K\n33.6\n37.5\n44.7\n51.1\n58.3\n67.2\n38.5\n46.1\n300M\n30K\n40.0\n44.6\n51.7\n57.0\n70.4\n75.1\n44.2\n52.0\n300M\n60K\n48.2\n52.8\n55.9\n61.3\n80.2\n83.8\n52.0\n59.7\n300M\n120K\n54.4\n58.3\n60.9\n65.9\n84.2\n88.6\n57.6\n65.5\n300M\n400K\n63.1\n66.1\n65.8\n70.8\n90.2\n91.4\n64.5\n71.9\n300M\n1.2M\n66.5\n69.6\n68.2\n72.1\n92.3\n92.9\n68.4\n74.7\n300M\n2M\n67.9\n70.9\n68.1\n72.7\n92.7\n92.8\n70.0\n76.2\n1B\n20K\n33.6\n37.9\n45.6\n51.",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "42] requires an efﬁcient strategy to retrieve a large number of negative samples\nin the training session, that requires the dataset module to return an additional object\n(e.g., negative sample indices). To support such extensions, we design dataset wrappers\nto return input batch, targets, and a supplementary dictionary, that can be empty when\nnot used. For the above case, the additional object can be stored in the supplementary\ndictionary, and used when computing the contrastive loss. This desig",
  "output": "The text discusses an efficient strategy for retrieving numerous negative samples during training sessions. This requires the dataset module to return an additional object, such as negative sample indices. Dataset wrappers are designed to return input batch, targets, and a supplementary dictionary, which can be empty if not in use. In the given scenario, the additional object is stored in the supplementary dictionary and utilized when calculating the contrastive loss.",
  "summary_type": "reports"
}{
  "input": "s of different losses in Table 3. We see the improve-\nment brought by the distillation loss and the KL loss is not very signiﬁcant, but it can consistently\nfurther boost the performance of various models.\nComparisons of different sparsiﬁcation strategies.\nAs illustrated in Figure 2, the dynamic token\nsparsiﬁcation is unstructured. To discuss whether the dynamic sparsiﬁcation is better than other\nstrategies, we perform ablation experiments and the results are shown in Table 4. For the structural",
  "output": "The distillation loss and KL loss slightly improve the performance of various models, but not significantly. Dynamic token sparsification is unstructured. Ablation experiments were conducted to compare dynamic sparsification with other strategies, with results shown in Table 4.",
  "summary_type": "reports"
}{
  "input": "nstrated that jointly learning disparate tasks\ndoes not harm performance [11], measuring the net ben-\neﬁt for all tasks still depends on a perceived ad-hoc sim-\nilarity between them. The symbiotic interaction between\nthe two objectives is thereby obscured, making it difﬁcult\nto establish if one or both tasks beneﬁt from one another\n(i.e. if the relation is mutualistic, commensalistic or para-\nsitic). Some surprising and rather unexpected relationships\nbetween tasks have been reported in the lite",
  "output": "The text discusses the challenge of measuring the net benefit of jointly learning different tasks. The relationship between the tasks can be mutualistic, commensalistic, or parasitic, and it's difficult to determine which. Unexpected relationships between tasks have been reported.",
  "summary_type": "reports"
}{
  "input": "experimented with hyperparameter stability by changing augmentations, optimizers and learning\nrates one at a time from the best combination for each of the methodologies. In Fig. 4(a), we\ncompare the top-1 accuracy of SupCon loss against cross-entropy across changes in augmentations\n(RandAugment [6], AutoAugment [5], SimAugment [3], Stacked RandAugment [49]); optimizers\n(LARS, SGD with Momentum and RMSProp); and learning rates. We observe signiﬁcantly lower\nvariance in the output of the contrast",
  "output": "The text discusses an experiment on hyperparameter stability, where augmentations, optimizers, and learning rates were individually altered. The top-1 accuracy of SupCon loss was compared against cross-entropy across different augmentations (RandAugment, AutoAugment, SimAugment, Stacked RandAugment), optimizers (LARS, SGD with Momentum and RMSProp), and learning rates. The results showed a significantly lower variance in the output of the contrast.",
  "summary_type": "reports"
}{
  "input": "nd\nHong-\nYuan Mark Liao. Yolov4: Optimal speed and accuracy of\nobject detection. arXiv preprint arXiv:2004.10934, 2020. 7\n[5] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and\nLarry S. Davis. Soft-nms – improving object detection with\none line of code. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), Oct 2017. 6, 9\n[6] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-\ning into high quality object detection. In Proceedings of the\nIEEE Conference on Computer Vis",
  "output": "The text discusses YOLOv4, an optimal speed and accuracy object detection system by Hong-Yuan Mark Liao. It also mentions Soft-NMS, a method to improve object detection with a single line of code by Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S. Davis. Lastly, it refers to Cascade R-CNN, a high-quality object detection system by Zhaowei Cai and Nuno Vasconcelos.",
  "summary_type": "reports"
}{
  "input": "and model that were\nused in the development of both of those algorithms. LAMB is a variant of Adam and performs about\nthe same as Adam with the same value of ϵ; LARS is more analogous to Momentum and indeed\nNesterov momentum and LARS have similar performance.\n5\nDiscussion\nOur results show that standard, generic optimizers sufﬁce for achieving strong results across batch\nsizes. Therefore, any research program to create new optimizers for training at larger batch sizes\nmust start from the fact th",
  "output": "The text discusses two algorithms, LAMB and LARS, used in development. LAMB is similar to Adam, performing at the same level, while LARS is more like Momentum, with comparable performance to Nesterov momentum. The results suggest that standard, generic optimizers are sufficient for achieving strong results across different batch sizes. This implies that any new optimizer research for larger batch sizes should consider this fact.",
  "summary_type": "reports"
}{
  "input": "acy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at training time (negligible w.r.t. the total\nnumber of weights).\n• Our architecture with speciﬁc class-attention offers a more effective pro-\ncessing of the class embedding.\n2\n\n--------------------------------------------------\n• Our best CaiT models establish the new state of the art on Imagenet-\nReal [6] and Imagenet V2 matched frequency [52] with no additional\ntraining data. On ImageNet1k-val [54",
  "output": "The text discusses an image transformer architecture that adds a few thousand parameters during training, improving the processing of class embedding. The best models of this architecture, CaiT, set a new standard on ImageNet-Real and ImageNet V2 matched frequency, without requiring additional training data.",
  "summary_type": "reports"
}{
  "input": "ore interpretable\nand can lead to insights that we can learn from. We analyze\nthe RegNet design space and arrive at interesting ﬁndings\nthat do not match the current practice of network design.\nFor example, we ﬁnd that the depth of the best models is sta-\nble across compute regimes (∼20 blocks) and that the best\nmodels do not use either a bottleneck or inverted bottleneck.\nWe compare top REGNET models to existing networks\nin various settings. First, REGNET models are surprisingly\neffective in th",
  "output": "The RegNet design space analysis reveals unexpected findings that contradict current network design practices. The depth of the best models remains stable across compute regimes (approximately 20 blocks), and the top models do not employ either a bottleneck or inverted bottleneck. When compared to existing networks in various settings, RegNet models prove to be surprisingly effective.",
  "summary_type": "reports"
}{
  "input": "utional neural networks. In\nICML, 2019. 9\n[33] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou.\nTraining data-efﬁcient image transformers & distillation through attention.\narXiv preprint\narXiv:2012.12877, 2020. 8\n[34] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution\ndiscrepancy. arXiv preprint arXiv:1906.06423, 2020. 12\n[35] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blak",
  "output": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou worked on training data-efficient image transformers and distillation through attention (arXiv:2012.12877, 2020). Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou also addressed the train-test resolution discrepancy (arXiv:1906.06423, 2020). Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, and Blak are also mentioned but their work is not specified.",
  "summary_type": "reports"
}{
  "input": "ani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998–6008, 2017. 1,\n2, 4\n[65] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\nTan, Xinggang Wang, et al. Deep high-resolution represen-\ntation learning for visual recognition. IEEE transactions on\npattern analysis and machine intelligence,",
  "output": "The text refers to two academic papers. The first, \"Attention is all you need\" by Noam Shazeer et al., was presented at the Advances in Neural Information Processing Systems in 2017. The second paper, by Jingdong Wang et al., is titled \"Deep high-resolution representation learning for visual recognition\" and was published in the IEEE Transactions on Pattern Analysis and Machine Intelligence.",
  "summary_type": "reports"
}{
  "input": "r moder-\nate augmentation. Furthermore, this transition can slightly\nimprove the elite performance.\n3.3.2\nReplacing patch ﬂattening with step-wise patch\nembedding\nDeiT and ViT models directly encode the image pixels with\na patch embedding layer which is equivalent to a convolu-\ntion with large kernel size and stride (e.g., 16). This oper-\nation ﬂattens the image patches to a sequence of tokens so\nthat Transformers can handle images. However, patch ﬂat-\ntening impairs the position information wit",
  "output": "The text discusses a transition in image processing models like DeiT and ViT. These models encode image pixels with a patch embedding layer, equivalent to a convolution with a large kernel size and stride. This operation transforms image patches into a sequence of tokens for Transformers to handle. However, patch flattening can impair the position information. The transition can moderately enhance and slightly improve elite performance.",
  "summary_type": "reports"
}{
  "input": "conversely how it is difﬁcult to compare different pro-\ncedures with a single architecture. We choose ResNet-50 and DeiT-S. The latter [45] is\nessentially a ViT parameterized so that it has approximately the same number of pa-\nrameters as a ResNet-50. For each architecture, we have put a signiﬁcant effort in opti-\nmizing the procedure to maximize the performance on Imagenet-val with the same 300\nepochs training schedule and same batch size. Under this constraint, the best training\nprocedure tha",
  "output": "The text discusses the difficulty of comparing different procedures with a single architecture. It mentions the use of ResNet-50 and DeiT-S architectures, with efforts made to optimize the procedure to maximize performance on Imagenet-val. This is done under the same 300 epochs training schedule and batch size. The text cuts off before revealing the best training procedure.",
  "summary_type": "reports"
}{
  "input": "last blocks of c3,c4) by RepMLP Bottlenecks with\nh = w = 6, r = 2, g = 4.\nFor training, we use a batch size of 512, momentum of\n0.9, AM-Softmax loss [28], and weight decay following\n[3]. All the models are trained for 420k iterations with a\nlearning rate beginning with 0.1 and divided by 10 at 252k,\n364k and 406k iterations. For evaluation, we report the top-\n1 accuracy on MegaFace. Table. 5 shows that FaceRes-\n\n--------------------------------------------------\nTable 5: Results of face recognit",
  "output": "The text discusses the use of RepMLP Bottlenecks in training models with specific parameters (h = w = 6, r = 2, g = 4). The models are trained for 420k iterations, with a learning rate starting at 0.1 and divided by 10 at specified intervals. The training uses a batch size of 512, momentum of 0.9, AM-Softmax loss, and weight decay. The models' performance is evaluated based on top-1 accuracy on MegaFace. The results are presented in Table 5.",
  "summary_type": "reports"
}{
  "input": "/5shot IN/10shot IN/25shot\nV-MoE-S/32\nNo\n0\n37.53\n51.75\n54.97\n57.44\nV-MoE-S/32\nYes\n0\n34.07\n49.34\n52.21\n55.11\nV-MoE-S/32\nYes\n1\n35.63\n51.95\n55.79\n58.19\nV-MoE-S/32\nYes\n2\n36.72\n53.09\n56.50\n58.84\n20\n\n--------------------------------------------------\n1\n2\n3\n4\n5\n6\n7\n8\n9\nK (Selected Experts per Token)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nImageNet/1shot\n1\n2\n3\n4\n5\n6\n7\n8\n9\nK (Selected Experts per Token)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nImageNet/5shot\n1\n2\n3\n4\n5\n6\n7\n8\n9\nK (Selected Experts per Token)\n0.0\n0.1\n0.2\n0.",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "image patches based on encoding vectors. For\ndownstream tasks (such as image classiﬁcation, and semantic segmentation), we append task layers\nupon pretrained BEIT and ﬁne-tune the parameters on the speciﬁc datasets.\n2.1\nImage Representations\nThe images have two views of representations in our method, namely, image patch, and visual tokens.\nThe two types serve as input and output representations during pre-training, respectively.\n2.1.1\nImage Patch\nThe 2D image is split into a sequence of patches",
  "output": "The text discusses a method involving pretrained BEIT for tasks like image classification and semantic segmentation. This method uses two types of image representations: image patch and visual tokens, which serve as input and output during pre-training. The 2D image is divided into a sequence of patches.",
  "summary_type": "reports"
}{
  "input": "with the 4000 labeled training samples as\nsupport. We report the mean top-1 accuracy and standard deviation across 5 seeds for the 4000 label split.\nsharpening temperature of T = 0.25. To construct the different image views, we use the multi-crop strategy, generating two large\ncrops (32 × 32), and six small crops (18 × 18) of each unlabeled image. We use the RandomResizedCrop method from the\ntorchvision.transforms module in PyTorch. The two large-crops (global views) are generated with scale (0",
  "output": "The text discusses a method using 4000 labeled training samples. The mean top-1 accuracy and standard deviation are reported across 5 seeds for the 4000 label split. A sharpening temperature of T=0.25 is used. Different image views are constructed using a multi-crop strategy, generating two large crops (32x32) and six small crops (18x18) of each unlabeled image. The RandomResizedCrop method from the torchvision.transforms module in PyTorch is used. The two large-crops are generated with a scale of 0.",
  "summary_type": "reports"
}{
  "input": "o the ImageNet ILSVRC 2012 validation set, we test it on\nthe ImageNet-ReaL test set [6]. On this test set, our model\nalso works well and achieves 91.02% Precision@1 which is\n0.4% better than Vision Transformer [14]. This gap is also\nbigger than the gap between Vision Transformer and Noisy\nStudent which is only 0.17%.\nA lite version of Meta Pseudo Labels.\nGiven the expen-\nsive training cost of Meta Pseudo Labels, we design a lite ver-\nsion of Meta Pseudo Labels, termed Reduced Meta Pseudo\nLabels.",
  "output": "The model tested on the ImageNet-ReaL test set achieved 91.02% Precision@1, outperforming the Vision Transformer by 0.4%. This difference is larger than the gap between Vision Transformer and Noisy Student, which is only 0.17%. Due to the high training cost of Meta Pseudo Labels, a lite version, called Reduced Meta Pseudo Labels, has been designed.",
  "summary_type": "reports"
}{
  "input": "ion.\nIn this work, we present new base-\nlines by improving the original Pyramid Vision Transformer\n(abbreviated as PVTv1) by adding three designs, including\n(1) overlapping patch embedding, (2) convolutional feed-\nforward networks, and (3) linear complexity attention lay-\ners.\nWith these modiﬁcations, our PVTv2 signiﬁcantly im-\nproves PVTv1 on three tasks e.g., classiﬁcation, detection,\nand segmentation.\nMoreover, PVTv2 achieves compara-\nble or better performances than recent works such as Swin",
  "output": "This work introduces improvements to the original Pyramid Vision Transformer (PVTv1) by adding three designs: overlapping patch embedding, convolutional feed-forward networks, and linear complexity attention layers. These modifications significantly enhance PVTv1's performance in classification, detection, and segmentation tasks. The updated version, PVTv2, achieves comparable or superior results to recent works like Swin.",
  "summary_type": "reports"
}{
  "input": "we proposed a Feature Coupling Unit to in-\nteract the local features and global representations in each\nblock to progressively align the features to ﬁll the seman-\ntic gap. To validate whether fusion should be done in each\nblock, we conduct experiments on fusion intervals and re-\nport the performance on ImageNet in Tab. 11. From Tab. 11,\none can see that smaller fusion intervals report higher per-\nformance, implying that frequent interaction facilities the\nrepresentation learning.\nInterval #Par",
  "output": "The text discusses a Feature Coupling Unit designed to interact local features and global representations in each block, aiming to progressively align features and bridge the semantic gap. Experiments on fusion intervals were conducted, with results on ImageNet shown in Tab. 11. The findings suggest that smaller fusion intervals yield higher performance, indicating that frequent interaction aids representation learning.",
  "summary_type": "reports"
}{
  "input": "he worst case to do\nbackpropagation for each given sentence. In this section,\nwe set a small m = 2 since the models are already well-\nregularized by other regularization techniques.\nImplement Details\nThe PTB corpus (Marcus et al.,\n1993) is a standard dataset for benchmarking language\nmodels. It consists of 923k training, 73k validation and\n82k test words. We use the processed version provided by\nMikolov et al. (2010) that is widely used for PTB.\nThe WT2 dataset is introduced in Merity et al. (20",
  "output": "The text discusses backpropagation in language models, using a small m=2 due to existing regularization techniques. The PTB corpus, a standard dataset for benchmarking language models, is used. It has 923k training, 73k validation, and 82k test words. The processed version by Mikolov et al. (2010) is utilized. The WT2 dataset is also mentioned.",
  "summary_type": "reports"
}{
  "input": "ntly\nprevents many researchers from being able to provide in-\nsight. This not only limits the ability to apply models in\ndifferent domains, but also limits reproducibility, making\nour ﬁeld vulnerable to the reproducibility crisis. Veriﬁca-\ntion of state of the art machine learning algorithms should\nnot be limited to those with large infrastructures and com-\nputational resources. Throughout the history of research,\nwe have learned that the speed of scientiﬁc advancement is\ndirectly proportional t",
  "output": "The text discusses the limitations in applying machine learning models across different domains due to lack of insight from researchers. This also affects reproducibility, making the field susceptible to a reproducibility crisis. The text argues that verifying advanced machine learning algorithms should not be exclusive to those with extensive infrastructures and computational resources. It suggests that the pace of scientific progress is directly related to these factors.",
  "summary_type": "reports"
}{
  "input": "in describing the mechanisms by which assimilation and accommodation occur to\nproduce mental representations of semantic concepts in sensorimotor observations [51]. It is perhaps for this reason that Piaget\nwas especially interested in the emerging ﬁeld of cybernetics (a precursor to artiﬁcial intelligence developed in the 40’s by Norbert\nWiener) and has gone so far as to say that “Life is essentially auto-regulation,” and “cybernetic models are, so far, the only ones\nthrowing any light on the",
  "output": "Piaget's work discusses how assimilation and accommodation create mental representations of semantic concepts in sensorimotor observations. He showed interest in the emerging field of cybernetics, a precursor to artificial intelligence developed in the 1940s by Norbert Wiener. Piaget believed that life is essentially auto-regulation and that cybernetic models are the only ones providing insight into this.",
  "summary_type": "reports"
}{
  "input": "laced with the LeFF.\nThe structure is given in Figure 3.\nA LeFF module performs following procedures. First,\ngiven tokens xh\nt ∈R(N+1)×C generated from the preced-\ning MSA module, we split them into patch tokens xh\np ∈\nR(N+1)×C and a class token xh\nc ∈RC accordingly. A lin-\near projection is conducted to expand the embeddings of\npatch tokens to a higher dimension of xl1\np ∈RN×(e×C),\nwhere e is the expand ratio. Second, the patch tokens are\nrestored to “images” of xs\np ∈R\n√\nN×\n√\nN×(e×C) on spatia",
  "output": "The text describes a LeFF module's procedures. It takes tokens generated from a preceding MSA module and splits them into patch tokens and a class token. A linear projection expands the patch tokens' embeddings to a higher dimension. The patch tokens are then restored to \"images\" on spatial dimensions.",
  "summary_type": "reports"
}{
  "input": "F. Massa, A. Sablayrolles, and H. Jégou. Training data-efﬁcient\nimage transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.\n[50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\nI. Polosukhin. Attention is all you need. In NeurIPS, 2017.\n[51] A. Vaswani, P. Ramachandran, A. Srinivas, N. Parmar, B. Hechtman, and J. Shlens. Scaling\nlocal self-attention for parameter efﬁcient visual backbones. arXiv preprint arXiv:2103.12731,\n20",
  "output": "The text references three academic papers. The first, by Massa, Sablayrolles, and Jégou, discusses training data-efficient image transformers and distillation through attention. The second, by Vaswani et al., presents the concept that 'Attention is all you need' in the NeurIPS conference in 2017. The third, also by Vaswani and team, explores scaling local self-attention for parameter efficient visual backbones.",
  "summary_type": "reports"
}{
  "input": "t [93] is a popular benchmark for object\ndetection and instance segmentation. It contains more than\n200,000 images with over 500,000 annotated object instances\nfrom 80 categories.\nMMDetection [47] is a widely-used toolkit for object\ndetection and instance segmentation. We conducted our\nobject detection and instance segmentation experiments\nusing MMDetection with a RestNet-50 backbone, applied to\nthe COCO dataset. We only added our external attention at\nthe end of Resnet stage 4. Results in Tabs.",
  "output": "The COCO dataset, a popular benchmark for object detection and instance segmentation, contains over 200,000 images with more than 500,000 annotated object instances across 80 categories. MMDetection, a widely-used toolkit for these tasks, was used in experiments with a RestNet-50 backbone applied to this dataset. An external attention was added at the end of Resnet stage 4.",
  "summary_type": "reports"
}{
  "input": "(or 1\n2\nof an ISAB-like block) whose output size is relatively large\n(e.g. 512) and task-independent rather than determined by\nthe task; it would be 1 (for classiﬁcation) if used as pro-\nposed in the Set Transformer. This is followed by a very\ndeep stack of (latent) self-attention blocks and a ﬁnal aver-\nage and project. In other words, Perceivers exploit similar\nprimitives to the Set Transformer, but compose them differ-\nently, in service of building an architecture with improved\nscaling prope",
  "output": "The Perceiver model uses a block with a large, task-independent output size, followed by a deep stack of self-attention blocks and a final average and project. It utilizes similar elements to the Set Transformer but arranges them differently to create an architecture with improved scaling properties.",
  "summary_type": "reports"
}{
  "input": "e, and Steven W. Su. Differentiable neural architecture\nsearch in equivalent space with exploration enhancement. In\nNeurIPS, 2020. 12\n[83] Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, and\nSteven W. Su. Overcoming multi-model forgetting in one-shot\nNAS with diversity maximization. In CVPR, 2020. 12\n[84] Mingyang Zhang and Linlin Ou. Stage-wise channel pruning\nfor model compression. arXiv:2011.04908, 2020. 2, 3\n[85] Man Zhang, Yong Zhou, Jiaqi Zhao, Shixiong Xia, Jiaqi\nWang, and Zizheng Huang.",
  "output": "The text refers to various research papers. Paper [83] by Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, and Steven W. Su discusses overcoming multi-model forgetting in one-shot NAS with diversity maximization, presented at CVPR 2020. Paper [84] by Mingyang Zhang and Linlin Ou is about stage-wise channel pruning for model compression, available on arXiv. The authors of paper [85] are Man Zhang, Yong Zhou, Jiaqi Zhao, Shixiong Xia, Jiaqi Wang, and Zizheng Huang, but the topic is not mentioned.",
  "summary_type": "reports"
}{
  "input": "rence time (see Sec. 6, supplementary)\nand parameter count.\nFinally, in Fig. 14 we assess how the order of the polynomial\nqualitatively reﬂects in the reconstruction of an exemplary mesh.\nIn particular, we color code the per vertex reconstruction error on\nthe reconstructed meshes (right) and compare them with the input\n(left). Notice that the overall shape resembles the input more as we\nincrease the order of the polynomial (especially in the head), while\nbody parts with strong articulations (e.g",
  "output": "The text discusses a study on mesh reconstruction using varying orders of polynomial. The per vertex reconstruction error is color-coded for comparison. The results show that increasing the polynomial order improves the overall shape resemblance to the input, particularly in the head area. However, body parts with strong articulations still present challenges.",
  "summary_type": "reports"
}{
  "input": "and position-based\ninteractions in global, local or masked contexts. The resulting neural networks, LambdaNetworks,\nare computationally efﬁcient, model long-range dependencies at a small memory cost and can there-\nfore be applied to large structured inputs such as high resolution images.\nWe evaluate LambdaNetworks on computer vision tasks where works using self-attention are hin-\ndered by large memory costs (Wang et al., 2018; Bello et al., 2019), suffer impractical implemen-\ntations (Ramachand",
  "output": "LambdaNetworks are neural networks that efficiently model long-range dependencies with minimal memory cost, making them suitable for large structured inputs like high-resolution images. They are evaluated on computer vision tasks where self-attention methods are limited due to high memory costs or impractical implementations.",
  "summary_type": "reports"
}{
  "input": "oved. Additionally, we show the differ-\nence between training a model with these parameters and\ninferring on a model that was trained on 32×32 sized im-\nages, denoting the generalizability of the models. We note\nthat the number of parameters for these competing models\nis roughly the same, see Table 9.\nB. Hyperparameter tuning\nWe tuned the hyperparamters per experiment and ar-\nrived at the following for each table in Sec. 4. We deter-\nmined these hyper parameters through a parameter sweep.\nFor Re",
  "output": "The text discusses model training with specific parameters and the difference when inferring on a model trained on 32x32 sized images, highlighting the models' generalizability. It mentions that the number of parameters for these models is roughly the same. The text also discusses hyperparameter tuning per experiment, with the parameters determined through a parameter sweep.",
  "summary_type": "reports"
}{
  "input": "split the multi-heads into parallel groups and apply\ndifferent self-attention operations onto different groups. This parallel strategy introduces no extra\ncomputation cost while enlarging the area for computing self-attention within each Transformer block.\nThis strategy is fundamentally different from existing self-attention mechanisms [57, 39, 70, 26]\nthat apply the same attention operation across multi-heads((Figure 1 b,c,d,e), and perform different\nattention operations sequentially(Figure 1 c",
  "output": "The text discusses a strategy of dividing multi-heads into parallel groups and applying different self-attention operations to each group in a Transformer block. This method doesn't increase computation cost and expands the self-attention calculation area. It differs from existing self-attention mechanisms that use the same attention operation across multi-heads and perform different attention operations sequentially.",
  "summary_type": "reports"
}{
  "input": "CeiT architecture\nthat combines the advantages of CNNs in extracting low-\nlevel features, strengthening locality, and the advantages\nof Transformers in establishing long-range dependencies.\nCeiT obtains state-of-the-art performances on ImageNet\n\n--------------------------------------------------\nand various downstream tasks, without requiring a large\namount of training data and extra CNN teachers.\nBe-\nsides, CeiT models demonstrate better convergence than\npure Transformer with 3× fewer training",
  "output": "CeiT architecture merges the benefits of CNNs for low-level feature extraction and locality strengthening, with the advantages of Transformers for establishing long-range dependencies. CeiT achieves top-tier performances on ImageNet and other tasks, without needing a large amount of training data or extra CNN teachers. Additionally, CeiT models show better convergence than pure Transformer with three times less training.",
  "summary_type": "reports"
}{
  "input": "[19, 59] as explained in experiments. Based on our experiments, it is\ninteresting to ﬁnd that, similar to our observations in image classiﬁcation, a carefully-designed block\nde-aggregation makes the model signiﬁcantly more effective in performance.\nAlgorithm 1 Gradient-based class-aware tree traversal (GradGAT).\nDeﬁne: Al denotes the feature maps at hierarchy l. Yc is the logit of predicted class c. [·]2×2 indexes one of\n2 × 2 partitions of input maps.\nInput: {Al|l = 2, ..., Td}, αTd = ATd, P =",
  "output": "The text discusses an experiment where a carefully-designed block de-aggregation significantly improves the model's performance in image classification. It introduces the Gradient-based class-aware tree traversal (GradGAT) algorithm, which uses feature maps at hierarchy l and the logit of predicted class c. The algorithm also involves 2x2 partitions of input maps.",
  "summary_type": "reports"
}{
  "input": "eeps\nthe same number of spatial tokens overall layer of the net-\nwork. Although the self-attention operation is not limited\nby spatial distance, the size of the spatial area participat-\ning in attention is affected by the spatial size of the feature.\nTherefore, in order to adjust the dimension conﬁguration\nlike ResNet, a spatial reduction layer is also required in ViT.\nTo utilize the advantages of the dimension conﬁgura-\ntion to ViT, we propose a new architecture called Pooling-\nbased Vision Tra",
  "output": "The text discusses a new architecture called Pooling-based Vision Transformer (ViT). The self-attention operation in ViT isn't limited by spatial distance, but the size of the spatial area participating in attention is influenced by the feature's spatial size. To adjust the dimension configuration like ResNet, a spatial reduction layer is required in ViT. The new architecture aims to utilize the benefits of the dimension configuration to ViT.",
  "summary_type": "reports"
}{
  "input": "s\nCIFAR-10\nSVHN\nImageNet\nSupervised\n97.18 ± 0.08\n98.17 ± 0.03\n84.49/97.18\nNoisyStudent\n98.22 ± 0.05\n98.71 ± 0.11\n85.81/97.53\nReduced Meta Pseudo Labels\n98.56 ± 0.07\n98.78 ± 0.07\n86.87/98.11\nTable 11: Image classiﬁcation accuracy of EfﬁcientNet-B0 on CIFAR-10 and SVHN, and EfﬁcientNet-B7 on ImageNet. Higher is better.\nCIFAR-10 results are mean ± std over 5 runs, and ImageNet results are top-1/top-5 accuracy of a single run. All numbers are produced in\nour codebase and are controlled experiments.",
  "output": "The table shows image classification accuracy of EfficientNet-B0 on CIFAR-10 and SVHN, and EfficientNet-B7 on ImageNet. The results indicate that the Reduced Meta Pseudo Labels method has the highest accuracy with 98.56 ± 0.07 on CIFAR-10, 98.78 ± 0.07 on SVHN, and 86.87/98.11 on ImageNet. The CIFAR-10 results are averaged over 5 runs, while ImageNet results are top-1/top-5 accuracy of a single run.",
  "summary_type": "reports"
}{
  "input": "g the Reﬁned-ViT model.\n4\nExperiments\n4.1\nExperiment Setup\nWe mainly evaluate Reﬁner for the image classiﬁcation task. Besides, we also conduct experiments\non natural language processing tasks to investigate its generalizability to NLP transformer models.\nComputer vision\nWe evaluate the effectiveness of the reﬁner on ImageNet [15]. For a fair com-\nparison with other methods, we ﬁrst replace the SA module with reﬁner on ViT-Base [19] model\nas it is the most frequently used one [19, 48, 61, 49]. W",
  "output": "The text discusses an experiment evaluating the Reﬁner for image classification tasks and its generalizability to NLP transformer models. The effectiveness of the reﬁner is tested on ImageNet. For fair comparison, the SA module is replaced with reﬁner on the frequently used ViT-Base model.",
  "summary_type": "reports"
}{
  "input": "mputer Vision (ECCV).\nSpringer, 2016, pp. 87–102. 8\n[75] J. Deng, J. Guo, D. Zhang, Y. Deng, X. Lu, and S. Shi, “Lightweight face\nrecognition challenge,” in CVPRW, 2019, pp. 0–0. 8\n[76] J. Deng, J. Guo, Y. Zhou, J. Yu, I. Kotsia, and S. Zafeiriou, “Retinaface:\nSingle-stage dense face localisation in the wild,” Conference on Computer\nVision and Pattern Recognition (CVPR), 2020. 8\n\n--------------------------------------------------\n13\n[77] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller, “L",
  "output": "The text refers to various academic papers on computer vision and face recognition. It mentions a \"Lightweight face recognition challenge\" presented at CVPRW 2019. Another paper discussed is \"RetinaFace: Single-stage dense face localisation in the wild,\" presented at the Conference on Computer Vision and Pattern Recognition (CVPR) in 2020. The text ends abruptly, mentioning an author named G. B. Huang, but provides no further information about the paper.",
  "summary_type": "reports"
}{
  "input": "l self-attention,\nwe abstain from enforcing translation equivariance in lieu\nof better hardware utilization, which improves the speed-\naccuracy tradeoff (Section 2.2). Also note that while we use\nlocal attention, our receptive ﬁelds per pixel are quite large\n(up to 18 × 18) and we show in Section 4.2.2 that larger\nreceptive ﬁelds help with larger images. In the remainder\nof this section, we will motivate self-attention for vision\ntasks and describe how we relax translational equivariance\nto efﬁc",
  "output": "The text discusses the use of self-attention in vision tasks, favoring better hardware utilization over enforcing translation equivariance for improved speed-accuracy tradeoff. It mentions the use of large receptive fields per pixel (up to 18x18), which are beneficial for larger images. The text also indicates a plan to explain how translational equivariance is relaxed for efficiency.",
  "summary_type": "reports"
}{
  "input": "ter and FLOPs efﬁciency.\nTop-1 Acc.\nParams\nFLOPs\nEfﬁcientNet-B6 (Tan & Le, 2019a)\n84.6%\n43M\n19B\nResNet-RS-420 (Bello et al., 2021)\n84.4%\n192M\n64B\nNFNet-F1 (Brock et al., 2021)\n84.7%\n133M\n36B\n3.2. Understanding Training Efﬁciency\nWe study the training bottlenecks of EfﬁcientNet (Tan & Le,\n2019a), henceforth is also called EfﬁcientNetV1, and a few\nsimple techniques to improve training speed.\nTraining with very large image sizes is slow: As pointed\nout by previous works (Radosavovic et al., 2020),",
  "output": "The text compares the performance of three models: EfficientNet-B6, ResNet-RS-420, and NFNet-F1. EfficientNet-B6 has an accuracy of 84.6% with 43M parameters and 19B FLOPs. ResNet-RS-420 has an accuracy of 84.4% with 192M parameters and 64B FLOPs. NFNet-F1 has the highest accuracy at 84.7% with 133M parameters and 36B FLOPs. The text also mentions a study on training bottlenecks of EfficientNet and techniques to improve training speed.",
  "summary_type": "reports"
}{
  "input": "ed the design of BoTNet for detection and\nsegmentation, it is a natural question to ask whether the\nBoTNet architecture design also helps improve the image\nclassiﬁcation performance on the ImageNet [52] benchmark.\nPrior work [65] has shown that adding Non-Local blocks\nto ResNets and training them using canonical settings does\nnot provide substantial gains. We observe a similar ﬁnd-\ning for BoTNet-50 when contrasted with ResNet-50, with\nboth models trained with the canonical hyperparameters for\nI",
  "output": "The text discusses the design of BoTNet for image detection and segmentation. It questions if the BoTNet architecture also enhances image classification performance on the ImageNet benchmark. Previous studies showed that adding Non-Local blocks to ResNets and training them with standard settings doesn't significantly improve results. A similar observation was made for BoTNet-50 compared to ResNet-50, both trained with standard hyperparameters.",
  "summary_type": "reports"
}{
  "input": "accuracy, compactness and efﬁciency of MUXNet\nand other baselines. Overall, MUXNet signiﬁcantly out-\nperforms previous methods on both CIFAR-10 and -100\ndatasets, indicating that our models also transfer well to\nother similar tasks. In particular, MUXNet-m achieves 1%\nhigher accuracy than NASNet-A mobile with 3× fewer pa-\nrameters while being 2× more efﬁcient in MAdds.\n5.4.2\nChestX-Ray14\nThe ChestX-Ray14 benchmark was recently introduced\nin [43]. The dataset consists of 112,120 high resolution\nf",
  "output": "MUXNet significantly outperforms previous methods on CIFAR-10 and -100 datasets, indicating good transferability to similar tasks. Specifically, MUXNet-m achieves 1% higher accuracy than NASNet-A mobile with 3 times fewer parameters and is twice as efficient in MAdds. The ChestX-Ray14 benchmark, consisting of 112,120 high resolution images, was recently introduced.",
  "summary_type": "reports"
}{
  "input": ".\nThey can be broadly classiﬁed into two categories depending on\nwhether the supernet training is coupled with architecture search\nor decoupled into a two-stage process. Approaches of the former\nkind [24], [26], [46] are computationally efﬁcient but return sub-\noptimal models. Numerous studies [47], [48], [49] allude to weak\ncorrelation between performance at the search and ﬁnal evaluation\nstages. Methods of the latter kind [10], [31], [50] use performance\nof subnets (obtained by sampling the tr",
  "output": "Supernet training can be classified into two categories: coupled with architecture search or decoupled into a two-stage process. The former is computationally efficient but yields sub-optimal models. Many studies suggest a weak correlation between search and final evaluation performance. The latter method uses performance of subnets obtained by sampling.",
  "summary_type": "reports"
}{
  "input": "Twins: Revisiting the Design of Spatial Attention in\nVision Transformers\nXiangxiang Chu1,\nZhi Tian2,\nYuqing Wang1,\nBo Zhang1,\nHaibing Ren1,\nXiaolin Wei1,\nHuaxia Xia1,\nChunhua Shen2∗\n1 Meituan Inc.\n2 The University of Adelaide, Australia\n1 {chuxiangxiang,wangyuqing06,zhangbo97,renhaibing,weixiaolin02,xiahuaxia}@meituan.com\n2 zhi.tian@outlook.com, chunhua@me.com\nAbstract\nVery recently, a variety of vision transformer architectures for dense prediction\ntasks have been proposed and they show that th",
  "output": "The text discusses a study on the design of spatial attention in Vision Transformers. The authors, from Meituan Inc. and The University of Adelaide, Australia, have recently proposed various vision transformer architectures for dense prediction tasks.",
  "summary_type": "reports"
}{
  "input": "], and in Figure 37 we compare directly to their results. V-MoE\nseems initially fairly robust to reduced data, but after reducing to 9M pre-training samples (3% of the\ndataset), it becomes slightly preferable to instead train a dense model.\nTraining on ImageNet21k.\nImageNet21k [16] is a large public dataset with approximately 14M\nimages and 21k classes. Previous works [20, 36] have successfully pre-trained on it to achieve\nstrong results in downstream tasks. In particular, dense ViT models train",
  "output": "The text discusses the performance of the V-MoE model with reduced data, indicating it becomes preferable to train a dense model after reducing to 9M pre-training samples. It also mentions ImageNet21k, a large public dataset with about 14M images and 21k classes, which has been used successfully in previous works for pre-training to achieve strong results in downstream tasks, particularly with dense ViT models.",
  "summary_type": "reports"
}{
  "input": "he order of the polynomial (especially in the head), while\nbody parts with strong articulations (e.g. hands) are reconstructed\nwith higher ﬁdelity.\nFig. 13: ProdPoly vs 1st order graph learnable operators for mesh\nautoencoding. Note that even without using activation functions the\nproposed methods signiﬁcantly improve upon the state-of-the-art.\n5\nFUTURE DIRECTIONS\nThe new class of Π´nets has strong experimental results and few\nempirical theoretical results already. We expect in the following\nyea",
  "output": "The text discusses a new class of Π´nets that have shown strong experimental results and some empirical theoretical results. These nets are used for mesh autoencoding, improving upon the state-of-the-art even without activation functions. They are particularly effective in reconstructing body parts with strong articulations, like hands. Future directions for this technology are anticipated.",
  "summary_type": "reports"
}{
  "input": "s to learn sufﬁcient domain-\nrelated information but gives disordered ranking of paths.\nThis smaller correlation coefﬁcient implies that it might be\nnot sensible to implement greedy sampling from a random\nsupernet since the ranking evaluated by 1000 validation im-\nages will be rather noisy. In this way, we record the trend of\nrank correlation coefﬁcients with uniform sampling in right\nFigure 4. It shows that with more iterations, the correla-\ntion coefﬁcients increase and at 10K iteration, they",
  "output": "The text discusses a method that learns domain-related information but results in disordered path ranking. It suggests that implementing greedy sampling from a random supernet may not be sensible due to noise from evaluation by 1000 validation images. The trend of rank correlation coefficients with uniform sampling is recorded, showing that with more iterations, the coefficients increase, particularly at 10K iteration.",
  "summary_type": "reports"
}{
  "input": "an be either shared\nor unshared across different heads. We show the effects of\nthese two schemes in bias and contextual modes in Tab.\n2, respectively. For bias mode, the accuracy drops signif-\nicantly when sharing encoding across the heads. By con-\ntrast, in contextual mode, the performance gap between two\nschemes is negligible. Both of them achieve an average\ntop-1 accuracy of 80.9%. We conjecture that different head\nMode\nShared\n#Param.\nMACs\nTop-1\n(M)\n(M)\nAcc(%)\nBias\n×\n22.05\n4613\n80.54 ± 0.06\n✓",
  "output": "The text discusses two schemes: shared and unshared across different heads. In bias mode, accuracy significantly decreases when encoding is shared across heads. In contrast, in contextual mode, the performance difference between the two schemes is negligible, both achieving an average top-1 accuracy of 80.9%. The shared mode in bias has a top-1 accuracy of 80.54% with 22.05M parameters and 4613M MACs.",
  "summary_type": "reports"
}{
  "input": "evel features like\nedges or textures compared with ResNet and more seman-\ntics compared with Pure-MLP Baseline .\n5. Conclusion\nIn this paper, we analyze the constraints of current MLP-\nbased models for visual representation learning: 1. Spa-\ntial MLPs only take inputs with ﬁxed resolutions, making\ntransfer to downstream tasks, such as object detection and\nsegmentation, difﬁcult. 2. Single-stage design and fully\nconnected layers further constrain usage due to the added\ncomplexity. To tackle these",
  "output": "The paper discusses the limitations of current MLP-based models for visual representation learning. These include spatial MLPs only accepting fixed resolution inputs, making transfer to tasks like object detection and segmentation challenging. Also, the single-stage design and fully connected layers add complexity, further limiting usage.",
  "summary_type": "reports"
}{
  "input": "s), and skip\nconnections [20]. More recently, building models solely on\nMLPs and skip connections without the self-attention lay-\ners has been a new trend for visual recognition tasks [49,\n50, 35].\n𝐻×𝑊\n𝐶\n(a) Channel FC\n𝐻×𝑊\n𝐶\n(b) Spatial FC\n𝐻×𝑊\n𝐶\n(c) Cycle FC\nFC\nO(HW)\nScale\nVariable\nImgNet\nTop-1\nCOCO\nAP\nADE20K\nmIoU\nChannel\nHW\n\u0013\n79.4\n35.0\n36.3\nSpatial\nH2W 2\n\u0017\n80.9\n\u0017\n\u0017\nCycle\nHW\n\u0013\n81.6\n41.7\n42.4\nFigure 1: Motivation of Cycle Fully-Connected Layer\n(FC) compared to Channel FC and Spatial FC. (a) Chan-",
  "output": "The text discusses a trend in visual recognition tasks of building models solely on MLPs and skip connections, without self-attention layers. It compares the effectiveness of three types of Fully-Connected Layers (FC): Channel FC, Spatial FC, and Cycle FC. The Cycle FC appears to perform better in terms of ImgNet Top-1, COCO AP, and ADE20K mIoU metrics.",
  "summary_type": "reports"
}{
  "input": "iv:1912.02781, 2020. 18\n[16] Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Youngjung\nUh, and Jung-Woo Ha. Adamp: Slowing down the weight norm increase in momentum-\nbased optimizers. arXiv preprint arXiv:2006.08217, 2020. 21\n[17] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry.\nAugment your batch: Improving generalization through instance repetition. In Conference\non Computer Vision and Pattern Recognition, 2020. 5\n[18] Grant Van Horn,",
  "output": "The text refers to various academic papers. \"Adamp: Slowing down the weight norm increase in momentum-based optimizers\" by Byeongho Heo et al. discusses a method to improve momentum-based optimizers. \"Augment your batch: Improving generalization through instance repetition\" by Elad Hoffer et al. presents a technique to enhance generalization in computer vision and pattern recognition. The text does not provide information about the work of Grant Van Horn.",
  "summary_type": "reports"
}{
  "input": "ø\nø\n(b) Image-relative coordinates\nFigure 4. For ImageNet experiments, we generate position encod-\nings using [-1, 1]-normalized (x, y)-coordinates drawn from (a)\ncrops rather than from (b) the raw images, as we ﬁnd the latter\nleads to overﬁtting.\nB. Ablations\nTo illustrate the effect of various network hyperparameters,\nwe considered a small Perceiver model and swept a num-\nber of options around it. Unlike ConvNets, each module\nin a Perceiver-based architecture takes as input the full in-\nput by",
  "output": "In ImageNet experiments, position encodings are generated using normalized (x, y)-coordinates from crops, not raw images, to avoid overfitting. The effect of network hyperparameters is demonstrated using a small Perceiver model. Unlike ConvNets, each module in a Perceiver-based architecture uses the full input.",
  "summary_type": "reports"
}{
  "input": "(xu, byu) to update its parameter θS. In expectation, the student’s new parameter is\nEbyu∼T (xu;θT )\n\u0002\nθS−ηS∇ηSCE(byu, S(xu; θS))\n\u0003\n. We will update the teacher’s parameter to minimize the student’s cross-entropy\non a batch of labeled data a this expected parameter. To this end, we need to compute the Jacobian:\n∂R\n∂θT\n|{z}\n1×|T |\n=\n∂\n∂θT\nCE\n\u0010\nyl, S\n\u0010\nxl; Ebyu∼T (xu;θT )\n\u0002\nθS −ηS∇ηSCE(byu, S(xu; θS))\n\u0003\u0011\u0011\n(4)\nTo simplify our notation, let us deﬁne\n¯θ′\nS\n|{z}\n|S|×1\n= Ebyu∼T (xu;θT )\n\u0002\nθS −ηS∇ηSCE(b",
  "output": "The text discusses a method of updating parameters in a machine learning model. The student's new parameter is updated using a cross-entropy function. The teacher's parameter is then updated to minimize the student's cross-entropy on a batch of labeled data. This involves computing the Jacobian. The text also introduces a simplified notation for the expected parameter.",
  "summary_type": "reports"
}{
  "input": "eed-accuracy Pareto curve of LambdaResNets compared to EfﬁcientNets (Tan & Le, 2019) on\nTPUv3 hardware. In order to isolate the beneﬁts of lambda layers, we additionally compare against\nthe same architectures when replacing lambda layers by (1) standard 3x3 convolutions (denoted\nResNet-RS wo/ SE) and (2) 3x3 convolutions with squeeze-and-excitation (denoted ResNet-RS w/\nSE). All architectures are trained for 350 epochs using the same regularization methods and evalu-\nated at the same resolution",
  "output": "The text compares the speed-accuracy Pareto curve of LambdaResNets and EfficientNets on TPUv3 hardware. It also evaluates the impact of replacing lambda layers with standard 3x3 convolutions (ResNet-RS without SE) and 3x3 convolutions with squeeze-and-excitation (ResNet-RS with SE). All architectures are trained for 350 epochs using the same regularization methods and evaluated at the same resolution.",
  "summary_type": "reports"
}{
  "input": "r visual representation.\nHowever, transformer-based vision models, including\nAutoFormer, now are still inferior to the models based on\ninverted residual blocks [44], such as MobileNetV3 [21]\nand EfﬁcientNet [49]. The reason is that inverted residu-\nals are optimized for edge devices, so the model sizes and\nFLOPs are much smaller than vision transformers.\n4.4. Transfer Learning Results\nClassiﬁcation.\nWe transfer Autoformer to a list of\ncommonly used recognition datasets: 1) general classiﬁca-\ntio",
  "output": "Transformer-based vision models like AutoFormer are currently less effective than models based on inverted residual blocks, such as MobileNetV3 and EfficientNet. This is because inverted residuals are optimized for edge devices, resulting in smaller model sizes and fewer FLOPs. AutoFormer has been transferred to commonly used recognition datasets for classification purposes.",
  "summary_type": "reports"
}{
  "input": "re\nfar below their true performances of training from scratch\n(see the right part of Fig. 4). This limits the ranking ca-\npacities of supernet. Furthermore, after the search, it is still\nnecessary to perform additional retraining for the searched\narchitectures since the weights are not fully optimized. In-\nspried by BigNAS [65] and slimmable networks [66, 64],\nwe propose the weight entanglement training strategy ded-\nicated to vision transformer architecture search. The cen-\ntral idea is to enab",
  "output": "The current supernet's ranking capacities are limited and underperform compared to training from scratch. Post-search, additional retraining is required for the searched architectures as the weights aren't fully optimized. Inspired by BigNAS and slimmable networks, a weight entanglement training strategy is proposed, specifically for vision transformer architecture search.",
  "summary_type": "reports"
}{
  "input": "approach, which let\nCNNs automatically ﬁgure out how to avoid being biased towards either shape or texture from their\ntraining samples. Speciﬁcally, we apply style transfer to generate cue conﬂict images, which breaks\nthe correlation between shape and texture, for augmenting the original training data. The most\nimportant recipe of training a successful shape-texture debiased model is that we need to provide\nsupervision from both shape and texture on these generated cue conﬂict images, otherwise",
  "output": "The text discusses a method to reduce bias in Convolutional Neural Networks (CNNs) towards shape or texture. This involves using style transfer to create cue conflict images, disrupting the correlation between shape and texture. These images are then used to augment the original training data. To successfully train a shape-texture debiased model, supervision from both shape and texture on these generated images is necessary.",
  "summary_type": "reports"
}{
  "input": "ween different tasks.\nThus, directly applying the vanilla transformer architecture\nto other tasks may be sub-optimal. It is natural that there\nexists better transformer architectures for image recogni-\ntion.\nHowever, hand-designing such an architecture is\ntime consuming since there are too many influential fac-\ntors to be considered. On one hand, Neural Architecture\nSearch (NAS) has achieved great progress in computer vi-\n\n--------------------------------------------------\nsion tasks [13, 22, 2]",
  "output": "The vanilla transformer architecture may not be ideal for all tasks, such as image recognition. Manually designing a better transformer architecture is time-consuming due to many influential factors. Neural Architecture Search (NAS) has shown significant progress in computer vision tasks.",
  "summary_type": "reports"
}{
  "input": "un. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 39:1137–1149, 2015. 1, 5, 6, 7, 14, 15\nSam Ringer, Will Williams, Tom Ash, Remi Francis, and David MacLeod. Texture bias of cnns\nlimits few-shot classiﬁcation performance, 2019. 1, 3\nRamprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,\nand Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradie",
  "output": "The text refers to three different studies. The first one discusses the Faster R-CNN, a method for real-time object detection using region proposal networks, published in IEEE Transactions on Pattern Analysis and Machine Intelligence in 2015. The second study by Sam Ringer et al., 2019, highlights the texture bias of Convolutional Neural Networks (CNNs) limiting few-shot classification performance. The third study introduces Grad-CAM, a technique for visual explanations from deep networks using gradients.",
  "summary_type": "reports"
}{
  "input": "Malik 2000). When\ninputs grow very large, this may introduce a bandwidth bot-\ntleneck. By using multiple cross-attentions, the Perceiver\ncan use a form of re-entrant processing to mitigate this ef-\nfect, by allowing ﬁrst-pass processing of an input to feed\nback and inﬂuence how the input is processed in subsequent\npasses. Re-entrant processing of this kind (sometimes re-\nferred to as top-down processing) has a long history in com-\nputer vision (Borenstein et al., 2004; Kumar et al., 2005;\nCarrei",
  "output": "The Perceiver uses multiple cross-attentions to manage large inputs and avoid bandwidth bottlenecks. This involves a re-entrant processing method where initial processing of an input influences how it's processed in subsequent passes. This approach, also known as top-down processing, has been widely used in computer vision.",
  "summary_type": "reports"
}{
  "input": "t of frozen patch embeddings\nxpatches. We discuss why we include xclass in the keys in Appendix B.\nConsidering a network with h heads and p patches, and denoting by d the\nembedding size, we parametrize the multi-head class-attention with several\nprojection matrices, Wq, Wk, Wv, Wo ∈Rd×d, and the corresponding biases\nbq, bk, bv, bo ∈Rd. With this notation, the computation of the CA residual block\nproceeds as follows. We ﬁrst augment the patch embeddings (in matrix form)\nas z = [xclass, xpatches]",
  "output": "The text discusses a network with h heads and p patches, denoted by d as the embedding size. It uses several projection matrices (Wq, Wk, Wv, Wo) and corresponding biases (bq, bk, bv, bo) to parameterize the multi-head class-attention. The computation of the Class-Attention (CA) residual block begins by augmenting the patch embeddings as z = [xclass, xpatches].",
  "summary_type": "reports"
}{
  "input": "9.17\n94.08\n95.95\n96\nResNet152x1\n7\n81.88\n87.96\n98.82\n90.22\n94.17\n96.94\n141\nResNet152x2\n7\n84.97\n89.69\n99.06\n92.05\n95.37\n98.62\n563\nResNet152x2\n14\n85.56\n89.89\n99.24\n91.92\n95.75\n98.75\n1126\nResNet200x3\n14\n87.22\n90.15\n99.34\n93.53\n96.32\n99.04\n3306\nR50x1+ViT-B/32\n7\n84.90\n89.15\n99.01\n92.24\n95.75\n99.46\n106\nR50x1+ViT-B/16\n7\n85.58\n89.65\n99.14\n92.63\n96.65\n99.40\n274\nR50x1+ViT-L/32\n7\n85.68\n89.04\n99.24\n92.93\n96.97\n99.43\n246\nR50x1+ViT-L/16\n7\n86.60\n89.72\n99.18\n93.64\n97.03\n99.40\n859\nR50x1+ViT-L/16\n14\n87.12\n89.76\n99",
  "output": "The text appears to be a comparison of different machine learning models' performances. ResNet152x1, ResNet152x2, ResNet200x3, and R50x1+ViT variants are evaluated with different parameters. The models' performance metrics are given, possibly including accuracy, precision, recall, and F1 score. The best performing model seems to be ResNet200x3 with a score of 99.34 on one of the metrics. The R50x1+ViT variants also perform well, with scores close to 99. The numbers following the model names could be the number of layers or other parameters. <NULL>",
  "summary_type": "reports"
}{
  "input": "(Left). Its intermediate performance in all\nobjectives indicate that this best trade-off solution makes a good\ncompromise on all 12 objectives among all 45 obtained solutions.\nIn Fig. 26 (Right), we compare this solution with different baseline\nmodels that are ﬁne-tuned to each dataset separately. Notably,\nour NATNet achieves better accuracy on all datasets with similar\nor less #MAdds than EfﬁcientNet-B0 [28], MobileNetV2 [56],\nNASNet-A [8], and ResNet-50 [3], making our highest trade-off\nsolut",
  "output": "The text discusses a solution, NATNet, that performs well across all 12 objectives compared to 45 other solutions. It is compared to different baseline models fine-tuned to each dataset. NATNet achieves better accuracy on all datasets with similar or fewer #MAdds than EfficientNet-B0, MobileNetV2, NASNet-A, and ResNet-50, making it the highest trade-off solution.",
  "summary_type": "reports"
}{
  "input": "es, each of which is denoted as Siloj with\na supply of one, as well as n demand sources, each of which is denoted as Millk with a demand\nof m\nn . Cj,k is the per-unit transportation cost from Siloj to Millk. Speciﬁcally, Cj,k = −Pj,k in\nWGM and Cj,k = −Sj,k in LGM. To make it clear, we illustrate this algorithm with a toy example,\nwhere the problem is simpliﬁed as Fig.6 (a) with m = 4, n = 2. In the ﬁrst step, we calculate the\npenalty cost pcrowj for each row and pccolk for each column of the ta",
  "output": "The text discusses an algorithm with supply sources, Siloj, and demand sources, Millk. The per-unit transportation cost from Siloj to Millk is denoted as Cj,k. An example is given with m=4, n=2. The first step involves calculating the penalty cost for each row and column.",
  "summary_type": "reports"
}{
  "input": "rs (e.g. number of heads\nand number of MSA layers). We mainly follow previous work to obtain right architectures that has\nsimilar capacity (e.g. number of parameters and throughput).\nRecall that the overall hierarchy can be determined by two key hyper-parameters: patch size S × S\nand hierarchy depth Td. Just like how ResNet [23] adapts to small and large input sizes, NesT also\nhas different conﬁguration for small input size and large input size. We follow [35, 49] to conﬁgure\nthe number of head,",
  "output": "The text discusses the configuration of a system architecture, influenced by factors like the number of heads and MSA layers. Key hyper-parameters include patch size (S × S) and hierarchy depth (Td). The system, similar to ResNet, adapts to different input sizes. The configuration of the number of heads is guided by previous work.",
  "summary_type": "reports"
}{
  "input": "s change\nby increasing the mixing weight λ, and in Figure 10, we\nvisualize how the results change as we increase the coefﬁ-\ncients of the smoothness terms, β and γ.\nE.2. More Samples\nIn this section, we provide Puzzle Mix results with various\nresolutions of the optimal mask and transport. Figure 11\nvisualizes the Puzzle Mix results along with the given inputs.\n\n--------------------------------------------------\nPuzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup\nReferences\nBen",
  "output": "The text discusses the Puzzle Mix method, where results vary based on the mixing weight λ and the smoothness terms coefficients, β and γ. The document also provides Puzzle Mix results at different resolutions of the optimal mask and transport. The results, along with the given inputs, are visualized in Figure 11.",
  "summary_type": "reports"
}{
  "input": "getting the routing\nright is the most important. The model is robust to mis-routing at most intermediate layers—layer 9 is\nan exception here. This observation motivated us into trying to train sparse models with MoE layers\nonly at the very end—21 and 23, for example—with excellent results (and computational savings).\n1 3 5 7 9\n11131517192123\nOriginal\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nImageNet 1-shot Acc\n1 3 5 7 9\n11131517192123\nOriginal\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nImageNet 5-shot Acc\n1 3 5 7 9\n11131517192123",
  "output": "The text emphasizes the importance of correct routing in the model, which can tolerate mis-routing except at layer 9. This led to experiments training sparse models with MoE layers only at the end (layers 21 and 23), yielding excellent results and computational savings. However, the text also includes data that appears to be related to ImageNet 1-shot and 5-shot accuracy, but without context, these cannot be meaningfully interpreted.",
  "summary_type": "reports"
}{
  "input": "nsidered applying\nstraddled Squeeze & Excite layers, where the input to the\nS&E is the input to the convolution, but the output of the\nS&E multiplies the output of the convolution. This is in con-\nstrast to the normal Squeeze & Excite formulation, which\nsimply operates directly on an activation (i.e. it takes in a\nvalue, and its output is used to multiply that same value).\nBoth forms of S&E block help to restore cross-channel con-\nnectivity (albeit not as strongly as using a fully connected\nconv",
  "output": "The text discusses the application of Squeeze & Excite (S&E) layers in convolutional neural networks. Unlike the traditional S&E formulation that operates directly on an activation, the new approach uses the input to the convolution as the input to the S&E, and the output of the S&E multiplies the output of the convolution. Both methods of S&E block help to restore cross-channel connectivity, though not as strongly as a fully connected convolution.",
  "summary_type": "reports"
}{
  "input": "ration for small input size and large input size. We follow [35, 49] to conﬁgure\nthe number of head, hidden dimensions for the tiny, small and base versions. For 32 × 32 image size,\nwe follow [49]. Speciﬁcally, we setup the same number of repeated MSANesT per block hierarchy.\nIn each hierarchy, the number of hidden dimensions and the number of heads are the same as well.\nFor 224 × 224 image size, we follow [35]. Therefore, different hierarchy has a gradually increased\nnumber of head, hidden dime",
  "output": "The text discusses configuring the number of head and hidden dimensions for different versions of an unspecified system, following references [35, 49]. It mentions setting up the same number of repeated MSANesT per block hierarchy, with equal numbers of hidden dimensions and heads in each hierarchy. For larger image sizes, a different hierarchy is used, which gradually increases the number of head and hidden dimensions.",
  "summary_type": "reports"
}{
  "input": "the reshaped tokens along channels. By reaching\na good combination of local and global features, our GLiT\nfocuses on more object regions than DeiT.\n5. Conclusion\nWe exploit better architectures for vision transformers\nin this paper through carefully designing the searching\nspace with local information and the hierarchical searching\nmethod. Transformer is applied to vision community not\nlong ago. Its architecture is not well exploited for image\nrecognition. Our method provides a feasible and auto",
  "output": "This paper presents a new method for vision transformers, called GLiT, which combines local and global features for better object region focus than DeiT. The method involves a carefully designed search space with local information and a hierarchical search method. This approach offers a feasible and automatic solution for image recognition, an area where transformer architecture has not been fully utilized.",
  "summary_type": "reports"
}{
  "input": "images\nfor training and 5k images for evaluating performances\nof object detectors. We follow standard practice of Reti-\nnaNet [20] and Mask R-CNN [8] with ResNet as backbones\nin mmdetection [1]. We replace ResNet backbones with\nConvMLP and adjust the dimension of convolution layers\nin feature pyramids accordingly. We also replace SGD op-\ntimizer with AdamW and adjust learning rate to 0.0001\nwith weight decay at 0.0001, which follows the conﬁgs in\nPVT [37]. We train both RetinaNet and Mask R-CNN",
  "output": "The text discusses the use of RetinaNet and Mask R-CNN with ResNet backbones in mmdetection for object detection. These backbones are replaced with ConvMLP and the dimension of convolution layers in feature pyramids is adjusted. The SGD optimizer is replaced with AdamW, and the learning rate is set to 0.0001 with a weight decay of 0.0001, following the configurations in PVT. Both RetinaNet and Mask R-CNN are trained.",
  "summary_type": "reports"
}{
  "input": "further iterate by adding\nconvolutional blocks to the tokenization step and thus creat-\ning the Compact Convolutional Transformer (CCT). Both of\nthese additions add to signiﬁcant increases in performance,\nleading to a top-1%accuracy of 94.72% on CIFAR-10. This\nis only slightly behind ResNet1001-v2, which obtains an\naccuracy of 95.38% but with 2.7× model size and 1.6×\nGMACs comparing to ours. Our model outperforms all pre-\nvious transformer based models within this domain. Addi-\ntionally, we show",
  "output": "The text discusses the Compact Convolutional Transformer (CCT), which improves performance significantly, achieving a top-1% accuracy of 94.72% on CIFAR-10. This is slightly less than ResNet1001-v2, which has 95.38% accuracy but is 2.7 times larger and uses 1.6 times more GMACs. The CCT outperforms all previous transformer-based models in this domain.",
  "summary_type": "reports"
}{
  "input": "ntal, as the same weights are used for two\ndifferent purposes: helping the attention process, and preparing the vector to be fed\nto the classiﬁer. We put this problem in evidence by showing that inserting CLS later\nimproves performance (middle). In the CaiT architecture (right), we further propose to\nfreeze the patch embeddings when inserting CLS to save compute, so that the last part\nof the network (typically 2 layers) is fully devoted to summarizing the information to\nbe fed to the linear clas",
  "output": "The text discusses the issue of using the same weights for two different purposes in a network. It suggests that inserting CLS later improves performance. In the CaiT architecture, it proposes to freeze the patch embeddings when inserting CLS to save computation. The last part of the network, typically 2 layers, is fully devoted to summarizing the information for the linear classifier.",
  "summary_type": "reports"
}{
  "input": "ible to adopt another way for acceleration rather than the\nmethods used for CNN acceleration like ﬁlter pruning [13], which removes non-critical or redundant\nneurons from a deep model. Our method aims at pruning the tokens of less importance instead of the\nneurons by exploiting the sparsity of informative image patches.\n3\nDynamic Vision Transformers\n3.1\nOverview\nThe overall framework of our DynamicViT is illustrated in Figure 2. Our DynamicViT consists of a\nnormal vision transformer as the backb",
  "output": "The text discusses a method for accelerating Convolutional Neural Networks (CNNs) that differs from traditional methods like filter pruning. This method focuses on pruning less important tokens instead of neurons, leveraging the sparsity of informative image patches. The text also mentions a framework called Dynamic Vision Transformers (DynamicViT), which includes a standard vision transformer.",
  "summary_type": "reports"
}{
  "input": "category include\nShufﬂeNet [27], MobileNetV2 [34], MnasNet [40] and Mo-\nbileNetV3 [13]. Such solutions sought to improve compu-\ntational efﬁciency by progressively replacing the parameter\nand compute intensive standard convolutions by a combina-\ntion of 1×1 convolutions and depth-wise separable 3×3 con-\nvolutions. Figure 2 depicts the trend in the relative contribu-\ntions of different layers in terms of parameters and MAdds.\nDepth-wise separable convolutions [36, 4] offer signif-\nicant computat",
  "output": "ShuffleNet, MobileNetV2, MnasNet, and MobileNetV3 are solutions that aim to enhance computational efficiency. They do this by progressively substituting standard convolutions with a mix of 1x1 convolutions and depth-wise separable 3x3 convolutions. These depth-wise separable convolutions significantly improve computational efficiency.",
  "summary_type": "reports"
}{
  "input": "otr Dollar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In CVPR, 2017. 2\n[55] Saining Xie, Alexander Kirillov, Ross Girshick, and Kaim-\ning He. Exploring randomly wired neural networks for im-\nage recognition. In ICCV, 2019. 9\n[56] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan\nNgiam. Condconv: Conditionally parameterized convolu-\ntions for efﬁcient inference. In NeurIPS, 2019. 4\n[57] Yibo Yang, Zhisheng Zhong, Tiancheng Shen, and Zhouchen\nLin.",
  "output": "The text refers to various research papers on deep learning and neural networks. Paper [55] discusses exploring randomly wired neural networks for image recognition. Paper [56] introduces Condconv, a method for efficient inference using conditionally parameterized convolutions. The authors of these papers include Zhuowen Tu, Kaiming He, Saining Xie, Alexander Kirillov, Ross Girshick, Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. The text does not provide information on paper [57].",
  "summary_type": "reports"
}{
  "input": "we now propose a training-aware NAS.\nNAS Search: Our training-aware NAS framework is\nlargely based on previous NAS works (Tan et al., 2019;\nTan & Le, 2019a), but aims to jointly optimize accuracy,\nparameter efﬁciency, and training efﬁciency on modern ac-\ncelerators. Speciﬁcally, we use EfﬁcientNet as our backbone.\nOur search space is a stage-based factorized space similar\nto (Tan et al., 2019), which consists of the design choices for\nconvolutional operation types {MBConv, Fused-MBConv},\nnumber",
  "output": "We propose a training-aware Neural Architecture Search (NAS) framework, based on previous NAS works but optimized for accuracy, parameter efficiency, and training efficiency on modern accelerators. We use EfficientNet as our backbone. Our search space is a stage-based factorized space, which includes design choices for convolutional operation types {MBConv, Fused-MBConv}, number.",
  "summary_type": "reports"
}{
  "input": "f TNT block\nare\nFLOPsT NT = 2nmc(6c + m) + nmcd + 2nd(6d + n).\n(17)\nSimilarly, the parameter complexity of TNT block is calculated as\nParamsT NT = 12cc + mcd + 12dd.\n(18)\nAlthough we add two more components in our TNT block, the increase of FLOPs is small since\nc ≪d and O(m) ≈O(n) in practice. For example, in the DeiT-S conﬁguration, we have d = 384\nand n = 196. We set c = 24 and m = 16 in our structure of TNT-S correspondingly. From Eq. 15\nand Eq. 17, we can obtain that FLOPsT = 376M and FLOPsT",
  "output": "The text discusses the parameter complexity of a TNT block, calculated using specific formulas. It mentions that despite adding two components to the TNT block, the increase in FLOPs (Floating Point Operations Per Second) is small. This is due to the relationship between the variables c, d, m, and n. In a specific configuration (DeiT-S), with d=384, n=196, c=24, and m=16, the FLOPsT is calculated to be 376M.",
  "summary_type": "reports"
}{
  "input": ", 51, 57].\nSome works have modernized the ResNet training procedure and obtained some\nimprovement over the original model (e.g. Dollar et al. [10]). This allows a more di-\nrect comparison when considering new models or methods involving more elaborate\ntraining procedures than the one initially used. Nevertheless, improving the ResNet-50\nbaseline [6, 10, 14, 47, 48, 54] was not the main objective of these works. As a conse-\nquence and as we will see, the best performance reported so far with a Re",
  "output": "Several studies have updated the ResNet training process, achieving improvements over the original model. This enables more direct comparisons when evaluating new models or methods that use more complex training procedures. However, enhancing the ResNet-50 baseline was not the primary goal of these studies. Consequently, the best performance reported to date with a ResNet model is not the main focus of these works.",
  "summary_type": "reports"
}{
  "input": "luated the effect of the latent array’s ini-\ntialization scale and the parameters of the Fourier frequency\nposition encoding on ImageNet performance. The results of\nthis experiment are shown in Fig. 6. These experiments use\nthe full-sized ImageNet architecture, but were trained with a\nsmaller batch size (256) and fewer TPUs (16) (for reasons of\ncompute availability). These experiments suggest that stan-\ndard and relatively small values for the initialization scale\nare best (values ≥1 may lead to",
  "output": "The study evaluated the impact of the latent array's initialization scale and the Fourier frequency position encoding parameters on ImageNet performance. Results suggest that standard and relatively small values for the initialization scale are optimal. The experiments used the full-sized ImageNet architecture, trained with a smaller batch size (256) and fewer TPUs (16) due to compute availability.",
  "summary_type": "reports"
}{
  "input": "e”\nbut with pdecay, twarmup, η0, ρ, ϵ set to their values in the LARS pipeline. Finally, “L2 variables” is\nthe same point as “Base” but where the L2 regularization is applied to all variables. The only ablation\n20 https://github.com/google-research/bert\n15\n\n--------------------------------------------------\n32k\n65k-32k\n65k\n90.25\n90.50\n90.75\n91.00\n91.25\n91.50\nF1 score\nFigure 3: 6 ﬁnetuning runs starting from the same pretraining checkpoint to show the stability of our\nresults, at each of the 32,7",
  "output": "The text discusses an experiment involving the LARS pipeline with specific parameters. An \"L2 variables\" condition applies L2 regularization to all variables. The experiment involves six fine-tuning runs starting from the same pre-training checkpoint to demonstrate result stability. The results are presented in a figure, likely showing F1 scores ranging from 90.25 to 91.50 for different conditions (32k, 65k-32k, 65k).",
  "summary_type": "reports"
}{
  "input": "te choice modelling economet-\nrics literature and theory. However unlike Collier et al. [9]\nwe use a low-rank approximation to a full (non-diagonal)\ncovariance matrix in the latent distribution. In the below\nexperiments, we demonstrate that our combination of (a)\nrecognizing the importance of the softmax temperature in\ncontrolling a bias-variance trade-off and (b) modelling the\ninter-class noise correlations yields signiﬁcantly improved\nperformance compared with individually using (a) or (b).\nOu",
  "output": "This text discusses the use of a low-rank approximation to a full covariance matrix in the latent distribution, differing from the approach of Collier et al. The authors highlight the importance of the softmax temperature in controlling a bias-variance trade-off and modeling inter-class noise correlations. They claim that combining these two factors significantly improves performance compared to using either factor individually.",
  "summary_type": "reports"
}{
  "input": "arameters and the script provided by (Fan et al. 2020) in the\nFairseq repository5, however, and unlike what they mention in their paper, this script does not include layer drop (Fan, Grave,\nand Joulin 2019). For DIFFQ, we tried the penalty levels λ in {1, 5, 10}, with group size 8, as well as λ = 10 and g = 16. For\nLSQ, we used the same training hyper-parameters as DIFFQ, except we initialized the model to a pre-trained model and used a\nlearning rate 10 time smaller for ﬁne tuning. Without this",
  "output": "The script provided by Fan et al. 2020 in the Fairseq repository doesn't include layer drop. For DIFFQ, penalty levels λ in {1, 5, 10} were tested with group size 8, and λ = 10 and g = 16. For LSQ, the same training hyper-parameters as DIFFQ were used, but the model was initialized to a pre-trained model and a learning rate 10 times smaller was used for fine tuning.",
  "summary_type": "reports"
}{
  "input": "ave more tradi-\ntional blocks while preserving the number of FLOPs. We\nTable 6:\nTimings for the components of the LeViT ar-\nchitecture on an Intel Xeon E5-2698 CPU core with batch\nsize 1.\nModel\nDeiT-tiny\nLeViT-256\nDimensions\nC = 192\nN = 3\nD = 64\nC = 256\nN = 4\nD = 32\nComponent\nRuntime (µs)\nRuntime (µs)\nLayerNorm\n49\nn/a\nKeys Q, K\n299\n275\nValues V\n172\n275\nProduct QKT\n228\n159\nProduct Attention AV\n161\n206\nAttention projection\n175\n310\nMLP\n1390\n1140\nTotal\n2474\n2365\ntherefore take Q, K, V to all have di",
  "output": "The text provides timings for components of the LeViT architecture on an Intel Xeon E5-2698 CPU core with batch size 1. Two models are compared: DeiT-tiny and LeViT-256. The components measured include LayerNorm, Keys Q, K, Values V, Product QKT, Product Attention AV, Attention projection, and MLP. The total runtime for DeiT-tiny is 2474µs, while for LeViT-256 it's 2365µs. The dimensions for DeiT-tiny are C=192, N=3, D=64, and for LeViT-256 they are C=256, N=4, D=32.",
  "summary_type": "reports"
}{
  "input": "vironments of ImageNet training.\nAnd, we extended the ImageNet comparison to architec-\ntures other than Transformer. In particular, we focus on the\ncomparison of the performance of ResNet and PiT, and in-\nvestigate whether PiT can beat ResNet. We also applied PiT\nto an object detector based on deformable DETR [44], and\ncompared the performance as a backbone architecture for\nobject detection. To analyze PiT in various views, we eval-\nuated the performance of PiT on robustness benchmarks.\n4.1. Ima",
  "output": "The text discusses a comparison between the performance of ResNet and PiT architectures in ImageNet training. It explores whether PiT can outperform ResNet. PiT was also applied to an object detector based on deformable DETR and its performance as a backbone architecture for object detection was compared. Additionally, PiT's performance was evaluated on robustness benchmarks.",
  "summary_type": "reports"
}{
  "input": "have also tried applying a smaller keeping ratio (larger acceleration\nrate). The results based on DeiT-S [25] and LV-ViT-S [16] models are presented in the following\ntables. We see that using ρ < 0.7 will lead to a signiﬁcant accuracy drop while reducing fewer\nFLOPs. Since only 22% and 13% tokens are remaining in the last stage when we set ρ to 0.6 and\n0.5 respectively, small ρ may cause a signiﬁcant information loss. Therefore, we use ρ ≥0.7 in our\nmain experiments. Jointly scaling ρ and the m",
  "output": "Experiments with DeiT-S and LV-ViT-S models show that using a keeping ratio (ρ) less than 0.7 leads to a significant accuracy drop while reducing fewer FLOPs. When ρ is set to 0.6 and 0.5, only 22% and 13% tokens remain in the last stage, potentially causing significant information loss. Therefore, a ρ of 0.7 or higher is used in main experiments.",
  "summary_type": "reports"
}{
  "input": "8, 34] in current T2T-ViT-7 and T2T-\nViT-12, and we only reduce model size by reducing the hid-\nden dimension, MLP ratio and depth of layers, indicating\nT2T-ViT is also very promising as a lite model. We also ap-\nply knowledge distillation on our T2T-ViT as the concurrent\nwork DeiT [38] and ﬁnd that our T2T-ViT-7 and T2T-ViT-\n12 can be further improved by distillation. Overall, the ex-\nperimental results show, our T2T-ViT can achieve superior\n\n--------------------------------------------------\nT",
  "output": "The text discusses the potential of T2T-ViT-7 and T2T-ViT-12 as lite models, achieved by reducing the hidden dimension, MLP ratio, and depth of layers. The models can be further improved through knowledge distillation, a technique also used in concurrent work DeiT. The experimental results suggest that T2T-ViT models can achieve superior performance.",
  "summary_type": "reports"
}{
  "input": "te their implemented modules/func-\ntions with this framework. Similar to AllenNLP [6] and Catalyst [17], this can be done\neven outside the framework by using a Python decorator. The following example shows\nthat a new model class, MyModel, is added to the framework by simply using @regis-\nter model (deﬁned in the framework), and the new class can be instantiated by deﬁning\n“MyModel” with required parameters at designated places in a conﬁguration ﬁle.\n2 https://pytorch.org/hub/\n3 https://github.co",
  "output": "The text discusses a framework similar to AllenNLP and Catalyst, where modules or functions can be implemented. This can be done outside the framework using a Python decorator. A new model class, MyModel, can be added to the framework using @register model. This new class can be instantiated by defining \"MyModel\" with required parameters at designated places in a configuration file.",
  "summary_type": "reports"
}{
  "input": "Our\nresults show that existing networks’ potential has not been\nfully exploited and there is still room to boost and enhance\nthrough our framework. We hope the proposed method can\ninspire more studies along this direction of boosting tiny\nand compact models through knowledge distillation.\n8\n\n--------------------------------------------------\nAppendix\nA. Implementation Details of Transferring\nFine-tuning backbone. On classiﬁcation task, we ﬁne-tune\nthe entire network of ResNet-50 using the parame",
  "output": "The study reveals that current networks have untapped potential that can be further enhanced through the proposed framework. The method aims to stimulate more research in improving small and compact models via knowledge distillation. The ResNet-50 network was fine-tuned in the classification task.",
  "summary_type": "reports"
}{
  "input": "on datasets\nwith larger training samples per class (e.g. 4,000 in CIFAR-10).\nOn datasets with fewer samples per class (e.g. 20 in Flowers\n102), there is still a large performance gap between supernet\nadaptation and additional ﬁne-tuning. Overall the results suggest\nthat supernet adaptation is more effective on tasks with limited\ntraining samples.\n5.6\nTowards Quantifying Architectural Advancement\nComparing the architectural contribution to the success of different\nNAS algorithms can be difﬁcult a",
  "output": "Supernet adaptation performs better on tasks with limited training samples. However, there's a significant performance gap between supernet adaptation and additional fine-tuning on datasets with fewer samples per class. The effectiveness of different NAS algorithms is challenging to compare due to architectural contributions.",
  "summary_type": "reports"
}{
  "input": "peed-\naccuracy trade-off among these methods on image classi-\nﬁcation, even though our work focuses on general-purpose\nperformance rather than speciﬁcally on classiﬁcation. An-\nother concurrent work [66] explores a similar line of think-\ning to build multi-resolution feature maps on Transform-\ners. Its complexity is still quadratic to image size, while\nours is linear and also operates locally which has proven\nbeneﬁcial in modeling the high correlation in visual sig-\nnals [36, 25, 41]. Our approa",
  "output": "The text discusses the speed-accuracy trade-off in image classification methods. It mentions a concurrent work that builds multi-resolution feature maps on Transformers, but its complexity is quadratic to image size. In contrast, the authors' approach is linear and operates locally, which has been beneficial in modeling the high correlation in visual signals.",
  "summary_type": "reports"
}{
  "input": "ResNeXt-50 ResNeXt-101\nCE(baseline)\n41.6\n44.4\n44.8\nDecouple-cRT\n47.3\n49.6\n49.4\nDecouple-τ-norm\n46.7\n49.4\n49.6\nDe-confound-TDE\n51.7\n51.8\n53.3\nResLT\n-\n52.9\n54.1\nMiSLAS\n52.7\n-\n-\nDecouple-τ-norm †\n54.5\n56.0\n57.9\nBalanced Softmax †\n55.0\n56.2\n58.0\nPaCo†\n57.0\n58.2\n60.0\nstate-of-the-arts of Decouple [28], Balanced Softmax [38]\nand RIDE [49] as mentioned in Section 4.2.\nFor Places-LT, following previous setting [35, 14], we\nchoose ResNet-152 as the backbone network, pre-train it on\nthe full ImageNet-201",
  "output": "The text presents performance scores of various models like ResNeXt-50, ResNeXt-101, CE(baseline), Decouple-cRT, Decouple-τ-norm, De-confound-TDE, ResLT, MiSLAS, Balanced Softmax, and PaCo. The highest score is achieved by PaCo with 60.0. The models are state-of-the-art of Decouple, Balanced Softmax, and RIDE. For Places-LT, ResNet-152 is chosen as the backbone network, pre-trained on the full ImageNet-201.",
  "summary_type": "reports"
}{
  "input": "0B\n86.1@224\n87.1@512\nD2 to VOLO-D5 yields nearly another 1% accuracy gain.\nIn addition, for all the ﬁve models, increasing the resolution\nduring ﬁnetuning brings around 1% performance gain.\nNumber of Outlookers: We observe that the number of\nOutlookers used in our VOLO has an impact on the classi-\nﬁcation performance. Here, we investigate the inﬂuence of\nusing different numbers of Outlookers in our VOLO. Note\nthat all Outlookers act on ﬁner-level token representations\n(28 × 28). The results have",
  "output": "The text discusses the performance of VOLO models. It notes that changing D2 to VOLO-D5 improves accuracy by approximately 1%. Additionally, increasing the resolution during fine-tuning also results in about 1% performance gain. The number of Outlookers used in VOLO impacts classification performance, with all Outlookers acting on finer-level token representations (28 × 28).",
  "summary_type": "reports"
}{
  "input": "trained models with token labeling are also beneﬁcial to downstream tasks with dense prediction,\nsuch as semantic segmentation.\n2\nRelated Work\nTransformers [39] refer to the models that entirely rely on the self-attention mechanism to build\nglobal dependencies, which are originally designed for natural language processing tasks. Due to their\nstrong capability of capturing spatial information, transformers have also been successfully applied\nto a variety of vision problems, including low-level vi",
  "output": "Trained models with token labeling aid downstream tasks like semantic segmentation. Transformers, models fully dependent on the self-attention mechanism for global dependencies, were initially designed for natural language processing tasks. Their ability to capture spatial information has led to their successful application in various vision problems.",
  "summary_type": "reports"
}{
  "input": "on of ω and we can obtain a sequence of X[k] by sampling X(ejω) at frequencies\nωk = 2πk/N:\nX[k] = X(ejω)|ω=2πk/N =\nN−1\nX\nn=0\nx[n]e−j(2π/N)kn,\n(A.9)\nwhich is exactly the formulation of DFT. The extension from 1D DFT to 2D DFT is straightforward.\nIn fact, The 2D DFT can be viewed as performing 1D DFT on the two dimensions alternatively, i.e.,\nthe 2D DFT of x[m, n] is given by:\nX[u, v] =\nM−1\nX\nm=0\nN−1\nX\nn=0\nx[m, n]e−j2π( um\nM + vn\nN ).\n(A.10)\nA.2\nSome properties of DFT\nDFT of real signals.\nGiven a",
  "output": "The text discusses the Discrete Fourier Transform (DFT). It explains that a sequence of X[k] can be obtained by sampling X(ejω) at frequencies ωk = 2πk/N. The 2D DFT is an extension of 1D DFT, performed on two dimensions alternatively. The 2D DFT of x[m, n] is given by a specific mathematical formula. The text also mentions some properties of DFT of real signals.",
  "summary_type": "reports"
}{
  "input": "-MSA denote window based multi-head\nself-attention using regular and shifted window partitioning\nconﬁgurations, respectively.\nThe shifted window partitioning approach introduces\nconnections between neighboring non-overlapping win-\ndows in the previous layer and is found to be effective in im-\nage classiﬁcation, object detection, and semantic segmenta-\ntion, as shown in Table 4.\nEfﬁcient batch computation for shifted conﬁguration\nAn issue with shifted window partitioning is that it will re-\nsult",
  "output": "MSA refers to a window-based multi-head self-attention using regular and shifted window partitioning configurations. The shifted window partitioning introduces connections between neighboring non-overlapping windows in the previous layer, proving effective in image classification, object detection, and semantic segmentation. However, an issue with this method is that it can affect efficient batch computation for shifted configuration.",
  "summary_type": "reports"
}{
  "input": "Santoro et al., 2018; Goyal et al., 2021), all while keep-\ning the cost of each operation linear in the input size. We\nuse cross-attention to induce a latent space for deep pro-\ncessing. This can be viewed as a fully attentional, domain-\nagnostic analogue of models that stacks self-attention on\ntop of convolutional feature maps to perform cheap but\nglobal processing on top or in conjunction with otherwise\nspatially localized convolutional feature maps (e.g. Carion\net al. 2020; Locatello et al. 2",
  "output": "The text discusses using cross-attention to create a latent space for deep processing. This approach is a fully attentional, domain-agnostic version of models that stack self-attention on convolutional feature maps. This allows for inexpensive but global processing in conjunction with spatially localized convolutional feature maps. The cost of each operation remains linear in the input size.",
  "summary_type": "reports"
}{
  "input": "ation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In Proc. Eur. Conf. Comp.\nVis., pages 213–229. Springer, 2020.\n[4] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\n[5]",
  "output": "This text refers to various academic papers. Paper [3] by Nicolas Carion et al. discusses end-to-end object detection with transformers, presented at the European Conference on Computer Vision. Paper [4] by Ze Liu et al. introduces the Swin Transformer, a hierarchical vision transformer using shifted windows, as per an arXiv preprint.",
  "summary_type": "reports"
}{
  "input": "ion [30]. We do not compare to\nthe distilled version of DeiT since it’s an orthogonal axis of improvement\napplicable to all models.\n\n--------------------------------------------------\nHoulsby, Alexey Dosovitskiy for feedback. We thank Zak\nStone for extensive compute support throughout this project\nthe through TFRC program providing Google Cloud TPUs\n(https://www.tensorflow.org/tfrc).\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv",
  "output": "The text acknowledges the feedback from Houlsby and Alexey Dosovitskiy and thanks Zak Stone for extensive compute support throughout the project via the TFRC program that provided Google Cloud TPUs. It also references a preprint arXiv paper by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton on layer normalization. The distilled version of DeiT is not compared as it's an orthogonal axis of improvement applicable to all models.",
  "summary_type": "reports"
}{
  "input": "than simply\nstacking it). For Broyden’s method (see Eq. (7)), the Broyden matrix B[i] is a full matrix that mixes\nall locations of the feature map (which is represented by gθ(z[i]; x)); whereas typical convolutional\nﬁlters only mix the signals locally. On the other hand, multiscale up- and downsamplings broaden the\neffective receptive ﬁeld on the high-resolution stream by direct interpolation from lower-resolution\nfeature maps.\nC\nQualitative Segmentation Results on Cityscapes\nWe demonstrate in F",
  "output": "Broyden's method uses a full matrix, B[i], that mixes all locations of the feature map, unlike typical convolutional filters that mix signals locally. Multiscale up- and downsamplings expand the effective receptive field on the high-resolution stream through direct interpolation from lower-resolution feature maps.",
  "summary_type": "reports"
}{
  "input": "ate\nmulti-scale attention maps, while the scale of feature con-\ntext aggregation inside the attention module remains single\n[2, 4, 45, 6, 35, 40]. The second type, which is also re-\nferred to as multi-scale spatial attention, aggregates feature\ncontexts by convolutional kernels of different sizes [20] or\nfrom a pyramid [20, 43] inside the attention module .\nThe proposed MS-CAM follows the idea of ParseNet\n[25] with combining local and global features in CNNs and\nthe idea of spatial attention wit",
  "output": "The text discusses two types of attention maps in convolutional neural networks (CNNs). The first type generates multi-scale attention maps but only uses single-scale feature context aggregation within the attention module. The second type, known as multi-scale spatial attention, aggregates feature contexts using convolutional kernels of different sizes or from a pyramid within the attention module. The proposed MS-CAM combines the ideas of ParseNet with local and global features in CNNs and spatial attention.",
  "summary_type": "reports"
}{
  "input": "-18\nDIFFQ (OURS)\n69.7\n4.1\nRESNET-50\nUNCOMPRESSED\n77.1\n97.5\nRESNET-50\nLSQ 8 BITS (ESSER ET AL. 2020)\n76.8\n24.5\nRESNET-50\nLSQ 4 BITS (ESSER ET AL. 2020)\n76.2\n12.3\nRESNET-50\nLSQ 3 BITS (ESSER ET AL. 2020)\n75.6\n9.3\nRESNET-50\nLSQ* 8 BITS (ESSER ET AL. 2020)\n76.8\n24.5\nRESNET-50\nLSQ* 4 BITS (ESSER ET AL. 2020)\n76.7\n12.3\nRESNET-50\nLSQ* 3 BITS (ESSER ET AL. 2020)\n75.8\n9.3\nRESNET-50\nDIFFQ (OURS)\n76.9\n14\nRESNET-50\nDIFFQ (OURS)\n76.6\n10.5\nRESNET-50\nDIFFQ (OURS)\n76.3\n8.8\nmust be smaller than 1 + 1/100 times t",
  "output": "The text presents data on RESNET-50 performance with different bit configurations. Uncompressed RESNET-50 has 77.1 and 97.5. LSQ 8, 4, and 3 bits have 76.8/24.5, 76.2/12.3, and 75.6/9.3 respectively. LSQ* 8, 4, and 3 bits have 76.8/24.5, 76.7/12.3, and 75.8/9.3 respectively. DIFFQ has 76.9/14, 76.6/10.5, and 76.3/8.8. The final line suggests a condition for a value to be smaller than 1 + 1/100 times t.",
  "summary_type": "reports"
}{
  "input": "rXiv, 2017. 3\n[27] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection.\nIn CVPR, 2018. 3\n[28] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018. 3\n[29] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\nCriss-cross attention for semantic segmentation. In ICCV, 2019. 3\n[30] Lukasz Kaiser, Aidan N Gomez, and Francois Chollet. Depthwise separable convolutions for neural\nmachine transl",
  "output": "The text mentions various research papers on neural networks and object detection. Han Hu et al. discuss Relation Networks for object detection (CVPR, 2018). Jie Hu et al. present Squeeze-and-Excitation networks (CVPR, 2018). Zilong Huang et al. introduce CCNet, a criss-cross attention mechanism for semantic segmentation (ICCV, 2019). Lukasz Kaiser et al. discuss depthwise separable convolutions for neural machine translation.",
  "summary_type": "reports"
}{
  "input": "ff\nthe expressivity and efﬁciency of convolutional layers. We\nintroduce MUXConv, a layer that leverages the efﬁciency of\ndepth-wise or group-wise convolutional layers along with a\nmechanism to enhance the ﬂow of information in the net-\nwork. MUXConv achieves this through two components,\nspatial multiplexing and channel multiplexing. Spatial mul-\ntiplexing extracts feature information at multiple scales via\nspatial shufﬂing, processes such information through depth-\nwise or group-wise convolution",
  "output": "MUXConv is a layer that combines the efficiency of depth-wise or group-wise convolutional layers with a mechanism to improve information flow in the network. It achieves this through spatial multiplexing and channel multiplexing. Spatial multiplexing extracts feature information at multiple scales through spatial shuffling and processes it through depth-wise or group-wise convolution.",
  "summary_type": "reports"
}{
  "input": "NetV2: Smaller Models and Faster Training\nTable 8. Transfer Learning Performance Comparison – All models are pretrained on ImageNet ILSVRC2012 and ﬁnetuned on\ndownstream datasets. Transfer learning accuracy is averaged over ﬁve runs.\nModel\nParams\nImageNet Acc.\nCIFAR-10\nCIFAR-100\nFlowers\nCars\nConvNets\nGPipe (Huang et al., 2019)\n556M\n84.4\n99.0\n91.3\n98.8\n94.7\nEfﬁcientNet-B7 (Tan & Le, 2019a)\n66M\n84.7\n98.9\n91.7\n98.8\n94.7\nVision\nTransformers\nViT-B/32 (Dosovitskiy et al., 2021)\n88M\n73.4\n97.8\n86.3\n85.4",
  "output": "The text compares the performance of different models pretrained on ImageNet ILSVRC2012 and finetuned on downstream datasets. GPipe has 556M parameters with ImageNet accuracy of 84.4% and high accuracy on other datasets. EfficientNet-B7 has 66M parameters with 84.7% ImageNet accuracy and similar performance on other datasets. ViT-B/32 has 88M parameters with 73.4% ImageNet accuracy and lower accuracy on other datasets.",
  "summary_type": "reports"
}{
  "input": "apply a center crop on\nthe validation set to benchmark, where a 224× 224 patch is\ncropped to evaluate the classiﬁcation accuracy.\nResults. In Table 2, we see that PVTv2 is the state-of-\nthe-art method on ImageNet-1K classiﬁcation. Compared\nto PVT, PVTv2 has similar ﬂops and parameters, but the\nimage classiﬁcation accuracy is greatly improved. For ex-\nample, PVTv2-B1 is 3.6% higher than PVTv1-Tiny, and\nPVTv2-B4 is 1.9% higher than PVT-Large.\nCompared to other recent counterparts, PVTv2 series\nal",
  "output": "The text discusses the use of a center crop on the validation set for benchmarking. A 224x224 patch is cropped to evaluate classification accuracy. PVTv2 is the best method for ImageNet-1K classification. It has similar flops and parameters to PVT, but its image classification accuracy is significantly better. For instance, PVTv2-B1 is 3.6% higher than PVTv1-Tiny, and PVTv2-B4 is 1.9% higher than PVT-Large.",
  "summary_type": "reports"
}{
  "input": "in\nthe supplemental document, we introduce the left settings\nthat not mentioned before.\nFor the experiments on the CIFAR-100 dataset, the\nweight decay is 1e-4, and we decay the learning rate by a\nfactor of 0.1 at epoch 300 and 350.\nFor the experiments on the ImageNet, we use the label\nsmoothing trick and a cosine annealing schedule for the\nlearning rate without weight decay.\nFor the semantic segmentation experiment, the StopSign\ndataset is a subset of the COCO dataset [24], which has\na large sca",
  "output": "The supplemental document introduces previously unmentioned settings. For CIFAR-100 dataset experiments, the weight decay is 1e-4, with learning rate decayed by 0.1 at epochs 300 and 350. For ImageNet experiments, label smoothing and a cosine annealing schedule are used for the learning rate, without weight decay. The StopSign dataset, used in the semantic segmentation experiment, is a subset of the COCO dataset.",
  "summary_type": "reports"
}{
  "input": "b)\nmethods which only model the diagonal of the covariance\nmatrix and (c) methods from the noisy labels literature.\nWe evaluate the effect of our probabilistic label noise\nmodel on the representations learned by the network. We\nshow that our method, when pre-trained on JFT, learns image\n1\narXiv:2105.10305v1  [cs.LG]  19 May 2021\n\n--------------------------------------------------\nrepresentations which transfer better to the 19 datasets from\nthe Visual Task Adaptation Benchmark (VTAB) [47].\nContr",
  "output": "The text discusses a probabilistic label noise model that improves the network's learned representations. This method, when pre-trained on JFT, enhances image representations, making them more adaptable to the 19 datasets from the Visual Task Adaptation Benchmark (VTAB).",
  "summary_type": "reports"
}{
  "input": "that models of this size overﬁt without\nweight sharing, so we use a model that shares weights for\nall but the ﬁrst cross-attend and latent Transformer mod-\nules. The resulting model has ∼45 million parameters,\nmaking it comparable in size to convolutional models used\non ImageNet.\nStandard ImageNet. As shown in Table 1, the Perceiver\nmodel we trained on ImageNet obtains results that are com-\npetitive with models speciﬁcally designed for processing\nimages. We include ResNet-50 results from (Cubuk",
  "output": "The text discusses a model with approximately 45 million parameters, similar in size to convolutional models used on ImageNet. This model uses weight sharing, excluding the first cross-attend and latent Transformer modules, to prevent overfitting. The Perceiver model, trained on ImageNet, achieves competitive results compared to models specifically designed for image processing, including ResNet-50.",
  "summary_type": "reports"
}{
  "input": "4, 68, 15] are feasible for input resolutions\n(224 × 224 (for classiﬁcation) and 640 × 640 (for detec-\ntion experiments in SASA [49])) considered in these papers.\nOur goal is to use attention in more realistic settings of high\nperformance instance segmentation models, where typically\nimages of larger resolution (1024 × 1024) are used. Con-\nsidering that self-attention when performed globally across\nn entities requires O(n2d) memory and computation [61],\nwe believe that the simplest setting that",
  "output": "The text discusses using attention in high performance instance segmentation models, typically with larger resolution images (1024 x 1024). It mentions that self-attention across n entities requires O(n2d) memory and computation. The goal is to apply this in more realistic settings. Input resolutions feasible are 224 x 224 (for classification) and 640 x 640 (for detection experiments in SASA).",
  "summary_type": "reports"
}{
  "input": "nstructured sparsiﬁcation can lead to better performance for vision\ntransformers compared to structural downsampling). The basic idea of our method is illustrated in\nFigure 1.\nIn this work, we propose to employ a lightweight prediction module to determine which tokens to be\npruned in a dynamic way, dubbed as DynamicViT. In particular, for each input instance, the prediction\nmodule produces a customized binary decision mask to decide which tokens are uninformative and\nneed to be abandoned. This m",
  "output": "This text discusses a method called DynamicViT, which uses a lightweight prediction module to improve the performance of vision transformers. The module dynamically determines which tokens to prune based on each input instance. It creates a custom binary decision mask to identify and discard uninformative tokens.",
  "summary_type": "reports"
}{
  "input": "insert blocks\nto the 8 × 8, 16 × 16 and 32 × 32 patch embedding stages,\nwhich correspond to 28 × 28, 14 × 14 and 7 × 7 feature\nresolutions respectively for 224 × 224 inputs. Additionally,\nwe halve the head dimension and feature dimension before\nself-attention in 28 × 28 stage to ensure that the blocks in\ndifferent stages utilize similar FLOPs.\nThis transition leads to interesting results. The base per-\nformance is further improved.\nIt is conjectured that the\nstage-wise design leverages the image",
  "output": "Blocks are inserted into the 8x8, 16x16, and 32x32 patch embedding stages, corresponding to 28x28, 14x14, and 7x7 feature resolutions for 224x224 inputs. The head and feature dimensions are halved before self-attention in the 28x28 stage to ensure similar FLOPs across different stages. This transition improves base performance, possibly due to the stage-wise design leveraging the image.",
  "summary_type": "reports"
}{
  "input": "mlps. arXiv preprint\narXiv:2105.08050, 2021. 2, 3, 7, 8\n[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021. 1, 3, 6, 7, 8, 16, 17\n[30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017. 16\n[31] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vlad",
  "output": "The text mentions several academic preprints from arXiv. \"Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows\" by Ze Liu et al. (arXiv:2103.14030, 2021) discusses a hierarchical vision transformer. \"Decoupled Weight Decay Regularization\" by Ilya Loshchilov and Frank Hutter (arXiv:1711.05101, 2017) talks about weight decay regularization. The text also refers to a preprint by Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vlad, but no details are provided.",
  "summary_type": "reports"
}{
  "input": "h SS. The NATS-Bench size search space\nSS [17] is a channel conﬁguration search space built\n12\n\n--------------------------------------------------\nupon a ﬁxed cell-based architecture with 5 layers, where\nthe 2-nd and 4-th layers have a down-sample rate of\n2. Number of channels in each layer is chosen from\n{8, 16, 24, 32, 40, 48, 56, 64}. SS has 85 = 32768\narchitecture candidates in total. Candidates of different\nchannel numbers in our supernet share the weights in\na slimmable manner [78, 77, 76,",
  "output": "The NATS-Bench size search space (SS) is a channel configuration search space built on a fixed cell-based architecture with 5 layers. The 2nd and 4th layers have a down-sample rate of 2. The number of channels in each layer can be 8, 16, 24, 32, 40, 48, 56, or 64. SS has 32,768 architecture candidates in total. Candidates with different channel numbers in the supernet share weights in a slimmable manner.",
  "summary_type": "reports"
}{
  "input": "f different models for image\nclassiﬁcation tasks on ImageNet, centroid @ means re-\nplacing the self-attention block with the centroid attention\nblock at speciﬁc layer.\nFigure 7: Comparing DeiT and centroid transformer for\nimage inputs. Upper panel: DeiT partitions the input im-\nage into non-lapping patches and keeps the same number\nof patches throughout the layers, which may lose infor-\nmation of the image because the patches do not overlap.\nIn comparison, thanks to the ability of down-sampling,",
  "output": "The text discusses image classification models on ImageNet. It compares DeiT and centroid transformer for image inputs. DeiT partitions the input image into non-overlapping patches, potentially losing information as the patches do not overlap. The centroid transformer, however, has the ability to down-sample, offering a potential advantage over DeiT.",
  "summary_type": "reports"
}{
  "input": "follow most of the training techniques used in DeiT [25].\nWe use the pre-trained vision transformer models to initialize the backbone models and jointly train\nthe backbone model as well as the prediction modules for 30 epochs. We set the learning rate of the\nprediction module to batch size\n1024\n× 0.001 and use 0.01× learning rate for the backbone model. The batch\nsize is adjusted adaptively for different models according to the GPU memory. We ﬁx the weights of\nthe backbone models in the ﬁrst 5 e",
  "output": "The training techniques from DeiT are used, with pre-trained vision transformer models initializing the backbone models. These are jointly trained with the prediction modules for 30 epochs. The learning rate for the prediction module is set to batch size 1024 x 0.001, with a 0.01x learning rate for the backbone model. The batch size is adjusted according to GPU memory. The backbone models' weights are fixed in the first 5 epochs.",
  "summary_type": "reports"
}{
  "input": "-------",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "oned techniques such that the reduc-\ntion in throughput is minimized. Therefore, among network\ntweaks, only SE was applied to MobileNet-V1 and boosted\nthe accuracy by 1.69 % (M0, M1). The top-1 accuracy gain\nof using SE-MobileNet-V1 together with LS+Mixup+KD\nwas 2.05% more than that of vanilla MobileNet-V1 with\nthe same regularizations applied (M5, M6). In other words,\nthe synergistic effect of using network tweaks and regu-\nlarizations is also demonstrated in mobile-oriented mod-\nels. Based on",
  "output": "The application of SE to MobileNet-V1 among network tweaks resulted in a 1.69% increase in accuracy. When combined with LS+Mixup+KD, SE-MobileNet-V1 showed a 2.05% higher top-1 accuracy than vanilla MobileNet-V1 with the same regularizations. This demonstrates the synergistic effect of using network tweaks and regularizations in mobile-oriented models.",
  "summary_type": "reports"
}{
  "input": "1\nN4 = 8\nE4 = 4\nL4 = 3\nTable 1: Detailed settings of PVTv2 series. “-Li” denotes PVTv2 with linear SRA.\n• Si: the stride of the overlapping patch embedding in\nStage i;\n• Ci: the channel number of the output of Stage i;\n• Li: the number of encoder layers in Stage i;\n• Ri: the reduction ratio of the SRA in Stage i;\n• Pi: the adaptive average pooling size of the linear SRA\nin Stage i;\n• Ni: the head number of the Efﬁcient Self-Attention in\nStage i;\n• Ei: the expansion ratio of the feed-forward laye",
  "output": "The text provides settings for the PVTv2 series. It includes parameters such as Si (stride of overlapping patch embedding), Ci (output channel number), Li (number of encoder layers), Ri (reduction ratio of SRA), Pi (adaptive average pooling size of linear SRA), Ni (head number of Efficient Self-Attention), and Ei (expansion ratio of the feed-forward layer) in Stage i. For instance, in Stage 4, N4 is 8 (head number), E4 is 4 (expansion ratio), and L4 is 3 (number of encoder layers).",
  "summary_type": "reports"
}{
  "input": "Knowledge distillation meets self-supervision. In: The\nEuropean Conference on Computer Vision (ECCV) (2020)\n50. Yim, J., Joo, D., Bae, J., Kim, J.: A gift from knowledge distillation: Fast optimization,\nnetwork minimization and transfer learning. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. pp. 4133–4141 (2017)\n51. Yuan, L., Tay, F.E., Li, G., Wang, T., Feng, J.: Revisiting knowledge distillation via label\nsmoothing regularization. In: Proceedings of the IEE",
  "output": "The text discusses knowledge distillation in the context of self-supervision at the European Conference on Computer Vision (ECCV) 2020. It also refers to a study by Yim et al. (2017) on the benefits of knowledge distillation, including fast optimization, network minimization, and transfer learning. Lastly, it mentions a study by Yuan et al. that revisits knowledge distillation through label smoothing regularization.",
  "summary_type": "reports"
}{
  "input": "50\n66.4\n1894.5\n78.2\nRedNet-50\n56.4\n1825.6\n80.6 (+2.4)\nPanoptic-DeepLab [7]\nAxial-DeepLab-S\n12.1\n220.8\n80.5\nAxial-DeepLab-M\n25.9\n419.6\n80.3\nAxial-DeepLab-XL\n173.0\n2446.8\n80.6\nTable 5: Performance comparison on Cityscapes segmentation\nbased on UPerNet. The efﬁciency of UPerNet is greatly boosted\nby the RedNet backbone, showing competitive performance to\nAxial-DeepLab-XL with only 32.6% parameter counts and 75.7%\ncomputational cost.\nder the single-scale mode and adopt the Intersection-over-\nUnion (",
  "output": "The text compares the performance of different models on Cityscapes segmentation based on UPerNet. RedNet-50 shows competitive performance to Axial-DeepLab-XL, with only 32.6% parameter counts and 75.7% computational cost. The efficiency of UPerNet is greatly boosted by the RedNet backbone.",
  "summary_type": "reports"
}{
  "input": "r an implementation and Table 8 for\nthe complexities. Experiments (see Appendix D.1) demonstrate that this variant results in accuracy\nimprovements but we ﬁnd that using |u|=1 (i.e. the default case) is optimal when controlling for\nspeed on modern machine learning accelerators.\n20\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\ndef compute position lambdas(embeddings, values, impl=’einsum’):\n\"\"\"Compute position lambdas with intra−depth u.\"\"\"\nif im",
  "output": "The text discusses an implementation variant that improves accuracy in a machine learning context. However, it is found that using |u|=1 (the default case) is optimal for speed on modern machine learning accelerators. A function, compute position lambdas, is also mentioned, which calculates position lambdas with intra-depth u.",
  "summary_type": "reports"
}{
  "input": "creases as training accuracy increases. The big difference in\ngeneralization performance causes the performance differ-\nence between PiT and ViT as shown in Figure 3 (c). The\nphenomenon that ViT does not increase performance even\nwhen FLOPs increase in ImageNet is reported in ViT pa-\nper [9]. In the training data of ImageNet scale, ViT shows\npoor generalization performance, and PiT alleviates this.\nSo, we believe that the spatial reduction layer is also nec-\nessary for the generalization of ViT.",
  "output": "The text discusses the performance difference between PiT and ViT in ImageNet training data. ViT does not show improved performance even with increased FLOPs, indicating poor generalization performance. PiT, however, mitigates this issue. The spatial reduction layer is deemed necessary for ViT's generalization.",
  "summary_type": "reports"
}{
  "input": "ion, Mixer achieves an overall strong\nperformance (84.15% top-1 on ImageNet), although slightly inferior to other models2. Regularization\nin this scenario is necessary and Mixer overﬁts without it, which is consistent with similar observations\nfor ViT [14]. The same conclusion holds when training Mixer from random initialization on ImageNet\n(see Section 3.2): Mixer-B/16 attains a reasonable score of 76.4% at resolution 224, but tends to\noverﬁt. This score is similar to a vanilla ResNet50, but be",
  "output": "Mixer shows a strong overall performance (84.15% top-1 on ImageNet), slightly inferior to other models. Regularization is necessary as Mixer tends to overfit without it, a trait observed in similar models like ViT. When trained from random initialization on ImageNet, Mixer-B/16 achieves a reasonable score of 76.4% at resolution 224, but tends to overfit. This score is comparable to a standard ResNet50.",
  "summary_type": "reports"
}{
  "input": "0\nImageNet Top-1 Accuracy (%)\nprogressive resize\nprogressive resize + adaptive reg\nFigure 6. Training curve comparison – Our adaptive regulariza-\ntion converges faster and achieves better ﬁnal accuracy.\n7. Conclusion\nThis paper presents EfﬁcientNetV2, a new family of smaller\nand faster neural networks for image recognition. Optimized\nwith training-aware NAS and model scaling, our Efﬁcient-\nNetV2 signiﬁcantly outperforms previous models, while\nbeing much faster and more efﬁcient in parameters. To",
  "output": "The text discusses EfficientNetV2, a new family of smaller and faster neural networks for image recognition. It is optimized with training-aware NAS and model scaling. EfficientNetV2 outperforms previous models, being faster and more efficient in parameters. The adaptive regularization feature allows it to converge faster and achieve better final accuracy.",
  "summary_type": "reports"
}{
  "input": "ble 2. The position encoding used here is initialized\nrandomly and trained end-to-end along with the network\n(using the same initialization type used for the latent array’s\nposition encoding, see Appendix Sec. C). Because the posi-\ntion encodings used here are unaware of the structure of the\ninput, it makes no difference whether inputs are permuted\nbefore or after the position encoding is constructed. We\nfound that the network with 8 cross-attends had stability is-\nsues when learned position enc",
  "output": "The text discusses a position encoding method that is initialized randomly and trained with the network. The encoding is not aware of the input structure, so input order doesn't affect it. However, a network with 8 cross-attends had stability issues with this learned position encoding.",
  "summary_type": "reports"
}{
  "input": "s that our gating is computed based on a projection over the spatial (cross-token)\ndimension rather than the channel (hidden) dimension. SGU is also related to Squeeze-and-Excite\n(SE) blocks [30] in terms of element-wise multiplication. However, different from SE blocks, SGU\ndoes not contain cross-channel projections at all, nor does it enforce permutation invariance (a\nkey feature for content-based attentive modules) due to its static parameterization for the spatial\ntransformation. The spatial",
  "output": "The Spatial Gating Unit (SGU) calculates gating based on spatial (cross-token) projection, not channel (hidden) dimension. Unlike Squeeze-and-Excite (SE) blocks, SGU doesn't have cross-channel projections and doesn't enforce permutation invariance due to its static parameterization for spatial transformation. It's related to SE blocks through element-wise multiplication.",
  "summary_type": "reports"
}{
  "input": "e image recognition. Based on this observation, we propose a\ndynamic token sparsiﬁcation framework to prune redundant tokens progressively\nand dynamically based on the input. Speciﬁcally, we devise a lightweight prediction\nmodule to estimate the importance score of each token given the current features.\nThe module is added to different layers to prune redundant tokens hierarchically. To\noptimize the prediction module in an end-to-end manner, we propose an attention\nmasking strategy to differenti",
  "output": "The text discusses a dynamic token sparsification framework for image recognition. This framework prunes redundant tokens based on the input. A lightweight prediction module is used to estimate the importance of each token. This module is added to different layers to prune tokens hierarchically. An attention masking strategy is proposed to optimize the prediction module in an end-to-end manner.",
  "summary_type": "reports"
}{
  "input": "and weight decay (wd) for each model.\nThese results use a batch size of 128 and are trained on 1 GPU.\nTop: Distribution of model error versus lr, wd, and also lr·wd\n(at 10 epochs and 400MF). Applying an empirical bootstrap, we\nsee that clear trends emerge, especially for lr and lr·wd. Middle:\nWe repeat this experiment but across various ﬂop regimes (trained\nfor 10 epochs each); the trends are stable. Bottom: Similarly, we\nrepeat the above while training for various number of epochs (in\nthe 400MF",
  "output": "The text discusses an experiment involving model error distribution versus learning rate (lr), weight decay (wd), and their product. The models are trained using a batch size of 128 on 1 GPU. The experiment is repeated across different flop regimes and various numbers of epochs, with clear trends emerging, particularly for lr and lr·wd. The trends remain stable across different conditions.",
  "summary_type": "reports"
}{
  "input": "rchitecture [34] signiﬁcantly increased the\nperformance of the NLP task with the self-attention mecha-\nnism. Funnel Transformer [7] improves the transformer ar-\nchitecture by reducing tokens by a pooling layer and skip-\nconnection. However, because of the basic difference be-\ntween the architecture of NLP and computer vision, the\nmethod of applying to pool is different from our method.\nSome studies are conducted to utilize the transformer archi-\ntecture to the backbone network for computer visio",
  "output": "The Funnel Transformer improves the transformer architecture for Natural Language Processing (NLP) tasks by reducing tokens through a pooling layer and skip-connection. However, due to differences between NLP and computer vision architectures, the pooling application method differs. Some studies aim to apply the transformer architecture to the backbone network for computer vision.",
  "summary_type": "reports"
}{
  "input": "pψ(xi|zi)] with an uniform prior as described in Equation (2). In the second stage,\nwe learn the prior pθ while keeping qφ and pψ ﬁxed. We simplify qφ(z|xi) to a one-point distribution\nwith the most likely visual tokens ˆzi = arg maxz qφ(z|xi). Then Equation (2) can be rewritten as:\nX\n(xi,˜xi)∈D\n\u0000Ezi∼qφ(z|xi)[log pψ(xi|zi)]\n|\n{z\n}\nStage 1: Visual Token Reconstruction\n+\nlog pθ(ˆzi|˜xi)\n|\n{z\n}\nStage 2: Masked Image Modeling\n\u0001\n(3)\nwhere the second term is our BEIT pre-training objective.\n2.5\nPre-T",
  "output": "The text describes a two-stage process in a visual modeling algorithm. In the first stage, the model learns the visual token reconstruction using a uniform prior. In the second stage, it learns the prior while keeping other parameters fixed. The model simplifies to a one-point distribution with the most likely visual tokens. The second term is the BEIT pre-training objective.",
  "summary_type": "reports"
}{
  "input": "G. Hinton, “A simple\nframework for contrastive learning of visual representations,”\npreprint arXiv:2002.05709, 2020. 1, 2, 5, 6, 7, 13\n[3] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski,\nand A. Joulin,\n“Unsupervised learning of visual fea-\ntures by contrasting cluster assignments,” arXiv preprint\narXiv:2006.09882, 2020. 1, 2, 5, 6, 7, 12\n[4] J.-B. Grill, F. Strub, F. Altch´e, C. Tallec, P. H. Richemond,\nE. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G.\nAzar, et al., “Bootstrap yo",
  "output": "G. Hinton proposed a simple framework for contrastive learning of visual representations in 2020. M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin also worked on unsupervised learning of visual features by contrasting cluster assignments in the same year. The text ends abruptly mentioning J.-B. Grill and others, possibly discussing another related work.",
  "summary_type": "reports"
}{
  "input": "ViTs are\nmore adversarially robust than CNNs [30] and the transferability of adversarial examples between\nCNNs and ViTs is remarkably low[31]. Follow up works [32, 33] extend the robustness study on\nViTs to much common image corruption and distribution shift, and indicate ViTs are more robust\nlearners. Although some ﬁnds are consistent with above works, in this paper, we do not make simple\ncomparison of robustness between ViTs and CNNs, but analyze the detailed robust component in\nViT and its v",
  "output": "ViTs (Vision Transformers) are more adversarially robust than CNNs (Convolutional Neural Networks), with low transferability of adversarial examples between them. Further studies extend the robustness analysis of ViTs to common image corruption and distribution shift, suggesting ViTs are more robust learners. This paper doesn't simply compare robustness between ViTs and CNNs, but analyzes the detailed robust component in ViT.",
  "summary_type": "reports"
}{
  "input": "eedy path ﬁltering w.t/w.o candaidate pool.\nInput: supernet N with parameter Ω,validation data Dval,\nnumber of sampled multiple paths m, number of kept\npaths k, candidate pool P with sampling probability ϵ.\n1: if without candidate pool P then\n2:\nsample m paths {ai}m\ni=1 i.i.d. w.r.t. ai ∼U(A)\n3: else\n4:\nsample m paths {ai}m\ni=1 i.i.d. w.r.t. ai ∼(1 −ϵ) ·\nU(A) + ϵ · U(P)\n5: end if\n6: randomly sample a batch ˆDval in Dval\n7: evaluate the loss ℓi of each path ai on ˆDval\n8: rank the paths by ℓi, an",
  "output": "The text describes a method for path filtering with or without a candidate pool. Given a supernet N, validation data Dval, a number of sampled paths m, kept paths k, and a candidate pool P with a sampling probability ϵ, the process involves sampling m paths. If there's no candidate pool, paths are sampled uniformly. If there is a pool, paths are sampled based on a combination of uniform sampling and the pool. A batch is then randomly sampled from Dval, the loss of each path is evaluated, and the paths are ranked accordingly.",
  "summary_type": "reports"
}{
  "input": "e processing [5, 58], image classiﬁcation [54, 67, 65, 14, 21,\n59, 12, 61, 12, 66, 31, 55, 19, 24], object detection [3, 74] and semantic segmentation [60, 71, 49].\nRather than concentrating on one special task, some recent works [59, 70, 39] try to design a general\nvision Transformer backbone for general-purpose vision tasks. They all follow the hierarchical\n2\n\n--------------------------------------------------\nℎ1\nConcat\n(a) Cross-Shaped Window Self-Attention\nSplit\nCross-Shaped Window\n𝑠𝑤\n𝑠𝑤\n𝑠𝑤",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "denoted as {Vj|j = 1, ..., m}, which are the weights for LossCE over\ndifferent samples. To obtain these values, VCM ﬁrstly calculates the suitability matrix Sm×n via\nEquation. 5. Then variance for elements of each row vector Sj,: can reﬂect the diversity for the\nexperts. A small diversity means xj can be assigned to an arbitrary expert with little to no drop of\naccuracy, thus Vj can be smaller. Therefore, VCM determines the value of Vj based on the normalized\nstandard deviation of Sj,::\nVj =\nSt",
  "output": "The text discusses the calculation of weights {Vj|j = 1, ..., m} for LossCE over different samples in VCM. It first calculates the suitability matrix Sm×n. The variance of each row vector Sj,: reflects the diversity of the experts. A small diversity implies xj can be assigned to any expert with minimal accuracy loss, hence a smaller Vj. VCM determines Vj based on the normalized standard deviation of Sj,::.",
  "summary_type": "reports"
}{
  "input": "ation. We explicitly analyze the differential of performance\nbetween the ImageNet-val and the distinct ImageNet-V2 test set. The relative offsets between curves\nreﬂect to which extent models are overﬁtted to ImageNet-val w.r.t. hyper-parameter selection. The\ndegree of overﬁtting of our MLP-based model is overall neutral or slightly higher to that of other\ntransformer-based architectures or convnets with same training procedure.\nNormalization & activation. Our network conﬁguration does not contai",
  "output": "The text discusses a performance analysis between the ImageNet-val and ImageNet-V2 test set, focusing on model overfitting with respect to hyper-parameter selection. The overfitting degree of the MLP-based model is similar or slightly higher than other transformer-based architectures or convnets with the same training procedure. The network configuration does not contain normalization and activation.",
  "summary_type": "reports"
}{
  "input": "egate context information due to its limited\nreceptive ﬁeld.\nThe motivation of Cycle FC is to enjoy channel FC’s\nmerit of taking input with arbitrary resolution and linear\ncomputational complexity while enlarging its receptive ﬁeld\nfor context aggregation. To achieve this goal, our Cycle\nFC samples points in a cyclical style along the channel di-\nmension (Figure 1c). In this way, Cycle FC has the same\ncomplexity (both the number of parameters and FLOPs) as\nchannel FC, while increasing the recept",
  "output": "Cycle FC aims to combine the benefits of channel FC's ability to handle inputs of any resolution and linear computational complexity, while also expanding its receptive field for context aggregation. This is achieved by sampling points in a cyclical manner along the channel dimension. As a result, Cycle FC maintains the same complexity as channel FC, in terms of both parameters and FLOPs, but with an increased receptive field.",
  "summary_type": "reports"
}{
  "input": "object detection and segmentation, multi-scale Vision\n\n--------------------------------------------------\nLongformer pretrained on ImageNet can be transferred to\nsuch high-resolution tasks, as we will show in Section 4.3.\nHowever, Linformer is not transferable because the\nweights of the linear projection layer is speciﬁc to a reso-\nlution. The Partial X-formers and Multi-scale ViT with full\nattention are not transferable due to its prohibitively large\nmemory usage after transferred to high-reso",
  "output": "Longformer, pretrained on ImageNet, is transferable to high-resolution tasks, as demonstrated in Section 4.3. However, Linformer is not transferable due to the specificity of the linear projection layer's weights to a resolution. The Partial X-formers and Multi-scale ViT with full attention are not transferable because of their excessive memory usage when transferred to high-resolution.",
  "summary_type": "reports"
}{
  "input": "s hold\nagainst all the other solutions aj (with j ̸= i) in the entire search\nspace of a.\nWith the above deﬁnition, we can sort solutions to different\nranks of domination, where solutions in the same rank are non-\ndominated to each other, and there exists at least one solution in\nlower rank that dominates any solution in the higher rank. Thus, a\nlower non-dominated ranked set is lexicographically better than a\nhigher ranked set. This process is referred as non dominated sort,\nand it is the ﬁrst s",
  "output": "The text discusses a process called non-dominated sort. Solutions are sorted into different ranks of domination, where solutions in the same rank are non-dominated to each other. There is at least one solution in a lower rank that dominates any solution in a higher rank. Therefore, a lower non-dominated ranked set is lexicographically better than a higher ranked set.",
  "summary_type": "reports"
}{
  "input": "tend to bias toward either shape or\ntexture, depending on the training dataset.\nWe verify that such biased representation learning (towards either shape or texture) weakens CNNs’\nperformance.1 Nonetheless, surprisingly, we also ﬁnd (1) the model with shape-biased representa-\ntions and the model with texture-biased representations are highly complementary to each other, e.g.,\nthey focus on completely different cues for predictions (an example is provided in Figure 1); and\n(2) being biased toward",
  "output": "CNNs' performance is weakened by biased representation learning towards either shape or texture, depending on the training dataset. However, models with shape-biased and texture-biased representations are highly complementary, focusing on different prediction cues.",
  "summary_type": "reports"
}{
  "input": ".7B\n85.3\nMulti-Stage TFM Only\nHaloNet-H4\n3842\n85M\n-\n85.6\nHaloNet-H4\n5122\n85M\n-\n85.8\nSwin-B\n3842\n88M\n47.0B\n86.0\nSwin-L\n3842\n197M\n103.9B\n86.4\nMulti-Stage Conv+TFM\nHaloNet-Conv-H4\n3842\n87M\n-\n85.5\nHaloNet-Conv-H4\n5122\n87M\n-\n85.8\nCvT-13\n3842\n20M\n16B\n83.3\nCvT-21\n3842\n32M\n25B\n84.9\nCvT-W24\n3842\n277M\n193.2B\n87.7\nProposed\nMulti-Stage Conv+TFM\nCoAtNet-2\n3842\n75M\n49.8B\n87.1\nCoAtNet-3\n3842\n168M\n107.4B\n87.6\nCoAtNet-4\n3842\n275M\n189.5B\n87.9\n+ PT-RA\n3842\n275M\n189.5B\n88.3\n+ PT-RA-E150\n3842\n275M\n189.5B\n88.4\nCoAtNe",
  "output": "The text appears to be a comparison of different models or systems, possibly related to machine learning or AI. Key details include the model names (e.g., HaloNet-H4, Swin-B, CvT-13), possibly their sizes (e.g., 85M, 88M, 20M), and performance metrics (e.g., 85.3, 86.0, 83.3). The highest performance metric is 88.4 for the \"+ PT-RA-E150\" model. The CoAtNet series seems to be a proposed solution, with CoAtNet-4 and its variations showing high performance.",
  "summary_type": "reports"
}{
  "input": "neural\nnetwork architectures that trade-off multiple objectives for a\ngiven image classiﬁcation task. We introduced Neural Architec-\nture Transfer (NAT), a practical and effective approach for this\npurpose. We described our efforts to harness the concept of a\nTABLE 9: Effect of different training setups. Details of the standard and\nadvanced settings under Random Initialization are provided in Table 7.\nTraining\nSettings\nRandom Initialization\nInherited from Supernet\nstandard\nadvanced\nw/o ﬁne-tune",
  "output": "The text discusses Neural Architecture Transfer (NAT), a method for balancing multiple objectives in image classification tasks. It also mentions different training setups, including standard and advanced settings under Random Initialization, and whether they are inherited from Supernet or not. The effect of these setups without fine-tuning is also mentioned.",
  "summary_type": "reports"
}{
  "input": "J.: Revisiting knowledge distillation via label\nsmoothing regularization. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. pp. 3903–3911 (2020)\n52. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving the perfor-\nmance of convolutional neural networks via attention transfer. In: Fifth International Confer-\nence on Learning Representations (2017)\n53. Zhang, Y., Lan, Z., Dai, Y., Zeng, F., Bai, Y., Chang, J., Wei, Y.: Prime-aware adaptiv",
  "output": "The text refers to two studies. The first one revisits knowledge distillation through label smoothing regularization, presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020). The second study focuses on improving the performance of convolutional neural networks via attention transfer, presented at the Fifth International Conference on Learning Representations (2017). The third reference seems incomplete.",
  "summary_type": "reports"
}{
  "input": "the gap between training, when the mask is used, and\ninference, when the mask is not used, we scale the values of\nthe non-drop neurons properly during training. Speciﬁcally,\nto apply a dropout pattern to a layer h of a neural network,\nwe randomly generate a binary mask m of the same shape\nwith h. We then scale the values in the mask m, and replace\nh with:\nDrop(h, m)\n△= h ⊗\n\u0012 Size(m)\nSum(m) · m\n\u0013\n(1)\nDimensional notations.\nIn modern deep learning frame-\nworks (Abadi et al. 2016; Paszke et al. 20",
  "output": "The text discusses the use of a mask in training a neural network. During training, a binary mask of the same shape as a layer in the network is generated and applied. The values in the mask are scaled and replace the original layer. This process, referred to as dropout, helps to manage the gap between training (when the mask is used) and inference (when it is not). The process is used in modern deep learning frameworks.",
  "summary_type": "reports"
}{
  "input": "19.\nDeVries, T. and Taylor, G. W.\nImproved regularization\nof convolutional neural networks with cutout.\narXiv\npreprint arXiv:1708.04552, 2017.\nGhiasi, G., Lin, T.-Y., and Le, Q. V. Dropblock: A regular-\nization method for convolutional networks. In NeurIPS,\npp. 10727–10737, 2018.\nGong, C., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.-\nY. Frage: Frequency-agnostic word representation. In\nNeurIPS, pp. 1334–1345, 2018.\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining\nand harnessing adve",
  "output": "DeVries and Taylor proposed an improved regularization method for convolutional neural networks called cutout in 2017. Ghiasi, Lin, and Le introduced Dropblock, another regularization method for these networks, at NeurIPS in 2018. Gong et al. presented Frage, a frequency-agnostic word representation, at the same conference. Goodfellow, Shlens, and Szegedy have also contributed to the field, but the text cuts off before their work is described.",
  "summary_type": "reports"
}{
  "input": "n as x. See Figure 5 for a zoomed-in version on the largest models (versus inference FLOPs).\n23\n\n--------------------------------------------------\nTable 10: Time and FLOPs unmatched inference results for JFT prec@1 and ImageNet 5shot.\nModel\nExperts\nRouting\nJFT prec@1\nINet/5shot\nTime[%]\nFLOPs[%]\nVIT-H/14\n-\n-\n56.68\n76.95\n100.00\n100.00\nVIT-L/16\n-\n-\n53.40\n74.36\n27.58\n36.83\nV-MoE-L/16\nLast-2\nVanilla\n56.76\n76.53\n32.56\n39.02\nV-MoE-L/16\nEvery-2\nVanilla\n57.64\n77.10\n57.40\n49.95\nV-MoE-H/14\nLast-5\nVanilla",
  "output": "The table presents unmatched inference results for JFT prec@1 and ImageNet 5shot. The models VIT-H/14, VIT-L/16, V-MoE-L/16 (Last-2 and Every-2), and V-MoE-H/14 are compared. The results show the percentage of time and FLOPs used. For example, VIT-H/14 used 100% of the time and FLOPs, while VIT-L/16 used 27.58% of the time and 36.83% of the FLOPs. The V-MoE-L/16 model with Last-2 routing used 32.56% of the time and 39.02% of the FLOPs.",
  "summary_type": "reports"
}{
  "input": "are 0.9\nbetter than K-means.\nVisualization We visualize our second centroid attention\nblocks’ feature cluster in Figure 4. We plot the sampled\npoint in white star and its K-Nearest-Neighbour (KNNs)\nin red point. We set K = 40 and use L2 distance in\nKNNs. From Figure 4 we can see that rather than gather-\ning around the sampled point in the 3D space, the KNNs\nin our features space tend to distribute within the same se-\nmantic part of the query part, which indicates our feature\ncaptures high-level",
  "output": "The text discusses a visualization of a feature cluster from a second centroid attention block. The K-Nearest-Neighbour (KNNs) points, set at K=40 and using L2 distance, are plotted. Instead of clustering around the sampled point in 3D space, the KNNs distribute within the same semantic part of the query, suggesting that the feature captures high-level semantics.",
  "summary_type": "reports"
}{
  "input": "es & Taylor, 2017).\nRecently, a line of research called mixup has been proposed.\nThese methods mainly focus on creating previously unseen\nvirtual mixup examples via convex combination or local\nreplacement of data for training (Zhang et al., 2018; Verma\net al., 2019; Yun et al., 2019; Guo et al., 2019).\nHowever, the underlying data domains contain rich re-\ngional saliency information (i.e. foreground objects in vi-\nsion, prominent syllables in speech, informative textual units\nin language) (Simon",
  "output": "Recent research, known as mixup, focuses on generating new virtual mixup examples through convex combination or local data replacement for training. However, these data domains contain valuable regional saliency information, such as foreground objects in vision, prominent syllables in speech, and informative textual units in language.",
  "summary_type": "reports"
}{
  "input": "der of this section, we\nprovide an overview of NSGA-III procedure and refer readers to\nthe original publication for more details.\nDomination is a widely-used partial ordering concept for\ncomparing two objective vectors. For a generic many-objective\noptimization problem: mina {f1(a), . . . , fm(a)}, where fi(·)\nare the objectives (say, loss functions) to be optimized and a is\nthe representation of a neural network architecture. For two given\nsolutions a1 and a2, solution a1 is said to dominate a2",
  "output": "This section provides an overview of the NSGA-III procedure, a method for comparing two objective vectors in many-objective optimization problems. This is often used in the context of optimizing objectives, such as loss functions, in neural network architecture. In this context, if two solutions, a1 and a2, are given, a1 is said to dominate a2 if it is superior.",
  "summary_type": "reports"
}{
  "input": "a large number\nof classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing, pp.\n722–729, 2008. doi: 10.1109/ICVGIP.2008.47.\n12\n\n--------------------------------------------------\nPreprint\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar.\nCats and dogs.\nIn\n2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3498–3505, 2012. doi:\n10.1109/CVPR.2012.6248092.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improvin",
  "output": "The text refers to two conference papers. The first, presented at the 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing, doesn't provide specific details. The second, titled \"Cats and Dogs\", was presented at the 2012 IEEE Conference on Computer Vision and Pattern Recognition. The authors include Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. There's also a mention of Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever, but without context.",
  "summary_type": "reports"
}{
  "input": "[46], 300 millions images). The paper concluded that\ntransformers “do not generalize well when trained on insufﬁcient amounts of data”,\nand the training of these models involved extensive computing resources.\nIn this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of ﬁne-tuning)\nthat is competitive with convnets having a similar number of parameters and\nefﬁciency. It uses Imagenet as the sole training set. We bui",
  "output": "The paper discusses training a vision transformer on an 8-GPU node within two to three days, using Imagenet as the sole training set. It suggests that transformers do not generalize well with insufficient data. However, the trained model competes with convnets of similar parameters and efficiency, despite the extensive computing resources involved in the training.",
  "summary_type": "reports"
}{
  "input": "e 2 do not conduct experiments on dense prediction\ntasks due to the incapability of dealing with variable input\nscales, which is discussed in Sec 1. However, CycleMLP\nsolved this issue by adopting Cycle FC. The experimental\nresults on dense prediction tasks are presented in Sec 4.3\nand Sec 4.4.\nComparison with SOTA Models.\nTable 3 further\ncompares CycleMLP with previous state-of-the-art CNN,\nTransformer and Hybrid architectures.\nIt is interesting\nto see that CycleMLP models achieve comparable pe",
  "output": "CycleMLP models, unlike e 2, can handle variable input scales, making them suitable for dense prediction tasks. This was achieved by adopting Cycle FC. Experimental results are detailed in sections 4.3 and 4.4. A comparison with previous state-of-the-art CNN, Transformer, and Hybrid architectures shows that CycleMLP models perform comparably.",
  "summary_type": "reports"
}{
  "input": ", and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88(2):303–338, 2010. 5, 9\n[9] Jonathan Frankle and Michael Carbin. The lottery ticket hy-\npothesis: Finding sparse, trainable neural networks. arXiv\npreprint arXiv:1803.03635, 2018. 10\n[10] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock:\nA regularization method for convolutional networks.\nIn\nAdvances in Neural Information Processing Systems, pages\n10727–10737, 2018. 6\n[11] R",
  "output": "The Pascal Visual Object Classes (VOC) challenge is a topic in the International Journal of Computer Vision. Jonathan Frankle and Michael Carbin proposed the Lottery Ticket Hypothesis, which involves finding sparse, trainable neural networks. Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le introduced DropBlock, a regularization method for convolutional networks, in Advances in Neural Information Processing Systems.",
  "summary_type": "reports"
}{
  "input": "Large setting with four experts.\nConsidering many works [9, 25] select branches based on the category, we ﬁrstly experiment to\nobserve the relationship between selection pattern and rough prediction of the delegator. We count\nthe probabilities for each expert to be selected within each predicted class on the ImageNet validation\nset. For better visualization, we cluster the 1000 probability vectors and then plot them on Fig. 8. It\ncan be seen that samples with the same predicted class are assigne",
  "output": "The text discusses an experiment observing the relationship between selection patterns and rough predictions in a large setting with four experts. The probabilities of each expert being selected within each predicted class on the ImageNet validation set are counted. For better visualization, the 1000 probability vectors are clustered and plotted. The results show that samples with the same predicted class are assigned similarly.",
  "summary_type": "reports"
}{
  "input": "jective optimization using evolutionary\nalgorithms — a comparative case study,” in Parallel Problem Solving\nfrom Nature — PPSN V, A. E. Eiben, T. B¨ack, M. Schoenauer, and H.-P.\nSchwefel, Eds.\nBerlin, Heidelberg: Springer Berlin Heidelberg, 1998,\npp. 292–301.\n[79] F. Hutter, H. H. Hoos, and K. Leyton-Brown, “Sequential model-based\noptimization for general algorithm conﬁguration,” in International Con-\nference on Learning and Intelligent Optimization.\nSpringer, 2011, pp.\n507–523.\n[80] P. Goyal, P",
  "output": "The text refers to two studies. The first, from 1998, discusses objective optimization using evolutionary algorithms and is a comparative case study. It was presented at the Parallel Problem Solving from Nature conference. The second, from 2011, discusses sequential model-based optimization for general algorithm configuration. It was presented at the International Conference on Learning and Intelligent Optimization.",
  "summary_type": "reports"
}{
  "input": ", indicating that incorporating these S&E layers\nhelps but does not fully recover the expressivity of dense\n1 × 1 convolutions.\nFinally, we did not experiment with any attention variants\n(Bello, 2021; Srinivas et al., 2021), and we expect that our\nresults could likely be improved by adopting these strategies\ninto our models.\n\n--------------------------------------------------",
  "output": "The text suggests that incorporating S&E layers aids but doesn't completely restore the expressivity of dense 1x1 convolutions. The authors haven't experimented with any attention variants, but anticipate that their results could potentially be enhanced by integrating these strategies into their models.",
  "summary_type": "reports"
}{
  "input": "context and\ng(X) =\n1\nH×W\nPH\ni=1\nPW\nj=1 X[:,i,j] is the global average\npooling (GAP). δ denotes the Rectiﬁed Linear Unit (ReLU)\n[27], and B denotes the Batch Normalization (BN) [18]. σ\nis the Sigmoid function. This is achieved by a bottleneck\nwith two fully connected (FC) layers, where W1 ∈R\nC\nr ×C\nis a dimension reduction layer, and W2 ∈RC× C\nr is a di-\nmension increasing layer. r is the channel reduction ratio.\nWe can see that the channel attention squeezes each fea-\nture map of size H × W into",
  "output": "The text discusses a global average pooling (GAP) function, g(X), used in machine learning. It involves Rectified Linear Unit (ReLU), Batch Normalization (BN), and Sigmoid functions. The process is facilitated by a bottleneck with two fully connected (FC) layers, which include a dimension reduction layer and a dimension increasing layer. The channel reduction ratio is denoted by 'r'. The channel attention process condenses each feature map of size H × W.",
  "summary_type": "reports"
}{
  "input": "[22].\nTo overcome these limitations, Transformer backbones are recently explored for their ability to\ncapture long-distance information [5, 32, 25, 18]. Unlike CNN backbones, the Transformer ones ﬁrst\n∗This work is funded by the Natural Science Foundation of China (No. 62176119).\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2105.13677v5  [cs.CV]  14 Oct 2021\n\n--------------------------------------------------\nFigure 1: Examples of backbone blo",
  "output": "Transformer backbones are being investigated for their potential to capture long-distance information, unlike CNN backbones. This research is funded by the Natural Science Foundation of China. The work was presented at the 35th Conference on Neural Information Processing Systems (NeurIPS 2021) in Sydney, Australia.",
  "summary_type": "reports"
}{
  "input": "128, G=8\n 5x5, 128, G=4\n 3x3, 128, G=1\n1x1, 512\nBN\nReLU\n1x1, 512\nBN\nReLU\nBN\nReLU\nUpsample\n(a) Local PyConv\n(b) Global PyConv\nFigure 6: PyConv blocks.\n5\nPyConv Network on Semantic Segmentation\nOur proposed framework for scene parsing (image segmentation) is illustrated in Fig. 5. To build an\neffective pipeline for scene parsing, it is necessary to create a head that can parse the feature maps\nprovided by the backbone and obtain not only local but also global information. The head should be\nable t",
  "output": "The text describes PyConv, a framework for scene parsing or image segmentation. It involves creating a head that can parse feature maps provided by the backbone, obtaining both local and global information. The process includes various PyConv blocks and steps like upsampling, and uses functions like ReLU and BN.",
  "summary_type": "reports"
}{
  "input": "The gain of Meta Pseudo Labels over UDA,\nalbeit smaller than the gain of UDA over RandAugment, is signiﬁcant as UDA is already very strong.\nmaking labeled data the same with unlabeled data in Figure 1. In this case, Meta Pseudo Labels can be seen as an adaptive\nform of Label Smoothing: the teacher generates soft labels on labeled data for the student, just like the way Label Smoothing\nsmooths the hard labels to regularize the model. The main difference is that the policy in Label Smoothing is ﬁ",
  "output": "Meta Pseudo Labels (MPL) improves upon Unsupervised Data Augmentation (UDA), although the gain is less than UDA's over RandAugment. MPL can be seen as an adaptive form of Label Smoothing, where the teacher generates soft labels on labeled data for the student. The key difference is that the policy in Label Smoothing is fixed, while MPL's is adaptive.",
  "summary_type": "reports"
}{
  "input": "y outperforms existing transformer-based visual models, being comparable to CNN models.†: reported\nby [50], ⋆: reported by [63].\nModels\nTop-1 Acc.\nTop-5 Acc.\n#Parameters\nFLOPs\nResolution\nModel Type\nDesign Type\nMobileNetV3Large1.0 [21]\n75.2%\n-\n5.4M\n0.22G\n2242\nCNN\nAuto\nEfﬁcietNet-B0[49]\n77.1%\n93.3%\n5.4M\n0.39G\n2242\nCNN\nAuto\nDeiT-tiny [50]\n72.2%\n91.1%\n5.7M\n1.2G\n2242\nTransformer\nManual\nAutoFormer-tiny (Ours)\n74.7%\n92.6%\n5.7M\n1.3G\n2242\nTransformer\nAuto\nResNet50⋆[18]\n79.1%\n-\n25.5M\n4.1G\n2242\nCNN\nManual",
  "output": "The text compares the performance of different visual models. MobileNetV3Large1.0 has a top-1 accuracy of 75.2% with 5.4M parameters. EfficietNet-B0 has a top-1 accuracy of 77.1% and top-5 accuracy of 93.3% with 5.4M parameters. DeiT-tiny has a top-1 accuracy of 72.2% and top-5 accuracy of 91.1% with 5.7M parameters. AutoFormer-tiny has a top-1 accuracy of 74.7% and top-5 accuracy of 92.6% with 5.7M parameters. ResNet50 has a top-1 accuracy of 79.1% with 25.5M parameters.",
  "summary_type": "reports"
}{
  "input": "at the convolution-induced translation equivariance and locality help ResNets es-\ncape from bad local minima when trained on visual data. However, we need improved learning\nalgorithms to prevent them from happening to the convolution-free ViTs and and MLP-Mixers. The\nﬁrst-order optimizers (e.g., SGD and Adam (Kingma & Ba, 2015)) only seek the model parameters\nthat minimize the training error. They dismiss the higher-order information such as ﬂatness that cor-\nrelates with the generalization (Kes",
  "output": "Convolution-induced translation equivariance and locality aid ResNets in avoiding bad local minima during visual data training. However, convolution-free ViTs and MLP-Mixers require enhanced learning algorithms to prevent this. First-order optimizers like SGD and Adam only target model parameters that reduce training error, ignoring higher-order information like flatness, which is related to generalization.",
  "summary_type": "reports"
}{
  "input": "R101+NL\nX3D-XL\nX3D-M\nMViT-B 16x4\nMViT-B 32x2\nX3D-S\nViViT-L ImageNet-21K\nTimeSformer ImageNet-21K\nVTN ImageNet-1K / 21K\nFigure A.4. Accuracy/complexity trade-off on K400-val for varying # of inference clips per video. The top-1 accuracy (vertical axis) is\nobtained by K-Center clip testing where the number of temporal clips K ∈{1, 3, 5, 7, 10} is shown in each curve. The horizontal axis\nmeasures the full inference cost per video. The left-sided plots show a linear and the right plots a logarithmic",
  "output": "The text discusses an accuracy/complexity trade-off on K400-val for varying numbers of inference clips per video. The top-1 accuracy is obtained by K-Center clip testing where the number of temporal clips K can be 1, 3, 5, 7, or 10, as shown in each curve. The horizontal axis measures the full inference cost per video. The left-sided plots show a linear and the right plots a logarithmic.",
  "summary_type": "reports"
}{
  "input": "nter-biased receptive ﬁeld [35], leading the model to\nignore the features at the image boundary. In contrast, self-attention based feature aggregation can\neffectively model the global-range relation among features but suffer the over-smoothing issue. DLA\ncan effectively overcome their limitations and better model the local and global context jointly.\n5\n\n--------------------------------------------------\n3.4\nReﬁner and Reﬁned-ViT\nThe attention expansion and distributed local attention can be easi",
  "output": "The text discusses the limitations of inter-biased receptive field models, which tend to ignore features at image boundaries, and self-attention based feature aggregation, which can over-smooth. Distributed Local Attention (DLA) is presented as a solution that can effectively model both local and global contexts. The text also mentions the attention expansion and distributed local attention as components of the Refiner and Refined-ViT models.",
  "summary_type": "reports"
}{
  "input": ". This explicit separation avoids the contradictory objective of guiding\nthe attention process while processing the class embedding. We refer to this\nnew architecture as CaiT (Class-Attention in Image Transformers).\nIn the experimental Section 4, we empirically show the effectiveness and\ncomplementary of our approaches:\n• LayerScale signiﬁcantly facilitates the convergence and improves the ac-\ncuracy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at",
  "output": "The new architecture, CaiT (Class-Attention in Image Transformers), separates guiding the attention process from processing the class embedding to avoid contradiction. The LayerScale approach significantly aids convergence and enhances the accuracy of image transformers at larger depths, adding a few thousand parameters to the network.",
  "summary_type": "reports"
}{
  "input": "ansformer to image clas-\nsiﬁcation.\nThey propose ViT [5] that reshapes image\nto patches for feature extraction by transformer encoder,\nwhich achieves comparable results to CNN-based mod-\nels.\nDeiT [35] further employs more data augmentation\nand makes ViT comparable to CNN-based model with-\nout ImageNet-22k or JFT-300M pretraining.\nDeiT also\nproposes an attention-based distillation method, which is\nused for student-teacher training, leading to even better\nperformance. PVT [37] proposes feature py",
  "output": "ViT [5] reshapes images into patches for feature extraction by a transformer encoder, achieving results similar to CNN-based models. DeiT [35] uses more data augmentation, making ViT comparable to CNN-based models without ImageNet-22k or JFT-300M pretraining. DeiT also introduces an attention-based distillation method for student-teacher training, improving performance. PVT [37] proposes a feature pyramid.",
  "summary_type": "reports"
}{
  "input": "recognition. In Computer Vision and Pattern Recognition, 2016.\n[29] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In International\nConference on Computer Vision, 2017.\n[30] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv preprint arXiv:1912.12180, 2019.\n[31] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig Adam, Pietro Perona,\nand Serge J. Belongie. The iNaturalist spec",
  "output": "This text mentions various research papers in the field of computer vision and pattern recognition. Notable works include Kaiming He et al.'s \"Mask R-CNN\" presented at the International Conference on Computer Vision in 2017, and Jonathan Ho et al.'s \"Axial attention in multidimensional transformers\" available on arXiv. Another work mentioned is related to iNaturalist but the text cuts off before providing full details.",
  "summary_type": "reports"
}{
  "input": "(Net1) is\nshown in Table 2. As can be seen, this transition can sub-\nstantially improve the base performance. Our further ex-\nperiments show that adding global pooling itself can im-\nprove the base performance from 64.17% to 69.44%. In\nother words, the global average pooling operation which is\nwidely used in convolution-based models since NIN [24],\nenables the network to learn more efﬁciently under moder-\nate augmentation. Furthermore, this transition can slightly\nimprove the elite performance.",
  "output": "The text discusses the improvement in the base performance of a network model, Net1, through the addition of global pooling. This transition increases the base performance from 64.17% to 69.44%. The global average pooling operation, commonly used in convolution-based models, allows the network to learn more efficiently under moderate augmentation. This transition also slightly enhances the elite performance.",
  "summary_type": "reports"
}{
  "input": "019. 3, 6\n[6] K. Deb. Multi-objective optimization using evolutionary al-\ngorithms. Chichester: Wiley, 2001. 4\n[7] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Ef-\nﬁcient multi-objective neural architecture search via lamar-\nckian evolution. In International Conference on Learning\nRepresentations (ICLR), 2019. 3\n[8] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\nWilliams, J. Winn, and A. Zisserman. The pascal visual ob-\nject classes challenge: A retrospective. International Journa",
  "output": "K. Deb discusses multi-objective optimization using evolutionary algorithms in his 2001 publication. Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter presented an efficient multi-objective neural architecture search via Lamarckian evolution at the 2019 International Conference on Learning Representations. M. Everingham et al. retrospectively examined the Pascal visual object classes challenge in an international journal.",
  "summary_type": "reports"
}{
  "input": "e of visual rep-\nresentation.\nIn this context, the desiderata of com-\nposing pixel pairs for relation modeling is challenged.\nFurthermore, we unify the view of self-attention and\nconvolution through the lens of our involution.\n3. The involution-powered architectures work universally\nwell across a wide array of vision tasks, including im-\nage classiﬁcation, object detection, instance and se-\nmantic segmentation, offering signiﬁcantly better per-\nformance than the convolution-based counterparts.\n2",
  "output": "The text discusses the challenges of composing pixel pairs for relation modeling in visual representation. It introduces the concept of involution, which unifies the view of self-attention and convolution. Involution-powered architectures are effective across various vision tasks such as image classification, object detection, and segmentation, performing significantly better than convolution-based counterparts.",
  "summary_type": "reports"
}{
  "input": "−1), and\nvalues in B are taken from ˆB.\n4To make the window size (M, M) divisible by the feature map size of\n(h, w), bottom-right padding is employed on the feature map if needed.\nWe observe signiﬁcant improvements over counterparts\nwithout this bias term or that use absolute position embed-\nding, as shown in Table 4. Further adding absolute posi-\ntion embedding to the input as in [20] drops performance\nslightly, thus it is not adopted in our implementation.\nThe learnt relative position bias in",
  "output": "The text discusses a method where the window size (M, M) is made divisible by the feature map size (h, w) using bottom-right padding if necessary. This method shows significant improvements over alternatives without this bias term or using absolute position embedding. Adding absolute position embedding to the input slightly reduces performance, so it isn't adopted in the implementation. The relative position bias is learned.",
  "summary_type": "reports"
}{
  "input": "popular way to fur-\nther improve accuracy is by training on additional sources\nof data (either labeled, weakly labeled, or unlabeled). Pre-\ntraining on large-scale datasets (Sun et al., 2017; Maha-\njan et al., 2018; Kolesnikov et al., 2019) has signiﬁcantly\npushed the state-of-the-art, with ViT (Dosovitskiy et al.,\n2020) and NFNets (Brock et al., 2021) recently achiev-\ning 88.6% and 89.2% ImageNet accuracy respectively.\nNoisy Student, a semi-supervised learning method, ob-\ntained 88.4% ImageNet",
  "output": "Training on additional data sources can improve accuracy. Pre-training on large-scale datasets has significantly advanced the field. ViT and NFNets recently achieved 88.6% and 89.2% ImageNet accuracy respectively. The semi-supervised learning method, Noisy Student, obtained 88.4% ImageNet accuracy.",
  "summary_type": "reports"
}{
  "input": "g Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122, 2021.\n[72] Ross\nWightman.\nPytorch\nimage\nmodels.\nhttps://github.com/rwightman/\npytorch-image-models, 2019.\n[73] Yuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision, 2018.\n[74] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual",
  "output": "The Pyramid Vision Transformer is a versatile backbone for dense prediction without convolutions, as per a 2021 preprint on arXiv. Ross Wightman has developed PyTorch image models, available on GitHub since 2019. Yuxin Wu and Kaiming He introduced Group Normalization at the European Conference on Computer Vision in 2018. Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun have worked on a Unified Perceptual.",
  "summary_type": "reports"
}{
  "input": "75.1\n44.6\n52.6\n1B\n60K\n47.2\n51.3\n50.6\n55.8\n78.2\n84.5\n53.9\n60.8\n1B\n120K\n51.5\n55.8\n53.6\n59.4\n83.3\n86.8\n59.0\n66.8\n1B\n400K\n58.9\n62.6\n56.7\n62.3\n88.0\n90.6\n65.7\n72.2\n1B\n1.2M\n61.5\n65.2\n59.6\n64.4\n90.7\n92.1\n67.3\n75.0\n1B\n2M\n62.8\n66.6\n60.8\n66.0\n90.7\n92.0\n69.1\n75.5\n1B\n4M\n64.0\n67.4\n61.4\n66.2\n91.2\n92.1\n69.5\n74.9\n3B\n20K\n32.3\n36.5\n38.4\n43.8\n56.2\n59.7\n37.7\n45.4\n3B\n30K\n38.8\n43.1\n43.8\n50.7\n68.8\n75.1\n45.9\n53.9\n3B\n120K\n52.6\n56.3\n53.5\n58.8\n83.8\n87.6\n58.5\n66.0\n3B\n400K\n59.1\n62.7\n56.9\n62.2\n88.7\n90.8\n65.8\n72.1\n3B\n1.2M\n62.1",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "thinking batch normalization in trans-\nformers. In International Conference on Machine Learn-\ning, pp. 8741–8751. PMLR, 2020.\nSimonyan, K. and Zisserman, A. Very deep convolutional\nnetworks for large-scale image recognition. In 3rd Inter-\nnational Conference on Learning Representations, ICLR,\n2015.\nSingh, S. and Shrivastava, A. Evalnorm: Estimating batch\nnormalization statistics for evaluation. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pp. 3633–3641, 2019.\nSmith",
  "output": "This text refers to several academic papers. The first discusses the application of batch normalization in transformers, presented at the International Conference on Machine Learning in 2020. The second paper, by Simonyan and Zisserman, focuses on very deep convolutional networks for large-scale image recognition, presented at the 3rd International Conference on Learning Representations in 2015. The third paper, by Singh and Shrivastava, introduces Evalnorm, a method for estimating batch normalization statistics for evaluation, presented at the IEEE/CVF International Conference on Computer Vision in 2019.",
  "summary_type": "reports"
}{
  "input": "arger datasets, ImageNet21-K and JFT. Although such regularization can\nharm the pre-training metrics, this allows more versatile regularization and augmentation during\nﬁnetuning, leading to improved down-stream performances.\n4.2\nMain Results\n0\n5\n10\n15\n20\n25\n30\n35\nFLOPs (Billions)\n80\n81\n82\n83\n84\nImageNet Top-1 Accuracy (%)\nDeiT\nCaiT\nT2T-ViT\nDeepViT\nCvT\nSwinTFM\nCoAtNet-0\nCoAtNet-1\nCoAtNet-2\nCoAtNet-3\nFigure 2: Accuracy-to-FLOPs scaling curve un-\nder ImageNet-1K only setting at 224x224.\n0\n50\n100\n15",
  "output": "The text discusses the use of regularization in large datasets like ImageNet21-K and JFT. Despite potentially harming pre-training metrics, this approach allows for more versatile regularization and augmentation during fine-tuning, which can improve downstream performances. The text also includes a figure showing the accuracy-to-FLOPs scaling curve under ImageNet-1K only setting at 224x224 for various models including DeiT, CaiT, T2T-ViT, DeepViT, CvT, SwinTFM, and CoAtNet-0 to CoAtNet-3.",
  "summary_type": "reports"
}{
  "input": "nd dﬀn=3072. Each gMLP model has L=36\nlayers with dmodel=512 and dﬀn = 3072. No positional encodings are used for Mixer or gMLPs.\nModel\nPerplexity∗\nParams (M)\nBERTbase\n4.37\n110\nBERTbase + rel pos\n4.26\n110\nBERTbase + rel pos - attn\n5.64\n96\nMLP-Mixer\n5.34\n112\nLinear gMLP, s(Z) = f(Z)\n5.14\n92\nAdditive gMLP, s(Z) = Z + f(Z)\n4.97\n92\nMultiplicative gMLP, s(Z) = Z ⊙f(Z)\n4.53\n92\nMultiplicative, Split gMLP, s(Z) = Z1 ⊙f(Z2), Z = Z1∥Z2\n4.35\n102\n* Standard deviation across multiple independent runs is arou",
  "output": "The text presents a comparison of different models, including BERTbase, MLP-Mixer, and various gMLP configurations, based on perplexity and parameter count. BERTbase has a perplexity of 4.37 with 110M parameters. The best performing model is the Multiplicative, Split gMLP with a perplexity of 4.35 and 102M parameters. The gMLP models have 36 layers with dmodel=512 and dﬀn=3072. No positional encodings are used for Mixer or gMLPs.",
  "summary_type": "reports"
}{
  "input": "-10 and ResNet-50, we compare\nAutoDropout against DropBlock (Ghiasi, Lin, and Le 2018),\nsince DropBlock has been well-tuned for these models.\nFor EfﬁcientNet, we compare AutoDropout with Stochastic\nDepth (Huang et al. 2016) since it is the default noise-based\nregularization scheme of this architecture. We implement\nthese baselines in our environment for fair comparison. Note\nthat large EfﬁcientNet models, such as B3, B5, B7 in our ex-\nperiments, enlarge the spatial dimensions of the input images",
  "output": "The text compares AutoDropout with DropBlock for ResNet-10 and ResNet-50 models, and with Stochastic Depth for EfficientNet models. The comparison is done in a fair environment. It also mentions that larger EfficientNet models like B3, B5, B7 increase the spatial dimensions of input images.",
  "summary_type": "reports"
}{
  "input": "aining cycle and complex\ntricks [31, 20, 38, 2]. Witnessing this, several works [1, 36] propose that convolution should be\nintroduced into the self-attention mechanism to improve its robustness and performance. In short,\ndifferent model assumptions are adopted to make convolution and self-attention complement each\nother in terms of optimization characteristics (i.e., well-condition/ill-condition), attention scope (i.e.,\nlocal/long-term), and content dependence (content-dependent/independent) e.t",
  "output": "Several studies propose incorporating convolution into the self-attention mechanism to enhance its robustness and performance. Different model assumptions are used to make convolution and self-attention complement each other in terms of optimization characteristics (well-conditioned/ill-conditioned), attention scope (local/long-term), and content dependence (content-dependent/independent).",
  "summary_type": "reports"
}{
  "input": "are trained for\n350 epochs with a batch size B of 4096 or 2048 distributed across 32 or 64 TPUv3 cores, depending\non memory constraints.\nWe tuned our models using a held-out validation set comprising ∼2% of the ImageNet training set\n(20 shards out of 1024). We perform early stopping on the held-out validation set for the largest\nmodels, starting with LambdaResNet-350 at resolution 288x288, and simply report the ﬁnal accura-\ncies for the smaller models.\nSemi-supervised learning with pseudo-label",
  "output": "The models are trained for 350 epochs with a batch size of 4096 or 2048 across 32 or 64 TPUv3 cores, depending on memory constraints. Models are tuned using a validation set comprising about 2% of the ImageNet training set. Early stopping is performed on the validation set for the largest models, starting with LambdaResNet-350 at resolution 288x288. The final accuracies are reported for the smaller models. The process also involves semi-supervised learning with pseudo-label.",
  "summary_type": "reports"
}{
  "input": "er-S\nDeiT-S\nResNet-50\nResNet-152\n(a)\n(b)\nFigure 5: Generalization capability. (a) Comparison of ro-\ntation invariance. The compared models are trained under\nthe same data augmentation settings and directly evaluated\non rotated images without model ﬁntuning. (b) Comparison\nof scale invariance. The models are trained on images with\nthe resolution of 224×224, and tested on different image\nresolutions without model ﬁnetuning.\n101) archives 81.8%, which is 1.6% lower than that of\nConformer-S (83.4%),",
  "output": "Figure 5 compares the generalization capability of different models. It examines rotation invariance, where models are trained and evaluated on rotated images without fine-tuning, and scale invariance, where models are trained on 224x224 resolution images and tested on different resolutions without fine-tuning. The archives model scored 81.8%, 1.6% lower than the Conformer-S model, which scored 83.4%.",
  "summary_type": "reports"
}{
  "input": "tic depth [19]. This set of techniques was inspired by the timm library [54]\nand Touvron et al. [48]. More details on these hyperparameters are provided in Supplementary B.\nFine-tuning We ﬁne-tune using momentum SGD, batch size 512, gradient clipping at global norm 1,\nand a cosine learning rate schedule with a linear warmup. We do not use weight decay when ﬁne-\ntuning. Following common practice [22, 48], we also ﬁne-tune at higher resolutions with respect to\nthose used during pre-training. Since",
  "output": "The text discusses fine-tuning techniques inspired by the timm library and Touvron et al. These involve using momentum SGD, a batch size of 512, gradient clipping at global norm 1, and a cosine learning rate schedule with a linear warmup. Weight decay is not used during fine-tuning. As per common practice, fine-tuning is done at higher resolutions compared to those used during pre-training.",
  "summary_type": "reports"
}{
  "input": "tNet models.\n3.1. Basic Training\nOur main benchmark for evaluating TResNet models is\nthe popular ImageNet dataset [18]. We trained the mod-\nels on input resolution 224, for 300 epochs, using a SGD\noptimizer and 1-cycle policy [34]. For regularization, we\nused AutoAugment [6], Cutout [7], Label-smoothing [35]\nand True-weight-decay [25]. We found that the common\nImageNet statistics normalization [20, 6, 36] does not im-\nprove the training accuracy, and instead normalized all the\nRGB channels to be",
  "output": "TResNet models were evaluated using the ImageNet dataset. Models were trained on a 224 input resolution for 300 epochs, using an SGD optimizer and 1-cycle policy. Regularization techniques used included AutoAugment, Cutout, Label-smoothing, and True-weight-decay. It was found that normalizing ImageNet statistics didn't improve training accuracy, so all RGB channels were normalized instead.",
  "summary_type": "reports"
}{
  "input": "8.0 \n77.9\n77.7\n77.5\n2\n5\n10\n25\n50\n(a) The impact of different 𝜆values\n(b) The impact of different 𝐾values\nFig. 4. Training EﬃcientNet-B0 with diﬀerent knowledge distillation parameters. All\nnumbers in the ﬁgure are top-1 accuracy (%). Left: The testing accuracy of diﬀerent λ\nvalues, while K is set as 10. Right: The testing accuracy of diﬀerent K values, while\nλ is set to be 0.5.\n1.3M training and 50K validation images. The number of images in each class is\napproximately the same for training data",
  "output": "The text discusses the impact of varying λ and K values on the training efficiency of EfficientNet-B0. It shows the testing accuracy of different λ values when K is set to 10 and the testing accuracy of different K values when λ is set to 0.5. The training involved 1.3M images and validation involved 50K images, with each class having approximately the same number of images.",
  "summary_type": "reports"
}{
  "input": "s\n94.43\n93.02\n94.73\n93.09\n94.82\nJFT-300M\nCIFAR-10\n99.00\n98.61\n99.38\n99.19\n99.50\nCIFAR-100\n91.87\n90.49\n94.04\n92.52\n94.55\nImageNet\n84.15\n80.73\n87.12\n84.37\n88.04\nImageNet ReaL\n88.85\n86.27\n89.99\n88.28\n90.33\nOxford Flowers-102\n99.56\n99.27\n99.56\n99.45\n99.68\nOxford-IIIT-Pets\n95.80\n93.40\n97.11\n95.83\n97.56\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\nare ﬁne-tun",
  "output": "The text presents the Top1 accuracy percentages of the Vision Transformer model when pre-trained on ImageNet, ImageNet-21k, or JFT300M. The model was tested on various datasets including CIFAR-10, CIFAR-100, ImageNet, ImageNet ReaL, Oxford Flowers-102, and Oxford-IIIT-Pets. The accuracy rates vary across datasets and pre-training models.",
  "summary_type": "reports"
}{
  "input": "wed to model the\nsemantic correlation among local regions, experimental re-\nsults also show its effectiveness.\nWe further optimize WSDAN and DCL by the following\nmethods:\n(1) About tricks for weakly supervised learning, we ﬁnd\nthat label smooth, warmup + Learning rate with cosine de-\ncay and 144-crop prediction can improved the results. Be-\nsides, larger batch size and image crop size can increase by\nabout 1.5 points;\n(2) With the increase of iterations of inclass data, the\nnumber of trusted pse",
  "output": "The text discusses optimizing WSDAN and DCL. Tricks for weakly supervised learning such as label smooth, warmup + learning rate with cosine decay, and 144-crop prediction can improve results. Larger batch size and image crop size can increase results by about 1.5 points. The number of trusted pseudo-labels increases with the iterations of in-class data.",
  "summary_type": "reports"
}{
  "input": "ing.\narXiv preprint arXiv:1312.5602, 2013.\n[25] W Pirie. S pearman rank correlation coefﬁcient. Encyclope-\ndia of statistical sciences, 2004.\n[26] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V\nLe. Regularized evolution for image classiﬁer architecture\nsearch. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 33, pages 4780–4789, 2019.\n[27] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla",
  "output": "The text refers to various academic resources. It mentions a preprint on arXiv from 2013, an article about Spearman rank correlation coefficient from the Encyclopedia of Statistical Sciences, and a paper by Esteban Real et al. on regularized evolution for image classifier architecture search, presented at the AAAI Conference on Artificial Intelligence in 2019. The last part seems to list authors of another work, including Olga Russakovsky and Aditya Khosla, but the specific work is not mentioned.",
  "summary_type": "reports"
}{
  "input": "cale vision longformer:\nA new vision transformer for high-resolution image encoding. arXiv preprint arXiv:2103.15358,\n2021. 3, 9\n12\n\n--------------------------------------------------\n[63] X. Zhang, Y. Wei, J. Feng, Y. Yang, and T. S. Huang. Adversarial complementary learning for\nweakly supervised object localization. In CVPR, 2018. 9\n[64] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang. Random erasing data augmentation. In AAAI,\n2020. 5\n[65] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. To",
  "output": "The text mentions three academic papers. The first is about \"Cale Vision Longformer\", a new vision transformer for high-resolution image encoding. The second paper discusses \"Adversarial Complementary Learning\" for weakly supervised object localization, presented at CVPR 2018. The third paper is about \"Random Erasing Data Augmentation\", presented at AAAI 2020. The last paper is incomplete, but it involves B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. To as authors or contributors.",
  "summary_type": "reports"
}{
  "input": "zes\nthe multi-level representations to improve the ﬁnal repre-\nsentation. In summary, our contributions are as follows:\n• We design a new visual Transformer architecture\nnamely Convolution-enhanced image Transformer\n(CeiT). It combines the advantages of CNNs in extract-\ning low-level features, strengthening locality, and the\nadvantages of Transformers in establishing long-range\ndependencies.\n• Experimental results on ImageNet and seven down-\nstream tasks show the effectiveness and generaliza-\nti",
  "output": "The text discusses a new visual Transformer architecture called Convolution-enhanced image Transformer (CeiT). This combines the benefits of Convolutional Neural Networks (CNNs) in extracting low-level features and strengthening locality, with the advantages of Transformers in establishing long-range dependencies. Experimental results on ImageNet and seven downstream tasks demonstrate its effectiveness and generalizability.",
  "summary_type": "reports"
}{
  "input": "that Transformers can handle images. However, patch ﬂat-\ntening impairs the position information within each patch\nand makes it more difﬁcult to extract the patterns within\npatches. To solve this problem, existing methods usually\nattach a preprocessing module before patch embedding.\nThe preprocessing module can be a feature extraction con-\nvnet [14] or a specially designed Transformer [40].\nWe found that there is a rather simple solution, which\nis factorizing the large patch embedding to step-wi",
  "output": "Transformers can process images, but patch flattening hinders position information extraction within each patch, making pattern extraction challenging. Existing solutions often use a preprocessing module before patch embedding, such as a feature extraction convnet or a specially designed Transformer. A simple solution found is to factorize the large patch embedding.",
  "summary_type": "reports"
}{
  "input": "rance, 6-11 July 2015, volume 37 of JMLR Workshop\nand Conference Proceedings, pages 448–456. JMLR.org, 2015.\n[14] Md. Amirul Islam, Sen Jia, and Neil D. B. Bruce. How much position information do convolutional neural\nnetworks encode? In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net, 2020.\n[15] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In IEEE Conference on\nComputer Vision and Patter",
  "output": "The text refers to various academic papers. One is from the 8th International Conference on Learning Representations, ICLR 2020, by Md. Amirul Islam, Sen Jia, and Neil D. B. Bruce, discussing the amount of position information encoded by convolutional neural networks. Another paper by Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang, presented at an IEEE Conference, discusses selective kernel networks.",
  "summary_type": "reports"
}{
  "input": "e\n(ms)\n(hr)\n(top-1)\nREGNETY-200MF\n0.2\n3.2\n2.2\n1024\n11\n3.1\n29.6±0.11\nREGNETY-400MF\n0.4\n4.3\n3.9\n1024\n19\n5.1\n25.9±0.16\nREGNETY-600MF\n0.6\n6.1\n4.3\n1024\n19\n5.2\n24.5±0.07\nREGNETY-800MF\n0.8\n6.3\n5.2\n1024\n22\n6.0\n23.7±0.03\nREGNETY-1.6GF\n1.6\n11.2\n8.0\n1024\n39\n10.1\n22.0±0.08\nREGNETY-3.2GF\n3.2\n19.4\n11.3\n512\n67\n16.5\n21.0±0.05\nREGNETY-4.0GF\n4.0\n20.6\n12.3\n512\n68\n16.8\n20.6±0.08\nREGNETY-6.4GF\n6.4\n30.6\n16.4\n512\n104\n26.1\n20.1±0.04\nREGNETY-8.0GF\n8.0\n39.2\n18.0\n512\n113\n28.1\n20.1±0.09\nREGNETY-12GF\n12.1\n51.8\n21.4\n512\n150",
  "output": "The text appears to be a data table for different models of REGNETY, with varying parameters. The models range from REGNETY-200MF to REGNETY-12GF. The parameters include values for e (from 0.2 to 12.1), ms (from 3.2 to 51.8), hr (from 2.2 to 21.4), top-1 (from 1024 to 512), and other unspecified metrics. Each model also has a corresponding value, possibly an accuracy or performance measure, given as a mean ± standard deviation (from 29.6±0.11 to 20.1±0.09).",
  "summary_type": "reports"
}{
  "input": "ts used for\neach group b ∈Rg\n∗+ is obtained from a logit parameter\nl ∈Rg, so that we have\nb = bmin + σ(l)(bmax −bmin),\n(11)\nwith σ is the sigmoid function, and bmin and bmax the minimal\nand maximal number of bits to use. The trainable parameter\nl is initialized so that b = binit. We set binit = 8.\nEvaluation and noise distribution. At evaluation time, we\nround the value b obtained from (10) as ˜b = round(b) and\nquantize w as Q(w,˜b). Thus, the amount of quantization\nnoise at evaluation can be la",
  "output": "The text discusses a method to determine the number of bits, b, used for each group in a system. It's calculated from a logit parameter, l, using a sigmoid function and the minimum and maximum number of bits, bmin and bmax. The parameter l is initialized so that b equals an initial value, binit, set to 8. At evaluation, b is rounded and quantized, affecting the amount of quantization noise.",
  "summary_type": "reports"
}{
  "input": "time and memory complexity with respect to the se-\nquence length. There are also some NLP literatures that\ntend to reduce the sequence length during processing. For\nexample, Goyal et al. [13] propose PoWER-BERT, which\nprogressively eliminates word tokens during the forward\npass. Funnel-Transformer [8] presents a pool-query-only\nstrategy, pooling the query vector within each self-attention\nlayer. However, there are few literatures targeting improv-\ning the efﬁciency of the ViT models.\nTo compromi",
  "output": "The text discusses the challenges of time and memory complexity in Natural Language Processing (NLP). Some literature suggests reducing sequence length during processing. For instance, PoWER-BERT progressively eliminates word tokens during the forward pass, and Funnel-Transformer uses a pool-query-only strategy. However, there is limited literature on improving the efficiency of Vision Transformer (ViT) models.",
  "summary_type": "reports"
}{
  "input": "e to refactor/build on the frame-\nworks for introducing new methods, models, datasets and designing experiments.\nIn this paper, we present our developed open-source framework built on PyTorch\nand dedicated for knowledge distillation studies. The framework is designed to\nenable users to design experiments by declarative PyYAML conﬁguration ﬁles,\nand helps researchers complete the recently proposed ML Code Completeness\nChecklist. Using the developed framework, we demonstrate its various efﬁcient\nt",
  "output": "The text discusses an open-source framework developed for knowledge distillation studies, built on PyTorch. This framework allows users to design experiments using declarative PyYAML configuration files, aiding researchers in completing the ML Code Completeness Checklist. The framework's efficiency is demonstrated within the text.",
  "summary_type": "reports"
}{
  "input": "iv:2011.00186,\n2020. 3\n[70] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing\nJia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient con-\nvnet design via differentiable neural architecture search. In\nCVPR, 2019. 3, 7, 12\n[71] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. In CVPR, 2018. 3\n[72] Shen Yan, Yu Zheng, Wei Ao, Xiao Zeng, and Mi Zhang.\nDoes unsu",
  "output": "The text refers to several academic papers. One is about FBNet, a hardware-aware efficient ConvNet design through differentiable neural architecture search, presented at CVPR 2019. Another paper, presented at CVPR 2018, discusses unsupervised feature learning via non-parametric instance discrimination. The text also mentions an incomplete reference to a paper by Shen Yan and others.",
  "summary_type": "reports"
}{
  "input": "T and concur-\nrent ViT [25] video variants: VTN [78], TimeSformer [6],\nViViT [1]. To achieve similar accuracy level as MViT, these\nmodels require signiﬁcant more computation and parameters\n(e.g. ViViT-L [1] has 6.8× higher FLOPs and 8.5× more pa-\nrameters at equal accuracy, more analysis in §A.1) and need\nlarge-scale external pre-training on ImageNet-21K (which\ncontains around 60× more labels than Kinetics-400).\nIN-1K\nIN-21K\nIN-21K\nIN-21K\n+4.6% acc\nat 1/5 FLOPs\nat 1/3 Params \nwithout ImageNet \nM",
  "output": "The text discusses the efficiency of different video variant models. MViT achieves similar accuracy levels with less computation and parameters compared to other models like VTN, TimeSformer, and ViViT. For instance, ViViT-L requires 6.8x higher FLOPs and 8.5x more parameters for equal accuracy. These models also need large-scale external pre-training on ImageNet-21K, which has around 60x more labels than Kinetics-400. MViT improves accuracy by 4.6% at 1/5 FLOPs and 1/3 parameters without ImageNet.",
  "summary_type": "reports"
}{
  "input": "per, we have presented a Hierarchical Visual\nTransformer, termed HVT, for image classiﬁcation. In par-\nticular, the proposed hierarchical pooling can signiﬁcantly\ncompress the sequential resolution to save computational\ncost in a simple yet effective form. More importantly, this\nstrategy greatly improves the scalability of visual Trans-\nformers, making it possible to scale various dimensions -\ndepth, width, resolution and patch size. By re-allocating\nthe saved computational cost, we can scale up",
  "output": "The Hierarchical Visual Transformer (HVT) has been introduced for image classification. It uses hierarchical pooling to significantly reduce sequential resolution, saving computational cost. This strategy enhances the scalability of visual Transformers, allowing for scaling in depth, width, resolution, and patch size. The saved computational cost can be reallocated for scaling up.",
  "summary_type": "reports"
}{
  "input": "ngs: multi-scale training (resizing the input such that the shorter\nside is between 480 and 800 while the longer side is at most 1333), AdamW [19] optimizer (initial\nlearning rate of 1e-4, weight decay of 0.05, and batch size of 16), and 1× schedule (12 epochs).\nUnlike CNN backbones, which adopt post normalization and can directly apply to downstream tasks.\nResT employs the pre-normalization strategy to accelerate network convergence, which means the\noutput of each stage is not normalized before",
  "output": "The text discusses a multi-scale training method that resizes the input so the shorter side is between 480 and 800 and the longer side is no more than 1333. It uses the AdamW optimizer with an initial learning rate of 1e-4, weight decay of 0.05, and batch size of 16. The process follows a 1× schedule over 12 epochs. Unlike CNN backbones that use post-normalization, ResT uses pre-normalization to speed up network convergence, meaning the output of each stage isn't normalized beforehand.",
  "summary_type": "reports"
}{
  "input": "tworks\noutperform more complex architectures, like SE-ResNet [17], despite that it uses an additional\nsqueeze-and-excitation block, which increases model complexity.\nThe above results mainly aim to show the advantages of our PyConv over the standard convolution\nby running all the networks with the same standard training settings for a fair comparison. Note\nthat there are other works which report better results on ImageNet, such as [29]–[31]. However, the\nimprovements are mainly due to the traini",
  "output": "The text discusses the superiority of PyConv over SE-ResNet and standard convolution in terms of performance and complexity. Despite SE-ResNet's additional squeeze-and-excitation block, PyConv still outperforms it. The results are based on standard training settings for fair comparison. Other works have reported better results on ImageNet, but these improvements are mainly attributed to the training.",
  "summary_type": "reports"
}{
  "input": "formance of shallow\narXiv:2103.11886v4  [cs.CV]  19 Apr 2021\n\n--------------------------------------------------\nViTs, a natural question arises: can we further improve\nperformance of ViTs by making it deeper, just like CNNs?\nThough it seems to be straightforward at the ﬁrst glance,\nthe answer may not be trivial since ViT is essentially dif-\nferent from CNNs in its heavy reliance on the self-attention\nmechanism. To settle the question, we investigate in detail\nthe scalability of ViTs along depth",
  "output": "The text discusses the possibility of improving the performance of Vision Transformers (ViTs) by making them deeper, similar to Convolutional Neural Networks (CNNs). However, the answer is not straightforward due to ViT's heavy reliance on the self-attention mechanism. The authors plan to investigate the scalability of ViTs in depth.",
  "summary_type": "reports"
}{
  "input": "ptimization.\narXiv preprint arXiv:1412.6980,\n2014. 5, 9\n[38] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\narXiv preprint arXiv:1912.11370, 6(2):8, 2019. 6\n[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In Advances in neural information processing sys-\ntems, pages 1097–1105, 2012. 1, 2\n[40] Y",
  "output": "The text discusses various research papers on machine learning. Paper [38] by Kolesnikov et al. is about Big Transfer (BiT), a method for general visual representation learning. Paper [39] by Krizhevsky et al. discusses ImageNet classification using deep convolutional neural networks. The text does not provide information about paper [40].",
  "summary_type": "reports"
}{
  "input": "020.\n[65] Daquan Zhou, Xiaojie Jin, Qibin Hou, Kaixin Wang, Jianchao Yang, and Jiashi Feng. Neural epitome search\nfor architecture-agnostic network compression. In International Conference on Learning Representations,\n2019.\n[66] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi Feng. Deepvit:\nTowards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\n[67] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end d",
  "output": "Daquan Zhou, Xiaojie Jin, Qibin Hou, Kaixin Wang, Jianchao Yang, and Jiashi Feng worked on Neural Epitome Search for architecture-agnostic network compression, presented at the International Conference on Learning Representations in 2019. Daquan Zhou and others also developed DeepViT, a deeper vision transformer, in 2021. Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong have worked on an end-to-end project.",
  "summary_type": "reports"
}{
  "input": "−Mean(TCP:,k)\nStd(TCP:,k)\n.\n(5)\nWe can denote the output of LGM as a binary matrix Lm×n, thus each row of Lm×n is a selection\nlabel. Because we aim at maximizing the total suitability, Lm×n can be determined by solving the\nfollowing problem:\nmin\nX\nj,k\n−Sj,k ∗Lj,k\ns.t.\nLj,k ∈{0, 1}\nP\nk\nLj,k = 1\nP\nj\nLj,k = m\nn\n(6)\nThis problem can also be modeled as BTP, and solved via VAM as described in section 3.1.\n3.3\nVariance Calculation Module (VCM)\nThe expert selection output by the delegator involves proba",
  "output": "The text discusses a binary matrix Lm×n output from the LGM. Each row of this matrix is a selection label. The goal is to maximize total suitability, which can be determined by solving a specific problem. This problem can be modeled as BTP and solved using VAM, as described in section 3.1. The Variance Calculation Module (VCM) is also mentioned, which is involved in expert selection output by the delegator.",
  "summary_type": "reports"
}{
  "input": "dAugment (Cubuk et al., 2020): a per-image data\naugmentation, with adjustable magnitude ϵ.\n• Mixup (Zhang et al., 2018): a cross-image data aug-\nmentation.\nGiven two images with labels (xi, yi)\nand (xj, yj), it combines them with mixup ratio λ:\n˜xi = λxj + (1 −λ)xi and ˜yi = λyj + (1 −λ)yi. We\nwould adjust mixup ratio λ during training.\n5. Main Results\nThis section presents our experimental setups, the main\nresults on ImageNet, and the transfer learning results on\nCIFAR-10, CIFAR-100, Cars, and",
  "output": "dAugment (Cubuk et al., 2020) is a per-image data augmentation method with an adjustable magnitude. Mixup (Zhang et al., 2018) is a cross-image data augmentation technique that combines two images and their labels using a mixup ratio, which can be adjusted during training. The document also presents experimental results on ImageNet and transfer learning results on CIFAR-10, CIFAR-100, and Cars.",
  "summary_type": "reports"
}{
  "input": "sampled class, sharpening the target predic-\ntions is equivalent to using a lower temperature in the cosine\nsimilarity between the unlabeled representation and support\nrepresentations. However, when the sampled support set con-\ntains more than one instance per sampled class, then sharpen-\ning the target predictions is actually different from adjusting\nthe cosine temperature. In this case, it is preferable to sharpen\nthe target predictions rather than use a different temperature\nin the cosine si",
  "output": "Sharpening target predictions in a sampled class is like using a lower temperature in the cosine similarity between unlabeled and support representations. However, if the sampled support set has more than one instance per sampled class, sharpening target predictions differs from adjusting the cosine temperature. In such cases, it's better to sharpen the target predictions instead of changing the cosine temperature.",
  "summary_type": "reports"
}{
  "input": "essing) has a long history in com-\nputer vision (Borenstein et al., 2004; Kumar et al., 2005;\nCarreira et al., 2016; Hu & Ramanan, 2016; Yang et al.,\n2018; Lin et al., 2020). There is widespread evidence that\nit plays an important role in human vision (e.g. Felleman\n& Essen 1991; Olshausen et al. 1993; Lollo et al. 2000),\nwhich is characterized by limited bandwidth input streams\n(Wolfe et al., 2006). In the Perceiver, attention to the full\nset of inputs can be inﬂuenced by a latent array produce",
  "output": "The concept of processing has a significant history in computer vision, with evidence suggesting its importance in human vision, which is characterized by limited bandwidth input streams. In the Perceiver model, attention to the full set of inputs can be influenced by a latent array.",
  "summary_type": "reports"
}{
  "input": "he local channel\ncontext aggregator, which only exploits point-wise channel\ninteractions for each spatial position. To save parameters,\nthe local channel context L(X) ∈RC×H×W is computed\nvia a bottleneck structure as follows:\nL(X) = B (PWConv2 (δ (B (PWConv1(X)))))\n(2)\nThe kernel sizes of PWConv1 and PWConv2 are C\nr ×C ×\n1 × 1 and PWConv2 is C × C\nr × 1 × 1, respectively. It\nis noteworthy that L(X) has the same shape as the input\nfeature, which can preserve and highlight the subtle details\nin th",
  "output": "The text discusses a local channel context aggregator that uses point-wise channel interactions for each spatial position. It uses a bottleneck structure to compute the local channel context, L(X), and applies two types of PWConv operations. The kernel sizes of these operations are specified. Notably, L(X) maintains the same shape as the input feature, allowing it to preserve and emphasize subtle details.",
  "summary_type": "reports"
}{
  "input": ", those straightforward scaling strategies\nonly work well with larger datasets and stronger augmen-\ntation policies [51, 46, 43] to alleviate the brought training\n\n--------------------------------------------------\nSelf-Attention\nNorm\nFeed Foward\nNorm\nAdd\nAdd\nPos. Embed\nPatch Embed\nInput\nLinear + Loss\n×N\nRe-Attention \nNorm\nFeed Foward\nNorm\nAdd\nAdd\nPos. Embed\nPatch Embed\nInput\nLinear + Loss\n×N\n(a)\n(b)\nTransformer block with self-attention\nTransformer block with Re-attention\nFigure 2: Comparison b",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "0 case for Adam updates. We note that the β2 = 0\ncase where the bound depends on Lavg instead of ||L||1 can be very similarly derived for Adam, but\nis also a very unrealistic condition in practice.\nProof. Under the assumption β1 = 0, λ = 0, one could write the Adam update rule as follows:\nx(i)\nt+1 = x(i)\nt\n−ηt\nq\n1 −βt\n2\ng(i)\nt\nq\nv(i)\nt\n,\nwhere vt = β2vt−1 + (1 −β2)g2\nt for all i ∈[h].\n12\n\n--------------------------------------------------\nSince the function f is L-smooth, we have the following:",
  "output": "The text discusses the Adam update rule in machine learning, under the assumption β1 = 0, λ = 0. The rule is written as x(i) t+1 = x(i) t -ηt q 1 -βt 2 g(i) t q v(i) t, where vt = β2vt−1 + (1 −β2)g2 t for all i ∈[h]. It also mentions that the β2 = 0 case, where the bound depends on Lavg instead of ||L||1, can be derived for Adam but is unrealistic in practice. The function f is L-smooth.",
  "summary_type": "reports"
}{
  "input": "nd exper-\niments to reproduce the reported results on ImageNet and COCO datasets.\n\n--------------------------------------------------\ntorchdistill: A Modular, Conﬁguration-Driven Framework for Knowledge Distillation\n11\nTable 3: Epoch-level training speed improvement by redesigning teacher and student\n(ResNet-18) models with required modules only for hint-training shown in Figure 3.\nTeacher ResNet-34 ResNet-50 ResNet-101 ResNet-152\nOriginal\n934 sec\n1,175 sec\n1,468 sec\n1,779 sec\nMinimal\n786 sec\n92",
  "output": "The text discusses \"torchdistill\", a modular, configuration-driven framework for knowledge distillation. It highlights the epoch-level training speed improvement achieved by redesigning teacher and student models (ResNet-18) with only necessary modules for hint-training. The original training times for ResNet-34, ResNet-50, ResNet-101, and ResNet-152 were 934 sec, 1,175 sec, 1,468 sec, and 1,779 sec respectively. The minimal training time for ResNet-34 was reduced to 786 sec.",
  "summary_type": "reports"
}{
  "input": "low previous\nwork [17] to generate three architectures that has comparable capacity (e.g. number of parameters\nand FLOPS), notated as tiny (NesT-T), small (NesT-S), and base (NesT-B). Most recent ViT based\nmethods follow the training techniques of DeiT [49], which include a mixture of data augmentation\nlike MixUp [58], CutMix [56], RandAugment [14], RandomErasing [64], and regularization like\nStochastic Depth [26]. We follow major settings with minor rationale modiﬁcations that is we ﬁnd\nuseful",
  "output": "The text discusses the generation of three architectures with similar capacities, named NesT-T, NesT-S, and NesT-B, based on previous work. These architectures use training techniques from DeiT, including various data augmentation methods like MixUp, CutMix, RandAugment, RandomErasing, and regularization like Stochastic Depth. The settings are mostly followed with minor modifications found useful.",
  "summary_type": "reports"
}{
  "input": "8748. IEEE\nComputer Society, 2018. 2\n[53] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In ICLR, 2021. 1, 2\n\n--------------------------------------------------",
  "output": "The text refers to a study by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. They developed \"Deformable DETR\", a deformable transformer for end-to-end object detection, presented at ICLR in 2021. The IEEE Computer Society recognized this work in 2018.",
  "summary_type": "reports"
}{
  "input": "rrelations in the\nJFT label noise, improves the VTAB1K score by 0.88%\nover the homoscedastic baseline and by 0.22% over the\nheteroscedastic diagonal model. We stress that the down-\nstream models are trained without a heteroscedastic output\nlayer, so our experiment demonstrates that a model trained\nupstream on JFT with a heteroscedastic output layer learns\nrepresentations which transfer better than a homoscedastic\nor a heteroscedastic diagonal model.\n4.3. Deep Ensembles for Full Predictive Uncert",
  "output": "A model trained on JFT with a heteroscedastic output layer improves the VTAB1K score by 0.88% over the homoscedastic baseline and 0.22% over the heteroscedastic diagonal model. This shows that such a model learns representations that transfer better than a homoscedastic or a heteroscedastic diagonal model. The downstream models are trained without a heteroscedastic output layer.",
  "summary_type": "reports"
}{
  "input": "split out and\nbypasses the feed-forward network. The derived new fea-\nture map is converted back to image tokens and concate-\nnated with the bypassed class token. The concatenated se-\nquence is processed by the next transformer layer.\nThe effectiveness of the introduced locality mechanism\nis validated in two ways. Firstly, its properties are investi-\ngated experimentally. We draw four basic conclusions. i.\nDepth-wise convolution alone can already improve the per-\nformance of the baseline transf",
  "output": "The text discusses a process where a new feature map bypasses the feed-forward network, is converted back to image tokens, and then combined with a bypassed class token. This sequence is processed by the next transformer layer. The effectiveness of this locality mechanism is validated through experimental investigation, with one conclusion being that depth-wise convolution can improve the performance of the baseline transformer.",
  "summary_type": "reports"
}{
  "input": "reprint arXiv:2010.03019, 2020. 7\n[59] Byeongho Heo, Sangdoo Yun, Dongyoon Han,\nSanghyuk Chun, Junsuk Choe, and Seong Joon Oh,\n“Rethinking spatial dimensions of vision transform-\ners,” 2021. 9, 10\n[60] Haiping Wu, Bin Xiao, Noel Codella, Mengchen\nLiu, Xiyang Dai, Lu Yuan, and Lei Zhang, “Cvt:\nIntroducing convolutions to vision transformers,”\n2021. 9, 10\n[61] A. Brock,\nSoham De,\nS. L. Smith,\nand K.\nSimonyan, “High-performance large-scale image\nrecognition without normalization,” arXiv preprint\nar",
  "output": "The text mentions three academic papers. The first, by Heo et al., 2021, discusses rethinking spatial dimensions of vision transformers. The second, by Wu et al., 2021, introduces convolutions to vision transformers. The third, by Brock et al., focuses on high-performance large-scale image recognition without normalization.",
  "summary_type": "reports"
}{
  "input": "the self-attention stage, which dominates the complexity, we increase the\nperformance signiﬁcantly by adding two blocks of CA+FFN.\n4.3\nOur CaiT models\nOur CaiT models are built upon ViT: the only difference is that we incorporate\nLayerScale in each residual block (see Section 2) and the two-stages architec-\nture with class-attention layers described in Section 3. Table 3 describes our\ndifferent models. The design parameters governing the capacity are the depth\nand the working dimensionality d.",
  "output": "The CaiT models are based on ViT, with the addition of LayerScale in each residual block and a two-stage architecture with class-attention layers. The performance is significantly improved by adding two blocks of CA+FFN. The design parameters that determine the capacity are the depth and the working dimensionality d.",
  "summary_type": "reports"
}{
  "input": "×8×8\n✓\n70.5\n6.8\n77.2\n2×8×8\n✓\n63.7\n6.3\n75.8\nTable 14. Key-Value pooling: Vary stride s = sT × sH × sW , for\npooling K and V . “adaptive” reduces stride w.r.t. stage resolution.\nKey-Value pooling. The ablation in Table 14 analyzes the\npooling stride s = sT × sH × sW , for pooling K and V\ntensors. Here, we compare an “adaptive” pooling that uses a\nstride w.r.t. stage resolution, and keeps the K, V resolution\nﬁxed across all stages, against a non-adaptive version that\nuses the same stride at every b",
  "output": "The text discusses Key-Value pooling in tensor analysis, comparing \"adaptive\" pooling that adjusts stride according to stage resolution and keeps K, V resolution constant across all stages, against a non-adaptive version that uses the same stride consistently. The ablation in Table 14 analyzes the pooling stride s = sT × sH × sW for pooling K and V tensors.",
  "summary_type": "reports"
}{
  "input": "the number of buckets. The accuracy increase\n6\n\n--------------------------------------------------\n0\n10 26 50 82 122\n226\n362\n730\nNumber of buckets\n80.0\n80.5\n81.0\nTop-1 Accuracy (%)\n79.9\n80.7\n80.8\n80.9\n80.9\n80.7\n81.0\n80.8\n80.9\nFigure 3: Ablation for the number of buckets in contextual\nproduct model with shared relative position encodings on\nImageNet [4].\n112\n224\n384\n448\n512\n800\nResolution\n10\n0\n10\n20\n30\nExtra MACC Ratio (%)\n1.1\n3.9\n9.9\n12.6\n15.3\n25.9\n1.1\n1.0\n0.9\n0.9\n0.9\n0.7\nInefficient\nEfficient\nF",
  "output": "The text discusses an ablation study for the number of buckets in a contextual product model with shared relative position encodings on ImageNet. The top-1 accuracy ranges from 79.9% to 81.0%. The extra MACC ratio, a measure of efficiency, ranges from 1.1% to 25.9%. The study suggests a trade-off between accuracy and efficiency.",
  "summary_type": "reports"
}{
  "input": "gregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492–1500,\n2017. 4\n[32] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017. 6\n[33] Hengshuang Zhao. Ofﬁcial pspnet. https://github.\ncom/hszhao/semseg, 2020. 9\n[34] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene",
  "output": "The text discusses various research papers and works related to deep neural networks. It mentions a paper on aggregated residual transformations for deep neural networks presented at the IEEE conference. Another paper titled \"mixup: Beyond empirical risk minimization\" by Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz is also referenced. Additionally, Hengshuang Zhao's official PSPNet on GitHub is mentioned. Lastly, a work by Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia on Pyramid scene is referred to.",
  "summary_type": "reports"
}{
  "input": "es signiﬁcantly by 3.95%. It reveals that position em-\nbedding is less important in the transition model than that\nin the pure Transformer-based models. It is because that\nthe position prior inter tokens is preserved by the feature\nmaps and convolutions with spatial kernels can encode and\nleverage it. Consequently, the harm of removing position\nembedding is remarkably reduced in the transition network.\nIt also explains why convolution-based models do not need\nposition embedding.\n3.3.7\nReplacing",
  "output": "The text suggests that position embedding is less crucial in the transition model than in pure Transformer-based models. This is because the position prior between tokens is maintained by feature maps and spatial kernel convolutions. As a result, removing position embedding is less detrimental in the transition network. This also explains why convolution-based models do not require position embedding.",
  "summary_type": "reports"
}{
  "input": "mutiplexing in stages two and three. For the kernel size\noptions in case of reduction blocks, we allow multiple parallel kernels to\ndown-sample the resolution, for example, “[3, 5, 7]” means three parallel\nconvolutions with kernel size of 3, 5, and 7.\nThe Mobilenet block consist of a 1 × 1 convolution to\nexpand the input channels, followed by a 3 × 3 depth-wise\nseparable convolution and another 1×1 convolution to com-\npress the channels (see Figure 12a). We use E to denote\nexpansion rate. Then t",
  "output": "The text discusses the use of multiplexing in stages two and three. It allows multiple parallel kernels to down-sample the resolution in reduction blocks, using kernel sizes of 3, 5, and 7. The Mobilenet block expands input channels with a 1x1 convolution, followed by a 3x3 depth-wise separable convolution, and another 1x1 convolution to compress the channels. E denotes the expansion rate.",
  "summary_type": "reports"
}{
  "input": "RPE methods into DeiT. We elab-\norate how to inject previous relative position encoding\nmethods into DeiT [22] in Tab. 5 of Section 4.3.\n• Training and test settings of DETR. We provide the\ndetails of training and test settings of DETR [1] in Sec-\ntion 4.4.\n• The effectiveness on other vision transformers. We\nshow the effectiveness of the proposed iRPE on the\nrecent Swin transformer [13].\n• Transfer learning on ﬁne-grained datasets.\nTo\nverify the generalizability, we evaluate our models\non ﬁne-g",
  "output": "The text discusses the integration of relative position encoding methods into DeiT, detailing the training and testing settings of DETR. It also highlights the effectiveness of the proposed iRPE on the Swin transformer and its transfer learning on fine-grained datasets, suggesting a focus on model generalizability.",
  "summary_type": "reports"
}{
  "input": "o the original single-layer patch\nembedding. However, when further increasing the number of convolutional layer in patch embedding\nto 6, we do not observe any performance gain. This indicates that using 4-layer convolutions in patch\nembedding is enough. Meanwhile, if we use a larger stride to reduce the size of the feature map,\nwe can largely reduce the computation cost, but the performance also drops. Thus, we only apply a\n13\n\n--------------------------------------------------\nconvolution of st",
  "output": "The text discusses the performance of convolutional layers in patch embedding. It suggests that increasing the number of convolutional layers to 6 doesn't improve performance, indicating that 4 layers are sufficient. However, using a larger stride to reduce the feature map size can decrease computation cost, but it also reduces performance. Therefore, a balance is maintained.",
  "summary_type": "reports"
}{
  "input": "nt network by balancing the resolu-\ntion, height, and width of the network. The performance of\nEfﬁcientNet for ILSVRC2012 top-1 accuracy was greatly\nimproved relative to AlexNet.\nUnlike these studies which focus on designing new net-\nwork architecture, He et al. [9] proposes different ap-\nproaches to improve model performance. They noted that\nperformance can be improved not only through changes in\nthe model structure, but also through other aspects of net-\nwork training such as data preprocessin",
  "output": "EfficientNet, a network balancing resolution, height, and width, significantly improved ILSVRC2012 top-1 accuracy compared to AlexNet. He et al. suggest that model performance can be enhanced not only by modifying the model structure but also by improving other network training aspects like data preprocessing.",
  "summary_type": "reports"
}{
  "input": "like data augmentations, which are domain-\nspeciﬁc, our dropout patterns for the hidden states have the\nsame design philosophy on ConvNets for image recogni-\ntion models and Transformer for text understanding models.\nCutMix (Yun et al. 2019) and ManifoldMixup (Verma et al.\n2We will release the datasets consisting of the dropout patterns\nthat our search algorithm has sampled and run. Like the similar\ndatasets collected from benchmarking various model architectures\n(Ying et al. 2019; Dong and Yang",
  "output": "The text discusses the use of dropout patterns for hidden states in ConvNets for image recognition models and Transformer for text understanding models, similar to data augmentations. It mentions CutMix and ManifoldMixup techniques. The authors plan to release datasets of dropout patterns sampled and run by their search algorithm, akin to datasets collected from benchmarking various model architectures.",
  "summary_type": "reports"
}{
  "input": "were used to ﬁlter the\nimages from the unlabelled set [8]. Therefore the pre-trained\nmodels may beneﬁt from more overﬁtting on the validation\nset. We quantify this in the experiments presented below.\nSince we use pre-trained EfﬁcientNet for our initializa-\ntion, our results are comparable to those from the Noisy Stu-\ndent [8], which uses the same degree of overﬁtting, but not\ndirectly with other semi-supervised approaches like that of\nYalniz et al. [2].\n3.4. Evaluation on ImageNet-V2\nThe ImageNe",
  "output": "The text discusses the use of pre-trained EfficientNet models, which may benefit from overfitting on the validation set. The results are comparable to the Noisy Student method, which uses a similar degree of overfitting. However, they are not directly comparable to other semi-supervised approaches like Yalniz et al.'s. The text also mentions an evaluation on ImageNet-V2.",
  "summary_type": "reports"
}{
  "input": "ed to play a central\nrole in the deep learning models [22, 29, 31, 15] for visual\nrecognition. This situation starts to change when the Trans-\nformer [35], a module that originates from natural language\nprocessing [35, 13, 25], is transplanted to the vision scenar-\nios. It was shown in the ViT model [14] that an image can\nbe partitioned into a grid of patches and the Transformer is\ndirectly applied upon the grid as if each patch is a visual\nword. ViT requires a large amount of training data (e.g",
  "output": "The Transformer, a module from natural language processing, is now being used in visual recognition models. In the ViT model, an image is divided into a grid of patches, with the Transformer applied directly to the grid, treating each patch as a visual word. However, ViT requires a large amount of training data.",
  "summary_type": "reports"
}{
  "input": "r attention variant which only has a\nlinear complexity w.r.t. the spatial size [12, 32, 33].\nWe brieﬂy experimented with option (C) without getting a reasonably good result. For option (B), we\nfound that implementing local attention involves many non-trivial shape formatting operations that\nrequires intensive memory access. On our accelerator of choice (TPU), such operation turns out to be\nextremely slow [34], which not only defeats the original purpose of speeding up global attention, but\nalso",
  "output": "The text discusses experiments with different attention variants. Option (C) did not yield satisfactory results. Implementing local attention (option B) involved complex shape formatting operations requiring intensive memory access. On the chosen accelerator (TPU), these operations were extremely slow, defeating the purpose of speeding up global attention.",
  "summary_type": "reports"
}{
  "input": "nd that self-attentions in lower\nresolutions play more important roles than those in higher\nresolutions. Additionally, replacing the self-attentions in\nthe ﬁrst stage almost has no effect on the network perfor-\nmance. Larger resolutions contain much more tokens and\n6\n\n--------------------------------------------------\nNetwork\nbase perf.(%)\nelite perf.(%)\nNet5\n77.37\n80.15\nNet5-DS1\n77.29 (-0.08)\n80.13 (-0.02)\nNet5-DS2\n77.34 (-0.02)\n79.75 (-0.40)\nNet5-DS3\n77.05 (-0.32)\n79.59 (-0.56)\nTable 3. Impact",
  "output": "The text discusses the impact of self-attentions in different resolutions on network performance. Lower resolutions have a more significant effect than higher ones. Replacing self-attentions in the first stage doesn't significantly impact performance. The table shows different network performances with Net5 having the highest performance at 80.15%, and Net5-DS3 showing the lowest at 79.59%.",
  "summary_type": "reports"
}{
  "input": "ern Recognition,\npp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby.\nAn image is worth 16x16 words: Tra",
  "output": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova published \"Bert: Pre-training of deep bidirectional transformers for language understanding\" in 2018. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby worked on a project titled \"An image is worth 16x16 words\".",
  "summary_type": "reports"
}{
  "input": "mostly focus on improving FLOPs\nefﬁciency (Tan & Le, 2019b;a) or inference efﬁciency (Tan\net al., 2019; Cai et al., 2019; Wu et al., 2019; Li et al., 2021).\n\n--------------------------------------------------\nEfﬁcientNetV2: Smaller Models and Faster Training\nUnlike prior works, this paper uses NAS to optimize training\nand parameter efﬁciency.\n3. EfﬁcientNetV2 Architecture Design\nIn this section, we study the training bottlenecks of Efﬁcient-\nNet (Tan & Le, 2019a), and introduce our training-awar",
  "output": "The text primarily discusses the improvement of FLOPs efficiency and inference efficiency. It introduces EfficientNetV2, a model that differs from previous works by using NAS to optimize training and parameter efficiency. The training bottlenecks of EfficientNet are also studied.",
  "summary_type": "reports"
}{
  "input": "a labeled data. We use the same\nhyper-parameters with Noisy Student, except that we use the\ntraining image resolution of 512x512 instead of 475x475.\nWe increase the input image resolution to be compatible with\nour model parallelism implementation which we discuss in\nthe next paragraph. In addition to EfﬁcientNet-L2, we also\nexperiment with a smaller model, which has the same depth\nwith EfﬁcientNet-B6 [63] but has the width factor increased\nfrom 2.1 to 5.0. This model, termed EfﬁcientNet-B6-Wide,",
  "output": "The text discusses using the same hyper-parameters as Noisy Student, but with a training image resolution of 512x512 instead of 475x475 to suit their model parallelism implementation. They also experiment with a smaller model, with the same depth as EfficientNet-B6 but a width factor increased from 2.1 to 5.0, termed EfficientNet-B6-Wide.",
  "summary_type": "reports"
}{
  "input": ".\nHyper-parameter\nImageNet-1K\nImageNet-21K\nJFT\nPre-Training Finetuning\nPre-Training Finetuning\nPre-Training\nFinetuning\n(CoAtNet-0/1/2/3)\n(CoAtNet-2/3/4)\n(CoAtNet-3/4/5)\nStochastic depth rate\n0.2 / 0.3 / 0.5 / 0.7\n0.3 / 0.5 / 0.7\n0.0 / 0.1 / 0.0\n0.1 / 0.3 / 0.2\nCenter crop\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nRandAugment\n2, 15\n2, 15⋄\nNone / None / 2, 5†\n2, 5\n2, 5\nMixup alpha\n0.8\n0.8\nNone\nNone\nNone\nNone\nLoss type\nSoftmax\nSoftmax\nSigmoid\nSoftmax\nSigmoid\nSoftmax\nLabel smoothing\n0.1\n0.1\n0.0001\n0.1\n0.0001",
  "output": "The text provides hyper-parameter details for different models (CoAtNet-0/1/2/3, CoAtNet-2/3/4, CoAtNet-3/4/5) under various conditions (ImageNet-1K, ImageNet-21K, JFT, Pre-Training, Finetuning). Parameters include stochastic depth rate, center crop (true/false), RandAugment values, Mixup alpha, loss type (Softmax/Sigmoid), and label smoothing values.",
  "summary_type": "reports"
}{
  "input": "er-term) using a search engine [27].\nBoth the crawled data and the original dataset are passed\nthrough a network with two corresponding branches and\ntrained jointly. Note that data for the “auxiliary” task (the\nbranch for super-classes) is disjoint to the one used for the\nﬁne-grained task. Moreover, there is no explicit correspon-\ndence between the classiﬁcation of the super-class and the\noriginal class. A different approach starts by assigning the\noriginal labels to visually similar groups and",
  "output": "The text discusses a network that uses both crawled data and an original dataset. The data for the \"auxiliary\" task is separate from the fine-grained task, with no explicit link between the super-class classification and the original class. A different method begins by assigning original labels to visually similar groups.",
  "summary_type": "reports"
}{
  "input": "1.9\n55.8\n61.2\n66.0\n82.3\n86.2\n53.5\n61.2\n300M\n400K\n60.1\n64.1\n65.4\n70.6\n88.9\n90.7\n61.5\n68.1\n300M\n1.2M\n64.0\n67.7\n66.3\n71.4\n90.9\n91.9\n65.1\n71.2\n300M\n2M\n66.4\n69.3\n67.7\n73.1\n91.6\n92.1\n67.5\n73.7\n300M\n4M\n67.5\n70.1\n68.4\n73.1\n91.7\n92.2\n68.3\n74.1\n1B\n20K\n30.6\n35.2\n43.9\n50.4\n57.1\n61.9\n35.1\n41.9\n1B\n30K\n37.1\n41.9\n49.5\n55.4\n65.3\n71.8\n41.6\n48.5\n1B\n60K\n46.3\n50.2\n56.0\n61.3\n76.2\n80.3\n48.3\n56.7\n1B\n120K\n51.9\n55.9\n60.8\n65.2\n81.5\n85.7\n54.6\n62.3\n1B\n400K\n60.8\n64.5\n65.7\n70.3\n88.6\n90.7\n62.5\n69.1\n1B\n1.2M\n65.1\n68.1\n66.6\n72.3",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "NAACL, 2018.\n[49] H. Pham, Z. Dai, Q. Xie, M.-T. Luong, and Q. V. Le. Meta pseudo labels. arXiv preprint\narXiv:2003.10580, 2020.\n[50] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019.\n[51] P. Ramachandran and Q. V. Le. Diversity and depth in per-example routing models. In ICLR,\n2018.\n[52] C. Rosenbaum, I. Cases, M. Riemer, and T.",
  "output": "The text refers to various academic papers. \"Meta pseudo labels\" by H. Pham et al. (2020) is available on arXiv. Another paper, \"Exploring the limits of transfer learning with a unified text-to-text transformer\" by C. Raffel et al. (2019), also on arXiv. \"Diversity and depth in per-example routing models\" by P. Ramachandran and Q. V. Le was presented at ICLR 2018. The text ends before fully citing the last paper by C. Rosenbaum et al.",
  "summary_type": "reports"
}{
  "input": "exander Ku, and\nDustin Tran, “Image transformer,” in International\nConference on Machine Learning.\nPMLR, 2018,\npp. 4055–4064. 2, 3\n[23] Rewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever, “Generating long sequences with sparse\ntransformers,” arXiv preprint arXiv:1904.10509,\n2019. 2, 3\n[24] Jean-Baptiste Cordonnier, Andreas Loukas, and\nMartin Jaggi, “On the relationship between self-\nattention and convolutional layers,” arXiv preprint\narXiv:1911.03584, 2020. 2\n[25] Patryk Chrabaszcz, Ilya",
  "output": "The text refers to various academic papers. \"Image transformer\" by Alexander Ku and Dustin Tran was presented at the International Conference on Machine Learning in 2018. \"Generating long sequences with sparse transformers\" by Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever was published on arXiv in 2019. \"On the relationship between self-attention and convolutional layers\" by Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi was also published on arXiv in 2020. The text ends with the mention of Patryk Chrabaszcz and Ilya.",
  "summary_type": "reports"
}{
  "input": "and general patch-wise aug-\nmentation method for patch sequences which\nadds rich afﬁnity and diversity to training data.\nPatch-wise augmentation also contributes to the\nmodel generalization by reducing the risk of\nover-ﬁtting. With the above proposed methods,\nwe can build an augmented Robust Vision Transformer∗. Contributions of this paper are three-folds:\n• We give systematic robustness analysis of ViTs and ﬁnd some components are shown to be harmful.\nInformed by this, we propose a Robust Visi",
  "output": "This text discusses a general patch-wise augmentation method for patch sequences, which enhances the diversity and affinity of training data and reduces overfitting risk, contributing to model generalization. The text also mentions the development of an augmented Robust Vision Transformer. The paper provides a robustness analysis of Vision Transformers (ViTs), identifying some harmful components, leading to the proposal of a Robust Vision Transformer.",
  "summary_type": "reports"
}{
  "input": "64.0\n51.1\n71.5\n300M\n400K\n50K\n0.03\n68.6\n55.5\n76.0\n300M\n1.2M\n50K\n0.03\n70.1\n57.1\n77.6\n300M\n2M\n50K\n0.03\n70.5\n57.1\n77.9\n1B\n20K\n10K\n0.03\n49.9\n37.8\n56.5\n1B\n30K\n10K\n0.03\n55.2\n42.8\n62.3\n1B\n60K\n10K\n0.03\n61.0\n47.9\n68.4\n1B\n120K\n50K\n0.03\n64.0\n51.1\n71.5\n1B\n400K\n50K\n0.03\n68.5\n55.7\n75.9\n1B\n1.2M\n50K\n0.03\n70.0\n57.3\n77.3\n3B\n20K\n10K\n0.03\n49.9\n38.0\n56.3\n3B\n30K\n10K\n0.03\n55.4\n43.5\n62.4\n31\n\n--------------------------------------------------",
  "output": "<NULL>",
  "summary_type": "reports"
}{
  "input": "tt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697,\n2021.\n11\n\n--------------------------------------------------\n[41] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? arXiv preprint\narXiv:1905.10650, 2019.\n[42] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking the positional encoding in language pre-training. arXiv\npreprint arXiv:2006.15595, 2020.\n[43] Hon",
  "output": "The text mentions several academic preprints. \"Convit: Improving vision transformers with soft convolutional inductive biases\" by Morcos, Biroli, and Sagun, discusses enhancing vision transformers. Michel, Levy, and Neubig's \"Are sixteen heads really better than one?\" questions the efficiency of certain models. Ke, He, and Liu's \"Rethinking the positional encoding in language pre-training\" reconsiders aspects of language pre-training.",
  "summary_type": "reports"
}{
  "input": "tactfully decoupled in\nour RedNet towards a favorable accuracy-efﬁciency trade-\noff, as empirically evidenced in Figure 2. To be speciﬁc,\nthe information encoded in the channel dimension of one\npixel is implicitly scattered to its spatial vicinity in the ker-\nnel generation step, after which the information in an en-\nriched receptive ﬁeld is gathered thanks to the vast and dy-\nnamic involution kernels. Indispensably, linear transforma-\ntions (realized by 1 × 1 convolutions) are interspersed for",
  "output": "The text discusses an improvement in RedNet's accuracy-efficiency trade-off, as shown in Figure 2. This is achieved by scattering the information encoded in one pixel's channel dimension to its spatial vicinity during the kernel generation step. The information in an enriched receptive field is then gathered using dynamic involution kernels. Linear transformations, realized by 1x1 convolutions, are also crucially interspersed.",
  "summary_type": "reports"
}{
  "input": "ly be deployed in other research\ndomains where self-attention has shown success.\nReferences\n[1] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham,\nAnirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured\ninputs in transformers. In Conference on Empirical Methods in Natural Language Processing, 2020.\n[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid.\nVivit: A video vision trans",
  "output": "The text refers to two research papers. The first one is \"Etc: Encoding long and structured inputs in transformers\" by Joshua Ainslie and others, presented at the Conference on Empirical Methods in Natural Language Processing in 2020. The second one is \"Vivit: A video vision trans\" by Anurag Arnab and others. The specific content or findings of these papers are not provided in the text.",
  "summary_type": "reports"
}{
  "input": ", Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. CoRR,\nabs/2012.00364, 2020. 1, 2\n[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In ICML, volume 119 of Proceedings\nof Machine Learning Research, pages 1691–1703. PMLR,\n2020. 1, 2\n[8] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\nYan, and Jiashi Feng. Aˆ2-nets: Double attention networks.\nIn NeurIPS, pages 350–3",
  "output": "The text refers to various academic papers on machine learning and image processing. Siwei Ma et al. discuss a pre-trained image processing transformer in their 2020 paper. Mark Chen et al. explore generative pre-training from pixels in their work presented at ICML 2020. Yunpeng Chen et al. introduce Aˆ2-nets, a concept of double attention networks, in their paper presented at NeurIPS.",
  "summary_type": "reports"
}{
  "input": "]. Due to the usage of absolution positional embeddings, standard\nTransformer (ViT) lacks this property. This partially explains why ConvNets are usually better than\nTransformers when the dataset is not enormously large.\n• Finally, the size of the receptive ﬁeld is one of the most crucial differences between self-attention\nand convolution. Generally speaking, a larger receptive ﬁeld provides more contextual information,\nwhich could lead to higher model capacity. Hence, the global receptive ﬁeld",
  "output": "The standard Transformer (ViT) lacks absolution positional embeddings, which partially explains why ConvNets usually outperform Transformers when the dataset isn't extremely large. A key difference between self-attention and convolution is the size of the receptive field. A larger receptive field offers more contextual information, potentially leading to a higher model capacity.",
  "summary_type": "reports"
}{
  "input": "hildon, Karan Grewal, Adam Trischler, and\nYoshua Bengio. Learning deep representations by mutual information estimation and maximiza-\ntion. ArXiv, abs/1808.06670, 2018. 3\nOlivier J. H´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\nand Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding,\n2019. 3\nA. Khosla, Nityananda Jayadevaprakash, B. Yao, and Li Fei-Fei. Novel dataset for ﬁne-grained\nimage categorization : Stanford dogs.",
  "output": "The text mentions research papers on deep learning and image recognition. The first paper discusses learning deep representations through mutual information estimation and maximization. The second paper focuses on data-efficient image recognition using contrastive predictive coding. The third paper introduces a novel dataset for fine-grained image categorization, specifically featuring Stanford dogs.",
  "summary_type": "reports"
}{
  "input": "strained to+ 1 or-1.\narXiv preprint arXiv:1602.02830,\n2016. 2\n[3] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmentation\npolicies from data. arXiv preprint arXiv:1805.09501, 2018.\n1, 5\n[4] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe.\nRandaugment:\nPractical automated data augmenta-\ntion with a reduced search space.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702–703, 20",
  "output": "The text refers to two studies published on arXiv. The first one, \"Autoaugment: Learning augmentation policies from data\" by Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le, discusses learning augmentation policies from data. The second study, \"Randaugment: Practical automated data augmentation with a reduced search space\" by Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le, was presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, and focuses on practical automated data augmentation with a reduced search space.",
  "summary_type": "reports"
}{
  "input": "th-wise Conv2d. In addition, we compensate short-length limitations\nof the input token for each head by projecting the interaction across the attention-heads dimension\nwhile keeping the diversity ability of multi-heads.\nWe comprehensively validate the effectiveness of the proposed ResT on the commonly used bench-\nmarks, including image classiﬁcation on ImageNet-1k and downstream tasks, such as object detection,\nand instance segmentation on MS COCO2017. Experimental results demonstrate the effect",
  "output": "The text discusses the ResT model, which compensates for short-length limitations of input tokens in Conv2d by projecting interactions across the attention-heads dimension, maintaining multi-head diversity. The model's effectiveness is validated on benchmarks like ImageNet-1k for image classification and MS COCO2017 for object detection and instance segmentation. Experimental results show positive outcomes.",
  "summary_type": "reports"
}{
  "input": "consistent settings\nfor future methods. Speciﬁcally, we report standard 1×-schedule (12 epochs) detection results on\nthe COCO 2017 dataset [48] in Tables 3 and 4. As for the evaluation based on RetinaNet, we train\n6\n\n--------------------------------------------------\nTable 1 – Comparisons with state-of-the-art methods for ImageNet-1K classiﬁcation. Throughput is tested on\nthe batch size of 192 on a single V100 GPU. All models are trained and evaluated on 224×224 resolution on\nImageNet-1K datase",
  "output": "The text discusses the use of standard 1x-schedule (12 epochs) detection results on the COCO 2017 dataset. It also mentions an evaluation based on RetinaNet training. Additionally, it compares various methods for ImageNet-1K classification, testing throughput on a batch size of 192 on a single V100 GPU. All models are trained and evaluated on 224x224 resolution on the ImageNet-1K dataset.",
  "summary_type": "reports"
}{
  "input": "samples. Yet, it obtains a 0.3% accuracy\ngain, implying that these samples, though rarely seen, can make the training\n1 https://github.com/tensorﬂow/tpu/tree/master/models/oﬃcial/eﬃcientnet\n\n--------------------------------------------------\nCircumventing Outliers of AutoAugment with Knowledge Distillation\n13\nprocess unstable. On the other hand, when λ is overly large, e.g., knowledge\ndistillation can dominate the training process and force the student model to\nhave a very similar behavior to th",
  "output": "The text discusses the impact of rarely seen samples on the training process, suggesting they can increase accuracy by 0.3% but may also destabilize the process. It also mentions that if the knowledge distillation factor (λ) is too large, it can dominate the training process, forcing the student model to closely mimic the behavior of the original model.",
  "summary_type": "reports"
}{
  "input": "4.15%). For reference, the current state\nof the art of 88.55% achieved with extra training data was obtained by the ViT-\nH model (600M parameters) trained on JFT-300M at resolution 512. Hereafter\nwe provide several analysis and observations.\nConvnets teachers.\nWe have observed that using a convnet teacher gives bet-\nter performance than using a transformer. Table 2 compares distillation results\nwith different teacher architectures. The fact that the convnet is a better teacher\nis probably due to",
  "output": "The text discusses the performance of the ViT-H model with 600M parameters, trained on JFT-300M at resolution 512, which achieved a state-of-the-art 88.55% with extra training data. It also mentions that using a convnet teacher yields better results than using a transformer. This is demonstrated through distillation results with different teacher architectures, suggesting that the convnet is a superior teacher.",
  "summary_type": "reports"
}{
  "input": "to ViT&T2T-\nViT as ViT-Dense&T2T-ViT-Dense. Similar to DenseNet,\nif each block in ViT-Dense&T2T-ViT-Dense has L Trans-\nformer layers, there are L(L + 1)/2 connections in this\nblock and l-th layer has l input from the early layers. Specif-\nically, we set the hidden dimension of the ﬁrst layer in ViT-\nDense&T2T-ViT-Dense as 128 and it increases 64 channels\n(‘’growth rate” as DenseNets) in each layer after concate-\nnating with the early layers channels. The ViT-Dense&T2T-\nViT-Dense has 4 blocks as",
  "output": "ViT-Dense and T2T-ViT-Dense are similar to DenseNet. Each block in ViT-Dense and T2T-ViT-Dense has L Transformer layers, with L(L + 1)/2 connections in each block. The l-th layer has l inputs from early layers. The first layer's hidden dimension is set as 128, increasing by 64 channels (\"growth rate\" as in DenseNets) in each subsequent layer after concatenating with early layers channels. ViT-Dense and T2T-ViT-Dense have 4 blocks.",
  "summary_type": "reports"
}{
  "input": "pixel-level representation. Tokens-to-Token (T2T) [41]\nmainly improves tokenization in ViT by concatenating mul-\ntiple tokens within a sliding window into one token. How-\never, this operation fundamentally differs from convolutions\nespecially in normalization details, and the concatenation\nof multiple tokens greatly increases complexity in compu-\ntation and memory. PVT [34] incorporates a multi-stage\ndesign (without convolutions) for Transformer similar to\nmulti-scales in CNNs, favoring dense pr",
  "output": "Tokens-to-Token (T2T) enhances tokenization in Vision Transformer (ViT) by merging multiple tokens within a sliding window into one. However, this method differs fundamentally from convolutions, particularly in normalization details, and the merging of multiple tokens significantly increases computational and memory complexity. Pyramid Vision Transformer (PVT) integrates a multi-stage design for Transformer, similar to multi-scales in Convolutional Neural Networks (CNNs), favoring dense processing.",
  "summary_type": "reports"
}{
  "input": "mechanism, our\nmulti-branch design of X-volution obtains a better performance, showing the complementarity and\nnecessity of convolution and self-attention.\n4.3\nAnalysis Study\n4.3.1\nEffect of Different Kernel Size\nTo study the inﬂuence of convolution branch in our X-volution, we conduct experiments of different\nconvolutional kernel size (i.e., ranges from 1 × 1 to 9 × 9). We show the results in Fig. 3 (a). When\nreducing the kernel size to 1 × 1, obvious performance decay occurs, which is possibl",
  "output": "The X-volution's multi-branch design enhances performance, demonstrating the importance of convolution and self-attention. An analysis of different convolutional kernel sizes, ranging from 1x1 to 9x9, was conducted. The results showed a significant performance drop when the kernel size was reduced to 1x1.",
  "summary_type": "reports"
}{
  "input": "use the CNN and Transformer architectures [36, 64, 40,\n24, 55, 69, 64, 47], validating our approach. We hope that our uniﬁed view helps place these different\nconcurrent proposals in context and leads to a better understanding of the landscape of these methods.\n2\n\n--------------------------------------------------\n2\nRelated Work\nVisual Backbones. Since AlexNet [33] revolutionized computer vision, a host of CNN based archi-\ntectures have provided further improvements in terms of accuracy including",
  "output": "The text discusses the use of CNN and Transformer architectures, aiming to provide a unified view of different concurrent proposals. This is hoped to enhance understanding of these methods. It also mentions the revolution in computer vision since AlexNet, with numerous CNN-based architectures improving accuracy.",
  "summary_type": "reports"
}{
  "input": "-q3\n42.3\n46.2\n47.6\nconc-q4\n40.2\n60.7\n57.8\nhard\n27.2\n28.8\n31.7\nThe results are listed in the Table 12 compared to Picture-\nbook (Kiros et al., 2018) and GloVe (Pennington et al., 2014)\nembeddings. Overall the learned ALIGN perform better\nthan Picturebook but slightly worse than GloVe embeddings.\nWhat is interesting is that the ALIGN word embeddings\nhas a similar trend of Picturebook embeddings, with bet-\nter performance on nouns and most concrete categories but\nworse on adjs and less concrete cat",
  "output": "The text presents a comparison of ALIGN, Picturebook, and GloVe embeddings. Overall, ALIGN performs better than Picturebook but slightly worse than GloVe. Interestingly, ALIGN's performance trend is similar to Picturebook's, with better results on nouns and most concrete categories, but poorer performance on adjectives and less concrete categories.",
  "summary_type": "reports"
}{
  "input": "fferent\nportions of the dataset.\n• We update the state-of-the-art on ImageNet for mobile setting, achieving 80.0% top-1\naccuracy with only 100M FLOPs for the ﬁrst time.\n2\n\n--------------------------------------------------\n2\nRelated Work\n2.1\nEnsemble Learning\nEnsemble learning [2] aims at combining the predictions from several models to get a more robust\none. Some recently proposed literatures [3, 4] demonstrate that signiﬁcant gains can be achieved\nwith negligible additional parameters compared",
  "output": "The text discusses an improvement in ImageNet for mobile settings, achieving 80.0% top-1 accuracy with only 100M FLOPs for the first time. It also mentions Ensemble Learning, a method that combines predictions from several models to get a more robust one. Recent literature suggests significant gains can be achieved with negligible additional parameters.",
  "summary_type": "reports"
}{
  "input": "DeiT-B\nViT-B\n768\n12\n12\n86M\n224\n292\nTable 2: We compare on ImageNet [42] the performance (top-1 acc., %) of the\nstudent as a function of the teacher model used for distillation.\nTeacher\nStudent: DeiT-B ⚗\nModels\nacc.\npretrain\n↑384\nDeiT-B\n81.8\n81.9\n83.1\nRegNetY-4GF\n80.0\n82.7\n83.6\nRegNetY-8GF\n81.7\n82.7\n83.8\nRegNetY-12GF\n82.4\n83.1\n84.1\nRegNetY-16GF\n82.9\n83.1\n84.2\n5.2\nDistillation\nOur distillation method produces a vision transformer that becomes on par\nwith the best convnets in terms of the trade-of",
  "output": "The text compares the performance of the DeiT-B student model on ImageNet with various teacher models used for distillation. The DeiT-B model has an accuracy of 81.8-83.1%. The RegNetY models show a range of accuracy from 80.0% to 84.2%. The distillation method produces a vision transformer that matches the best convnets in terms of trade-off.",
  "summary_type": "reports"
}{
  "input": "ions in Table 4 exceed the MLPerf\ntarget accuracy of 75.9% at batch size 32,768 with minimal tuning. Training with larger batch\nsizes is not fundamentally unstable: stringent step budgets make hyperparameter tuning trickier.\nWeights\nOptimizer\nBias/BN\nOptimizer\nTop-1\nNesterov\nNesterov\n76.7\nLARS\nMomentum\n76.9\nLARS\nLARS\n76.9\nAdam (ϵ = 10−8)\nAdam (ϵ = 10−8)\n76.2\nAdam (ϵ = 10−6)\nAdam (ϵ = 10−6)\n76.4\nLAMB\nLAMB\n27.3\nLAMB\nAdam (ϵ = 10−8)\n76.3\nLAMB\nAdam (ϵ = 10−6)\n76.3\nTable 4: Validation accuracy of Res",
  "output": "The text presents a table with validation accuracy results of different optimizer combinations for a machine learning model. All combinations exceed the MLPerf target accuracy of 75.9% at batch size 32,768, except for LAMB/LAMB which only achieves 27.3%. The highest accuracy is achieved with LARS/Momentum and LARS/LARS, both at 76.9%. Training with larger batch sizes is not fundamentally unstable, but it makes hyperparameter tuning more challenging.",
  "summary_type": "reports"
}{
  "input": "model variants, termed Conformer-\nTi, -S, and -B, respectively. The details of Conformer-S\nare described in Tab. 1, and those of Conformer-Ti/B are\nin the Appendix. Conformer-S/32 splits the feature maps to\n7×7 patches, i.e., the patch size is 32×32 in the transformer\nbranch.\n4.2. Image Classiﬁcation\nExperimental Setting.\nConformer is trained on the\nImageNet-1k [14] training set with 1.3M images and tested\nupon the validation set.\nThe Top-1 accuracy is reported\nin Tab. 2. To make the transformer",
  "output": "The text discusses different Conformer models: Ti, S, and B. Conformer-S/32 divides feature maps into 7x7 patches with a patch size of 32x32 in the transformer branch. The Conformer is trained on the ImageNet-1k training set, which contains 1.3M images, and tested on the validation set. The Top-1 accuracy is reported.",
  "summary_type": "reports"
}{
  "input": "Z. Han, Y. Liu, and M. Zwicker, “Point2sequence: Learning\nthe shape representation of 3d point clouds with an attention-\nbased sequence to sequence network,” in AAAI, 2019, pp. 8778–\n8785.\n[75]\nW. Wu, Z. Qi, and F. Li, “Pointconv: Deep convolutional networks\non 3d point clouds,” in CVPR, 2019, pp. 9621–9630.\n[76]\nY. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “Pointcnn:\nConvolution on x-transformed points,” in NeurIPS, 2018, pp. 828–\n838.\n[77]\nX. Yan, C. Zheng, Z. Li, S. Wang, and S. Cui, “Po",
  "output": "Z. Han, Y. Liu, and M. Zwicker presented \"Point2sequence\", a shape representation learning method for 3D point clouds using an attention-based sequence to sequence network at AAAI 2019. W. Wu, Z. Qi, and F. Li introduced \"Pointconv\", deep convolutional networks for 3D point clouds at CVPR 2019. Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen discussed \"Pointcnn\", a convolution method on x-transformed points at NeurIPS 2018.",
  "summary_type": "reports"
}{
  "input": "of comparison experiments\nto show the advantages of our proposed Re-attention.\nRe-attention v.s. Self-attention: We ﬁrst evaluate the ef-\nfectiveness of Re-attention by comparing to the pure ViT\nmodels using the same set of training hyper-parameters. We\ndirectly replace the self-attention module in ViT with Re-\nattention and show the results in Tab. 4 with different num-\nber of transformer blocks. As can be seen, the vanilla ViT\narchitecture suffers performance saturation when adding\nmore transf",
  "output": "The text discusses a comparison between Re-attention and Self-attention in Vision Transformer (ViT) models. The same training hyper-parameters were used for both. The self-attention module in ViT was replaced with Re-attention, and results showed that the standard ViT architecture's performance stagnates when more transformer blocks are added.",
  "summary_type": "reports"
}{
  "input": "e, all experimental results\nin this section report ImageNet accuracies obtained by training a LambdaNetwork architecture that\nreplaces the spatial convolutions in the ResNet-50 with lambda layers.\nVarying query depth, number of heads and intra-depth.\nTable 9 presents the impact of the\nquery depth |k|, number of heads |h| and intra depth |u| on performance (See Appendix B.4 for\na presentation of the intra-depth |u|). Our experiments indicate that the lambda layer outperforms\nconvolutional and att",
  "output": "The text discusses an experiment where a LambdaNetwork architecture replaces the spatial convolutions in the ResNet-50 with lambda layers. The impact of query depth, number of heads, and intra-depth on performance is presented. The results suggest that the lambda layer performs better than convolutional and attention mechanisms.",
  "summary_type": "reports"
}{
  "input": "in GluonCV [19].\nAs comparison we replace the backbone with ResNeSt50\nand ResNeSt101 respectively while keeping other settings\nunchanged. The input image size is ﬁxed to 256x192 for all\nruns. We use Adam optimizer with batch size 32 and ini-\ntial learning rate 0.001 with no weight decay. The learning\nrate is divided by 10 at the 90th and 120th epoch. The ex-\nperiments are conducted on COCO Keypoints dataset, and\nwe report the OKS AP for results without and with ﬂip test.\nFlip test ﬁrst makes pre",
  "output": "In GluonCV, the backbone is replaced with ResNeSt50 and ResNeSt101, maintaining other settings. The input image size is 256x192 for all runs. Adam optimizer is used with a batch size of 32 and an initial learning rate of 0.001 with no weight decay. The learning rate is divided by 10 at the 90th and 120th epoch. Experiments are conducted on the COCO Keypoints dataset, reporting the OKS AP for results without and with flip test.",
  "summary_type": "reports"
}{
  "input": "of architectures\nfrom its accuracy in the ﬁrst 25% of training iterations. PNAS\n[23] uses a multilayer perceptron (MLP) and a recurrent neural\nnetwork to estimate the expected improvement in accuracy if the\ncurrent modular structure (which is later stacked together to form\na network) is expanded with a new branch. Conceptually, both\nof these methods seek to learn a prediction model that extrapolate\n(rather than interpolate), resulting in poor correlation in prediction.\nOnceForAll [31] also uses",
  "output": "PNAS uses a multilayer perceptron and a recurrent neural network to estimate the potential improvement in accuracy if the current modular structure is expanded with a new branch. However, these methods aim to extrapolate rather than interpolate, leading to poor prediction correlation. The OnceForAll method is also mentioned but without further context.",
  "summary_type": "reports"
}{
  "input": "on\n(ECA) module [45], and their combinations. A thorough\nanalysis of the activation function is discussed in the exper-\niments section.\n3.3. Class token\nTo apply vision transformers to image classiﬁcation, a\ntrainable class token is added and inserted into the token\nembedding, i.e.\nˆX ←Concat(Xcls, ˆX),\n(7)\n4\n\n--------------------------------------------------\nNetwork\nγ\nDW\nParams\nFLOPs\nTop-1\n(M)\n(G)\nAcc. (%)\nDeiT-T [41]\n4\nNo\n5.7\n1.3\n72.2\nLocalViT-T\n4\nNo\n5.7\n1.3\n72.5 (0.3↑)\nLocalViT-T*\n4\nYes\n5.8",
  "output": "The text discusses the application of vision transformers to image classification, where a trainable class token is added and inserted into the token embedding. It also compares the performance of DeiT-T and LocalViT-T networks. Both have similar parameters and FLOPs, but LocalViT-T shows a slight increase in Top-1 Accuracy.",
  "summary_type": "reports"
}{
  "input": "ch bounding box, the network\nshould output the conﬁdences for each class category (in total C class categories). For providing the\ndetections the framework uses a classiﬁer which is represented by a 3×3 convolution, that outputs for\neach bounding box the conﬁdences for all class categories (C). For localization the framework uses\nalso a 3×3 convolution to output the four localization values for each regressed default bounding box.\nIn total, the framework outputs 8732 detections (for 300×300 inpu",
  "output": "The network outputs confidence levels for each class category using a 3x3 convolution classifier. This classifier provides detections and outputs confidence levels for all class categories. For localization, a 3x3 convolution is also used to output four localization values for each default bounding box. In total, the framework outputs 8732 detections for a 300x300 input.",
  "summary_type": "reports"
}{
  "input": ". . .\n6\n3.3\nMaking lambda layers translation equivariant. . . . . . . . . . . . . . . . . . . . .\n7\n3.4\nLambda convolution: modeling longer range interactions in local contexts. . . . . .\n7\n4\nRelated Work\n8\n5\nExperiments\n9\n5.1\nLambda layers outperform convolutions and attention layers. . . . . . . . . . . . .\n9\n5.2\nComputational beneﬁts of lambda layers over self-attention.\n. . . . . . . . . . . .\n9\n5.3\nHybrids improve the speed-accuracy tradeoff of image classiﬁcation. . . . . . . . .\n10\n5.4\nO",
  "output": "The text discusses the use of lambda layers in translation equivariance and modeling longer range interactions in local contexts. It compares lambda layers to convolutions and attention layers, stating that lambda layers outperform them. It also highlights the computational benefits of lambda layers over self-attention and how hybrids improve the speed-accuracy tradeoff of image classification.",
  "summary_type": "reports"
}{
  "input": "and have a two stage evaluation process we also\ncompared against using models trained with cross-entropy loss for representation learning. We\ndo this by ﬁrst training the model with cross entropy and then re-initializing the ﬁnal layer of the\nnetwork randomly. In this second stage of training we again train with cross entropy but keep the\nweights of the network ﬁxed. Table 7 shows that the representations learnt by cross-entropy for a\nResNet-50 network are not robust and just the re-initializati",
  "output": "The text discusses a two-stage evaluation process for training models with cross-entropy loss for representation learning. Initially, the model is trained with cross entropy, then the final layer of the network is randomly re-initialized. In the second stage, the model is trained again with cross entropy, but the network weights are kept fixed. The results indicate that the representations learned by cross-entropy for a ResNet-50 network are not robust, especially after re-initialization.",
  "summary_type": "reports"
}{
  "input": "ikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\nD.10\nVTAB BREAKDOWN\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\n20\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82",
  "output": "The study by Ikov et al. (2020) achieved 82.1% top-5 accuracy and 61.7% top-1 accuracy. The scores for each of the VTAB-1k tasks are detailed in Table 9. This research was presented as a conference paper at ICLR 2021.",
  "summary_type": "reports"
}{
  "input": "39.5\n61.9\n42.5\nCycleMLP-B4\n61.5M\n43.2\n63.9\n46.2\n26.6\n46.5\n57.4\n71.5M\n44.1\n65.7\n48.1\n40.2\n62.7\n43.5\nResNeXt101-64x4d [61]\n95.5M\n41.0\n60.9\n44.0\n23.9\n45.2\n54.0\n101.9M\n42.8\n63.8\n47.3\n38.4\n60.6\n41.3\nCycleMLP-B5\n85.9M\n42.7\n63.3\n45.3\n24.1\n46.3\n57.4\n95.3M\n44.1\n65.5\n48.4\n40.1\n62.8\n43.0\nTable 5: Object detection and instance segmentation on COCO val2017 [34].\nare initialized with ImageNet pre-trained weights and other\nnewly added layers are initialized via Xavier [17]. We use\nthe AdamW [37] optimizer with",
  "output": "The text refers to object detection and instance segmentation on COCO val2017. It mentions models CycleMLP-B4, ResNeXt101-64x4d, and CycleMLP-B5, with respective sizes of 61.5M, 95.5M, and 85.9M. The models are initialized with ImageNet pre-trained weights and other layers are initialized via Xavier. The AdamW optimizer is used.",
  "summary_type": "reports"
}{
  "input": "object detection with Mask R-CNN. We notice that\nthe window size plays a crucial role and the default window\nsize 15 gives the best performance. Smaller window sizes\nlead to serious performance drop. As shown in Figure 3\nFigure 3. Effects of window size (Left) and number of global to-\nkens (Right) in Vision Longformer for object detection with Mask\nR-CNN. All use the same ImageNet1K pre-trained checkpoint\n(ViL-Small-RPB in Table 4).\n(Right), as long as there is one global token, adding more\nglob",
  "output": "The window size is critical in object detection with Mask R-CNN, with the default size of 15 providing the best performance. Smaller window sizes result in a significant performance drop. The number of global tokens also impacts performance, but even a single global token can enhance it. All tests use the same ImageNet1K pre-trained checkpoint.",
  "summary_type": "reports"
}{
  "input": "rk (for re-entrant processing), while in “at\nstart” all cross-attends are placed at the start of the network fol-\nlowed by all latent self-attend layers. All cross-attention layers\nexcept the initial one are shared, and self-attends are shared as\nusual (using 8 blocks of 6 self-attention modules). Results are\ntop-1 validation accuracy (in %) on ImageNet (higher is better).\ninitializing the position encoding: this value is used for the\nmodel reported in Tab. 2.\nFor all FLOPs numbers reported here",
  "output": "The text discusses a network model with re-entrant processing. Cross-attention layers are placed at the start, followed by latent self-attend layers. Except for the first, all cross-attention layers are shared, as are self-attends (using 8 blocks of 6 self-attention modules). The model's performance is measured by top-1 validation accuracy on ImageNet. The position encoding is initialized for the model reported in Tab. 2. The text also mentions FLOPs numbers.",
  "summary_type": "reports"
}{
  "input": "1097–1105, 2012.\n[20] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks\nwithout residuals. arXiv preprint arXiv:1605.07648, 2016.\n[21] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature\npyramid networks for object detection. In CVPR, 2017.\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. Microsoft coco: Common objects i",
  "output": "Gustav Larsson, Michael Maire, and Gregory Shakhnarovich developed Fractalnet, ultra-deep neural networks without residuals in 2016. Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie proposed Feature Pyramid Networks for object detection in 2017. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick worked on Microsoft COCO, a common objects project.",
  "summary_type": "reports"
}{
  "input": "e later transformer encoder. More formally, the output\nzi of this fusion module can be represented as\nzi =\n\ngi(\nX\nj∈{l,s}\nf j(xj\ncls)) || xi\npatch\n\n,\n(3)\nwhere f i(·) and gi(·) play the same role as Eq. 2.\nPairwise Fusion. Figure 3(c) shows how both branches are\nfused in pairwise fusion. Since patch tokens are located at\nits own spatial location of an image, a simple heuristic way\nfor fusion is to combine them based on their spatial loca-\ntion. However, the two branches process patches of di",
  "output": "The text discusses a fusion module in a transformer encoder. The output, zi, is represented by a formula. It also mentions pairwise fusion, where two branches are fused. Patch tokens, located at their own spatial location of an image, are combined based on their spatial location. However, the two branches process patches differently.",
  "summary_type": "reports"
}{
  "input": "al if we are to understand neural network training,\nespecially at larger batch sizes where we lose some of the regularization effect of gradient noise.\nHypothetically, if the primary beneﬁt of a training procedure is regularization, then it would be better\nto compare the method with other regularization baselines than other optimizers.\nUltimately, we only care about batch size to the extent that higher degrees of data parallelism lead\nto faster training. Training with a larger batch size is a me",
  "output": "The text discusses neural network training, emphasizing the loss of regularization effect with larger batch sizes due to gradient noise. It suggests that if a training procedure's main advantage is regularization, it should be compared with other regularization baselines rather than other optimizers. The importance of batch size is only relevant in terms of data parallelism, which can speed up training. Training with a larger batch size is mentioned but not fully explained.",
  "summary_type": "reports"
}{
  "input": "output layer of the network is removed and\nreplaced with a untrained homoscedastic output layer for\nﬁne-tuning. For downstream ﬁne-tuning we use the standard\nhyperparameters and data augmentation settings speciﬁed\nby Kolesnikov et al. [26]. The VTAB1K score is an average\nof the accuracy on all 19 datasets.\nTable 6 shows VTAB1K scores. Our parameter-efﬁcient\nheteroscedastic model, which captures correlations in the\nJFT label noise, improves the VTAB1K score by 0.88%\nover the homoscedastic baselin",
  "output": "The network's output layer is replaced with an untrained homoscedastic output layer for fine-tuning, using standard hyperparameters and data augmentation settings. The VTAB1K score, an average accuracy of all 19 datasets, is improved by 0.88% with the heteroscedastic model, which captures correlations in the JFT label noise, over the homoscedastic baseline.",
  "summary_type": "reports"
}{
  "input": "iT [22] as the baseline for most experiments. The rela-\ntive position encoding is added into all self-attention layers.\nIf not speciﬁed, the relative position encoding is only added\non keys. We set α:β:γ = 1:2:8 for the piecewise function\ng(x), and adjust the number of buckets by changing β. An\n5\n\n--------------------------------------------------\nMethod\nIs\nMode\nTop-1\n∆\nbased on DeiT-S [22]\nDirected\nAcc(%)\nAcc(%)\nOriginal [22]\n-\n-\n79.9\n–\nEuclidean\n×\nbias\n80.1\n+0.2\ncontextual\n80.4\n+0.5\nQuantizati",
  "output": "The text discusses an experiment using iT [22] as a baseline. The relative position encoding is added to all self-attention layers, primarily on keys. The piecewise function g(x) is set with α:β:γ = 1:2:8, and the number of buckets is adjusted by changing β. The method is based on DeiT-S [22]. The original accuracy is 79.9%, but with Euclidean bias, it increases to 80.1%, and with contextual adjustment, it reaches 80.4%.",
  "summary_type": "reports"
}{
  "input": "d implementation details that are not often discussed. It was not\nobvious to us a priori which ones would prove crucial. These details do not involve changes to the\noptimizer, but they interact with the optimizer in a regime where all hyperparameters need to be well\ntuned to stay competitive, making it necessary to re-tune everything for a new optimizer.\nIn neural network optimization research, training loss is rarely discussed in detail and evaluation\ncenters on validation/test performance sinc",
  "output": "The text discusses the importance of fine-tuning hyperparameters in neural network optimization. It highlights that certain implementation details, which do not involve changes to the optimizer, can significantly impact the performance. These details are often overlooked in research, where focus is mainly on training loss and validation/test performance.",
  "summary_type": "reports"
}{
  "input": ".2\n86.5\n152\n256\n0.49\n83.8\n-\n152\n288\n0.63\n-\n86.7\n270\n256\n0.91\n84.2\n-\n350\n256\n1.16\n84.4\n-\n350\n288\n1.48\n84.5\n-\n350\n320\n1.91\n84.7\n-\n420\n320\n2.25\n84.9\n-\nTable 19: Detailed LambdaResNets results. Latency refers to the time per training step for a batch\nsize of 1024 on 8 TPU-v3 cores using bfloat16 activations.\nSupervised ImageNet 90 epochs training setup with vanilla ResNet.\nIn the 90 epoch setup, we\nuse the vanilla ResNet for fair comparison with prior works. We used the default hyperparameters\nas fo",
  "output": "The text presents detailed LambdaResNets results, referring to latency per training step for a batch size of 1024 on 8 TPU-v3 cores using bfloat16 activations. It mentions a supervised ImageNet 90 epochs training setup with vanilla ResNet, using default hyperparameters for fair comparison with prior works.",
  "summary_type": "reports"
}{
  "input": "ortantly, XCA has a linear\ncomplexity in the number of patches. To construct our Cross-Covariance Image Transformers\n(XCiT), we combine XCA with local patch interaction modules that rely on efﬁcient depth-\nwise convolutions and point-wise feedforward networks commonly used in transformers, see\nFigure 1. XCA can be regarded as a form of a dynamic 1×1 convolution, which multiplies\nall tokens with the same data-dependent weight matrix. We ﬁnd that the performance of\nour XCA layer can be further imp",
  "output": "The Cross-Covariance Image Transformers (XCiT) combines Cross-Covariance Analysis (XCA) with local patch interaction modules that use efficient depth-wise convolutions and point-wise feedforward networks. XCA, which can be seen as a dynamic 1x1 convolution, multiplies all tokens with the same data-dependent weight matrix. The performance of the XCA layer can be further improved.",
  "summary_type": "reports"
}{
  "input": "f semi-supervised methods related\nto self-training [5] that explicitly generate pseudo-labels for\nthe unlabeled samples and that optimize prediction accuracy\non both the ground truth labels (for the labeled samples) and\nthe pseudo-labels (for the unlabeled samples). For example\nPseudo-Label [18] and earlier related methods [19, 20, 21]\nﬁrst train a model on the labeled samples, use this model\nto assign pseudo-labels to unlabeled samples, and then re-\ntrain the model using both the labeled and un",
  "output": "The text discusses semi-supervised methods like self-training that generate pseudo-labels for unlabeled samples and optimize prediction accuracy for both actual and pseudo-labels. Methods like Pseudo-Label train a model on labeled samples, assign pseudo-labels to unlabeled samples, and then retrain the model using both types of labels.",
  "summary_type": "reports"
}{
  "input": "meterized supernet model,\nand then share the weights across subnets. Among them,\nSPOS is simple and representative [16]. In each iteration, it\nonly samples one random path and trains the path using one\nbatch data. Once the training process is ﬁnished, the subnets\ncan be ranked by inheriting the shared weights. However,\nmost weight-sharing methods need an additional retraining\nstep after the best architecture is identiﬁed [16, 31, 59].\nRecent works, OFA [6], BigNAS [65] and slimmable\nnetworks [66",
  "output": "The text discusses a meterized supernet model where weights are shared across subnets. The Single Path One-Shot (SPOS) method is simple and representative, sampling one random path and training it with one batch data. After training, subnets can be ranked by inheriting the shared weights. However, most weight-sharing methods require additional retraining after the best architecture is identified. Recent works include OFA, BigNAS, and slimmable networks.",
  "summary_type": "reports"
}{
  "input": "markable 0.8% improvement on ImageNet compared to\nN=16. However, the improvement comes to be saturated when adding more parts into the network.\nEffects of the part-wise linear. Different from the original design in Transformer [62] that uses a\nself-attention module for part-wise communication, we replace it with a simple linear operation to\nsave the computational cost. Introducing such a simple linear operation into ViP-S can lead to a 0.4%\ngain on ImageNet with only a fractional increase in par",
  "output": "The text discusses an improvement of 0.8% on ImageNet compared to N=16. However, adding more parts to the network saturates this improvement. The original design in Transformer uses a self-attention module for part-wise communication, but this has been replaced with a simple linear operation to reduce computational cost. This change leads to a 0.4% gain on ImageNet with only a slight increase in parameters.",
  "summary_type": "reports"
}{
  "input": "ets.\nDataset\nTrain Size\nTest Size\n# Classes\nFood-101\n75,750\n25,250\n101\nCARS196\n8,144\n8,041\n196\nOxford-Flowers\n2,040\n6,149\n102\nFGVC Aircraft\n6,667\n3,333\n100\nOxford-IIIT Pets\n3,680\n3,669\n37\nTable 16. Statistics of FGVC datasets.\nC. IR Task Conﬁguration\nIn this section, we will describe experimental conﬁg-\nurations for three public ﬁne-grained image retrieval (IR)\ndatasets: Stanford Online Products (SOP) [28], CUB200\n[33] and CARS196 [17]. The basic experimental setup and\nhyperparameters are descri",
  "output": "The text provides statistics for five Fine-Grained Visual Categorization (FGVC) datasets: Food-101 with 75,750 training and 25,250 testing images across 101 classes; CARS196 with 8,144 training and 8,041 testing images across 196 classes; Oxford-Flowers with 2,040 training and 6,149 testing images across 102 classes; FGVC Aircraft with 6,667 training and 3,333 testing images across 100 classes; and Oxford-IIIT Pets with 3,680 training and 3,669 testing images across 37 classes. It also mentions experimental configurations for three public fine-grained image retrieval datasets: Stanford Online Products, CUB200, and CARS196.",
  "summary_type": "reports"
}