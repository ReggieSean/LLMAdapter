[
  ".\n[53] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00b4e\nJ\u00b4egou. Fixing the train-test resolution discrepancy. arXiv\npreprint arXiv:1906.06423, 2019.\n[54] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas,\nNiki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling\nlocal self-attention for parameter ef\ufb01cient visual backbones.\narXiv preprint arXiv:2103.12731, 2021.\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolo",
  "g\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 764\u2013773, 2017. 1, 3\n[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009. 5\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterth",
  "cientNet variants because\nwe report the wallclock time, whereas Srinivas et al. (2021)\nreport the \u201ccompute time\u201d which will ignore cross-device\ncommunication. For very small models the inter-device\ncommunication costs can be non-negligible relative to the\ncompute time, especially for Ef\ufb01cientNets which employ\ncross-replica batch normalization. For larger models this\ncost is generally negligible on hardware like TPUv3 with\nvery fast interconnects, so in practice one can expect that\nthe compute ti",
  "asurement in blind audio source separation. IEEE\nTransactions on Audio, Speech and Language Processing.\nWang, K.; et al. 2019. Haq: Hardware-aware automated\nquantization with mixed precision. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 8612\u20138620.\nWidrow, B.; Kollar, I.; and Liu, M.-C. 1996. Statistical theory\nof quantization. IEEE Transactions on instrumentation and\nmeasurement, 45(2): 353\u2013361.\nWightman, R. 2019. PyTorch Image Models. https://github.\ncom",
  "asks considered therein:\n5\n\n--------------------------------------------------\nTable 2: Main V-MoE & VIT models; Table 8 shows results for additional models and datasets.\nModel\nParams\nJFT prec@1\nIN/1shot IN/5shot IN/10shot IN/Fine-t.\nExaFLOPs TPUv3-days\nVIT-H/14\n656M\n56.68\n62.34\n76.95\n79.02\n88.08\n4.27k\n2.38k\nV-MoE-L/16, Every-2\n3.4B\n57.65\n62.41\n77.10\n79.01\n87.41\n2.17k\n1.20k\nV-MoE-H/14, Last-5\n2.7B\n60.12\n62.95\n78.08\n80.10\n88.23\n4.75k\n2.73k\nV-MoE-H/14, Every-2\n7.2B\n60.62\n63.38\n78.21\n80.33\n88.36\n5.",
  "ngs of IEEE Conference\non Computer Vision and Pattern Recognition Workshops\n(CVPRW), 2020.\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and\nSalakhutdinov, R. Transformer-XL: Attentive language\nmodels beyond a \ufb01xed-length context. In Annual Meet-\nings of the Association for Computational Linguistics,\n2019.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and\nKaiser, L. Universal Transformers. In Proceedings of\nInternational Conference on Learning Representations\n(ICLR), 2019.\nDeng, ",
  "sequence with 256\ntokens. In the centroid attention block, we \ufb01rst initialize\nour shorter sequence by reshaping the N length patch to-\nkens into a\n\u221a\nN \u00d7\n\u221a\nN order. We then apply a depth-wise\nModels\n#layers\n#tokens\nembedding\ndimension\ncentroid @\nDeiT-tiny\n12\n196\n192\n-\nOurs-1\n196\n6\nOurs-2\n256\n5\nOurs-3\n256\n7\nDeiT-small\n12\n196\n384\n-\nOurs-4\n196\n6\nOurs-5\n256\n5\nOurs-6\n256\n7\nTable 5: Architecture design of different models for image\nclassi\ufb01cation tasks on ImageNet, centroid @ means re-\nplacing the self-",
  "petitive with\nCNN based architectures given suf\ufb01ciently large amount\nof training data. ViT is composed of several parts: Image\nTokenization, Positional Embedding, Classi\ufb01cation Token,\nthe Transformer Encoder, and a Classi\ufb01cation Head. These\nsubjects are discussed in more detail below.\nImage Tokenization: A standard transformer takes as\ninput a sequence of vectors, called tokens. For traditional\nNLP based transformers the word ordering provides a nat-\nural order to sequence the data, but this is ",
  "m, Nm \u2212j\u2217\nm), ..., (j\u2217\nM, NM \u2212j\u2217\nM)].\nSecond Stage. After obtaining the optimal the distribution\nof global-local modules in all blocks at the first stage, we\nfix this distribution and search the detailed architecture of\nall modules. Similar to the first stage, we adopt SPOS [13]\nto find the optimal architecture S\u2217in the search space. The\nmain difference is changed search space and correspond-\ningly the random index of a block is an array with four el-\nements, instead of a single number jm. The r",
  "pernet adaptation and evolutionary search, that are iteratively executed. NAT also uses an online accuracy predictor model to improve its\ncomputational ef\ufb01ciency.\n(i) subnets that span the entire objective trade-off front, and (ii)\na task-speci\ufb01c supernet. The latter can now be utilized for all\nfuture deployment-speci\ufb01c NAS, i.e., new and different hardware\nor objectives, without any additional training.\nThe core of NAT\u2019s ef\ufb01ciency lies in only adapting the subnets\nof the supernet that will lie ",
  "erature has shown that features obtained from supervised training of\nCNNs may over-emphasize texture rather than encoding high-level information.\nIn self-supervised learning in particular, texture as a low-level cue may provide\nshortcuts that prevent the network from learning higher level representations. To\naddress these problems we propose to use classic methods based on anisotropic\ndiffusion to augment training using images with suppressed texture. This simple\nmethod helps retain important ed",
  "that when the normalization is combined with an inner product (as we do here), this is equivalent\nto cosine similarity. Some contrastive learning approaches [3] use a cosine similarity explicitly in their loss\nformulation. We decouple the normalization here to highlight the bene\ufb01ts it provides.\n16\n\n--------------------------------------------------\nThus, for the gradient of Lsup\nin (where Xip = Xin\nip ):\n\u2225(zp \u2212(zi \u2022 zp)zi\u2225|Pip \u2212Xin\nip |\n\u2248|Pip \u2212Xin\nip |\n=\n\f\f\f\f\f\f\f\n1\nP\np\u2032\u2208P (i)\nexp (zi \u2022 zp\u2032/\u03c4) +\nP",
  "arning Sys-\ntems, 28(10):2222\u20132232, October 2017. ISSN 2162-2388.\ndoi: 10.1109/TNNLS.2016.2582924. Conference Name:\nIEEE Transactions on Neural Networks and Learning\nSystems.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770\u2013778, 2016.\nHinton, G., Vinyals, O., and Dean, J.\nDistilling\nthe knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015.\nHu, H., Gu, J",
  "s\n1-10-1\n2-9-1\n1-1-9-1\n1-2-8-1\n1-10-1\n2-9-1\n1-1-9-1\n1-2-8-1\nFull Attention\n7.1, 1.35\n6.8, 1.45\n6.7, 2.29 6.4, 2.39\n27.6, 4.84 26.3, 5.05\n26.0, 6.74 24.6, 6.95\nVision Longformer\n7.1, 1.27\n6.8, 1.29\n6.7, 1.33 6.4, 1.35\n27.6, 4.67 26.3, 4.71\n26.0, 4.82 24.6, 4.86\nLinformer [46]\n7.8, 1.57\n7.7, 1.6\n8.2, 1.69 8.0, 1.73\n28.3, 5.27 27.1, 5.35\n27.4, 5.55 26.3, 5.62\nPartial Linformer\n7.3, 1.31\n7.2, 1.37\n7.7, 1.46 7.6, 1.52\n27.8, 4.76 26.7, 4.88\n27.0, 5.08 25.8, 5.21\nSRA/64 [47]\n14.2, 0.99 13.9, 0.99\n13.8,",
  "123, 2019. 1, 2, 6, 7, 8, 21\n[6] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical data\naugmentation with no separate search. arXiv preprint arXiv:1909.13719, 2019. 1, 6, 8, 21, 22\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In 2009 IEEE conference on computer vision and pattern\nrecognition, 2009. 1, 5, 6\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: ",
  "approximated global self-attention features. Once trained, we\nconditionally re-parameterize it as an atomic operation as shown on the right. At inference phase, X-volution\nis actually a dynamic convolution operator, and its weight is composed of the attention map that needs to be\ndynamically calculated and the convolution weight that has been trained and solidi\ufb01ed.\nto a compatible pattern of convolution. Finally, we describe that in the inference phase how to\nconditionally merge the branches of ",
  "riments. Stochastic Depth is also applied here as what proposed on ImageNet. We\nembed ViP into two popular frameworks for object detection and instance segmentation, RetinaNet\n[44] and Cascade Mask-RCNN [6]. When incorporating ViP into these frameworks, ViP serves as\nthe backbone followed by a Feature Pyramid Network (FPN) [43] re\ufb01ning the multi-scale whole\nrepresentations. All weights within the backbone are \ufb01rst pre-trained on ImageNet-1K, while those\noutside the backbone are initialized with ",
  "Semi-Supervised Recognition under a Noisy and Fine-grained Dataset\nCheng Cui*, Zhi Ye*, Yangxi Li*, Xinjian Li*,\nMinYang*, Kai Wei *, Bing Dai*, Yanmei Zhao*, Zhongji Liu*, Rong Pang*\nNational Internet Emergency Center(CNCERT), Baidu\n{laxlyezhi, weikai105b, cuicheng0101, yangminbupt}@gmail.com\nAbstract\nSimi-Supervised Recognition Challenge-FGVC7 is a\nchallenging \ufb01ne-grained recognition competition. One of\nthe dif\ufb01culties of this competition is how to use unlabeled\ndata. We adopted pseudo-tag dat",
  "practically not\nfeasible and, more importantly, not necessary. Firstly, it is evident\nfrom existing NAS approaches [26], [62] that different objectives\n(#Params, #MAdds, latency on different hardware, etc.) require\n\n--------------------------------------------------\n8\nTABLE 2: Hyperparameter Settings\nCategory\nParameter\nSetting\nGlobal\nArchive size\n300\nNumber of iterations\n30\nAccuracy predictor\nTrain size\n100\nEnsemble size\n500\nEvolutionary search\nPopulation size\n100\nNumber of generations per itera",
  "\nposition encodings.\n\n--------------------------------------------------\nPerceiver: General Perception with Iterative Attention\nFigure 11. Example attention maps from the \ufb01rst cross-attend of an ImageNet network trained with 2D Fourier feature position\nencodings.\n\n--------------------------------------------------\nPerceiver: General Perception with Iterative Attention\nFigure 12. Example attention maps from the eighth (\ufb01nal) cross-attend of an ImageNet network trained with 2D Fourier feature\nposi",
  "capacity itself is a cru-\ncial factor. If we compare MEAL V1 and V2 we can further\nderive the conclusion that teacher\u2019s performance, i.e. the\nquality of supervision, is another factor for the student, gen-\nerally, the stronger teachers can consistently distill stronger\nstudents. In the following, we would like to exam the im-\npact of the initialization and discriminator.\nEffects of initialization. To verify whether the initializa-\ntion of a student has a big impact, we conduct the ablation\nstudy",
  "\nFor the h-th SA head, the attention map is computed as\nAh,:,: = Softmax(QhKh\n\u22a4/\n\u221a\nd) with Qh and Kh from the\ncorresponding head. When the context is clear, we omit the\nsubscript h.\n3.2. Attention Collapse\nMotivated by the success of deep CNNs [9, 30, 35, 36],\nwe conduct systematic study in the changes of the perfor-\nmance of ViTs as depth increases. Without loss of gen-\nerality, we \ufb01rst \ufb01x the hidden dimension and the number\nof heads to 384 and 12 respectively1, following the com-\nmon practice ",
  "r greedy super-\nnet is more discriminative since it can use less images to\nidentify whether a path is good or weak. Nevertheless, as\nleft Figure 4 shows, too few evaluation images might have\nweak correlation while too many evaluation images mean\ngreater evaluation cost. But 1000 evaluation images enjoy a\nbalanced trade-off between the rank correlation and evalua-\ntion cost. Note that we also report the results w.r.t. ranking\nusing the ACC of 1000 images, which is smaller than that\nusing loss. Th",
  "t,\napply the Normalizer-Free setup of Brock et al. (2021) to\nan SE-ResNeXt-D, with modi\ufb01ed width and depth patterns,\nand a second spatial convolution. Second, apply AGC to\nevery parameter except for the linear weight of the classi\ufb01er\nlayer. For batch size 1024 to 4096, set \u03bb = 0.01, and make\nuse of strong regularization and data augmentation. See\nTable 1 for additional information on each model variant.\n6. Experiments\n6.1. Evaluating NFNets on ImageNet\nWe now turn our attention to evaluating our",
  "77] both use JFT as unlabeled data like\nMeta Pseudo Labels. Similar to Section 3.2, we only com-\npare Meta Pseudo Labels to results that are obtained with\nResNet-50 and without distillation.\nResults.\nTable 3 presents the results. As can be seen from\nthe table, Meta Pseudo Labels boosts the top-1 accuracy of\nResNet-50 from 76.9% to 83.2%, which is a large margin\nof improvement for ImageNet, outperforming both UDA\nand Noisy Student. Meta Pseudo Labels also outperforms\nBillion-scale SSL [68, 79] in",
  "[14]. However, the relative\nimprovement of larger Mixer models are even more pronounced. The performance gap between\nMixer-L/16 and ViT-L/16 shrinks with data scale. It appears that Mixer bene\ufb01ts from the growing\ndataset size even more than ViT. One could speculate and explain it again with the difference in\ninductive biases: self-attention layers in ViT lead to certain properties of the learned functions that are\nless compatible with the true underlying distribution than those discovered with M",
  "s are not able to\nbene\ufb01t from either the largest dataset, or compute resources. Figure 1, left and center, show the Ti/16\nmodel tending towards a high error rate, even when trained on a large number of images.\nThird, large models bene\ufb01t from additional data, even beyond 1B images. When scaling up the\nmodel size, the representation quality can be limited by smaller datasets; even 30-300M images is\nnot suf\ufb01cient to saturate the largest models. In Figure 1, center, the error rate of L/16 model on t",
  "und that the learnable pooling performs the\nbest. We believe that the learnable weighting is more ef-\n\ufb01cient because each embedded patch does not contain the\nsame amount of entropy. This allows our model to give\nhigher weight to patches that contain more information rel-\nevant to the classi\ufb01er. Additionally, the sequence pooling\nallows our model to better utilize information across spa-\ntially sparse data. We will further study the effects of this\npooling in the ablation study (Sec 4.4).\n3.4. Sm",
  "tion to 12\nobjective problem in Section F.\n4.6\nUtility on Dense Image Prediction\nDense image prediction is another series of important computer\nvision tasks, that assigns a label to each pixel in the input image\n[72], [73]. Success in these tasks relies on both feature extraction\nvia a backbone CNN, e.g. ResNet [3], and feature aggregation, e.g.\nFPN [74], at multiple scales. In this section, we use NAT to design\nef\ufb01cient backbone feature extractors for semantic segmentation, to\ndemonstrate its u",
  "g ones, starting at 30M (around 10% of the original dataset), it does.\nin the sense of reducing validation accuracy on the pre-training dataset, but also in reduced transfer\nperformance as training continued. As an initial attempt at tackling this, we used RandAugment [14]\nwith N = 2 transformations of magnitude M = 10. This is shown in Figure 38. Interestingly,\nRandAug typically helps expert models while harming dense models. With this applied, for each\narchitecture, there is an expert model wh",
  "d-to-Head comparison on image segmentation (ResNet-50 as backbone) on ADE20K.\nHead\noutput stride backbone: 8\noutput stride backbone: 16\nmean IoU\npixel Acc.\nparams\nGFLOPs\nmean IoU\npixel Acc.\nparams\nGFLOPs\nbaseline [23]: 3\u00d73 conv\n37.87\n78.17\n35.42\n131.37\n36.84\n77.84\n35.42\n39.52\nDeepLabv3 [24]: ASPP\n40.91\n79.92\n41.48\n151.17\n40.34\n79.44\n41.48\n44.47\nPSPNet [23]: PPM\n41.24\n80.01\n49.06\n165.42\n39.75\n79.17\n49.06\n48.08\nPyConvSegNet: PyConvPH\n41.54\n80.18\n34.40\n116.84\n40.43\n79.45\n34.40\n36.08\nPyConv results ",
  " the advantages of Transformers: dynamic\nattention, global context fusion, and better generalization.\nOur results demonstrate that this approach attains state-of-\nart performance when CvT is pre-trained with ImageNet-\n1k, while being lightweight and ef\ufb01cient: CvT improves the\nperformance compared to CNN-based models (e.g. ResNet)\nand prior Transformer-based models (e.g. ViT, DeiT) while\nutilizing fewer FLOPS and parameters. In addition, CvT\nachieves state-of-the-art performance when evaluated at",
  "d Aditya V. Nori. Semi-supervised learning via\ncompact latent space clustering. In International Conference on Machine Learning, 2018. 3\n[25] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain\nGelly, and Neil Houlsby. Large scale learning of general visual representations for transfer.\narXiv preprint arXiv:1912.11370, 2019. 1\n[26] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better?\nIn Proceedings of the IEEE conference ",
  "ms 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5754\u20135764,\n2019.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\narXiv preprint arXiv:2106.04560, 2021.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu,\nJianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic segmentatio",
  "18] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J.\nBelongie. Feature pyramid networks for object detection. In IEEE Conference on Computer\nVision and Pattern Recognition, 2017. 6\n[19] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In\nEuropean Conference on Computer Vision, 2014. 2, 6\n[20] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan",
  "mputer Vision and Pattern\nRecognition, pages 1580\u20131589, 2020.\n[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n770\u2013778, 2016.\n[18] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge\nin a neural network. arXiv preprint arXiv:1503.02531, 2015.\n[19] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam. Mobilenets: Ef\ufb01-\n",
  ",\n(ii) between different spatial locations, or both at once. In CNNs, (ii) is implemented with N \u00d7 N\nconvolutions (for N > 1) and pooling. Neurons in deeper layers have a larger receptive \ufb01eld [1, 28].\nAt the same time, 1\u00d71 convolutions also perform (i), and larger kernels perform both (i) and (ii).\nIn Vision Transformers and other attention-based architectures, self-attention layers allow both (i)\nand (ii) and the MLP-blocks perform (i). The idea behind the Mixer architecture is to clearly sepa",
  "Ef\ufb01cientnet: Rethinking model scaling for convolutional neural networks. In\nInternational Conference on Machine Learning, pages 6105\u20136114. PMLR, 2019.\n[53] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In\nInternational Conference on Machine Learning, pages 9438\u20139447. PMLR, 2020.\n[54] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9\nJ\u00e9gou.\nTraining data-ef\ufb01cient image transformers & distillation through a",
  "nd Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners.\narXiv preprint\narXiv:2005.14165, 2020.\n[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit:\nCross-attention multi-scale vision transformer for image\nclassi\ufb01cation. arXiv preprint arXiv:2103.14899, 2021.\n[5] Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George\nPapandreou, Barret Zoph, Florian Schroff, Hartwig Adam,\nand Jon Shlens. Searching for ef\ufb01cient multi-scale architec-\ntures fo",
  "lly a bottleneck\nstructure [10], which can be de\ufb01ned as a stack of 1 \u00d7 1, 3 \u00d7 3, and 1 \u00d7 1 convolution layers\nwith residual learning (shown in Figure 1a). The 1 \u00d7 1 layers are responsible for reducing and\nthen increasing channel dimensions, leaving the 3 \u00d7 3 layer a bottleneck with smaller input/output\nchannel dimensions. The CNN backbones are generally faster and require less inference time thanks\nto parameter sharing, local information aggregation, and dimension reduction. However, due to\nthe ",
  "dation set.\n5.1. Implementation Details\nWe use data sharding for distributed training on Ima-\ngeNet, evenly partitioning the data across GPUs. At each\ntraining iteration, a mini-batch of training data is sampled\nfrom the corresponding shard (without replacement). We\napply the transformations from the learned Auto Augmen-\ntation policy to each individual image. Then we further ap-\nply standard transformations including: random size crop,\nrandom horizontal \ufb02ip, color jittering, and changing the\nli",
  "r than stated in Figure 5.\n4. Transfer Learning Results\nIn this section, we will present transfer learning results\nof TResNet models on four well-known single-label classi-\n\ufb01cation downstream datasets. We will also present transfer\nlearning results on multi-label classi\ufb01cation and object de-\ntection tasks.\n4.1. Single-Label Classi\ufb01cation\nWe evaluated TResNet on four commonly used, compet-\nitive transfer learning datasets: Stanford-Cars [16], CIFAR-\n10 [17], CIFAR-100 [17] and Oxford-Flowers [29]",
  "in several of the \ufb01rst module\u2019s attention maps).\narchitectural inductive bias to do so. In other words, under\nthese conditions, networks that use 2D convolutions cannot\nexploit the local neighorhood structure of inputs to reason\nabout space, but must reason in terms of features, just like\nstructure-agnostic architectures like Transformers and Per-\nceivers. The results of this experiment are shown in Table 2.\nAs the Transformer and Perceiver effectively treat any input\nas a permuted input, their ",
  "ture advantages of both CNN and transform-\ners but also retains the representation capability of local fea-\ntures and global representations to the maximum extent.\n3. Conformer\n3.1. Overview\nLocal features and global representations are important\ncounterparts, which have been extensively studied in the\nlong history of visual descriptors. Local features and their\ndescriptors [33, 26, 34], which are compact vector represen-\ntations of local image neighborhoods, have been the build-\ning blocks of m",
  "\nProdpoly-ResNet50\n98.78 (\u00d2 0.50)\n98.95 (\u00d2 0.31)\nIn Fig. 13, we compare the reconstruction error of the proposed\nmethod to the baseline spiral convolutions along with other popular\ngraph learnable operators, i.e., the Graph Attention Network\n(GAT) [90], FeastNet [91], Mixture model CNNs (MoNet) [92],\nConvolutional Mesh Autoencoders (COMA) [88] which are based\non the spectral graph \ufb01lters of ChebNet [93], as well as with\nPrincipal Component Analysis (PCA), which is quite popular\nin shape analysis",
  "cally this is either a learned embedding\nor tokens are given weights from two sine waves with high\nfrequencies, which is suf\ufb01cient for the model to learn that\nthere exists a positional relationship between these tokens.\nClassi\ufb01cation Token: Vision transformers typically add\nan extra learnable [class] token to the sequence of the\nembedded patches, representing the class parameter of an\nentire image and its state after transformer encoder can be\nused for classi\ufb01cation. [class] token contains the l",
  "magenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 1\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An Image is Worth 16x",
  "rmers enhanced by the proposed\nlocality mechanism outperform their baselines.\ntures, which is essential for the success of CNNs. Mean-\nwhile, the local, repetitive connections save many parame-\nters compared with fully connected layers. Yet, one prob-\nlem is that a larger receptive \ufb01eld can only be achieved by\ncombining layers, despite alternative attempts at enlarging\nthe receptive \ufb01eld [51].\nA parallel, thriving research strand incorporates global\nconnectivity into the network via self-attenti",
  "ith the state-of-the-art. We \ufb01rst report a simple ViT-B\n(our baseline) that uses ImageNet-21K pre-training. Our\nMViT-B with 16 frames has 64.7% top-1 accuracy, which is\nbetter than the SlowFast R101 [30] which shares the same\nsetting (K400 pre-training and 3\u00d71 view testing). With more\ninput frames, our MViT-B achieves 67.7% and the deeper\nMViT-B-24 achieves 68.7% using our K600 pre-trained\nmodel of above. In general, Table 6 veri\ufb01es the capability of\ntemporal modeling for MViT.\nmodel\npretrain\nmA",
  "to changes in image resolution at test time,\nand are therefore more amenable to process variable-size images.\n\u2022 For image classi\ufb01cation, we demonstrate that our models are on par with state-of-the-\nart vision transformers for multiple model sizes using a simple columnar architecture,\ni.e., in which we keep the resolution constant across layers. In particular, our XCiT-L24\nmodel achieves 86.0% top-1 accuracy on ImageNet, outperforming its CaiT-M24 [68] and\nNFNet-F2 [10] counterparts with comparab",
  "nique portion of the\ndataset. To accomplish this goal, we propose WGM to reweight losses of different experts.\nThe expert selection can be denoted as a probability matrix Pm\u00d7n \u2208Rm\u00d7n, whose element Pj,k \u2208\n[0, 1] represents the probability of assigning the j-th sample to the k-th expert. WGM takes Pm\u00d7n as\ninput and outputs a weight matrix Wm\u00d7n. WGM \ufb01rstly obtain an assignment matrix Am\u00d7n, which\nis a binary matrix with one-hot row vectors, then get Wm\u00d7n via smoothing and normalizing. For\nexample, A",
  "he median over 50 trials exceeds the\ntarget of 75.9%.\n2.1\nNesterov momentum at batch size 32k\nThis section describes how we used the standard Nesterov momentum optimizer to train the ResNet-\n50 v1.5 on ImageNet to 75.9% validation accuracy in 2,512 update steps at a batch size of 32,768,\nmatching the best published LARS result at this batch size. Although we implemented our own\ntraining program, the only logical changes we made to the published LARS pipeline were to the\noptimizer and the optimiz",
  " possible. This requires that, for any two\nblocks b(i)\nj\nand b(i)\nk in the same layer, we have\nw(i)\nj\n\u2286w(i)\nk\nor w(i)\nk\n\u2286w(i)\nj .\n(6)\nSuch within layer weight sharing makes the weight updates\nof w(i)\nj\nand w(i)\nk\nentangled with each other. The training of\nany block will affect the weights of others for their inter-\nsected portion, as demonstrated in Fig. 5. This is different\nfrom the classical weight sharing strategy in one-shot NAS,\nwhere the building blocks in the same layer are isolated. In\no",
  "d Momentum instead of RMSProp. Both simpli\ufb01cations reduce the total number of hyper-\nparameters as (1) cosine decay has no hyperparameters associated with it and (2) Momentum has one less than RMSProp.\nResNet (2015)\nResNet-RS (2021)\nEf\ufb01cientNets (2019)\nEpochs Trained\n90\n350\n350\nLR Decay Schedule\nStepwise\nCosine\nExponential Decay\nOptimizer\nMomentum\nMomentum\nRMSProp\nEMA of Weights\n\u2713\n\u2713\nLabel Smoothing\n\u2713\n\u2713\nStochastic Depth\n\u2713\n\u2713\nRandAugment\n\u2713\n\u2713\nDropout on FC\n\u2713\n\u2713\nSmaller Weight Decay\n\u2713\n\u2713\nSqueeze-Excita",
  "W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[18] X. Ding, X. Zhang, J. Han, and G. Ding. Repmlp: Re-parameterizing convolutions into fully-connected\nlayers for image recognition. arXiv preprint arXiv:2105.01883, 2021.\n[19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 w",
  "s.\nWe have considered different optimizers and\ncross-validated different learning rates and weight decays. Transformers are\nsensitive to the setting of optimization hyper-parameters. Therefore, during\ncross-validation, we tried 3 different learning rates (5.10\u22124, 3.10\u22124, 5.10\u22125) and\n3 weight decay (0.03, 0.04, 0.05). We scale the learning rate according to the\nbatch size with the formula: lrscaled =\nlr\n512 \u00d7 batchsize, similarly to Goyal et\nal. [19] except that we use 512 instead of 256 as the b",
  ". First note that if we\nchoose each stochastic component to be distributed standard\nGumbel independently, \u03f5j \u223ci.i.d. G(0, 1) \u2200j, then the pre-\ndictive probabilities pc have a closed form solution that is\nprecisely the popular softmax cross-entropy model used in\ntraining neural network classi\ufb01cation models [42, 9]:\npc = P(arg max\nj\u2208[K]\nuj(x) = c)\n=\nexp(\u00b5c)\nPK\nj=1 exp(\u00b5j)\n\u21d0\u21d2\u03f5j \u223ci.i.d. G(0, 1) \u2200j\n(2)\nIn other words, this generative process with Gumbel noise\ndistribution is already an implicit stand",
  "mall 0.75 and 1.0, we\nstill obtain 1.72% and 1.49% increase on ImageNet. Note\nthat for Ef\ufb01cientNet-B0, 77.3/93.5 accuracy is from their\npaper [47] and 76.8/93.2 is the actual accuracy from their\npre-trained models in timm.\nWith More Data Augmentation. We\u2019d like to further ex-\nplore whether our models have been saturated on the tar-\nget data by injecting more data augmentation like CutMix\nin training. The results are shown in Table 3, we involve\nCutMix and keep other settings the same as our basi",
  "s. Swin Transformer depends on\ntorch.roll() to perform cyclic shift and its reverse on features. This operation is memory unfriendly and\nrarely supported by popular inference frameworks such as NVIDIA TensorRT, Google Tensor\ufb02ow-\nLite, and Snapdragon Neural Processing Engine SDK (SNPE), etc. This hinders the deployment of\nSwin either on the server-side or on end devices in a production environment. In contrast, Twins\nmodels don\u2019t require such an operation and only involve matrix multiplications t",
  "namicViT.\n1\nIntroduction\nThese years have witnessed the great progress in computer vision brought by the evolution of CNN-\ntype architectures [12, 18]. Some recent works start to replace CNN by using transformer for many\nvision tasks, like object detection [36, 20] and classi\ufb01cation [25]. Just like what has been done to the\nCNN-type architectures in the past few years, it is also desirable to accelerate the transformer-like\nmodels to make them more suitable for real-time applications.\nOne common",
  "sults. While being better in ImageNet1K, our model is on\npar with DeiT models on all the downstream classi\ufb01cation\ntasks. This result assures that our models still have good\ngeneralization ability rather than only \ufb01t to ImageNet1K.\n4.3. Ablation Studies\nIn this section, we \ufb01rst compare the different fusion ap-\nproaches (Section 3.3), and then analyze the effects of dif-\nferent parameters of our architecture design, including the\npatch sizes, the channel width and depth of the small branch\nand num",
  "ognition (CVPR), 2018,\npp. 7132\u20137141. 7\n[73] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular\nmargin loss for deep face recognition,\u201d in Conference on Computer Vision\nand Pattern Recognition (CVPR), 2019, pp. 4690\u20134699. 8, 9, 10\n[74] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao, \u201cMs-celeb-1m: A dataset\nand benchmark for large-scale face recognition,\u201d in European Conference\non Computer Vision (ECCV).\nSpringer, 2016, pp. 87\u2013102. 8\n[75] J. Deng, J. Guo, D. Zhang, Y. Deng, X. Lu, ",
  "However, both have\nsophisticated designs (with non-trivial implementation) and add extra complexity.\nDifferent from these methods, every block in NesT processes information independently via standard\ntransformer layers, and only communicate and mix global information during block aggregation\nstep via some simple spatial operations (e.g. convolution and pooling). One key ingredient of block\naggregation is to perform it in image plane so that information can be exchanged between nearby\nblocks. The",
  " propose the random-shifting training strategy for Vi-\nsion Longformer, to further accelerate the training speed of\nVision Longformer. More speci\ufb01cally, instead of attending\nto all 8 neighbor patches, one patch can only attend to itself\nand one random neighbor patch during training. To achieve\nFigure 9. Illustration of the 8 modes in the random-shifting train-\ning strategy. For mode i (1 <= i <= 8), the query chunk (dark\nbrown) attends to itself and the i\u2019th neighbor chunk.\nFigure 10. The random",
  "ields an improvement of 1.9\nBLEU scores over the Transformer model with vanilla\nDropout.\nAlthough the search cost of AutoDropout can be high, a\nsimple use case of AutoDropout is to drop our found patterns\ninto existing pipelines in the same way that AutoAugment\npolicies (Cubuk et al. 2019a) were used to improve state-of-\nthe-art models.\nRelated works.\nOur work has the same philosophy with\nexisting neural architecture search and AutoAugment lines of\nresearch (Pham et al. 2018; Liu, Simonyan, and ",
  "l complexity to image scale. (c) Our proposed Cycle\nFully-Connected Layer (Cycle FC) has linear complexity\nthe same as channel FC and a larger receptive \ufb01eld than\nChannel FC.\nThe MLP-like models [49, 50, 35] are much simpler\nthan Transformer-based models in: They replace the self-\nattention sub-layers by the purely fully-connected lay-\ners1 (equal to the 1 \u00d7 1 convolution), removing the at-\n1MLP and FC are are used interchangeably.\n1\narXiv:2107.10224v1  [cs.CV]  21 Jul 2021\n\n--------------------",
  "on each input\nimage patches at training time. Our motivation comes from the difference of ViTs and CNNs that\nViTs not only extract intra-patch features but also concern the inter-patch relations. We think the\ntraditional augmentation which randomly transforms the whole image could provide enough intra-\npatch augmentation. However, it lacks the diversity on inter-patch augmentation, as all of patches\nhave the same transformation at one time. To impose more inter-patch diversity, we retain the ori",
  "bly effective at numerous downstream tasks such as object\ndetection [37], instance segmentation [22] and image captioning [1]. Similarly, in natural language\nprocessing, Transformers rule the roost [13, 43, 42, 4]. Their effectiveness at capturing short and long\nrange information have led to state-of-the-art results across tasks such as question answering [45]\nand language understanding [58].\nIn computer vision, Transformers were initially employed as long range information aggregators\nacross sp",
  "y\ncheck marks (\u2713) in the Required additional modules columns such as KD, AT, PKT,\nRKD, HND, SPKD, Tf-KD, GHND and L2 can be reimplemented simply by adding\nthe new loss modules to the registry in the framework (Section 2.2).\nDifferent from the existing frameworks [52,29,12,28,42,49], all the methods in Ta-\nble 4 are reimplemented independently from models in torchvision so that users can\neasily switch models by specifying a model name and its parameters in a con\ufb01guration\n\ufb01le. Taking image classi\ufb01",
  "nts of the inputs, and\nmodify the inputs by propagating information between\nthe input pairs. As a result, it maps N inputs to N outputs\nand casts a quadratic O(N 2) memory and time complex-\nity. We propose centroid attention, a generalization of\nself-attention that maps N inputs to M outputs (M \u2264N),\nsuch that the key information in the inputs is summarized\nin the smaller number of outputs (called centroids). We\ndesign centroid attention by amortizing the gradient de-\nscent update rule of a clust",
  "\nwith training improvements makes accurate comparisons\nbetween architectures dif\ufb01cult. The large improvements\ncoming from training strategies, when not being controlled\nfor, can overshadow architectural differences.\nHow should one compare different architectures?\nSince training methods and scale typically improve perfor-\nmance (Lee et al., 2020; Kaplan et al., 2020), it is critical\nto control for both aspects when comparing different ar-\nchitectures. Controlling for scale can be achieved through",
  "ongside model sparsity in\norder to scale models up. In order to scale the base architecture to which we add sparse mixture of\nexpert layers, we make the following changes based on [68]:\n\u2022 Low precision: We use bfloat16 instead of float32 to store the gradient moving average.\n\u2022 Learning-rate decay: We replace the linear schedule with an inverse square root schedule\n(rsqrt).\n\u2022 Weight decay: We apply weight decay to the kernel weights in the model with value 0.03\n(while biases are not regularized),",
  "9]\n219M\n75.2\n-\nMnasNet-A3 [63]\n403M\n76.7\n93.3\nEf\ufb01cientNet-B0 [64]\n399M\n76.3\n93.2\nDNA-b [37]\n406M\n77.5\n93.3\nBossNet-M2\n403M\n77.4\n93.6\nTable 2: ImageNet results of state-of-the-art NAS models\non MBConv search space.\nMethod\nSearch Cost\n\u03c4\n\u03c1\nR\nSPOS [22]\n8.5 Gds\n-0.18\n-0.27\n-0.29\nDARTS [43]\n50 Gds\n0.08\n0.14\n0.06\nMnasNet [63]\n288 Tds\n0.61\n0.77\n0.78\nDNA [37] (EffNetB0)\n8.5 Gds\n0.62\n0.77\n0.83\nDNA [37] (MBNetV1)\n8.5 Gds\n0.23\n0.27\n0.37\nBossNAS\n10 Gds\n0.65\n0.78\n0.85\nTable 3: Comparison of the effectiveness ",
  "ablation studies for pre-training and analyze the representations learned by BEIT.\n5\n\n--------------------------------------------------\nModels\nCIFAR-100\nImageNet\nTraining from scratch (i.e., random initialization)\nViT384 (Dosovitskiy et al., 2020)\n48.5*\n77.9\nDeiT (Touvron et al., 2020)\nn/a\n81.8\nSupervised Pre-Training on ImageNet-1K (using labeled data)\nViT384 (Dosovitskiy et al., 2020)\n87.1\n77.9\nDeiT (Touvron et al., 2020)\n90.8\n81.8\nSelf-Supervised Pre-Training on ImageNet-1K (without labeled ",
  "ary\n74.50\n326M\nuniform-E\nevolutionary\n74.17\n320M\ngreedy\nevolutionary\n74.85\n320M\nFrom Table 7, we can see that a greedy supernet improves consistently the classi\ufb01cation accuracy in terms of different\nsearchers. This validates the superiority of our greedy supernet since it helps searchers to probe better architectures. More-\nover, to comprehensively investigate the effect of supernets, we implement systematic sampling 4 to sample 30 paths from\n50\u00d720 = 1000 paths, which are discovered by the evolu",
  "et, CIFAR-10, CIFAR-\n100, Cars, and Flowers dataset. On ImageNet, we achieve\n85.7% top-1 accuracy while training 3x - 9x faster and being\nup to 6.8x smaller than previous models (Figure 1). Our Ef-\n\ufb01cientNetV2 and progressive learning also make it easier to\ntrain models on larger datasets. For example, ImageNet21k\n(Russakovsky et al., 2015) is about 10x larger than ImageNet\nILSVRC2012, but our Ef\ufb01cientNetV2 can \ufb01nish the training\nwithin two days using moderate computing resources of 32\nTPUv3 cor",
  "okens of different sizes (Ps and Pl, Ps < Pl) and\nfuse the tokens at the end by an ef\ufb01cient module based on\ncross attention of the CLS tokens. Our design includes dif-\nferent numbers of regular transformer encoders in the two\nbranches (i.e. N and M) to balance computational costs.\n3.1. Overview of Vision Transformer\nVision Transformer (ViT) [11] \ufb01rst converts an image\ninto a sequence of patch tokens by dividing it with a cer-\ntain patch size and then linearly projecting each patch into\ntokens. A",
  "eports the TPUv3 compute time for a batch size of 32 per core. The\ninference speed is also computed on a single TPUv3 core.\ntrain with Nesterov\u2019s Accelerated Gradient [39, 55] during\npretraining and \ufb01netuning. We pretrain on 256 \u00d7 256 size\nimages and \ufb01netune on different image sizes, as shown in\nTable 6. Our wider H4 and hybrid-H4 models achieves bet-\nter accuracy than the Vision Transformer and a 4\u00d7 wide\nResNet-152 from [29] and are also faster at inference on\nlarger images. We \ufb01netune for 8 ep",
  "LBERT (Lan et al. 2019)). Model compres-\nsion (Xu et al. 2020; Sun et al. 2019) can also make trans-\nformer more parameter ef\ufb01cient.\nThe two existing methods both have their own limitations.\nFor huge models, one typical and effective method to scale\ntrainable parameters is replacing part of the feed-forward\nnetwork (FFN) layer in transformer blocks with MoE lay-\ners. In each MoE layer, to re\ufb01ne one single token represen-\ntation, only a few experts are activated, so the MoE based\ntransformer hold",
  ", Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic\ndepth. In European conference on computer vision, pages 646\u2013661. Springer, 2016.\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining",
  " batch\nsize of 128 on each GPU card. We use 8 NVIDIA RTX\nA6000 GPUs to train all models for 300 epochs and the to-\ntal batch size is 1024. All other training settings and hyper-\nparameters are adopted from Deit [35] for fair comparisons.\nFor those results in ablation study, we train these models\nfor 100 epochs with batch size 256 on each GPU and use 4\nGPUs with learning rate at 0.001.\n4.2. Ablation Study\nOur baseline model Pure-MLP Baseline is composed of\none patch converter and a sequence of ch",
  "-M (both give similar\nGPU throughput). Comparison results appear in Table 10.\nMethod\nBabkbone\nmAP %\nFCOS\nResNet50\n42.8\nFCOS\nTResNet-M\n44.0\nTable 10. Comparison of TResNet-M to ResNet50 on MS-\nCOCO object detection task. Results were obtained using mm-\ndetection package, with FCOS as the object detection method .\nWe can see from Table 10 that TResNet-M outperform\nResNet50 on this object-detection task, increasing COCO\nmAP score from 42.8 to 44.0. This is consistent with the\nimprovement we saw in ",
  "5), but they are not stable at large batch sizes and do not\nmatch the performance of Ef\ufb01cientNets (Tan & Le, 2019),\nthe current state of the art (Gong et al., 2020). This paper\nbuilds on this line of work and seeks to address these central\nlimitations. Our main contributions are as follows:\n\u2022 We propose Adaptive Gradient Clipping (AGC), which\nclips gradients based on the unit-wise ratio of gradient\nnorms to parameter norms, and we demonstrate that\nAGC allows us to train Normalizer-Free Networks ",
  " layers.\nIn Proceedings of International Conference on Learning\nRepresentations (ICLR), 2020.\nCorreia, G. M., Niculae, V., and Martins, A. F. Adaptively\nsparse Transformers. In Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), 2019.\nCubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Ran-\ndaugment: Practical automated data augmentation with a\nreduced search space. In Proceedings of IEEE Conference\non Computer Vision and Pattern Recognition Workshops\n(CVPRW), 2020.\nDai, Z., ",
  "d\u2019Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697,\n2021.\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.\nIeee, 2009.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. ",
  "retraining, etc.\n3. Analysis: Problems in Existing Classi\ufb01ca-\ntion System\nConsider a classi\ufb01cation task that distinguishes various\nbreeds of dogs, e.g. toy poodle, miniature poodle, etc., the\noutput predictive distribution of a higher capability teacher\nalways provides the student model with the extra informa-\ntion of how alike one breed of dogs looks to the other. This\nhelps the student learn more generalized features of each\n2\n\n--------------------------------------------------\n4%\n0%\n8%\n12%\nSe",
  "local self-attention because it improves accuracy without\nsacri\ufb01cing performance.\nAnother difference with SASA is our implementation\nof downsampling. We replace attention followed by post-\nattention strided average pooling by a single strided attention\nlayer that subsamples queries similar to strided convolutions,\nas shown in Figure 2. Note that we use the same neighbor-\nhood as is extracted in the stride 1 case (Figure 1). This\nchange does not impact accuracy while also reducing the\nFLOPs 4\u00d7 in",
  "nsformer (ViT) in performance and ef\ufb01ciency by intro-\nducing convolutions into ViT to yield the best of both de-\nsigns. This is accomplished through two primary modi\ufb01ca-\ntions: a hierarchy of Transformers containing a new convo-\nlutional token embedding, and a convolutional Transformer\nblock leveraging a convolutional projection. These changes\nintroduce desirable properties of convolutional neural net-\nworks (CNNs) to the ViT architecture (i.e. shift, scale,\nand distortion invariance) while main",
  "ottleneck\nlayers have higher GPU usage than BasicBlock layers, but\nusually give better accuracy. However, BasicBlock layers\nhave larger receptive \ufb01eld, so they might be more suited to\nbe placed at the early stages of a network.\nWe found that the uniform block selection of ResNet\nmodels is not optimal, and a better speed-accuracy trade-off\ncan be obtained using a novel design, which uses a mixture\nof BasicBlock and Bottleneck layers. Since BasicBlock lay-\ners have a larger receptive \ufb01eld, we plac",
  " exclusive grouping.\nquest for explanations an ill-posed endeavor. The \ufb01eld of\nadversarial perturbations is a good example of this seman-\ntic gap for explanability [24]. Despite having input sam-\nples that preserve all the perceptually relevant information,\nadversarially perturbed samples can be misclassi\ufb01ed with\nhigh probability. The effectiveness of adversarial attacks\nprovides a strong body of evidence that patterns extracted\nby neural networks are effective but fundamentally differ-\nent from",
  "er. Speci\ufb01cally, the PAWS-NN\nresults in Table 1 are reported by directly applying a soft nearest neighbours classi\ufb01er to the pre-trained representations. To\ndetermine a class prediction for an image x, we compare its representation, z = f\u03b8(x), to the representations of the available\nlabeled training samples, zS \u2208RM\u00d7d, and subsequently choose the class label with the highest probability under the similarity\nclassi\ufb01er; i.e., argmaxk\u2208[1000] [\u03c0d (z, zS)]k. All results are reported using a single cen",
  "s the training loss function, while x\nand y denote the input data and the labels, respectively.\nSubsequently, architectures \u03b1 are searched based on the\nranking of their ratings with these shared network weights.\nWithout loss of generality, we choose the evaluation loss\nfunction Lval as the rating metric; the searching phase can\nbe formulated as: \u03b1\u2217= arg min\n\u2200\u03b1\u2208A\nLval(W\u2217, \u03b1; x, y). However,\nthe architecture ranking based on the shared weights W\u2217\ndoes not necessarily represents the correct ranking",
  "and original models depends\non the experiment but in essence, a mixture of convolutional\nand pooling layers are used. A more detailed speci\ufb01cation\nof all networks used in this work can be found in Section B\nof the supplementary material.\nUnder this last perspective, an important meta-parameter\nof the architecture is the point at which the auxiliary clas-\nsi\ufb01er attaches to the original model. Having a junction in\nearlier layers allows both branches to work with generic,\nlower-level features but l",
  " Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-\nheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,\nJifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\nChen Change Loy, and Dahua Lin.\nMMDetection: Open\nmmlab detection toolbox and benchmark.\narXiv preprint\narXiv:1906.07155, 2019. 7\n[7] MMSegmentation\nContributors.\nMMSegmentation:\nOpenmmlab\nsemantic\nsegmentation\ntoolbox\nand\nbenchmark.\nh",
  "magenet-val1k.\n4.5.4\nLonger training schedules\nAs shown in Table 8 , increasing the number of training epochs from 300 to\n400 improves the performance of CaiT-S-36. However, increasing the number\nof training epochs from 400 to 500 does not change performance signi\ufb01cantly\n(83.44 with 400 epochs 83.42 with 500 epochs). This is consistent with the ob-\nservation of the DeiT [63] paper, which notes a saturation of performance from\n400 epochs for the models trained without distillation.\n5\nVisualizatio",
  " than that of other compar-\nison methods under the same FLOPs or latency level, but\nreduces approximate 40% of supernet training cost.\nBy\nsearching on a larger space, we can also obtain new state-\nof-the-art architectures.\n1For example, in a same supernet, MobileNetV2 [28] can achieve\n72.0% Top-1 accuracy on ImageNet dataset while an extreme case of al-\nmost all identity operations only has 24.1% [3].\n2. Related Work\nOne-shot NAS methods mainly aim to train an over-\nparameterized network (a.k.a ",
  "at\nDraw a mini-batch M from Dn, and update \u03b8 via\n\u03b8 \u2190\u03b8 \u2212\u03b7Ex\u223cM\n\u0014\n\u2207\u03b8\n\u0012\nmax\ni\u2208[m] L(x\u2032\ni, \u03b8)\n\u0013\u0015\n,\nwhere {x\u2032\ni}m\ni=1 are drawn i.i.d. from P(\u00b7|x) for each x in the mini batch M. See Equation 3.\nuntil convergence\n3. Related Methods and Discussion\nMaxUp is closely related to both data augmentation and ad-\nversarial training. It can be viewed as an adversarial vari-\nant of data augmentation, in that it minimizes the worse\ncase loss on the perturbed data, instead of an average loss\nlike typical data augm",
  "competitive image classi\ufb01cation perfor-\nmance at large scale. The ViT design adapts Transformer\n*This work is done when Haiping Wu was an intern at Microsoft.\n\u2020Corresponding author\nCvT\nViT\nBiT\n(a)\n78\n80\n82\n84\n86\n88\nImageNet top-1 accuracy (%)\n20M\n32M\n277M\n86M\n307M\n25M\n928M\nCvT\nViT\nBiT\n20\n40\n60\n80\nModel Paramters (M)\n(b)\n80.0\n80.5\n81.0\n81.5\n82.0\n82.5\nImageNet top-1 accuracy (%)\nCvT\nDeiT\nT2T\nPVT\nTNT\nFigure 1: Top-1 Accuracy on ImageNet validation com-\npared to other methods with respect to model p",
  "r forward propagation. Backward passes proceed in reverse\norder through the same graph. This approach is the core of popular deep learning frameworks [1]\nand is associated with the very concept of \u201carchitecture\u201d. In contrast, implicit models do not have\nprescribed computation graphs. They instead posit a speci\ufb01c criterion that the model must satisfy\n(e.g., the endpoint of an ODE \ufb02ow, or the root of an equation). Importantly, the algorithm that drives\nthe model to ful\ufb01ll this criterion is not pre",
  "hich this parameterization has a\nhigher \ufb01tting error e\ufb01t tend to perform poorly. See text for details.\n3.3. The RegNet Design Space\nTo gain further insight into the model structure, we show\nthe best 20 models from AnyNetXE in a single plot, see Fig-\nure 8 (top-left). For each model, we plot the per-block width\nwj of every block j up to the network depth d (we use i and\nj to index over stages and blocks, respectively). See Fig-\nure 6 for reference of our model visualization.\nWhile there is signi\ufb01",
  "h. The RandAug-\nment magnitude is set to 10 for image resolution 224 or\nsmaller, 20 for image resolution larger than 320 and 15 oth-\nerwise. All other hyperparameters are kept the same as per\nthe original Ef\ufb01cientNets. Figure 5 demonstrates a marked\nimprovement of the re-scaled Ef\ufb01cientNets (Ef\ufb01cientNet-\nRS) on the speed-accuracy Pareto curve over the original\nEf\ufb01cientNets.\n3Activations are typically stored during training as they are\nused in backpropagation. At inference, activations can be dis",
  "e of the\nconvolutional prior.\n\n--------------------------------------------------\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n0\n100\n200\n300\nEpochs\n3\n4\n5\n6\n7\nNon-locality\nDeiT\n0\n100\n200\n300\nEpochs\n3\n4\n5\n6\n7\nConViT\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\nLayer 9\nLayer 10\nLayer 11\nLayer 12\n(a) DeiT-Ti and ConViT-Ti\n0\n100\n200\n300\nEpochs\n3\n4\n5\n6\n7\nNon-locality\nDeiT\n0\n100\n200\n300\nEpochs\n3\n4\n5\n6\n7\nConViT\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\n",
  "(3) In detail, the token extraction and the bi-directional pathway\ndesigned in ViP are quite different from their pipelines.\n4\nExperiments\n4.1\nImage Classi\ufb01cation on ImageNet-1K\nExperimental Settings. For image classi\ufb01cation, we evaluate our models on ImageNet-1K [16],\nwhich consists of 1.28M training images and 50K validation images categorized into 1,000 classes.\nThe network is trained for 300 epochs using AdamW [39] and a half-cosine annealing learning rate\n6\n\n--------------------------------",
  "ation bound can be written as follows:\nE\u03f5i\u223cN(0,\u03c3)[LD(w+\u03f5)] \u2264E\u03f5i\u223cN(0,\u03c3)[LS(w+\u03f5)]+\nv\nu\nu\nt\n1\n4k log\n\u0010\n1 + \u2225w\u22252\n2\nk\u03c32\n\u0011\n+ 1\n4 + log n\n\u03b4 + 2 log (6n + 3k)\nn \u22121\n(13)\nIn the above bound, we have \u03f5i \u223cN(0, \u03c3). Therefore, \u2225\u03f5\u22252\n2 has chi-square distribution and by\nLemma 1 in Laurent & Massart (2000), we have that for any positive t:\nP(\u2225\u03f5\u22252\n2 \u2212k\u03c32 \u22652\u03c32\u221a\nkt + 2t\u03c32) \u2264exp(\u2212t)\n(14)\nTherefore, with probability 1 \u22121/\u221an we have that:\n\u2225\u03f5\u22252\n2 \u2264\u03c32(2 ln(\u221an) + k + 2\nq\nk ln(\u221an)) \u2264\u03c32k\n \n1 +\nr\nln(n)\nk\n!2\n\u2264\u03c12\n15\n\n--------",
  ", with input size 224. Tiny-4stage / 4 means that the model has\ncomparable size with DeiT-Tiny, has 4 stages and uses patch size 4x4 in the initial pixel space. 1-2-8-1 means that the model contains 4\nstages, each stage has 1/2/8/1 MSA-FFN blocks, respectively. *Partial* means that the last two stages, which contain most of the attention\nblocks, still use full attention. Vision Longformer does not have *Partial* version because its window size is set as 15 (comparable with\nthe ViT(DeiT)/16 featu",
  "nLearner with Video Vision Transformer\n5.1\nNetwork architecture implementation\nViViT [2] is a direct extension of ViT for videos, which uses spatio-temporal patches from videos\nas its tokens. The size of the space-time patches are typically 16x16x2, which are given to the\nTransformer layers similar to ViT. ViViT and ViT share the architecture. For our experiments, we\ninsert the TokenLearner module within the ViViT architecture, identically to how we inserted it\nwithin ViT in Figure 3. ViViT has ",
  "where h = \u230ak\n2\u230b\n4(b + 2h)2c\nBlocked local (ours)\nHW\nb2 (b + 2h)2c\n(b + 2h) \u00d7 (b + 2h)\n4(b + 2h)2c\nTable 1. Scaling behavior of self-attention mechanisms. f is the\nnumber of heads, b is the size of the block, c is the total number of\nchannels, and h is the size of the halo\na factor of k2 due to replicating the pixel contents for each of\nthe k2 neighborhoods it participates in. This solution quickly\nleads to out-of-memory errors. Global attention (Row 4) is\nat the other end of the spectrum, where ",
  "led dot-product attention is a key component in Multi-Head Self\nAttention layer (MHSA) of transformer. MHSA \ufb01rst generates set of query Q \u2208RN\u00d7d, key\nK \u2208RN\u00d7d, value V \u2208RN\u00d7d with the corresponding projection. Then the query vector q \u2208Rd is\nmatched against the each key vector in K. The output is the weighted sum of a set of N value vectors\nv based on the matching score. This process is called scaled dot-product attention:\nAttention(Q, K, V ) = Softmax(QKT /\n\u221a\nd)V\n(1)\nFor preventing extremly small g",
  "ut. Vis., 60(2):91\u2013110, 2004.\n3\n[34] Timo Ojala, Matti Pietik\u00a8ainen, and Topi M\u00a8aenp\u00a8a\u00a8a. Multires-\nolution gray-scale and rotation invariant texture classi\ufb01cation\nwith local binary patterns. IEEE Trans. Pattern Anal. Mach.\nIntell., 24(7):971\u2013987, 2002. 3\n[35] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll\u00b4ar. Designing network design\nspaces. arXiv preprint arXiv:2003.13678, 2020. 6, 11\n[36] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networ",
  "w the im-\nage feature is obtained for the linear classi\ufb01cation head.\nModel\nTiny\nSmall\nCLS\nAve Pool\nCLS\nAve Pool\nViL-1,2,8,1-APE\n75.72\n75.98\n81.65\n81.99\nViL-1,1,8,2-APE\n76.18\n76.25\n82.12\n82.08\nTable 12. For ViL models that has only one attention block in the\nlast stage (ViL 1-2-8-1), the average pooled feature from all tokens\nworks better than the feature of the CLS token. When there are\nmore than 2 attention blocks in the last stage (ViL 1-1-8-2), the\ndifference between these two features disapp",
  " input size): a fully-ISAB model scales as\nO(LMN), rather than O(MN + LN 2), like the Perceiver\n(where M is the index dimension of the input, N the index\ndimension of the latent, and L the network depth).\nPMA is used to map an input array to an output array with a\nsized determined by the task (e.g. 1 point for classi\ufb01cation\nor 4 points for a 4-way clustering task). It is used to map\nto a target output size and not to induce a latent space. In\ncontrast, the Perceiver\u2019s latent space has a size tha",
  "cture on object detection and the exact numbers of the results.\nAs our proposed PyConv uses different levels of kernel sizes in parallel, it can provide signi\ufb01cant\nbene\ufb01ts for object detection task, where the objects can appear in the image at different scales. For\nobject detection, we integrate our PyConv in a powerful approach, Single Shot Detector (SSD) [48].\nSSD is a very ef\ufb01cient single stage framework for object detection, which performs the detection at\nmultiple feature maps resolutions. ",
  "AS, when equipped with the stopping principle of candidate pool, the supernet training is stopped at\napproximately 46-th epoch. Thus the accumulated number of examples calculated for a whole optimization step is equal to\n#optimization=1.23M \u00d7 46,\nwhere 1.23M refers to the quantity of training dataset. As for the path \ufb01ltering, we evaluate 10 paths based on 1000 validation\nimages, and select 5 paths for training, whose batch size is 1024. In this way, the number of images for evaluation amounts\nt",
  "ow, the correct design of block aggregation is critical.\nNesT improves consistently given bigger model size. Most variants of NesT in Figure 6 outperform\ncompared methods with far better throughput. For example, NesT3-T (S = 2) leads to 94.5%\nCIFAR10 accuracy with 5384 images/s throughout, 10\u00d7 faster than Swin-B with 94.6% accuracy.\nImageNet We test NesT on standard ImageNet 2012 benchmarks [16] with commonly used 300\nepoch training on TPU in Table 2. The input size is 224 \u00d7 224 and no extra pre",
  "re as in ResNet. We also investigate the ben-\ne\ufb01ts of PiT compared to ViT and con\ufb01rm that ResNet-style\ndimension setting also improves the performance of ViT.\nFinally, to analyze the effect of PiT compared to ViT, we\narXiv:2103.16302v2  [cs.CV]  18 Aug 2021\n\n--------------------------------------------------\n(a) ResNet-50\n56\u00d756\u00d7256\n224\u00d7224\n\u00d73\n28\u00d728\u00d7512\n14\u00d714\u00d71024\n7\u00d77\u00d72048\n(b) ViT-S/16\n(14\u00d714)\u00d7384\nSpatial tokens\n1\u00d7384\nClass token\n224\u00d7224\n\u00d73\n(14\u00d714)\u00d7288\nSpatial tokens\n1\u00d7144\nClass token\n224\u00d7224\n\u00d73\n",
  " Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting the design of spatial attention in vision\ntransformers. arXiv preprint arXiv:2104.13840, 2021. 1, 2,\n3, 4\n[5] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-\naolin Wei, Huaxia Xia, and Chunhua Shen. Conditional po-\nsitional encodings for vision transformers.\narXiv preprint\narXiv:2102.10882, 2021. 2\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei.\nImagenet: A large-scale hierarchical im-\nage database. In Proc. IEEE C",
  "he\nframework (e.g., by forking the repository) to conduct their knowledge distillation stud-\nies, and their studies can be integrated to the framework by sending a pull request.\nThis will help the research community ensure the reproducibility of the work, and ad-\nvance the deep learning research while supporting fair method comparison on bench-\nmarks. Speci\ufb01cally, researchers can publish the log, con\ufb01guration, and pretrained model\nweights for their champion performance, that will help them ensur",
  "normalized projection net-\nwork representations provide an intrinsic mechanism for hard positive/negative mining during train-\ning. For losses such as the triplet loss or max-margin, hard mining is known to be crucial to their\nperformance. For contrastive loss, we show analytically that hard mining is intrinsic and thus re-\nmoves the need for complicated hard mining algorithms.\nAs shown in Sec. 6, the gradients of both Lsup\nout and Lsup\nin\nare given by Eq. 12. Additionally, note\nthat the self-su",
  "alization [68] and im-\nage scale augmentation (short size of a image is picked ran-\ndomly from 640 to 800). 1x learning rate schedule is used.\nWe conduct Faster-RCNNs and Cascade-RCNNs experi-\nments using Detectron2 [60]. For comparison, we simply\nreplaced the vanilla ResNet backbones with our ResNeSt,\nwhile using the default settings for the hyper-parameters\nand detection heads [20,60].\nCompared to the baselines using standard ResNet, Our\nbackbone is able to boost mean average precision by arou",
  "lasses\nCIFAR-10 (Krizhevsky & Hinton, 2009)\n50,000\n10,000\n10\nCIFAR-100 (Krizhevsky & Hinton, 2009)\n50,000\n10,000\n100\nFlowers (Nilsback & Zisserman, 2008)\n2,040\n6,149\n102\nCars (Krause et al., 2013)\n8,144\n8,041\n196\nFor this experiment, we use the checkpoints trained on Ima-\ngeNet ILSVRC2012. For fair comparison, no ImageNet21k\nimages are used here. Our \ufb01netuning settings are mostly the\nsame as ImageNet training with a few modi\ufb01cations similar\nto (Dosovitskiy et al., 2021; Touvron et al., 2021): We",
  "odels obtaining an accuracy of 89.2%.2\n1. Introduction\nThe vast majority of recent models in computer vision are\nvariants of deep residual networks (He et al., 2016b;a),\ntrained with batch normalization (Ioffe & Szegedy, 2015).\nThe combination of these two architectural innovations has\nenabled practitioners to train signi\ufb01cantly deeper networks\nwhich can achieve higher accuracies on both the training\nset and the test set. Batch normalization also smoothens the\nloss landscape (Santurkar et al., 2",
  "y. Journal of Machine Learning Research,\n2019.\nGupta, S. and Akin, B. Accelerator-aware neural network\ndesign using automl. On-device Intelligence Workshop in\nSysML, 2020.\nGupta, S. and Tan, M.\nEf\ufb01cientnet-edgetpu:\nCre-\nating accelerator-optimized neural networks with au-\ntoml.\nhttps://ai.googleblog.com/2019/08/ef\ufb01cientnet-\nedgetpu-creating.html, 2019.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition.\nCVPR, pp. 770\u2013778,\n2016.\nHoffer, E., Weinstein, B., Hubara,",
  "ve the \ufb01xed-size position encoding [7], and introduce\nzero padding position encoding into PVT. As shown in Fig-\nure 1(b), we add a 3 \u00d7 3 depth-wise convolution [15] with\nthe padding size of 1 between the \ufb01rst fully-connected (FC)\nlayer and GELU [14] in feed-forward networks.\nLinear Spatial Reduction Attention. To further reduce\nthe computation cost of PVT, we propose linear spatial re-\nduction attention (SRA) as illustrated in Figure 2. Different\nfrom SRA [31], linear SRA enjoys linear computati",
  "\narXiv:2102.05918v2  [cs.CV]  11 Jun 2021\n\n--------------------------------------------------\nScaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision\nText \nEncoder\nImage \nEncoder\nNoisy Image-Text \nData\nContrastive Learning\n(Zero-shot) Visual Tasks\n   Fine-grained Image-Text Retrieval\nPre-training\n\u201cRoppongi Hills Spider at night\u201d\n\u201coriginal picture of \nmonet haystack\u201d\n\u201cmonet haystack png\u201d\n\u201chaystack series \nmonet art institute of \nchicago\u201d\n...\n\u201csnow\u201d\n(A) Text -> Im",
  "al learning rate from 0.1 to 0.01.\nThe second option achieved better overall results and that is the one we report. We also tried training with LSQ from a randomly\ninitialized model, but that performed the worst of all approaches.\n2https://sigsep.github.io/datasets/musdb.html\n3https://github.com/sigsep/website/blob/master/content/datasets/assets/tracklist.csv\n4https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/\n5https://github.com/pytorch/fairseq/tree/master/exa",
  "spondingly, the tokens associated\nwith image and feature patches are referred to as lo-\ncal tokens afterwards.\n3. We replace the vanilla full self-attention with an ef\ufb01-\ncient attention mechanism, denoted by a, which will\nbe described in detail in Sections 3.2 and 3.3.\n4. We use either an Absolute 2-D Positional Embedding\n(APE for short, separately encoding x and y coordi-\nnates and concatenating them) or a Relative Positional\nBias (RPB for short) to replace the original absolute\n1-D positional ",
  "interactions between the pruned tokens and other\ntokens. We also \ufb01nd that merely zero-out the tokens to be dropped using the binary mask \u02c6D is not\nfeasible, because in the calculation of self-attention matrix [26]\nA = Softmax\n\u0012QKT\n\u221a\nC\n\u0013\n(8)\nthe zeroed tokens will still in\ufb02uence other tokens through the Softmax operation. To this end, we\ndevise a strategy called attention masking which can totally eliminate the effects of the dropped\ntokens. Speci\ufb01cally, we compute the attention matrix by:\nP = QK",
  " that none of the models\nconverge with shorter training schedules.\nMultilabel datasets.\nWe can also apply our method to\nmultilabel datasets which may have more than one class in\neach image. The same latent variable formulation can be\nused but with temperature parameterized sigmoid smoothing\nfunction (see Appendix A). Imagenet-21k and JFT are two\nlarge-scale multilabel image classi\ufb01cation datasets.\nImagenet-21k is a larger version of the standard ILSVRC-\n2012 Imagenet benchmark [10, 26, 2, 9]. It",
  "eir detailed con\ufb01gurations are shown in Tab. 8. We\ntrain all models with 8 NVIDIA Telsa-V100 GPUs.\nTable 8: Default training hyper-parameters for our experiments.\nH-param.\nStandard\nAdvanced\nEpoch\n300\n300\nBatch size\n256\n512\nLR\n5e-3 \u00b7 batch_size\n256\n5e-3\u00b7 batch_size\n512\nLR decay\ncosine\ncosine\nWeight decay\n0.05\n0.05\nWarmup epochs\n5\n5\nDropout\n0\n0\nStoch. Depth\n0.1\n0.1 \u00b7 #Blocks\n12\nMixUp\n0.2\n0.8\nCutMix\n0\n1.0\nErasing prob.\n0.25\n0.25\nRandAug\n9/0.5\n9/0.5\nA.2\nFine-tuning with larger image resolutions\nOn I",
  "ear activation functions could be used. In the\nfollowing experiments, we use the combination of h-swish\nand SE as the non-linear activation function after depth-\nwise convolution. Additionally, the reduction ratio of the\nsqueeze-and-excitation module is chosen such that only 4\nchannels are kept after the squeeze operation. This choice\nof design achieves a good balance between the number of\nparameters and the model accuracy. Thus, local informa-\ntion is also important in vision transformers. A wi",
  "Drop\nCutout\n1.6\n1.9\n11.3\n11.8\nPyramidNet+ShakeDrop\nAA\n1.4\n1.6\n10.3\n10.6\nTable 9:\nResults for the CIFAR-10/CIFAR-100 experiments, using \u03c1 = 0.05 for all mod-\nels/datasets/augmentations\nboth types of updates becomes weaker. Fortunately, the model trained without the second order\nterms reaches a lower test error, showing that the most ef\ufb01cient method is also the one providing the\nbest generalization on this example. The reason for this is quite unclear and should be analyzed in\nfollow up work.\nC.5\n",
  " To verify whether the initializa-\ntion of a student has a big impact, we conduct the ablation\nstudy through adopting (1) randomly initialized weights, (2)\npytorch moderate weights and (3) timm superior weights.\nResults are illustrated in Fig. 6, we trained with additional\n120 epochs with lr = 0.1 if the parameters are randomly\ninitialized.\nIntriguingly, the convergence with randomly\ninitialized weights is not as good as using pre-trained pa-\nrameters, especially the second and third stages when",
  "odel [25] using the visualization method proposed in [3]. These results demonstrate the \ufb01nal\nprediction in vision transformers is only based on a subset of most informative tokens, which suggests\na large proportion of tokens can be removed without hurting the performance.\ndownsampling strategy for vision transformers to further leverage the advantages of self-attention\n(our experiments also show unstructured sparsi\ufb01cation can lead to better performance for vision\ntransformers compared to structu",
  "on, Ulrike von Luxburg, Samy\nBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in\nNeural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998\u20136008, 2017.\n[28] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convoluti",
  "residual learning for image recognition. In CVPR, 2016.\n1\n[23] Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang.\nLearning deep representation for imbalanced classi\ufb01cation.\nIn CVPR, 2016. 2\n[24] Chen Huang, Yining Li, Change Loy Chen, and Xiaoou Tang.\nDeep imbalanced learning for face recognition and attribute\nprediction. IEEE TPAMI, 2019. 2\n[25] Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang.\nLearning deep representation for imbalanced classi\ufb01cation.\nIn CVPR, 2016. 1, 5, 14\n[26] ",
  " with\nVQ-VAE-2. In Advances in Neural Information Processing Systems, volume 32. Curran Associates,\nInc., 2019.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet\nlarge scale visual recognition challenge. IJCV, 2015.\n13\n\n--------------------------------------------------\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words",
  "sual and vision-language representation learning with noisy text supervision. arXiv\npreprint arXiv:2102.05918, 2021.\n[22] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big transfer\n(BiT): General visual representation learning. In ECCV, 2020.\n[23] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report,\nUniversity of Toronto, 2009.\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classi\ufb01cation with deep convolutiona",
  "ctor of |h|. The number of heads |h| controls the size\nof the lambdas \u03bbn \u2208R|k|\u00d7|d|/|h| relative to the total size of the queries qn \u2208R|hk|.\n5Attention maps typically need to be stored for back-propagation (Kitaev et al., 2020).\n6\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\ndef lambda layer(queries, keys, embeddings, values):\n\"\"\"Multi\u2212query lambda layer.\"\"\"\n# b: batch, n: input length, m: context length,\n# k: query/key depth, v: value depth,\n# ",
  "ich consists of the design choices for\nconvolutional operation types {MBConv, Fused-MBConv},\nnumber of layers, kernel size {3x3, 5x5}, expansion ratio\n{1, 4, 6}. On the other hand, we reduce the search space size\nby (1) removing unnecessary search options such as pooling\nskip ops, since they are never used in the original Ef\ufb01cient-\nNets; (2) reusing the same channel sizes from the backbone\nas they are already searched in (Tan & Le, 2019a). Since the\nsearch space is smaller, we can apply reinforc",
  " learning\nperformance on object detection.\n4.2.2\nSAME DOMAIN\nWe also observe consistent performance improvements in the same domain setups.\nImageNet. In Table 9, we show results by using Anisotropic ImageNet for supervised classi\ufb01cation.\nWe observe that Anisotropic ImageNet improves performance in both ImageNet classi\ufb01cation and\nobject detection. For Gaussian blurring experiments, we closely follow Chen et al. (2020b) and\nadd a Gaussian blur operator with random radius from 10 to 20 and train in",
  "ets is provided in Section E.\n\n--------------------------------------------------\n11\nFig. 11: Top row: NATNets obtained from tri-objective search to maximize ImageNet top-1 accuracy, minimize model size (#Params), and minimize {#MAdds,\nCPU latency, GPU latency} from left to right. Pareto surfaces emerge at higher model complexity regime (i.e. top right corner) suggesting that trade-offs exist\nbetween model size (#params) and model ef\ufb01ciency (#MAdds and latency). Bottom row: 2D projections from a",
  "Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16\nwords: Transformers for image recognition at scale. In ICLR, 2021.\n[21] D. Eigen, M. Ranzato, and I. Sutskever. Learning factored representations in a deep mixture of\nexperts. arXiv preprint arXiv:1312.4314, 2013.\n[22] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models\nwith simple and ef\ufb01cient sparsity. arXiv preprint arXiv:2101.03961, 2021.\n[23] D. M. Gavr",
  "\n\u2020Corresponding author.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2107.00645v2  [cs.CV]  26 Oct 2021\n\n--------------------------------------------------\nPatch Embedding\nGlobal Filter Layer\nFeed Forward Network (FFN)\n\ud835\udc41\u00d7\nGlobal Average Pooling\nLinear\nClass\n2D FFT\n\u00d7\nlearnable\nglobal filters\nfrequency domain\nfeatures\n2D IFFT\nGlobal Filter Layer\nLayer Norm\nLayer Norm\nMLP\nFFN\nFigure 1: The overall architecture of the Global Filter Network. Our architecture is based",
  "be maintained. In this way, our method can be\na generic plugin in most ViTs of both \ufb02at and deep-narrow\nstructures from the very beginning of training.\nConcretely, Evo-ViT 1 , a self-motivated slow-fast token\nevolution approach for dynamic ViTs is proposed in this\nwork. We claim that since transformers have insights into\nglobal dependencies among image tokens and learn for clas-\nsi\ufb01cation, it is naturally able to distinguish informative to-\nkens from placeholder tokens for each instance, which i",
  "eiT. It is possible\nto design a self-attention network with high base perfor-\nmance and elite performance.\n3.3.8\nAdjusting the shape of network\nThere are still many differences between Net7 and ResNet-\n50.\nFirst, the shape of Net7 is different from ResNet-\n50. Their depths, widths, bottleneck ratios and block num-\nbers in network stages are different. Second, they normal-\nize the features in different positions. Net7 only normal-\nizes input features in a block, while ResNet-50 normalizes\nfeature",
  "plmented KD outperformed their proposed state-of-the-art method, CRD (71.17%), and\nachieved the comparable accuracy with their CRD+KD (71.38%) method.\n3.3\nReproducing COCO experiments\nTo demonstrate that our framework can 1) be applied to different tasks, and 2) col-\nlaborate with model architectures that are not implemented in torchvision, we apply\nthe generalized head network distillation (GHND) to bottleneck-injected R-CNN ob-\nject detectors for split computing [25], using COCO 2017 dataset. ",
  ". These approaches rely on heuristics (e.g., EA)\nto ef\ufb01ciently navigate the search space allowing practitioners to\nvisualize the trade-off between the objectives and to choose a\nsuitable network a posteriori to the search. NAT falls into the\nlatter category and uses an accuracy prediction model and weight\nsharing for ef\ufb01cient architecture transfer to new tasks.\n3\nPROPOSED APPROACH\nNeural Architecture Transfer consists of three main components:\nan accuracy predictor, an evolutionary search routin",
  "s a special serial case of the resid-\nual structure. (b) The CNN (e.g., ResNet); (c) A special\nhybrid structure where the transformer block is embedded\nto bottlenecks. (d) The visual transformers (e.g., ViT); (e)\nA special case where the bottlenecks are embedded to the\ntransformer blocks.\noverlap, by a linear projection layer, which is a 4\u00d74\nconvolution with stride 4. A class token is then pretended\nto the patch embeddings for classi\ufb01cation.\nConsidering\nthat the CNN branch (3\u00d73 convolution) enco",
  "print arXiv:1611.03530, 2016.\nZhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D.\nmixup: Beyond empirical risk minimization. ICLR, 2018.\nZhang, Y., Pezeshki, M., Brakel, P., Zhang, S., Bengio, C.\nL. Y., and Courville, A. Towards end-to-end speech recog-\nnition with deep convolutional neural networks. arXiv\npreprint arXiv:1701.02720, 2017.\nZhao, R., Ouyang, W., Li, H., and Wang, X. Saliency\ndetection by multi-context deep learning. CVPR, 2015.\nZhou, B., Khosla, A., Lapedriza, A., Oliva, A., a",
  "Mixup+KD\n200\n224\n74.18\n77.51\n64.41\n12.98 / 22.49\nM5\nMobileNet-V1\nLS+Mixup+KD\n360\n224\n74.37\n76.39\n65.02\n12.98 / 22.49\nM6\nSE-MobileNet-V1 (r=16)\nLS+Mixup+KD\n360\n224\n76.42\n71.67\n56.73\n10.33 / 19.71\nM7\nSE-MobileNet-V1 (r=2)\nLS+Mixup+KD\n360\n224\n76.82\n70.67\n55.76\n9.9 / 19.32\nM8\nSE-MobileNet-V1 (r=2)\nLS+Mixup+KD+DropBlock\n900\n224\n77.30\n68.12\n49.99\n9.9 / 19.32\nTable 9. Ablation study for assembling the network tweaks and regularization with MobileNet on ILSVRC2012 dataset. In order to measure\nthroughput",
  "different types of \ufb01lters with varying size and depth, which are\nable to capture different levels of details in the scene. On top of these improved\nrecognition capabilities, PyConv is also ef\ufb01cient and, with our formulation, it\ndoes not increase the computational cost and parameters compared to standard\nconvolution. Moreover, it is very \ufb02exible and extensible, providing a large space of\npotential network architectures for different applications. PyConv has the potential\nto impact nearly every co",
  "input data. Spatial relationships are essential for sensory\nreasoning (Kant, 1781) and this limitation is clearly unsat-\nisfying. In the attention literature, position information is\ntypically injected by tagging position encodings onto the\ninput features (Vaswani et al., 2017); we pursue this strategy\nhere as well. While position information is typically used\nto encode sequence position in the context of language, it\ncan also be used to encode spatial, temporal, and modality\nidentity.\nScalable ",
  "s, Jeffrey Dean, Matthieu Devin, et al.\nTensor\ufb02ow:\nLarge-scale machine learning on heterogeneous distributed\nsystems. arXiv preprint arXiv:1603.04467, 2016.\n[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101\u2013mining discriminative components with random\nforests. In European Conference on Computer Vision, pages\n446\u2013461. Springer, 2014.\n[3] Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and\nRogerio Feris. Big-little net: An ef\ufb01cient multi-scale fea-\nture representation for visu",
  "nsformer over CNN\nmainly come with strong data augmentation and training tricks. This can promote the ongoing and\nfuture research of Vision Transformer.\n14\n\n--------------------------------------------------\n",
  "arning applied to document recognition. Proceed-\nings of the IEEE, 86(11):2278\u20132324, 1998.\nLeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Na-\nture, 521(7553):436\u2013444, 2015.\nLee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh,\nY. W. Set Transformer: A framework for attention-based\npermutation-invariant neural networks. In Proceedings of\nInternational Conference on Machine Learning (ICML),\n2019.\nLee, M. A., Zhu, Y., Zachares, P., Tan, M., Srinivasan, K.,\nSavarese, S., Fei-Fei, L., Gar",
  "aluate\nit under the transfer learning setup in [20] on three different\ndatasets; CIFAR-10, CIFAR-100 and ChestX-Ray14 [43].\n5.4.1\nCIFAR-10 and CIFAR-100\nBoth CIFAR-10 and -100 datasets have 50,000 and 10,000\nimages for training and testing, respectively. CIFAR-100\nextends CIFAR-10 by adding 90 more classes resulting in\n10\u00d7 fewer training examples per class. For training on both\ndatasets, the models are initialized with weights pre-trained\non ImageNet. The model is then \ufb01ne-tuned using SGD with\nm",
  " cause a prohibitively larger memory requirement\nbecause of the (D + h \u2217THW/fqfk) term. Hence, Multi-\nscale Transformer starts with a small number of heads which\nis increased as the resolution factor THW/fqfk decreases,\nto hold the effect of (D +h\u2217THW/fqfk) roughly constant.\nD. Additional Implementation Details\nWe implement our model with PySlowFast [28]. Code\nand models are available at: https://github.com/\nfacebookresearch/SlowFast.\nD.1. Details: Kinetics Action Classi\ufb01cation\nArchitecture deta",
  "naries Section, the class attention\nAcls is calculated by Eqn. 3. We select k tokens whose\nscores in the class attention are among the top k as the infor-\nmative tokens. The remaining N \u2212k tokens are recognized\nas placeholder tokens that contain less information. Differ-\nent from token pruning, the placeholder tokens are kept and\nfast-updated rather than dropped.\nFor better capability of capturing the underlying informa-\ntion among tokens in different layers, we propose a global\nclass attention ",
  "e that lambda layers (or linear attention) can successfully be used complemen-\ntary to pixel patches.\nC.6\nCONNECTIONS TO HYPERNETWORKS AND EXPERT MODELS\nLambdaNetworks generate their own computations, i.e. lambdas such that yn = \u03bbnqn. As such,\nthey can alternatively be viewed as an extension of HyperNetworks (Ha et al., 2016) that dynamically\ngenerate their computations based on contextual information.\nLastly, LambdaNetworks share some connections with sparsely-activated expert models (Shazeer\ne",
  "able B.3: Penalty \u03bb and group size g for the v1 and v2 DIFFQ models reported on Table B.2\nMOBILENET\nRESNET-18\nWIDERESNET\n\u03bb\ng\n\u03bb\ng\n\u03bb\ng\nCIFAR-10\nDIFFQ V1\n1\n16\n0.1\n8\n5\n16\nDIFFQ V2\n5\n8\n5\n4\n5\n16\nCIFAR-100\nDIFFQ V1\n1\n16\n0.05\n4\n5\n16\nDIFFQ V2\n5\n16\n5\n8\n1\n16\n8 bits, DIFFQ achieves the same accuracy as the uncompressed baseline, for a slightly smaller model than QAT 8bits. As we\nlower the number of bits, we again see a clear advantage for DIFFQ, with both a smaller model (5.7MB against 6.1MB) than\nQAT 4bits",
  "two basic operations. First,\nis an attention operation [4] for modeling inter-element re-\nlations. Second, is a multi-layer perceptron (MLP), which\nmodels relations within an element. Intertwining these oper-\nations with normalization [2] and residual connections [44]\nallows transformers to generalize to a wide variety of tasks.\nRecently, transformers have been applied to key com-\nputer vision tasks such as image classi\ufb01cation. In the spirit\nof architectural universalism, vision transformers [25",
  "pendix B.5. There are three additional\nmajor design decisions that affect the cost (and potentially the quality) of our model:\nNumber of MoE layers. Following [39], we place the MoEs on every other layer (we refer to these\nas V-MoE Every-2). In addition, we experimented with using fewer MoE layers, by placing them on\nthe last-n even blocks (thus we dub these V-MoE Last-n). In Appendix E.1 we observe that, although\nusing fewer MoE layers decreases the number of parameters of the model, it has typ",
  "arison of class-token and global average pooling classi\ufb01ers. Both work similarly\nwell, but require different learning-rates.\nPos. Emb.\nDefault/Stem\nEvery Layer\nEvery Layer-Shared\nNo Pos. Emb.\n0.61382\nN/A\nN/A\n1-D Pos. Emb.\n0.64206\n0.63964\n0.64292\n2-D Pos. Emb.\n0.64001\n0.64046\n0.64022\nRel. Pos. Emb.\n0.64032\nN/A\nN/A\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\nImageNet 5-shot linear.\nthe difference in performance is fully explained by the requirem",
  "tails). Since X[k] repeats on intervals of length N, it\nis suf\ufb01ce to take the value of X[k] at N consecutive points k = 0, 1, . . . , N \u22121. Speci\ufb01cally, X[k]\nrepresents to the spectrum of the sequence x[n] at the frequency \u03c9k = 2\u03c0k/N.\nIt is also worth noting that DFT is a one-to-one transformation. Given the DFT X[k], we can recover\nthe original signal x[n] by the inverse DFT (IDFT):\nx[n] = 1\nN\nN\u22121\nX\nk=0\nX[k]ej(2\u03c0/N)kn.\n(3.2)\nFor real input x[n], it can be proved that (see Appendix A) its DFT is",
  "architecture search.\narXiv preprint\narXiv:2103.12424, 2021. 3\n[32] Min Lin, Qiang Chen, and Shuicheng Yan. Network in net-\nwork. arXiv preprint arXiv:1312.4400, 2013. 2\n[33] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll\u00b4ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980\u20132988, 2017. 7\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Law",
  "associ-\nated alt-texts are discarded. To ensure that we don\u2019t train on\ntest images, we also remove duplicates or near-duplicates\nof test images in all downstream evaluation datasets (e.g.,\nILSVRC-2012, Flickr30K, and MSCOCO). See Appendix\nA for more details.\nText-based \ufb01ltering.\nWe exclude alt-texts that are shared\nby more than 10 images. These alt-texts are often irrelevant\nto the content of the images (e.g., \u201c1920x1080\u201d, \u201calt img\u201d,\nand \u201ccristina\u201d). We also discard alt-texts that contain any\nra",
  "ls. A SSAL objective is real-\nized through one or more additional targets that are derived\nfrom the original supervised classi\ufb01cation task, following\narchitectural principles found in multi-task learning. SSAL\nbranches impose low-level priors into the optimization pro-\ncess (e.g., grouping). The ability of using SSAL branches\nduring inference, allow models to converge faster, focus-\ning on a richer set of class-relevant features. We show that\nSSAL models consistently outperform the state-of-the-",
  "53]\nC. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, \u201cLearning\na discriminative feature network for semantic segmentation,\u201d in\nCVPR, 2018, pp. 1857\u20131866.\n[54]\nH. Zhang, K. J. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and\nA. Agrawal, \u201cContext encoding for semantic segmentation,\u201d in\nCVPR, 2018, pp. 7151\u20137160.\n[55]\nZ. Zhong, Z. Q. Lin, R. Bidart, X. Hu, I. B. Daya, Z. Li, W. Zheng,\nJ. Li, and A. Wong, \u201cSqueeze-and-attention networks for semantic\nsegmentation,\u201d in CVPR, 2020, pp. 13 062\u201313 071.",
  "rate multiple types of data that appear in vision, such as\npixels [61, 3, 46, 68], point clouds [65], sequence condition-\ning information [64], and graphs [32]. Self-attention may\nalso be regarded as an adaptive nonlinearity paralleling a\nlong history of nonlinear processing techniques in computer\nvision, such as bilateral \ufb01ltering [41] and non-local means\n[4].\nSeveral recent papers [3, 45, 12, 68, 52] have attempted\nusing self-attention primitives to improve image classi\ufb01ca-\ntion accuracy over ",
  "ccu-\nracy is the percentage of images that are certi\ufb01ably correct.\nFollowing Salman et al. (2019), we calculate the certi\ufb01ed\naccuracy of all the classi\ufb01ers for various radius and report\nthe best results overall of the classi\ufb01ers. We use the codes\nprovided by Cohen et al. (2019) to calculate certi\ufb01ed ac-\ncuracy.4\nFollowing Salman et al. (2019), we select the best hyperpa-\nrameters with grid search. The only two hyperparameters\nof our MaxUp+Gauss are the sample size m and the vari-\nance \u03c32 of the ",
  "020; Shao\net al., 2020). Additionally, Zhang et al. (2019a) and De &\nSmith (2020) observed that the performance of unnormal-\nized ResNets can be improved with additional regulariza-\ntion. However only recovering these two bene\ufb01ts of batch\nnormalization is not suf\ufb01cient to achieve competitive test\naccuracies on challenging benchmarks (De & Smith, 2020).\nIn this work, we adopt and build on \u201cNormalizer-Free\nResNets\u201d (NF-ResNets) (Brock et al., 2021), a class of pre-\nactivation ResNets (He et al., 2",
  "n (4), p(1)\ni\nand p(2)\ni\ncorrespond to the two large crop views, and p(3)\ni , . . . , p(8)\ni\ncorrespond to the six small crop views.\nThus, from equation (4), the target for p(1)\ni\nis the sharpened positive view prediction \u03c1(p(2)\ni ), and similarly, the target for p(2)\ni\nis\nthe sharpened positive view prediction \u03c1(p(1)\ni ). For the small views, p(3)\ni , . . . , p(8)\ni , we use both \u03c1(p(1)\ni ) and \u03c1(p(2)\ni ) as positive\nview predictions and average those to produce a single target. This is similar",
  "t the Top-1 accuracy on ImageNet-1K in Ta-\nble 1 and Cifar10 in Appendix. Observe that WideNet-H\nachieves the best performance and signi\ufb01cantly outperforms\nViT and ViT-MoE models on ImageNet-1K. Compared with\nthe strongest baseline, our WideNet-H outperforms ViT-B\nby 1.5% with less trainable parameters. Even if we use the\nsmallest model, WideNet-B, it still achieves comparable per-\nformance with ViT-L and ViT-MoE-B with over 4\u00d7 less\ntrainable parameters. When we scale up to WideNet-L, it\nhas sur",
  "ny improvements for the\nlarger scales.\nPyramids\nAll layers in Mixer retain the same, isotropic design. Recent improvements on the ViT\narchitecture hint that this might not be ideal [52]. We tried using the token-mixing MLP to reduce the\nnumber of tokens by mapping from S input tokens to S\u2032 < S output tokens. While \ufb01rst experiments\nshowed that on JFT-300M such models signi\ufb01cantly reduced training time without losing much\nperformance, we were unable to transfer these \ufb01ndings to ImageNet or ImageNe",
  "earner do not drop (e.g., TokenLearner-B/16 vs.\nViT-B/16), despite the difference in the number of tokens.\nTokenLearner on larger models.\nWe also evaluated our TokenLearner inserted into a \u2018large\u2019\nmodel: ViT-L. In addition to the standard L/16 model that splits the scene into 16x16 patches, the\nsame model with \ufb01ner tokens were used including L/14, L/10, and L/8. Note that the model size stays\nthe same, with 464M parameters, and only the inputs change for these models. As discussed above,\nthe num",
  " computer vision to predict\nthe class. The transformer thus process batches of (N + 1) tokens of dimension\nD, of which only the class vector is used to predict the output. This architecture\nforces the self-attention to spread information between the patch tokens and\nthe class token: at training time the supervision signal comes only from the\nclass embedding, while the patch tokens are the model\u2019s only variable input.\n5\n\n--------------------------------------------------\nFixing the positional enc",
  "r set of class-relevant features. We show that\nSSAL models consistently outperform the state-of-the-art\nwhile also providing structured predictions that are more\ninterpretable.\n1. Introduction\nMachine learning models tackling classi\ufb01cation prob-\nlems are isolated in nature i.e., they are de\ufb01ned, and op-\nerate under a closed world paradigm [2] where all possible\ninputs belong to one out of multiple but \ufb01nite pre-de\ufb01ned\nclasses. This simpli\ufb01cation goes against emerging needs\nfor more interpretable",
  "9216\n9728\n10240\n1024//16\n1024//8\n1280//20\n1280//16\n1280//10\n1408//22\n1408//16\n1408//11\n1536//24\n1536//16\n1536//12\n1664//26\n1664//16\n1664//13\n1792//28\n1792//16\n1792//14\n1920//30\n1920//24\n1920//16\n1920//15\n2048//32\n2048//16\nSA-width // num heads\nViT-L\n24 layers\n4096\n4608\n5120\n5632\n6144\n6656\n7168\n7680\n8192\n8704\n9216\n9728\n10240\nViT-H\n32 layers\n4096\n4608\n5120\n5632\n6144\n6656\n7168\n7680\n8192\n8704\n9216\n9728\n10240\nMLP-width\nViT-g\n40 layers\n4096\n4608\n5120\n5632\n6144\n6656\n7168\n7680\n8192\n8704\n9216\n9728\n10240\n",
  "ing or special regularization mechanisms.\n3. Methods\nIn this section, we describe the algorithmic components\nfrom SSAL and how they integrate into a traditional clas-\nsi\ufb01cation problem for training and prediction. There are\nfour main components to discuss: grouping criterion, ar-\nchitectural design, training objectives and joint prediction.\nFor each of these components, we introduce emergent meta-\nparameters that need to be considered during evaluation.\n3.1. Grouping Criterion\nWe propose that th",
  "\nCourty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A.\nOptimal transport for domain adaptation. IEEE transac-\ntions on pattern analysis and machine intelligence, 2016.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Li,\nF. F. Imagenet: a large-scale hierarchical image database.\nCVPR, 2009.\nDeVries, T. and Taylor, G. W. Improved regularization of\nconvolutional neural networks with cutout. arXiv preprint\narXiv:1708.04552, 2017.\nErkan, G. and Radev, D. R. Lexrank: Graph-based lexical\ncent",
  "d generalization ability of our ViTAE.\n4.6\nVisual inspection of ViTAE\n40\n60\n80\n100\n0\n1\n2\n3\n4\n5\n6\n7\n8\nAttention Distance (pixel)\nLayers\nT2T-ViT\nViTAE\nFigure 3: The average per-layer attention distance\nof T2T-ViT-7 and our ViTAE-T.\nTo further analyze the property of our ViTAE,\nwe \ufb01rst calculate the average attention distance\nof each layer in ViTAE-T and the baseline T2T-\nViT-7 on the ImageNet test set, respectively. The\nresults are shown in Figure 3. It can be observed\nthat with the usage of PCM, ",
  "tisfactory result, 0.76 \u03c1,\nsmoothly and quickly within only 20 epochs on CIFAR-100,\nand continues to be stable for the subsequent 10 epochs.\nA.4. Visualization of Human-designed Architec-\ntures in HyTra\nThe\narchitectures\nof\nResNet50-T,\nViT-T/16\nand\nBoTNet50-T from our HyTra search space are illustrated\nin Fig. 10. Their architectures follow as closely as possible\nto the architectures of their prototypes.\n14\n\n--------------------------------------------------\n",
  "rameters, at\nany point in time, a signi\ufb01cant fraction of the weights could\nbe quantized to a suboptimal value due to the oscillations\nimplied by the STE method. We con\ufb01rm such behavior exper-\nimentally using Ef\ufb01cientNet-b3 with QAT4 on ImageNet (see\nFigure 1(b) in the Appendix). In the following section, we\nintroduce DIFFQ, a method based on independent additive\npseudo quantization noise, that does not suffer from such a\nbias, while approximating well enough quantization noise to\nperform ef\ufb01cien",
  "xtensive connections [34], and\nmore sophisticated forms of convolution [70, 18, 84]. With\nCNNs serving as backbone networks for a variety of vision\ntasks, these architectural advances have led to performance\nimprovements that have broadly lifted the entire \ufb01eld.\nOn the other hand, the evolution of network architectures\nin natural language processing (NLP) has taken a different\npath, where the prevalent architecture today is instead the\nTransformer [64].\nDesigned for sequence modeling and\ntransdu",
  " and the softmax of the student model.\nLet Zt be the logits of the teacher model, Zs the logits of the student model.\nWe denote by \u03c4 the temperature for the distillation, \u03bb the coef\ufb01cient balanc-\ning the Kullback\u2013Leibler divergence loss (KL) and the cross-entropy (LCE) on\nground truth labels y, and \u03c8 the softmax function. The distillation objective is\nLglobal = (1 \u2212\u03bb)LCE(\u03c8(Zs), y) + \u03bb\u03c4 2KL(\u03c8(Zs/\u03c4), \u03c8(Zt/\u03c4)).\n(2)\nHard-label distillation.\nWe introduce a variant of distillation where we take\nthe ha",
  "ation quality may pro-\nfoundly affect \ufb01nal fusion weights. Since it is still a feature\nfusion problem, an intuitive way is to have another attention\nmodule to fuse input features. We call this two-stage ap-\nproach iterative Attentional Feature Fusion (iAFF), which\nis illustrated in Fig. 2(b). Then, the initial integration X\u228eY\nin Eq. (4) can be reformulated as\nX \u228eY = M(X + Y) \u2297X + (1 \u2212M(X + Y)) \u2297Y (5)\n4.3. Examples: InceptionNet, ResNet, and FPN\nTo validate the proposed AFF/iAFF as a uniform and\n",
  "uch networks in memory for the\nteacher and the student would vastly exceed the available\n6\n\n--------------------------------------------------\nMethod\n# Params\nExtra Data\nImageNet\nImageNet-ReaL [6]\nTop-1\nTop-5\nPrecision@1\nResNet-50 [24]\n26M\n\u2212\n76.0\n93.0\n82.94\nResNet-152 [24]\n60M\n\u2212\n77.8\n93.8\n84.79\nDenseNet-264 [28]\n34M\n\u2212\n77.9\n93.9\n\u2212\nInception-v3 [62]\n24M\n\u2212\n78.8\n94.4\n83.58\nXception [11]\n23M\n\u2212\n79.0\n94.5\n\u2212\nInception-v4 [61]\n48M\n\u2212\n80.0\n95.0\n\u2212\nInception-resnet-v2 [61]\n56M\n\u2212\n80.1\n95.1\n\u2212\nResNeXt-101 [78]\n",
  "tion on MS-COCO dataset. KSSNet [40],\nis the known SOTA, based on ResNet101 backbone.\nWe can see from Table 9 that the TResNet-based solu-\ntion signi\ufb01cantly outperforms previous top solution for MS-\nCOCO multi-label dataset, increasing the known SOTA by\na large margin, from 83.7 mAP to 86.4 mAP. All additional\nevaluation metrics also show improvement.\n4.3. Object Detection\nWhile our main focus was on various classi\ufb01cation tasks,\nwe wanted to further test TResNet on another popular com-\nputer vis",
  "d interpretation or action.\nClosed world models are thereby semantically disconnected\nfrom the patterns we may deem reasonable, making the\nFigure 1. Overview of a SSAL model. Starting from a common\nfeature representation h, a supervised goal f and an auxiliary\nbranch g are used for training and prediction. Training objective\nfor g is derived from the original labels used for f following a\nmutually exclusive grouping.\nquest for explanations an ill-posed endeavor. The \ufb01eld of\nadversarial perturbat",
  "r fast inference image classi\ufb01cation. We consider dif-\nferent measures of ef\ufb01ciency on different hardware plat-\nforms, so as to best re\ufb02ect a wide range of application\nscenarios.\nOur extensive experiments empirically vali-\ndate our technical choices and show they are suitable to\nmost architectures. Overall, LeViT signi\ufb01cantly outper-\nforms existing convnets and vision transformers with re-\nspect to the speed/accuracy tradeoff. For example, at 80%\nImageNet top-1 accuracy, LeViT is 5 times faster ",
  " sub-optimal initial architecture can skew the scaling\nresults. For example, the compound scaling rule derived\nfrom a small grid search around Ef\ufb01cientNet-B0, which\nwas obtained by architecture search using a \ufb01xed FLOPs\nbudget and a speci\ufb01c image resolution. However, since\nthis image resolution can be sub-optimal for that FLOPs\nbudget, the resulting scaling strategy can be sub-optimal.\nIn contrast, our work designs scaling strategies by training\nmodels across a variety of widths, depths and imag",
  "---------------------------------------------\nPerceiver: General Perception with Iterative Attention\nFigure 26. Example attention maps from the \ufb01rst cross-attend over the video input subset of an AudioSet network trained on video and\nmel-spectrogram.\n\n--------------------------------------------------\nPerceiver: General Perception with Iterative Attention\nFigure 27. Example attention maps from the \ufb01rst cross-attend over the mel-spectrogram input subset of an AudioSet network trained on\nvideo and",
  "utput features as shown in Eqn. (2):\nFk = CKA(fk, fout)\nCKA(fin, fout),\n(2)\n15\n\n--------------------------------------------------\nTable 10: Model architecture con\ufb01gurations.\nModel\n#Blocks Hidden dim #Head #Params Training Resolution\nRe\ufb01ned-ViT-Base\n12\n768\n12\n86M\n224\nRe\ufb01ned-ViT-S\n16\n384\n12\n25M\n224\nRe\ufb01ned-ViT-M\n32\n420\n12\n55M\n224\nRe\ufb01ned-ViT-L\n32\n512\n16\n81M\n224\nwhere fk denotes the token features at layer k, fin denotes the token features at the \ufb01rst transformer\nblock and fout denotes the features ",
  " worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[20] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan.\nThe third pascal recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and\nparaphrasing, pages 1\u20139. Association for Computational Linguistics, 2007.\n11\n\n--------------------------------------------------\n[21] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu.",
  "ew(B, G, C//G, K*K, H, W)\n# kernel generation, Eqn.(6)\nkernel = span(reduce(o(x))) # B,KxKxG,H,W\nkernel = kernel.view(B, G, K*K, H, W).unsqueeze(2)\n# Multiply-Add operation, Eqn.(4)\nout = mul(kernel, x_unfolded).sum(dim=3) # B,G,C/G,H,W\nout = out.view(B, C, H, W)\nreturn out\n1, 2, \u00b7 \u00b7 \u00b7 , G, is specially tailored for the pixel Xi,j \u2208RC\n(the subscript of C is omitted for notation brevity) located\nat the corresponding coordinate (i, j), but shared over the\nchannels. G counts the number of groups wh",
  "Table 3), indicating that the commonly used\nweight decay may not effectively regularize ViTs and MLP-Mixers.\nSparser active neurons in MLP-Mixers. Given the recursive formulation Equation (3), we identify\nanother intrinsic measure of MLP-Mixers that contribute to the Hessian: the number of activated\nneurons. Indeed, Bk is determined by the activated neurons whose values are greater than zero,\nsince the \ufb01rst-order derivative of GELU becomes much smaller when the input is negative. As\na result, th",
  "P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. In AISTATS,\n2015.\n[34] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable\nunsupervised learning of hierarchical representations. In International Conference on Machine\nLearning (ICML), 2009.\n[35] H. Lee, P. Pham, Y. Largman, and A. Y. Ng. Unsupervised feature learning for audio classi\ufb01ca-\ntion using convolutional deep belief networks. In Advances in Neural Information Processing\nSystems, 2009.\n",
  "ilable.\nConvolutional arithmetic circuits (ConvACs) are also related\nto our work. Arithmetic circuits are networks with two types of\nnodes: sum nodes (weighted sum of their inputs), and product\nnodes (computing the product of their inputs). Those two types\nof nodes are suf\ufb01cient to express a polynomial expansion. On\n[40], the authors want to characterize the depth ef\ufb01ciency of\n(deep) convolutional neural networks. The CP decomposition is\nused to factorize the weights of a shallow convolutional n",
  "yin Zhou.\nTransunet: Transformers make strong encoders for medi-\ncal image segmentation. arXiv preprint arXiv:2102.04306,\n2021. 2\n[14] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In Proc. ICML, pages 1691\u20131703.\nPMLR, 2020. 2\n[15] Yunpeng Chen, Haoqi Fang, Bing Xu, Zhicheng Yan, Yannis\nKalantidis, Marcus Rohrbach, Shuicheng Yan, and Jiashi\nFeng. Drop an octave: Reducing spatial redundancy in con-\nvolutional ",
  "T-B [95]\n81.8\n17.6\n86.6\nDeiT-B \u21913842 [95]\n83.1\n55.5\n87.0\nMViT-B-16, max-pool\n82.5\n7.8\n37.0\nMViT-B-24, max-pool\n83.1\n10.9\n53.5\nMViT-B-24-wide-3202, max-pool 84.3\n32.7\n72.9\nMViT-B-16\n83.0\n7.8\n37.0\nMViT-B-24-wide-3202\n84.8\n32.7\n72.9\nTable 17. Comparison to prior work on ImageNet.\nRegNet\nand Ef\ufb01cientNet are ConvNet examples that use different training\nrecipes. DeiT/MViT are ViT-based and use identical recipes [95].\nWe show models of different depth, MViT-B-Depth, (16,\n24, and 32), where MViT-B-16 is",
  "100 (Krizhevsky\n& Geoffrey, 2009), Tiny-ImageNet (Chrabaszcz et al., 2017),\nand ImageNet (Deng et al., 2009) datasets. We \ufb01rst study\nthe generalization performance and adversarial robustness\nof our method (Section 6.1). Next, we show that our method\ncan be used in conjunction with the existing augmentation\nmethod (AugMix) to simultaneously improve the corruption\nrobustness and generalization performance (Section 6.2).\nFinally, we perform ablation studies for our method (Sec-\ntion 6.3).\n6.1. Gene",
  "distance, while the\nclip function assigns the same encoding when the relative\ndistance is larger than \u03b2.\nNumber of buckets. The number of buckets largely af-\nfects model parameters, computational complexities and\nperformance. In order to \ufb01nd a balance, we explore the in-\n\ufb02uence of varying the number of buckets for the contextual\nProduct method. Fig. 3 shows the change of top-1 accuracy\nalong with the number of buckets. The accuracy increase\n6\n\n--------------------------------------------------\n0",
  "ing the kernels size\nprovides more reliable details about larger objects and/or context information.\n(2) Ef\ufb01ciency. In comparison with the standard convolution, PyConv maintains, by default, a similar\nnumber of model parameters and requirements in computational resources, as shown in Equation 1.\nFurthermore, PyConv offers a high degree of parallelism due to the fact that the pyramid levels can\nbe independently computed in parallel. Thus, PyConv can also offer the possibility of customizable\nheav",
  "given suf\ufb01cient pre-training data) it is more ef\ufb01cient to\ntrain larger models with a shorter epoch budget than to train\nsmaller models for longer, consistent with the observations\nin (Kaplan et al., 2020).\nWe \ufb01ne-tune NFNet models for 15,000 steps at a batch\nsize of 2048 using a learning rate of 0.1, which is warmed\nup from zero over 5000 steps, then annealed to zero with\ncosine decay through the rest of training. We use SAM with\n\u03c1 = 0.05, weight decay of 10\u22125, a DropOut rate of 0.25, and\na stoc",
  " the PSA layer can achieve a strictly convolutional\nattention map by setting the centers of attention \u2206h to\nPatches\nClass\ntoken\nImage \nembedding\nNonlocal \nGPSA\nFFN\nSA\nFFN\nSA\nFFN\nLocal \n\ud835\udc4a\ud835\udc5e\ud835\udc5f\ud835\udc66\n\ud835\udc4a\ud835\udc58\ud835\udc52\ud835\udc66\n\ud835\udc63\ud835\udc5d\ud835\udc5c\ud835\udc60\n\ud835\udc5f\ud835\udc56\ud835\udc57\n\u2217\n\u2217\nsoftmax\nsoftmax\n+\n1 \u2212\ud835\udf0e(\ud835\udf06)\n\ud835\udf0e(\ud835\udf06)\nnormalize\nGated Positional\n Self-Attention\nConViT\n\ud835\udc4b\ud835\udc56\n\ud835\udc4b\ud835\udc57\n\ud835\udc34\ud835\udc56\ud835\udc57\nGPSA\nFFN\nFigure 4. Architecture of the ConViT. The ConViT (left) is a ver-\nsion of the ViT in which some of the self-attention (SA) layers\nare replaced with gated positional self-attention layers (GPSA",
  "s prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 \u00d7 4 downsized version of the 16 \u00d7 16\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\nwell, though L2 was slightly worse. We report \ufb01nal results only for option 1) because it has ",
  "rd. Are we done with\nimagenet? CoRR, abs/2006.07159, 2020. 6\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV (1), volume\n12346 of Lecture Notes in Computer Science, pages 213\u2013\n229. Springer, 2020. 1, 2\n[6] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. CoRR,\nabs/2",
  " Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[11] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer. Multiscale vision\ntransformers. arXiv preprint arXiv:2104.11227, 2021.\n[12] C. Feichtenhofer. X3D: expanding architectures for ef\ufb01cient video recognition. In Proceeding",
  "stafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929,\n2020.\nJean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi\nAzar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent:\nA new approach to self-supervised learning",
  "------------\nRevisiting ResNets: Improved Training and Scaling Strategies\nHu, H., Zhang, Z., Xie, Z., and Lin, S.\nLocal rela-\ntion networks for image recognition.\narXiv preprint\narXiv:1904.11491, 2019.\nHu, J., Shen, L., and Sun, G. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pp. 7132\u20137141,\n2018.\nHuang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger,\nK. Q. Deep networks with stochastic depth. In European\nconference on com",
  "phase. Figure 2 provides an intuitive interpretation. Given the output\npatch tokens X1, ..., XN and the corresponding labels [y1, ..., yN], the token labeling objective can\nbe de\ufb01ned as\nLtl = 1\nN\nN\nX\ni=1\nH(Xi, yi).\n(2)\nRecall that H is the cross-entropy loss. Therefore, the total loss function can be written as\nLtotal = H(Xcls, ycls) + \u03b2 \u00b7 Ltl,\n(3)\n= H(Xcls, ycls) + \u03b2 \u00b7 1\nN\nN\nX\ni=1\nH(Xi, yi),\n(4)\nwhere \u03b2 is a hyper-parameter to balance the two terms. In our experiment, we empirically set it to\n0",
  "ent. To\nadjust for the staleness of their gradients, we need to reweigh\nthe gradient properly as explained later.\nOur implementation maintains two queues: a queue\nqun\ufb01nished of un\ufb01nished jobs and a queue q\ufb01nished of \ufb01nished\njobs. Whenever the qun\ufb01nished contains less than its capacity\nC, the controller generates n = C \u2212|qun\ufb01nished| new dropout\npatterns r1, r2, ..., rn and \ufb01lls up qun\ufb01nished with the pairs\n(ri, P(ri; \u03b8i)), where \u03b8i is the value of the controller\u2019s param-\neters at the time ri is s",
  "y and makes\nlearning easier. We also notice that the temperature usually\nquickly decrease to only around 1.2x of the converged\nvalues in the \ufb01rst 100k steps, and then slowly converges\nuntil the end of training.\nTable 8. Ablation study of key architecture parameters. Baseline\nmodel (\ufb01rst row) is trained with embedding dimension 640, using\nall negatives in the batch, and a learnable softmax temperature.\nModel\nMSCOCO\nImangeNet KNN\nI2T R@1\nT2I R@1\nR@1\nB5 + BERT-base\n51.7\n37.5\n64.6\nw/ embedding dim=3",
  "n addition, the local context modeling of\nthe proposed Convolutional Projection compensates for the\nloss of information incurred by resolution reduction.\n3.3. Methodological Discussions\nRemoving Positional Embeddings:\nThe introduction of\nConvolutional Projections for every Transformer block,\ncombined with the Convolutional Token Embedding, gives\nus the ability to model local spatial relationships through the\nnetwork. This built-in property allows dropping the position\nembedding from the network ",
  "rk, we revisit the design of the spatial attention in vision\ntransformers. Our \ufb01rst \ufb01nding is that the global sub-sampled attention in PVT is highly effective,\nand with the applicable positional encodings [9], its performance can be on par or even better\nthan state-of-the-art vision transformers (e.g., Swin). This results in our \ufb01rst proposed architecture,\ntermed Twins-PCPVT. On top of that, we further propose a carefully-designed yet simple spatial\nattention mechanism, making our architectures ",
  "ROUGE-1\nROUGE-2\nROUGE-L\nTransformer\n523.2\n192\n32.987\n15.286\n30.771\nOurs-Random\n262.9\n230\n30.310\n12.752\n27.823\nOurs-MP\n262.9\n230\n34.651\n16.468\n32.415\nTable 1: Results on Gigaword text summarization task (MBS=Maximal Batch Size, MP = Mean-Pooling). The MACs\n(Multiply-add ACcumulation) is only computed for the encoder, assuming the length of sequence is 45 (the maximal\nlength of sequence in the dataset). Though centroid transformer with random initialization (Ours-Random) performs\nworse than the ba",
  "sion alignment.\nFinally, the feature maps are added with patch embeddings,\nas shown in Fig. 2(b). When fed back from the transformer\nbranch to the CNN branch, the patch embeddings require\nto be up-sampled (Fig. 2(a)) to align the spatial scale. The\nchannel dimension is then aligned with that of CNN feature\nmaps through the 1\u00d71 convolution, and added to the fea-\nResNet-101\n(a) Class activation maps\nDeiT-S\nOurs-CNN\nOurs-Trans.\n(b) Attention maps\nFigure 4: Feature analysis. (a) Class activation map",
  "\n95.07\n96.03\nSAM-\ufb01nal\n99.65\n97.10\n95.96\n96.18\nALIGN\n99.65\n96.19\n96.13\n95.88\n6. Ablation Study\nIn the ablation study, we compare model performance\nmostly on MSCOCO zero-shot retrieval and ImageNet K-\nNearest-neighbor (KNN) tasks.3 We \ufb01nd these two met-\n2ViT (Dosovitskiy et al., 2021) uses different hyper-parameters\nfor different tasks and hence is not included in comparison.\n3For each image in the validation set of ImageNet, we retrieve\nits nearest neighbors from the training set w/ pre-trained i",
  " A \u2208RN\u00d7N and Aij represents for the attention\nscore between the Qi and Kj. Then, the self-attention op-\neration is applied on the value vectors to produce an output\nmatrix\nO = AV,\n(11)\nwhere O \u2208RN\u00d7Dh. For a multi-head self-attention layer\nwith D/Dh heads, the outputs can be calculated by a linear\nprojection for the concatenated self-attention outputs\nX\n\u2032 = [O1; O2; ...; OD/Dh]Wproj,\n(12)\nwhere Wproj \u2208RD\u00d7D is a learnable parameter and [\u00b7] de-\nnotes the concatenation operation.\nS1.2. Position-wise",
  "ization performance\n(c) Model performance\nFigure 3. Effects of the spatial dimensions in vision transformer (ViT) [9]. We compare our Pooling-based Vision Transformer (PiT)\nwith original ViT at various aspects. PiT outperforms ViT in capability, generalization performance, and model performance.\nSpatial tokens\nClass token\n(\ud835\udc64\u00d7\u210e)\u00d7\ud835\udc51\n1\u00d7\ud835\udc51\n\ud835\udc64\u00d7\u210e\u00d7\ud835\udc51\nReshape\n\ud835\udc64\n2 \u00d7 \u210e\n2 \u00d72\ud835\udc51\nDepth-wise\nConvolution\nReshape\nSpatial tokens\n(\ud835\udc64\n2 \u00d7 \u210e\n2)\u00d72\ud835\udc51\n1\u00d72\ud835\udc51\nFully-connected layer\nClass token\nFigure 4. Pooling layer of PiT archi",
  " a factor of r. Our superpixel operation bears sim-\nilarity to the concept of tiled convolution [28], a particular\nrealization of locally connected layers. This idea has also\nbeen particularly effective for image super-resolution [35]\nin the form of \u201csubpixel\" convolution.\n3.2. Channel Multiplexing\nWhile the spatial multiplexing operation described above\nis effective, it still suffers from some limitations. Firstly, the\ngroup convolutions in spatial multiplexing are more com-\nputationally expens",
  "n, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\ntion. arXiv, 2020.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens van der Maaten.\nExploring the limits of weakly supervised\npretraining. In ECCV,",
  "ford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019. 6\n[8] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi.\nOn the relationship between self-attention and convolutional\nlayers. arXiv preprint arXiv:1911.03584, 2019. 2\n[9] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmentation\nstrategies from data. In Proceedings of the IEEE conference\non computer vision and pattern ",
  " object detection\nframeworks: Cascade Mask R-CNN [29, 6], ATSS [79],\nRepPoints v2 [12], and Sparse RCNN [56] in mmdetec-\ntion [10]. For these four frameworks, we utilize the same\nsettings: multi-scale training [8, 56] (resizing the input such\nthat the shorter side is between 480 and 800 while the longer\nside is at most 1333), AdamW [44] optimizer (initial learn-\ning rate of 0.0001, weight decay of 0.05, and batch size of\n16), and 3x schedule (36 epochs). For system-level compar-\nison, we adopt a",
  "which alternates be-\ntween two partitioning con\ufb01gurations in consecutive Swin\nTransformer blocks.\nAs illustrated in Figure 2, the \ufb01rst module uses a regular\nwindow partitioning strategy which starts from the top-left\npixel, and the 8 \u00d7 8 feature map is evenly partitioned into\n2 \u00d7 2 windows of size 4 \u00d7 4 (M = 4). Then, the next mod-\nule adopts a windowing con\ufb01guration that is shifted from\nthat of the preceding layer, by displacing the windows by\n(\u230aM\n2 \u230b, \u230aM\n2 \u230b) pixels from the regularly partitio",
  " (Imagenet-21k and JFT). We param-\neterize V (x) = v(x)1\u22ba\nR \u2299V where v(x) is a vector of\ndimension R, 1R is a vector of ones of dimension R and V\nis a K\u00d7R matrix of learnable parameters which is not a func-\ntion of x. Sampling the correlated noise component can be\nsimpli\ufb01ed, V (x)\u03f5R = (v(x)1\u22ba\nK \u2299V )\u03f5R = v(x) \u2299(V \u03f5R).\nThe total parameter count of this parameter-ef\ufb01cient ver-\nsion is O(DK + KR) which typically reduces the memory\nand computational requirements dramatically for large-scale\nAlgorithm",
  "bels assigned to the cue con\ufb02ict images here will be\nexclusively based on the texture information, e.g., the image of chimpanzee shape but with lemon\ntexture will be labelled as lemon, shown in Figure 2(b). By this way, the texture information is highly\nrelated to the \u201cground-truth\u201d while the shape information only serves as a nuisance factor during\nlearning. Similarly, to learn a shape-biased model, the label assignment of cue con\ufb02ict images will\nbe based on shape only, e.g., the image of chimp",
  "omputation in self-attention. Several methods have been proposed\nto make Transformer computations more ef\ufb01cient for high resolution input. Reformer [32], Clus-\nterform [57], Adaptive Clustering Transformer [73] and Asymmetric Clustering [10] propose to\nuse Locality Sensitivity Hashing to cluster keys or queries and reduce quadratic computation into\nlinear computation. Lightweight convolution [62] explore convolution architectures for replacing\nTransformers but only explore applications in NLP. R",
  "t\narXiv:2103.12424, 2021. 8\n[33] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom\nGoldstein.\nVisualizing the loss landscape of neural nets.\nNeurIPS, 2017. 5\n[34] Liam Li and Ameet Talwalkar. Random search and repro-\nducibility for neural architecture search. In UAI, 2019. 3,\n8\n[35] Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Yuwen\nXiong, Rui Hu, and Raquel Urtasun. Polytransform: Deep\npolygon transformer for instance segmentation. In CVPR,\n2020. 8\n[36] Paul Michel, Omer Levy, and Gra",
  " under 330 FLOPs),\nwe just ditch it for good.\nBesides, evolutionary algorithms need to initialize pop-\nulation with size Npop before implementing iterative muta-\ntion and crossover. Current methods usually random sam-\nple Npop paths under the constraint as initial population. In\ncontrast, our method makes the initialization with the help\nof candidate pool P, and select its Top-Npop paths instead.\nAs Figure 3 shows, searching with candidate pool can boost\nthe evolutionary performance for supplyin",
  "x\n64\n128\n256\n512\n1024\n2048\n4096\nRegNetX-16GF\ndi = [2, 6, 13, 1]\nwi = [256, 512, 896, 2048]\ng = 128, b = 1, e = 20.0%\nwa = 56, w0 = 216, wm = 2.1\n0\n3\n6\n9\n12\n15\n18\n21\nblock index\n64\n128\n256\n512\n1024\n2048\n4096\nRegNetX-32GF\ndi = [2, 7, 13, 1]\nwi = [336, 672, 1344, 2520]\ng = 168, b = 1, e = 19.5%\nwa = 70, w0 = 320, wm = 2.0\n\ufb02ops\nparams\nacts\nbatch\ninfer\ntrain\nerror\n(B)\n(M)\n(M)\nsize\n(ms)\n(hr)\n(top-1)\nREGNETX-200MF\n0.2\n2.7\n2.2\n1024\n10\n2.8\n31.1\u00b10.09\nREGNETX-400MF\n0.4\n5.2\n3.1\n1024\n15\n3.9\n27.3\u00b10.15\nREGNETX",
  "able parameters. Therefore, with\none attention layer and one single stronger MoE layer learn-\ning complex representations, and independent normalization\nlayers for diversi\ufb01ed semantic representations, going wider\ninstead of deeper is a more parameter-ef\ufb01cient and effective\nframework.\nCompared with simply scaling along the width, going\nwider instead of deeper is a more parameter-ef\ufb01cient frame-\nwork, which makes the models small enough to be adapted\nto downstream tasks without advanced parallelis",
  "ed Task Papers,\npp. 304\u2013323, 2018.\nBossard, L., Guillaumin, M., and Van Gool, L. Food-101 \u2013\nmining discriminative components with random forests.\nIn European Conference on Computer Vision, 2014.\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P.,\nand Joulin, A. Unsupervised learning of visual features\nby contrasting cluster assignments. In Advances in Neural\nInformation Processing Systems, 2020.\nChen, J., Hu, H., Wu, H., Jiang, Y., and Wang, C. Learning\nthe best pooling strategy for vis",
  " compare to their best Ef\ufb01cientNet-L2 image encoder.\n3.1\nMain results\nTable 2 presents comparison of the largest Mixer models to state-of-the-art models from the literature.\n\u201cImNet\u201d and \u201cReaL\u201d columns refer to the original ImageNet validation [13] and cleaned-up ReaL [5]\n4\n\n--------------------------------------------------\nTable 2: Transfer performance, inference throughput, and training cost. The rows are sorted by\ninference throughput (\ufb01fth column). Mixer has comparable transfer accuracy to s",
  "ention,\nwe have explored different sizes of the latent layer, by adjusting the expansion factor e in the sequence:\nlinear N \u00d7 e \u00d7 N \u2014 GELU \u2014 linear e \u00d7 N \u00d7 N. For this experiment we used average pooling to\naggregating the patches before the classi\ufb01cation layer.\nexpansion factor \u00d7e\n\u00d70.25\n\u00d70.5\n\u00d71\n\u00d72\n\u00d73\n\u00d74\nImnet-val top-1 acc.\n78.6\n79.2\n79.2\n79.3\n78.8\n78.8\nWe observe that a large expansion factor is detrimental in the patch communication, possibly because\nwe should not introduce too much capacity i",
  "nction for the\nInplace-ABN, we chose to use Leaky-ReLU instead of\nResNet50\u2019s plain ReLU.\nUsing Inplace-ABN in TResNet models offers the fol-\nlowing advantages:\n\u2022 BatchNorm layers are major consumers of GPU mem-\nory. Replacing BatchNorm layers with Inplace-ABN\nenables to signi\ufb01cantly increase the maximal possible\nbatch size, which can improve the GPU utilization.\n\u2022 For TResNet models, Leaky-ReLU provides better ac-\ncuracy than plain ReLU. While some modern activa-\ntion, like Swish and Mish [28], ",
  "eng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115(3):211\u2013252, 2015. 2, 6, 8\n[50] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nattention with relative position representations. arXiv preprint\narXiv:1803.02155, 2018. 2, 3\n[51] Laurent Sifre. Rigid-motion scattering for image classi\ufb01ca-\ntion. Ph. D. thesis, 2014.",
  "0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncumulative prob.\nflops=800M\n[35.8|44.5] AnyNetXA\n[35.1|38.5] AnyNetXE\n[34.6|36.8] RegNetX\n30\n35\n40\n45\n50\n55\n60\nepochs=50\n[30.0|38.8] AnyNetXA\n[30.0|32.5] AnyNetXE\n[29.4|31.5] RegNetX\n40\n45\n50\n55\n60\n65\n70\nstages=5\n[40.4|49.9] AnyNetXA\n[38.4|42.8] AnyNetXE\n[37.9|41.4] RegNetX\n45\n50\n55\n60\n65\n70\nerror\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncumulative prob.\n[42.9|53.1] AnyNetRA\n[42.0|45.7] AnyNetRE\n[41.9|44.3] RegNetR\n50\n60\n70\n80\nerror\n[47.0|61.2] AnyNetVA\n[46.8|56.4] AnyNetVE\n[46.0|49.2] Re",
  "tion\nfrom a sequence-to-sequence perspective with transformers. In CVPR, 2021. 3\n[35] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi\nFeng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021. 3\n[36] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159,\n2020. 1\n12\n\n--------------------------------------",
  ", 7, 8\n[14] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Jun-\nyuan Xie, and Mu Li. Bag of tricks for image classi\ufb01ca-\ntion with convolutional neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2019. 7\n[15] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\nhardt, and Dawn Song. Natural adversarial examples. arXiv\npreprint arXiv:1907.07174, 2019. 8\n[16] Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon\nHan, Sangdoo Yun, Gyu",
  "-long-term-dependency-language-modeling-dataset/\n5https://github.com/pytorch/fairseq/tree/master/examples/quant noise\n\n--------------------------------------------------\nThe dataset has been obtained from the torchvision package6. The input images are augmented with a random crop of\nsize 32 with padding of 4, and a random horizontal \ufb02ip. The RGB pixel values are normalized to mean 0 and standard deviation\n1. We use the default split between train and valid as obtained from the torchvision packag",
  "-\nnamic convolutions. arXiv preprint arXiv:1901.10430, 2019.\n3\n[39] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. Lite transformer with long-short range attention. arXiv\npreprint arXiv:2004.11886, 2020. 3, 4\n[40] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-\ning Guo. Learning texture transformer network for image\nsuper-resolution. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n5791\u20135800, 2020. 2\n[41] Li Yuan, Yunpeng Chen, T",
  "an Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[6] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to\nvision transformers. arXiv preprint arXiv:2104.05707, 2021.\n[7] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv",
  ") + \u03c8(y, x)) + \u03b3(\u03c6(x, y) + \u03c6(y, x))\n= (\u03b2\u03c8 + \u03b3\u03c6)(x, y) + (\u03b2\u03c8 + \u03b3\u03c6)(y, x).\nA.2. Proof of Proposition 5\nProposition 5. Algorithm 1 converges to a local-minimum\nwith respect to the update rule at most n(n\u22121)/2+1 steps.\nProof. Let C(0) has a minimum at (i1, j1). Then, \u2200t =\n0, 1, ..., \u03a0(t)\nwin[i1, j1]\n=\n1.\nNext, let\u2019s de\ufb01ne I2\n=\n{(i, j)|i \u0338= i1, j \u0338= j1} and (i2, j2) = argmin(i,j)\u2208I2\nC(1)[i, j]. If \u03a0(0)[i2, j1] = 1, C(0)[i2, j1] will be added\nby the large value, and thus, \u03a0(1)\nwin[i2, j2] = 1. Otherwi",
  "n, 2016. 8\n[51] Ilija Radosavovic, Piotr Doll\u00e1r, Ross Girshick, Georgia\nGkioxari, and Kaiming He. Data distillation: Towards omni-\n10\n\n--------------------------------------------------\nsupervised learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2018. 1, 8\n[52] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V\nLe. Regularized evolution for image classi\ufb01er architecture\nsearch. In Proceedings of the AAAI conference on arti\ufb01cial\nintelligence, volume 3",
  "Model Size\nImage Size\nImageNet\nTraining from scratch (i.e., random initialization)\nViT384-B (Dosovitskiy et al., 2020)\n86M\n3842\n77.9\nViT384-L (Dosovitskiy et al., 2020)\n307M\n3842\n76.5\nDeiT-B (Touvron et al., 2020)\n86M\n2242\n81.8\nDeiT384-B (Touvron et al., 2020)\n86M\n3842\n83.1\nSupervised Pre-Training on ImageNet-22K (using labeled data)\nViT384-B (Dosovitskiy et al., 2020)\n86M\n3842\n84.0\nViT384-L (Dosovitskiy et al., 2020)\n307M\n3842\n85.2\nSelf-Supervised Pre-Training on ImageNet-1K (without labeled da",
  "reasonable performance on ImageNet-1K [14] alone. More importantly, when\npre-trained on large-scale weakly labeled JFT-300M dataset [15], ViT achieves comparable results\nto state-of-the-art (SOTA) ConvNets, indicating that Transformer models potentially have higher\ncapacity at scale than ConvNets.\nWhile ViT has shown impressive results with enormous JFT 300M training images, its performance\nstill falls behind ConvNets in the low data regime. For example, without extra JFT-300M pre-training,\nthe ",
  "tep\nstep\nstep\ncosine\nstep\ncosine\ncosine\ncosine\ndecay rate\n0.1\n0.1\n0.1\n0.02t/400\ndecay epochs\n30\n30\n30\n1\nWeight decay\n10\u22124\n10\u22124\n10\u22124\n0.05\n10\u22124\n0.01\n0.02\n0.02\nWarmup epochs\n\u0017\n\u0017\n\u0017\n5\n5\n5\n5\n5\nLabel smoothing \u03b5\n\u0017\n\u0017\n\u0017\n0.1\n0.1\n0.1\n\u0017\n\u0017\nDropout\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\nStoch. Depth\n\u0017\n\u0017\n\u0017\n0.1\n\u0017\n0.05\n0.05\n\u0017\nRepeated Aug\n\u0017\n\u0017\n\u0013\n\u0013\n\u0017\n\u0013\n\u0013\n\u0017\nGradient Clip.\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\nH. \ufb02ip\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\nRRC\n\u0017\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\nRand Augment\n\u0017\n\u0017\n\u0017\n9/0.5\n\u0017\n7/0.5\n7/0.5\n6/0.5\nAuto Augment\n\u0017\n\u0017\n\u0017\n\u0017\n\u0013\n\u0017\n\u0017\n\u0017\nMixup alpha\n\u0017\n\u0017\n\u0017\n0.8\n0.2\n0.2\n0.1\n0",
  "ns. Scaling local self-attention for parameter ef\ufb01cient visual backbones. CoRR,\nabs/2103.12731, 2021. 1, 2, 3, 5\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\ntion Processing Systems, 2017. 1, 2, 3, 6, 7, 8, 9\n[33] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu. Eca-net:\nEf\ufb01cient channel attention for deep convolutional neura",
  "accuracy drop, we propose an\nimproved method of progressive learning, which\nadaptively adjusts regularization (e.g. data aug-\nmentation) along with image size.\nWith progressive learning, our Ef\ufb01cientNetV2 sig-\nni\ufb01cantly outperforms previous models on Im-\nageNet and CIFAR/Cars/Flowers datasets. By\npretraining on the same ImageNet21k, our Ef\ufb01-\ncientNetV2 achieves 87.3% top-1 accuracy on\nImageNet ILSVRC2012, outperforming the re-\ncent ViT by 2.0% accuracy while training 5x-11x\nfaster using the same",
  "ontrastive learning to the fully supervised setting. Analytically and\nempirically, we show that a na\u00a8\u0131ve extension performs much worse than our proposed version.\n2\n\n--------------------------------------------------\n2. We show that our loss provides consistent boosts in top-1 accuracy for a number of datasets. It is\nalso more robust to natural corruptions.\n3. We demonstrate analytically that the gradient of our loss function encourages learning from hard\npositives and hard negatives.\n4. We show ",
  "d data from ImageNet (see Section 4), we use the\nLARS optimizer [82] with their default parameters, i.e., momentum 0.9 and learning rate 0.001, training for 20,000 steps with\na batch size of 4,096. We \ufb01netune using this optimizer instead of SGD in Noisy Student [77] because unlike Noisy Student,\nthe student model in Meta Pseudo Labels never trains directly on any labeled example, and hence can bene\ufb01t from a more\n\u201caggressive\u201d \ufb01netuning process with stronger optimiziers.\nNumerical Hyper-parameters",
  "s would like to reuse parameters de\ufb01ned in the con\ufb01guration \ufb01le such as root\ndirectory path for datasets, parameters and model names as part of checkpoint \ufb01le paths\nfor better data management. In a con\ufb01guration \ufb01le, there are three main components to\nbe de\ufb01ned: datasets, teacher and student models, and training. Each of the key compo-\nnents is de\ufb01ned by using abstracted and registered modules described in Sections 2.1\nand 2.2. A con\ufb01guration \ufb01le gives users a summary of the experiment, and shows",
  "performance gap in unnormal-\nized resnets. In 9th International Conference on Learning\nRepresentations, ICLR, 2021.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. In International conference on machine\nlearning, pp. 1597\u20131607. PMLR, 2020.\nCubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Ran-\ndaugment: Practical automated data augmentation with a\nreduced search space. In Proceedings of the IEEE/CVF\nConference on Compu",
  "--------------------------------------------\nPublished as a conference paper at ICLR 2021\nModel\nLayers\nHidden size D\nMLP size\nHeads\nParams\nViT-Base\n12\n768\n3072\n12\n86M\nViT-Large\n24\n1024\n4096\n16\n307M\nViT-Huge\n32\n1280\n5120\n16\n632M\nTable 1: Details of Vision Transformer model variants.\nWe also evaluate on the 19-task VTAB classi\ufb01cation suite (Zhai et al., 2019b). VTAB evaluates\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\nthree groups: Natura",
  " we plot the accuracy percentage at each\ntraining batch throughout the training process of a teacher and a student in Meta Pseudo Labels. We also plot the same data for\na supervised model. From the \ufb01gure, we have two observations:\n\u2022 On CIFAR-10-4K (Figure 4-Left), the student\u2019s training accuracy in Meta Pseudo Labels is much lower that of the same\nnetwork in Supervised Learning. As CIFAR-10-4K has very few labeled data, if the teacher converges quickly like in\nSupervised Learning, it will not ge",
  "proximation is:\nxn \u201c\n\u00b4\nU T\nrnsz\n\u00af\n\u02da xn\u00b41 ` xn\u00b41\n(6)\nfor n \u201c 2, . . . , N with x1 \u201c U T\nr1sz and x \u201c CxN ` \u03b2. The\nparameters C P Ro\u02c6k, Urns P Rd\u02c6k for n \u201c 1, . . . , N are\nlearnable.\nz\nU[1]\n\u2217\n+\nU[2]\nG(z)\n\u2217\n+\nU[3]\n\u2217\n+\nC\n\u03b2\nFig. 2: Schematic illustration of the CCP (for third order approxima-\ntion). Symbol \u02da refers to the Hadamard product.\nModel 2: NCP (Nested coupled CP decomposition)\nInstead of explicitly separating the interactions between layers,\nwe can utilize a joint hierarchical decomposition",
  "bution of a much deeper\nteacher model the student tries to \ufb01nd a compact solution\nof transformation. This inherently enforces the student to\nexplore more informative knowledge and generalize better.\nImpressively, in certain cases the student manages to out-\nperform its teacher due to this superior generalization.\nTraining with one-hot labels accompanying with cross-\nentropy loss is a \u201cbalanced\u201d learning system, which means\nthe objective will enforce each class to be equidistant to all\nremaining ",
  "s because in Axial-ViT models, each Transformer block with global\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\nattention and although the sequence length that self-attention operates on is smaller in axial case,\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\non TPUs (Figure 13, right).\nD.7\nATTENTION",
  " W., and\nPlumbley, M. D. Panns: Large-scale pretrained audio\nneural networks for audio pattern recognition. IEEE/ACM\nTransactions on Audio, Speech, and Language Process-\ning, 2020.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassi\ufb01cation with deep convolutional neural networks.\nAdvances in neural information processing systems, 25,\n2012.\nKumar, M. P., Ton, P., and Zisserman, A. Obj cut. In\nProceedings of IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2005.\nLan,",
  "tive extensions over past work (outlined in Sec 3.3), several of which are compared in Table 2.\nH-DeiT-S is a hierarchical version of DeiT-S obtained by simply using Asa within our hierarchical\narchitecture and provides 1.2 gain. Conv-3 (naive convolution (conv) with 3 \u00d7 3 kernel) aggregates\nspatial and channel information, where as Group Conv-3 splits input features and performs convs\nusing different kernels \u2013 it is cheaper and more effective. When group size = channel dim., we\nget depth-wise c",
  "ecay of 5e-2, 10 warmup and cooldown\nepochs. We used the same training script and therefore aug-\nmentations as the ImageNet-1k experiments. We also re-\nsized all images to 224\u00d7224.\nResults The results are presented in Table 4. We report\nresults from ResMLP, ViT and DeiT as well.\n4.5. Object Detection\nDataset MS COCO [21] is a widely-used benchmark for\nevaluating object detection model.\nIt has 118k images\nfor training and 5k images for evaluating performances\nof object detectors. We follow standa",
  "at an image as 1D sequence of\nvisual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual\nstructures and dealing with scale variance. Alternatively, they require large-scale\ntraining data and longer training schedules to learn the IB implicitly. In this\npaper, we propose a novel Vision Transformer Advanced by Exploring intrinsic\nIB from convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid\nreduction modules to downsample and embed the input image into tok",
  " of inferior\nquality. Since the weights are highly shared in the same\nsupernet, training on these weak paths does have negative\nin\ufb02uence on the evaluation of those potentially-good paths.\nTo alleviate this disturbance, an intuitive idea is to block the\ntraining of these weak paths.\nFor simplifying the analysis, we assume the search space\nA can be partitioned into two subsets Agood and Aweak by\nan Oracle good but unknown supernet No, where\nA = Agood\n[\nAweak, Agood\n\\\nAweak = \u2205,\n(4)\nand Agood indic",
  " have adopted the recent CaiT variant [57] as\na starting point. This transformer-based architecture achieves state-of performance with Imagenet-\ntraining only (achieving 86.5% top-1 accuracy on Imagenet-val for the best model). Most importantly,\nthe training is relatively stable with increasing depth.\nIn our exploration phase, our objective was to radically simplify this model. For this purpose, we\nhave considered the Cait-S24 model for faster iterations. This network consists of 24-layer with a",
  "pproaches [36,50,52,12,1,31,42,49,53] introduce auxiliary modules, which are\nused only in training session. Such auxiliary modules use tensors from intermediate\nlayers in models, and introducing the modules to the models often results in branching\ntheir feedforward path as shown in Figs. 1 and 2. This paradigm, however, is also one of\nthe backgrounds that researchers decide to hard-code the models (e.g., modify the orig-\ninal implementations of models in torchvision every time they change the pl",
  "All subnets share the weights\nof their common parts. The supernet is the largest model\nin the space, and its architecture is shown in Fig. 3. In par-\nticular, the supernet stacks the maximum number of trans-\nformer blocks with the largest embedding dimension, Q-K-\nV dimension and MLP ratio as de\ufb01ned in the space. During\ntraining, all possible subnets are uniformly sampled, and the\ncorresponding weights are updated.\nAccording to the constraints on model parameters, we\npartition the large-scale se",
  "ains a projection 1\u00d71 conv to adapt the number of feature maps and their spatial\nresolution for the summation with the output of the block). This is similar to projection shortcut\nin [26]. Therefore, for the original ResNet, the downsampling is not performed by the \ufb01rst stage (as\nthe max pooling performs this before), the next three main stages perform the downsampling on their\n\ufb01rst block. In our networks, all four main stages perform the downsampling in their \ufb01rst block.\nThis change does not in",
  " structure); (3)\nNL blocks are inserted as additional blocks into a ResNet\nbackbone as opposed to replacing existing convolutional\nblocks as done by BoTNet. Section 4.6 offers a comparison\nbetween BoTNet, NLNet as well as a NL-like version of\nBoTNet where we insert BoT blocks in the same manner as\nNL blocks instead of replacing.\n3. Method\nstage\noutput\nResNet-50\nBoTNet-50\nc1 512 \u00d7 512\n7\u00d77, 64, stride 2\n7\u00d77, 64, stride 2\nc2 256 \u00d7 256\n3\u00d73 max pool, stride 2\n3\u00d73 max pool, stride 2\n\uf8ee\n\uf8ef\uf8f0\n1\u00d71, 64\n3\u00d73, ",
  "ViTAE: Vision Transformer Advanced by Exploring\nIntrinsic Inductive Bias\nYufei Xu1\u2217\nQiming Zhang1\u2217\nJing Zhang1\nDacheng Tao2\n1The University of Sydney\n2JD Explore Academy, JD.com\nAbstract\nTransformers have shown great potential in various computer vision tasks owing to\ntheir strong capability in modeling long-range dependency using the self-attention\nmechanism. Nevertheless, vision transformers treat an image as 1D sequence of\nvisual tokens, lacking an intrinsic inductive bias (IB) in modeling lo",
  "e study the training bottlenecks of Ef\ufb01cient-\nNet (Tan & Le, 2019a), and introduce our training-aware\nNAS and scaling, as well as Ef\ufb01cientNetV2 models.\n3.1. Review of Ef\ufb01cientNet\nEf\ufb01cientNet (Tan & Le, 2019a) is a family of models that are\noptimized for FLOPs and parameter ef\ufb01ciency. It leverages\nNAS to search for the baseline Ef\ufb01cientNet-B0 that has\nbetter trade-off on accuracy and FLOPs. The baseline model\nis then scaled up with a compound scaling strategy to obtain\na family of models B1-B7. W",
  "-------------------------------------------------\nTable 1: Model details of two variants of ViTAE.\nModel\nReduction Cell\nNormal Cell\nParams Macs\nDilation\nCells Heads Embed Cells\n(M)\n(G)\nViTAE-T [1, 2, 3, 4] \u2193\n3\n4\n256\n7\n4.8\n1.5\nViTAE-S [1, 2, 3, 4] \u2193\n3\n6\n384\n14\n23.6\n5.6\nWe use two variants of ViTAE in\nour experiments for a fair com-\nparison of other models with sim-\nilar model sizes. The details of\nthem are summarized in Table 1.\nIn the \ufb01rst RC, the default convo-\nlution kernel size is 7 \u00d7 7 with ",
  "1\n2\n3\n0\n1\nFigure 2: Overview of the proposed Hierarchical Visual Transformer. To reduce the redundancy in the full-length patch\nsequence and construct a hierarchical representation, we propose to progressively pool visual tokens to shrink the sequence\nlength. To this end, we partition the ViT [11] blocks into several stages. At each stage, we insert a pooling layer after the \ufb01rst\nTransformer block to perform down-sampling. In addition to the pooling layer, we perform predictions using the result",
  "eprint arXiv:1906.01787,\n2019a.\nWang, S., Li, B., Khabsa, M., Fang, H., and Ma, H. Lin-\nformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768, 2020.\nWang, Y., Sun, Y., Liu, Z., Sarma, S. E., Bronstein, M. M.,\nand Solomon, J. M. Dynamic graph cnn for learning\non point clouds. Acm Transactions On Graphics (tog),\n38(5):1\u201312, 2019b.\nWu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang,\nX., and Xiao, J. 3d shapenets: A deep representation\nfor volumetric shapes. In Proceeding",
  "Jiashi Feng, and S. Yan.\nTokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021. 3\n[55] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and\nYoungjoon Yoo. CutMix: Regularization strategy to train strong classi\ufb01ers with localizable\nfeatures. arXiv preprint arXiv:1905.04899, 2019. 3, 5, 18\n[56] Hongyi Zhang, Moustapha Ciss\u00b4e, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond\nempirical risk minimization. arXiv prepri",
  "to map an input array (inter-\npreted as a set) to a low-dimensional array and immediately\nmap it back to the input space. Stacking these blocks leads\nto an architecture that scales linearly in compute/memory\nwith input size like the Perceiver\u2019s cross-attention module,\nbut without the advantage of the Perceiver\u2019s latent array\n(which completely decouples the cost of the latent Trans-\nformer from the input size): a fully-ISAB model scales as\nO(LMN), rather than O(MN + LN 2), like the Perceiver\n(whe",
  "top-1 accuracy, surpassing the human-\ndesigned hybrid CNN-transformer, BoTNet50, by 2.2%\nwhile being 1.19\u00d7 faster in terms of compute time; when\nequipped with SE and SiLU activation, BossNet-T0 fur-\nther achieves 80.8% top-1 accuracy, surpassing the NAS\nsearched Ef\ufb01cientNet-B1 by 1.7% while being 1.14\u00d7 faster.\nSecondly, our searched model demonstrates absolute su-\nperiority over manually and randomly selected models from\nsearch space HyTra. In particular, BossNet-T0 achieves\nup to 6.0% improveme",
  "ConViT: Improving Vision Transformers\nwith Soft Convolutional Inductive Biases\nSt\u00b4ephane d\u2019Ascoli 1 2 Hugo Touvron 2 Matthew L. Leavitt 2 Ari S. Morcos 2 Giulio Biroli 1 2 Levent Sagun 2\nAbstract\nConvolutional architectures have proven ex-\ntremely successful for vision tasks. Their hard\ninductive biases enable sample-ef\ufb01cient learning,\nbut come at the cost of a potentially lower perfor-\nmance ceiling. Vision Transformers (ViTs) rely\non more \ufb02exible self-attention layers, and have\nrecently outper",
  "e show Batch Prioritized Routing versus vanilla routing.\n0.01\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ncapacity C\n10\n15\n20\n25\n30\n35\nImageNet/1shot\n0.01\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ncapacity C\n15\n20\n25\n30\n35\n40\n45\n50\nImageNet/5shot\n0.01\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ncapacity C\n20\n25\n30\n35\n40\n45\n50\nImageNet/10shot\n0.01\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\ncapacity C\n5\n10\n15\n20\n25\n30\n35\nJFT validation prec@1\nDense VIT-S/32\nV-MoE-S/32 (BPR)\nV-MoE-S/32 (Vanilla)\nFigure 19: Infe",
  "uence length n and dimension d. Each block is de\ufb01ned as:\nZ = \u03c3(XU),\n\u02dcZ = s(Z),\nY = \u02dcZV\n(1)\nwhere \u03c3 is an activation function such as GeLU [24]. U and V de\ufb01ne linear projections along the\nchannel dimension\u2014the same as those in the FFNs of Transformers (e.g., their shapes are 768\u00d7 3072\nand 3072\u00d7 768 for BERTbase). Shortcuts, normalizations and biases are omitted for brevity.\nA key ingredient in the aforementioned formulation is s(\u00b7), a layer which captures spatial interactions\n(see below). When s ",
  "]\n22\n5.2\n-\n81.5\nTNT-S [15]\n23.8\n5.2\n-\n81.3\nCoaT Mini [17]\n10\n6.8\n-\n80.8\nCoaT-Lite Small [17]\n20\n4.0\n-\n81.9\nPVT-Small [8]\n24.5\n3.8\n820\n79.8\nCPVT-Small-GAP [9]\n23\n4.6\n817\n81.5\nTwins-PCPVT-S (ours)\n24.1\n3.8\n815\n81.2 (+1.3)\nSwin-T [4]\n29\n4.5\n766\n81.3\nSwin-T + CPVT\u2020\n28\n4.4\n766\n81.2\nTwins-SVT-S (ours)\n24\n2.9\n1059\n81.7 (+1.8)\nT2T-ViT-19 [27]\n39.2\n8.9\n-\n81.9\nPVT-Medium [8]\n44.2\n6.7\n526\n81.2\nTwins-PCPVT-B(ours)\n43.8\n6.7\n525\n82.7 (+0.8)\nSwin-S [4]\n50\n8.7\n444\n83.0\nTwins-SVT-B (ours)\n56\n8.6\n469\n83.2 (+1.3)\n",
  "context,\u201d in ECCV, 2014.\n[50]\nW. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back,\nP. Natsev, et al., \u201cThe kinetics human action video dataset,\u201d ArXiv:1705.06950, 2017.\n[51]\nK. He, X. Zhang, S. Ren, and J. Sun, \u201cDelving deep into recti\ufb01ers: Surpassing human-level performance\non imagenet classi\ufb01cation,\u201d in ICCV, 2015.\n[52]\nG. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, \u201cImproving neural\nnetworks by preventing",
  "CNN-transformers [62] at different scales.\n5\n\n--------------------------------------------------\nMethod\nMAdds\nSteptime\nTop-1 (%)\nTop-5 (%)\nResNet50 [25]\n4.1B\n100ms\n77.7\n93.9\nViT-B/32 [20]\n-\n68ms\n73.4\n-\nViT-B/16 [20]\n17.6B\n158ms\n77.9\n-\nBoT50 [62]\n4.0B\n120ms\n78.3\n94.2\nR50-T\nConv-Only\n4.1B\n104ms\n78.2\n94.2\nViT-T/32\nAtt-Only\n2.9B\n92ms\n74.5\n91.7\nViT-T/16\nAtt-Only\n3.2B\n96ms\n76.5\n93.0\nBoT50-T\nHybrid\n3.9B\n103ms\n79.5\n94.8\nRandom-T Hybrid\n3.7B\n84ms\n76.7\n93.1\nBossNet-T0 w/o SE\n3.4B\n101ms\n80.5\n95.0\nSENet50 [",
  ".\narXiv preprint\narXiv:2010.11929, 2020.\n[8] Xavier Glorot and Yoshua Bengio. Understanding the dif\ufb01-\nculty of training deep feedforward neural networks. In Pro-\nceedings of the thirteenth international conference on arti\ufb01-\ncial intelligence and statistics, pages 249\u2013256. JMLR Work-\nshop and Conference Proceedings, 2010.\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nr",
  "ince the origi-\nnal methods do not converge: we have re-introduced the Layer-normalization \u03b7 and\nwarmup. We have adapted the drop rate dr for all the methods, including the baseline.\nThe column \u03b1 = \u03b5 reports the performance when initializing the scalar with the same\nvalue as for LayerScale. \u2020: failed before the end of the training.\ndepth\nbaseline\nscalar \u03b1 weighting\nLayerScale\ndr = 0.05\nadjust [dr]\nRezero\nT-Fixup\nFixup\n\u03b1 = \u03b5\n12\n79.9\n79.9 [0.05]\n78.3\n79.4\n80.7\n80.4\n80.5\n18\n80.1\n80.7 [0.10]\n80.1\n81",
  " feature of experts with the predicted\ngate-values of Router. These methods aim at conditionally selecting a speci\ufb01c layer or block.\nDifferently, our method can take more advantage of conditional computation as the selection of whole\nnetwork making every parameter input-dependent. The recently proposed Dynamic Convolution\nmethods [11, 12, 23] share the same idea and achieve remarkable performance with low FLOPs but\nhigh latency. It is because these methods need to load many basic models or exper",
  "Dynamic Routing module fails to abstract many prime\ncapsules into a small amount of latent capsules. On the\ncontrast, our centorid transformer can aggregate informa-\ntion no matter the output length is short or long.\nFurther, our centroid attention block has a much smaller\nparameter size as well as a larger max batch size per GPU.\nTjos means we can process more data in the same time\nand lead a much faster training speed comparing with the\ncapsule network using Dynamic Routing. This roughly\nleads",
  " stands for knowledge distillation [16].\nMethods\nTop-1\n(%)\nFLOPs\n(G)\nBatch Time\n(ms)\nResNet-50\u2217\n78.7\n4.1\n34.2\nDeiT-S\u2217\n80.1\n4.6\n36.9\nRegNetY-4GF\u2217\n80.0\n4.0\n40.2\nEf\ufb01cientNet-B3 [33]\n81.6\n1.8\n48.3\nVisformer-S (ours)\n82.2\n4.9\n36.7\nTable 8. Comparison of inference ef\ufb01ciency among Visformer-S\nand other models. A batch size of 32 is used for testing. Besides\nEf\ufb01cientNet-B3, other models are trained using the elite setting.\n4.2. Training with limited data\nLast but not least, we evaluate the performance o",
  "e dotted red line is at the target accuracy of 75.9%,\nand the boxes show the min, max, and quartiles of the distribution of accuracies over the 50 training\nruns.\nwhose median over 50 seeds continues to beat the target 75.9% accuracy (noted by the dotted red\nline) is \u201cBN & LR tuning\u201d, with the rest having between 0.1%-0.3% drops in median accuracy.\nD\nHyperparameter tuning\nD.1\nNesterov momentum training speed on ResNet-50\nWe considered two con\ufb01gurations of Nesterov hyperparameters: Con\ufb01guration A,",
  "ig. 12 compares the mIoU-MAdds trade-off obtained by\nNAT and the original BiSeNet [75] on the Cityscapes dataset.\nEmpirically, we observe that NAT based backbones consistently\noutperform the original BiSeNets, which are based on ResNets. To\nrealize the full potential of the searched NATNets, we further \ufb01ne-\ntune the obtained models for 4K iterations. As shown in Table 5,\nthe resulting NAT model yields comparable performance against\nstate-of-the-art methods, including PSPNet [76], DeepLabv3 [77],",
  "raining + ImageNet-1K \ufb01netuning\nset up. \u201cPT-RA\u201d denotes applying RandAugment during 21K pre-training and \u201cE150\u201d means 150\nepochs of pre-training, which is longer than the standard 90 epochs.\nModels\nEval Size\n#Params\n#FLOPs\nTop-1 Accuracy\nConv Only\nENetV2-S\n3842\n24M\n8.8B\n85.0\nENetV2-M\n4802\n55M\n24B\n86.1\nENetV2-L\n4802\n121M\n53B\n86.8\nViT-Stem TFM Only\nViT-B/16\n3842\n87M\n55.4B\n84.6\nViT-L/16\n3842\n304M\n190.7B\n85.3\nMulti-Stage TFM Only\nHaloNet-H4\n3842\n85M\n-\n85.6\nHaloNet-H4\n5122\n85M\n-\n85.8\nSwin-B\n3842\n88M\n",
  "perts in the last layers.\nWe would like to remark that each image is subject to a large number of routing decisions through\nits patches. Concretely, Figure 32 shows how most images use \u2013on aggregate by pooling over all\ntheir patches\u2013 most of the experts in every layer. This motivated our efforts to try to save compute by\ndiscarding, or not processing, patches that are not useful for the \ufb01nal classi\ufb01cation. We cover this in\ndetail in Section 4.\n35\n\n------------------------------------------------",
  "wing.\nminimize\n\u03a00,\u03a01\u2208{0,1}n\u00d7n \u2212\u2225(1 \u2212z\u2217) \u2299\u03a0\u22ba\n0s(x0)\u22251\n\u2212\u2225(z\u2217\u2299\u03a0\u22ba\n1s(x1)\u22251\n+ \u03be\nX\nk=0,1\n\u27e8\u03a0k, C\u27e9\nsubject to \u03a0k1n = 1n, \u03a0\u22ba\nk1n = 1n\nfor k = 0, 1.\nNote the problem is completely separable as two inde-\npendent optimization problems of each \u03a0k. Let s(x1)i\n\n--------------------------------------------------\nPuzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup\nAlgorithm 1 Masked Transport\nInput: mask z\u2217, cost C\u2032, large value v\nInitialize C(0) = C\u2032, t = 0\nrepeat\ntarget = argmin(C(t), dim = ",
  "ve loss. Models are all trained without RandAugment.\nMethod\nTop-1 Accuracy\nCE\n48.4\nmulti-task (CE+Re-weighting)\n49.0\nmulti-task (CE+Blance Softmax)\n48.6\nPaCo\n51.0\nE. Implementation details for Table 1\nWe train models with cross-entropy, parametric contrastive loss 400 epochs without RandAugment respectively. For\nsupervised contrastive loss, following the original paper, we \ufb01rstly train the model 400 epochs. Then we \ufb01x the backbone\nand train a linear classi\ufb01er 400 epochs.\nF. Ablation Study\nRe-wei",
  " by vote. However, it requires\nn forward passes, leading to a signi\ufb01cant runtime cost. MIMO [5] draws inspiration from model\nsparsity [6] and tries to ensemble several subnetworks within one regular network. It only needs one\nsingle forward pass of the regular network but is incompatible with compact models. Conditional\ncomputation methods [7, 8, 9, 10] alleviates this problem via delegation scheme, i.e. assigning one\nor several, rather than all, models conditionally to make the prediction. Some",
  " a 1\u00d71\nconv lacks the spatial resolution for performing downsampling, we maintain a max pooling on the\nprojection shortcut in the \ufb01rst block of each following stages. Our \ufb01nal network result is 22.12 top-1\nerror requiring only 24.85 million parameters and 3.88 GFLOPs. The conclusion is that regardless of\nthe settings of PyConv, using our formulation, it consistently provides better results than the baseline.\nFig. 7 shows the training and validation curves for comparing our networks, PyConvResNet",
  "n that only\nchanges the shape speci\ufb01cation of tensors but not the or-\nder of data in memory, which is cost-free. The input is\n\ufb01rst \ufb02attened into N vectors of length CHW, which is\nV(in) = RS(M(in), (N, CHW)), multiplied by the kernel\nW(OHW, CHW), then the output V(out)(N, OHW) is re-\nshaped back into M(out)(N, O, H, W). For the better read-\n\n--------------------------------------------------\nability, we omit the RS if there is no ambiguity,\nM(out) = MMUL(M(in), W) .\n(3)\nSuch an FC cannot take adv",
  "mkv). Unlike linear attention mechanisms, lambda\nlayers have quadratic time complexity with respect to the input length (in the global context case)\nbecause they consider position-based interactions.\n22\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\n2) Lambda layers do not necessarily attempt to approximate an attention kernel.\nWhile ap-\nproximations of the attention kernel are theoretically motivated, we argue that they may be unnec-\nessarily re",
  "en. Vision transformers are robust learners. arXiv preprint arXiv:2105.07581,\n2021.\n[34] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv\u00e9 J\u00e9gou, and\nMatthijs Douze. Levit: a vision transformer in convnet\u2019s clothing for faster inference. arXiv preprint\narXiv:2104.01136, 2021.\n[35] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv preprint arXiv:2103.00112, 2021.\n[36] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, ",
  "iki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jon Shlens. Stand-alone self-\nattention in vision models. In NeurIPS, pages 68\u201380, 2019.\n2\n[30] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll\u00b4ar. Designing network design\nspaces, 2020. 6, 7\n[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael S. Bernstein, Alexander C. Berg,\nand Fei-Fei Li.",
  "n select a subset of tokens from the\noriginal sequence, while the emphasis of our work is\nsummarize the information into several centroids. This\nleads to completely distinct structure design. (3) Their\nscoring function is human-designed. In contrast, we start\nfrom clustering algorithm, and derives a novel connection\nbetween gradient-based clustering and attention.\nCapsule Networks\nSimilar to our method, capsule net-\nworks (Hinton et al., 2011) are also based on the idea\nof \u201cbuilding clustering a",
  "ko & Komodakis, 2016). We set m = 10\nfor WideResNet and m = 4 for the other models. We use\nthe public code2 and keep their hyper-parameters.\nImplementation Details\nFor CIFAR-10 and CIFAR-\n100, we use the standard data processing pipeline (mirror+\ncrop) and train the model with 200 epochs. All the results\nreported in this section are averaged over \ufb01ve runs.\nWe train the models for 200 epochs on the training\nset with 256 examples per mini-batch, and evaluate\nthe trained models on the test set.\nThe",
  "geNet\nclassi\ufb01cation with a top-1 accuracy of 85.8%.\nKeywords: AutoML, AutoAugment, Knowledge Distillation\n1\nIntroduction\nAutomated machine learning (AutoML) has been attracting increasing atten-\ntions in recent years. In standard image classi\ufb01cation tasks, there are mainly\ntwo categories of AutoML techniques, namely, neural architecture search (NAS)\nand hyper-parameter optimization (HPO), both of which focus on the possibility\nof using automatically learned strategies to replace human expertise.",
  " on ImageNet-21k. Our hyper-parameters are\nmostly adopted from Touvron et al. [56, 57].\nHyper-parameter settings. In the case of supervised learning, we train our network with the Lamb\noptimizer [64] with a learning rate of 5 \u00d7 10\u22123 and weight decay 0.2. We initialize the LayerScale\nparameters as a function of the depth by following CaiT [57]. The rest of the hyper-parameters follow\nthe default setting used in DeiT [56]. For the knowledge distillation paradigm, we use the same\nRegNety-16GF [48] ",
  "ur T2T-ViT backbone. Speci\ufb01cally, it has a\nsmall channel number and a hidden dimension d but more\nlayers b. For tokens with \ufb01xed length Tf from the last layer\nof T2T module, we concatenate a class token to it and then\nadd Sinusoidal Position Embedding (PE) to it, the same as\nViT to do classi\ufb01cation:\nTf0 = [tcls; Tf] + E,\nE \u2208R(l+1)\u00d7d\nTfi = MLP(MSA(Tfi\u22121)),\ni = 1...b\ny = fc(LN(Tfb))\n(5)\n\n--------------------------------------------------\nFixed Tokens\nImage\n224 x 224\n7\n7\nTransformer  layer\n+ PE\ncls",
  "ilar to Linformer, but uses a convolution layer with ker-\nnel size R and stride R to project the key-value pairs, hence\nresulting in nl/R2 compressed key-value pairs. Therefore,\nThe memory complexity of SRA is O(n2\nl /R2), which is\nstill quadratic w.r.t. nl but with a much smaller constant\n1/R2. When transferring the ImageNet-pretrained SRA-\nmodels to high-resolution tasks, SRA still suffers from the\nquartic computation/memory blow-up w.r.t. the feature map\nresolution. Pyramid Vision Transformer",
  "out-\nput complexity. So, in deep layers, concentrated interaction\nmight be enough. There are signi\ufb01cant differences between\nthe vision and the language domain, and we believe that the\nattention of PiT is suitable for image recognition backbone.\n3.4. Architecture design\nThe architectures proposed in ViT paper [9] aimed at\ndatasets larger than ImageNet. These architectures (ViT-\nLarge, ViT-Huge) have an extremely large scale than gen-\neral ImageNet networks, so it is not easy to compare them\nwith ",
  " properties of\nconvolution into the Transformer backbone [25, 24, 23, 22, 21, 43, 44].\nWhile our work also belongs to this category, we show that our relative attention instantiation is a\nnatural mixture of depthwise convolution and content-based attention with minimum additional cost.\nMore importantly, starting from the perspectives of generalization and model capacity, we take a\nsystematic approach to the vertical layout design and show how and why different network stages\nprefer different typ",
  "rXiv\npreprint arXiv:1409.1556, 2014.\n[31] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani.\nBottle-\nneck transformers for visual recognition.\narXiv preprint\narXiv:2101.11605, 2021.\n[32] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy,\nand Cordelia Schmid. Videobert: A joint model for video\nand language representation learning. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 7464\u20137473, 2019.\n[33] Christian Szeged",
  ". arXiv preprint arXiv:2101.11605, 2021.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\nand Salakhutdinov, R. Dropout: a simple way to prevent\n\n--------------------------------------------------\nHigh-Performance Normalizer-Free ResNets\nneural networks from over\ufb01tting. The Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\nSrivastava, R. K., Greff, K., and Schmidhuber, J. Highway\nnetworks. arXiv preprint arXiv:1505.00387, 2015.\nSummers, C. and Dinneen, M. J. Four things ever",
  " image-text pairs as positive and all other random\nimage-text pairs that can be formed in a training batch as\nnegative.\nWe minimize the sum of two losses: one for image-to-text\nclassi\ufb01cation\nLi2t = \u22121\nN\nN\nX\ni\nlog\nexp(x\u22a4\ni yi/\u03c3)\nPN\nj=1 exp(x\u22a4\ni yj/\u03c3)\n(1)\nand the other for text-to-image classi\ufb01cation\nLt2i = \u22121\nN\nN\nX\ni\nlog\nexp(y\u22a4\ni xi/\u03c3)\nPN\nj=1 exp(y\u22a4\ni xj/\u03c3)\n(2)\nHere, xi and yj are the normalized embedding of image in\nthe i-th pair and that of text in the j-th pair, respectively. N\nis the batch si",
  "stributed computing. They were proved both ef\ufb01cient and effective in speeding up the\nnetworks. As an extreme case, depthwise convolutions [12, 36] use the number of groups that is\n2\n\n--------------------------------------------------\nequal to the input or output channels, which is followed by point-wise convolutions to aggregate the\ninformation across different channels. Here, the proposed spatially separable self-attention shares\nsome similarities with them.\nPositional Encodings. Most vision tr",
  " sparsity-based ef\ufb01cient at-\ntentions that are developed for image generation. The sec-\nond is the memory-based mechanism, including Compres-\nsive Transformers [32] and Set Transformer [20]. These\nmodels use some extra global tokens as static memory and\nallow all the other tokens to attend only to those global\ntokens. The third is the low-rank based mechanism. For\nexample the Linformer [46] projects the input key-value\npairs into a smaller chunk, and performs cross-attention be-\ntween the querie",
  "nd 194M respectively. Compared with OFA [22],\nour method reduces the FLOPs from 230M to 100M and from 595M to 194M respectively, with\nbetter top-1 accuracy. Compared with Ef\ufb01cientNet-B1 with noisy student training [35], our method\nalso reduces the FLOPs by 3.6x while improving the accuracy by 0.5%. Though dynamic networks\nlike GFNet [37], CondConv [12] and BasisNet [7] are more ef\ufb01cient than traditional networks, our\nmethod still has signi\ufb01cantly higher accuracy with smaller FLOPs. Compared with",
  " similar principles than\nthe one used by Yan et al. [28] but imposing a constrain that\nensures balanced clusters. Concretely, given a set of classes\nYc = {y1, y2, . . . yc} we de\ufb01ne Yk as a partition of Yc into\nk subsets. The grouping starts by constructing a distance\nmatrix Dc, based on the confusion matrix from a pre-trained\nmodel. Given a normalized confusion matrix F with the di-\nagonal set to zero, the distance matrix Dc is constructed by\nsubtracting 1 from it and then making it symmetric b",
  "ng [69] with the\ninput tokens. We generate them in 64 dimensions from the 2d patch coordinates and then\nlinearly project to the transformer working dimension d. This choice is orthogonal to the use\nof learned positional encoding, as in ViT [22]. However, it is more \ufb02exible since there is no\nneed to interpolate or \ufb01ne-tune the network when changing the image size.\nModel con\ufb01gurations.\nIn Table 1 we list different variants of our model which we use in\nour experiments, with different choices for mo",
  ". We compare BEIT with vision Transformers\ntrained by random initialization, supervised pre-training, and previous self-supervised learning\nmethods. All the compared models are base-size, except iGPT has 1.36B parameters. Pre-training is\nconducted on ImageNet for the comparison purpose, except ViT-JFT300M is pretrained on Google\u2019s\nin-house 300M images.\nCompared with the models trained by random initialization, we \ufb01nd that pre-trained BEIT signif-\nicantly improves performance on both datasets. No",
  "n Table D.4 one can see there is a large bene\ufb01t in employing a patch based method\nalong with our XCiT transformers: XCiT-VLAD performs signi\ufb01cantly better than the CLS\ntoken, likely thanks to the higher dimensionality. This is further magni\ufb01ed with AMSK,\nwhere we obtain results approaching the absolute state of the art on Holidays, despite a sub-\noptimal training setting for image retrieval. This is interesting since our method has not been\n\ufb01ne-tuned for retrieval tasks and we have not been adap",
  "JFT-300M [54] can achieve better\naccuracies without the need for regularization. To under-\nstand the transfer properties of HaloNet models, we scale up\nHaloNet-H4 by increasing the base width to 128 and evalu-\nate the transfer protocol from [29], pretraining on the public\nImageNet-21k dataset, and \ufb01netuning on ImageNet. Follow-\ning our observation in Table 4, we train a hybrid version of\nthis model with convolutions in the \ufb01rst two stages. Note\nthat our hybrids can be seen as using a series of c",
  "oral axis, similar as done in previous work [39,89,58],\nfollowed by application of frame-wise RoIAlign [43] and\ntemporal global average pooling. The RoI features are then\nmax-pooled and fed to a per-class, sigmoid classi\ufb01er for\nprediction.\nTraining. We initialize the network weights from the Ki-\nnetics models and adopt synchronized SGD training on 64\nGPUs. We use 8 clips per GPU as the mini-batch size and a\nhalf-period cosine schedule of learning rate decaying. The\nbase learning rate is set as 0",
  "ated in multiple\nworks [24, 39] and a head width of 64 is recommended for visual tasks [10, 35]. We adopt the head\nwidth of 64 in outer transformer block in our model. The number of heads in inner transformer block\n7\n\n--------------------------------------------------\nis another hyper-parameter for investigation. We evaluate the effect of #heads in inner transformer\nblock (Table 7). We can see that a proper number of heads (e.g., 2 or 4) achieve the best performance.\nTable 7: Effect of #heads in",
  "tandard training hyperparameters (see Table 7). We then\ncompare the outcome to two other recently proposed ef\ufb01cient\nmodels, MobileNetV3 [22] and FBNetV2 [32]. The results are\nsummarized in Table 8, where we observe that the NAT searched\n\n--------------------------------------------------\n15\nFig. 17: Comparing the performance of adapting supernet, adapting subnet and additional \ufb01ne-tuning under a bi-objective search setup on four datasets. Details\nare provided in Section 5.5.\nmodel, NAT-M1, is 0.",
  "ral Information Processing Systems,\nvolume 32. Curran Associates, Inc., 2019.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao,\nSonghao Piao, Ming Zhou, and Hsiao-Wuen Hon. UniLMv2: Pseudo-masked language models\nfor uni\ufb01ed language model pre-training. In Proceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of\nMachine Learning Research, pages 642\u2013652. PMLR, 2020. URL http",
  "ses signi\ufb01cantly. Right: The bene\ufb01-\ncial effect of locality is particularly strong in the early epochs.\nof the convolutional initialization ([a], +3.1), and is un-\nhelpful otherwise. These mild improvements due to gating\nand convolutional initialization (likely due to performance\nsaturation above 80% top-1) become much clearer in the\nlow data regime. Here, GPSA alone brings +6.8, with an\nextra +2.3 coming from gating, +2.8 from convolution ini-\ntialization and +5.1 with the two together, illustr",
  " images\nare randomly resized and cropped to 512 \u00d7 512.\nResults All experimental results on ADE20K are presented\nin Figure 3. Similar to the results of object detection, it\nshows that visual representations learned by ConvMLP can\nbe transferred to pixel-level prediction task like semantic\nsegmentation. More details of the results can be found in\nAppendix.\n4.7. Visualization\nWe\nvisualize\nfeature\nmaps\nof\nResNet50,\nMLP-\nMixer-B/16, Pure-MLP Baseline and ConvMLP-M under\n(1024, 1024) input size (MLP-M",
  "sover operator on four datasets; ImageNet [1], CIFAR-10 [9],\nOxford Flowers102 [19], and Stanford Cars [15]. The hyperpa-\nrameters that we compare are:\n1)\nw/ crx: crossover probability of 0.9; mutation probability\nof 0.1; mutation index \u03b7m of 3.\n2)\nw/o crx: crossover probability of 0.0; mutation probability\nof 0.2; mutation index \u03b7m of 3.\nWe double the mutation probability when crossover is not used to\ncompensate for the reduced exploration ability of the search. On\neach dataset, we run each set",
  "etV3 [13], and MixNet [42]. In terms of accuracy and\nef\ufb01ciency, MUXNets are on par with current state-of-the-art\nmodels, i.e. MobileNetV3 and MixNet.\nIn terms of latency, the performance of MUXNet models\nis mixed since they, (i) use non-standard primitives that do\nnot have readily available ef\ufb01cient low-level implementa-\ntions, and (ii) are not explicitly optimized for latency. Com-\npared to methods that use optimized convolutional prim-\nitives but do not directly optimize for latency (Ef\ufb01cient-",
  "olutional\nToken Embedding. Conv. Proj.: Convolutional Projection. Hi and Di is the number of heads and embedding feature\ndimension in the ith MHSA module. Ri is the feature dimension expansion ratio in the ith MLP layer.\nstrate that the fusion of local neighboring information plays\nan important role on the performance.\n4. Experiments\nIn this section, we evaluate the CvT model on large-scale\nimage classi\ufb01cation datasets and transfer to various down-\nstream datasets. In addition, we perform throug",
  "\n86.56\nRe\ufb01ned-ViT-448\n1.000\n85.94\n1.130\n85.98\n4.5\nApplied to NLP tasks\nWe also evaluate the performance of Re\ufb01ner-ViT models for natural language processing tasks on the\nGLUE benchmark, to investigate whether Re\ufb01ner also improves other transformer-based models. We\nuse the BERT-small [11] as the baseline model and replace the self-attention module with re\ufb01ner,\nusing the same pre-training dataset and recipes. From the results in Tab. 6, Re\ufb01ner boosts the model\nperformance across all the tasks sign",
  "magnitude gradients\nmake the optimization problem simpler by allowing for larger learning rates. In Section\n3.3 of the paper, it is shown that in the case of a single positive and negative, the contrastive\nloss is equivalent to a triplet loss with margin \u221d\u03c4. Therefore, in these cases, a larger\ntemperature makes the optimization easier, and classes more separated.\n2. Hard positives/negatives: On the other hand, as shown in Sec 7, the supervised contrastive\nloss has structure that cause hard posit",
  "k 2. We suppose pl, ph \u2208(0, 0.71) and pl < ph. To\nachieve the optimal value, when Psup=p, the supervised con-\ntrastive loss value Lsupcon decreases as shown in Eq. (7).\nHere Psup increases from pl to ph, Lsupcon decreases to\na much smaller loss value to achieve the optimal solution,\nwhich implies the need to make two different class samples\nLsupcon = \u2212\nX\nz+\u2208P (i)\nlog\nexp(z+ \u00b7 G(xi))\nP\nzk\u2208A(i) exp(zk \u00b7 G(xi))\n= \u2212\nX\nz+\u2208P (i)\nlog\nexp(z+\u00b7G(xi))\nExpSum\nP\nzk\u2208A(i) exp(zk\u00b7G(xi))\nExpSum\n= \u2212K\u2217log 0.035\n1 ",
  "Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.\nImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.\nLavanya Sharan, Ruth Rosenholtz, and Edward H. Adelson. Accuracy and speed of material cate-\ngorization in real-world images. Journal of Vision, 14(10), 2014.\nBaifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, and Jingdong Wang. Informative\ndropout f",
  "nd\nCrossViT-15\u2020 incur 30-50% more FLOPs and parameters\nthan the baselines. However, their accuracy is considerably\nimproved by \u223c2.5-5%. On the other hand, CrossViT-18\u2020\nreduces the FLOPs and parameters almost by half compared\nto DeiT-B while still being 1.0% more accurate.\nComparisons with SOTA Transformers. We further com-\npare our proposed approach with some very recent concur-\nrent works on vision transformers. They all improve the\noriginal ViT [11] with respect to ef\ufb01ciency, accuracy or\nboth.",
  "m\nImageNet (we include additional results in the Appendix).\nEach attention map shows the output of the QKT opera-\ntion at each of the model\u2019s 512 latent indices and each input\npixel. We visualize attention maps before the softmax, as the\n\n--------------------------------------------------\nPerceiver: General Perception with Iterative Attention\nModel / Inputs\nAudio\nVideo\nA+V\nBenchmark (Gemmeke et al., 2017)\n31.4\n-\n-\nAttention (Kong et al., 2018)\n32.7\n-\n-\nMulti-level Attention (Yu et al., 2018)\n36.",
  " correct prediction,\nand the ImageNet model fails. For example in Fig. 7(e), we observe that the network trained on\nImageNet alone is not focusing on the whole bird and is only focusing on the body to make the\ndecision; whereas the one trained with Anisotropic ImageNet is focusing on complete bird to make\na decision.\nWe include more saliency maps on Sketch-ImageNet, and cases where ImageNet trained models are\ncorrect and our model fails in the supplementary material. We show more analysis about ",
  "erleaving local and global attention can produce impressive\nresults, yet it comes with higher throughputs. Both transformer models set a new state of the art in\nimage classi\ufb01cation, objection detection and semantic/instance segmentation.\n9\n\n--------------------------------------------------\nReferences\n[1] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, an",
  "3], etc. CNNs are\nbased on locality in that convolutional \ufb01lters only perceive\na local region of the input image, i.e. the receptive \ufb01eld. By\nstacking multiple layers, the effective receptive \ufb01elds of a\ndeep neural network can be enlarged progressively. This\ndesign enables the network to learn a hierarchy of deep fea-\nFigure 1: Comparison between LocalViT and the baseline\ntransformers. The transformers enhanced by the proposed\nlocality mechanism outperform their baselines.\ntures, which is essent",
  " since the success of\nAlexNet in 2012 (Krizhevsky et al., 2017), epitomizes this\ntrend. Inductive biases are hard-coded into the architectural\nstructure of CNNs in the form of two strong constraints\non the weights: locality and weight sharing. By encourag-\ning translation equivariance (without pooling layers) and\ntranslation invariance (with pooling layers) (Scherer et al.,\n2010; Schmidhuber, 2015; Goodfellow et al., 2016), the\nconvolutional inductive bias makes models more sample-\nef\ufb01cient and ",
  "mer for visual tasks.\nTo utilize the transformer architectures for conducting visual tasks, a number of researchers have\nexplored for representing the sequence information from different data. For example, Wang et al. ex-\nplore self-attention mechanism in non-local networks [41] for capturing long-range dependencies\nin video and image recognition. Carion et al. present DETR [3], which treats object detection\nas a direct set prediction problem and solve it using a transformer encoder-decoder arch",
  "w-parameter, low-memory models without af-\nfecting accuracy. In Figure 13, we test RegNetX with theses\nconstraints and observe that the constrained version is supe-\nrior across all \ufb02op regimes. We use this version in \u00a75, and\nfurther limit depth to 12 \u2264d \u226428 (see also Appendix D).\nAlternate design choices. Modern mobile networks often\nemploy the inverted bottleneck (b < 1) proposed in [25]\nalong with depthwise conv [1] (g = 1). In Figure 14 (left),\nwe observe that the inverted bottleneck degrades",
  "ation\nresults. The following describes the details of the experiments, results and analysis.\nProgressive sparsi\ufb01cation.\nTo verify the effectiveness of the progressive sparsi\ufb01cation strategy,\nwe test different sparsi\ufb01cation methods that result in similar overall complexity. Here we provide\nmore detailed results and more analysis. We \ufb01nd that progressive sparsi\ufb01cation is much better than\nsingle-shot sparsi\ufb01cation. Increasing the number of stages will lead to better performance. Since\nfurther incre",
  "of both convolutions and self-attention\ncompared to pure attention models on ImageNet-1K.\n5. Conclusion\nThe design of vision backbone architectures that use\nself-attention is an exciting topic.\nWe hope that our\nwork helps in improving the understanding of architec-\nture design in this space. Incorporating self-attention for\nother computer vision tasks such as keypoint detection [9]\nand 3D shape prediction [23]; studying self-attention ar-\nchitectures for self-supervised learning in computer vi-\n",
  "rap.\nCompressive transform-\ners for long-range sequence modelling.\narXiv preprint\narXiv:1911.05507, 2019. 2\n\n--------------------------------------------------\n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learn-\ning with a uni\ufb01ed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019. 5\n[34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards rea",
  " input feature into stripes of equal width. This stripe width is an important\nparameter of the cross-shaped window because it allows us to achieve strong modelling capability\nwhile limiting the computation cost. Speci\ufb01cally, we adjust the stripe width according to the depth of\nthe network: small widths for shallow layers and larger widths for deep layers. A larger stripe width\nencourages a stronger connection between long-range elements and achieves better network capacity\nwith a small increase ",
  "tails).\n5\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\nCIFAR-10\nCIFAR-100\nModel\nAugmentation\nSAM\nSGD\nSAM\nSGD\nWRN-28-10 (200 epochs)\nBasic\n2.7\u00b10.1\n3.5\u00b10.1\n16.5\u00b10.2\n18.8\u00b10.2\nWRN-28-10 (200 epochs)\nCutout\n2.3\u00b10.1\n2.6\u00b10.1\n14.9\u00b10.2\n16.9\u00b10.1\nWRN-28-10 (200 epochs)\nAA\n2.1\u00b1<0.1\n2.3\u00b10.1\n13.6\u00b10.2\n15.8\u00b10.2\nWRN-28-10 (1800 epochs)\nBasic\n2.4\u00b10.1\n3.5\u00b10.1\n16.3\u00b10.2\n19.1\u00b10.1\nWRN-28-10 (1800 epochs)\nCutout\n2.1\u00b10.1\n2.7\u00b10.1\n14.0\u00b10.1\n17.4\u00b10.1\nWRN-28-10 (1800 epochs",
  "sically\napplied to all our models described in Section 3.\nWe\nbrie\ufb02y describe the default hyperparameters and training\ntechniques as follows.\nPreprocessing In the training phase, a rectangular region\nis randomly cropped using a randomly sampled aspect ratio\n3https://github.com/tensor\ufb02ow/models\nfrom 3/4 to 4/3, and the fraction of cropped area over whole\nimage is randomly chosen from 5% to 100%. Then, the\ncropped region is resized to 224 \u00d7 224 and \ufb02ipped horizon-\ntally with a random probability of",
  "e but decreases\nthe elite performance. To study the reasons, we replace the\nself-attention blocks with bottleneck blocks in each stage\nseparately for Net5, by which we can estimate the impor-\ntance of self-attention in different stages. The results are\nshown in Table 3. The replacement of self-attention in all\nthree stages reduces both the base performance and the elite\nperformance. There is a trend that self-attentions in lower\nresolutions play more important roles than those in higher\nresoluti",
  "elf-attention in existing architectures. Similar ob-\nservations have been made in natural language processing. Notably, Synthesizer [54] shows that\ndot-product self-attention can be replaced by a feedforward network, with competitive performance\non sentence representation benchmarks. As opposed to our work, Synthesizer does use data dependent\nweights, but in contrast to transformers the weights are determined from the queries only.\n5\nConclusion\nIn this paper we have shown that a simple residual ",
  "onal space spanned by a larger number of attention\nmaps. As such, the number of attention heads (used for computing the attention maps) are implicitly\nincreased without reducing the embedding dimension per head, enabling the model to enjoy both\nbene\ufb01ts from more SA heads and high embedding dimension.\nAdditionally, we argue that ignoring the local relationship among the tokens is another main cause\nof the above mentioned over-smoothing issue of global SA. The locality (local receptive \ufb01elds)\nand ",
  "educed FLOPs, we\nmay construct wider or deeper HVT-S, with 11 heads or 48\nblocks, then the overall FLOPs would be around 4.51G and\n4.33G, respectively. Moreover, we may consider a longer\nsequence by setting a smaller patch size or using a larger\nresolution. For example, with a patch size of 8 and an im-\nage resolution of 192\u00d7192, the FLOPs for HVT-S is around\n4.35G. Alternatively, enlarging the image resolution into\n384\u00d7384 will lead to 4.48G FLOPs. In all of the above\nmentioned cases, the compu",
  "e\n83.0\n83.5\n84.0\n84.5\n85.0\n85.5\nImagenet Top-1 Accuracy (%)\nEffNetV2\nEffNet(baseline)\nEffNet(reprod)\nNFNet\nBoTNet\nFigure 3. ImageNet accuracy and training step time on TPUv3 \u2013\nLower step time is better; all models are trained with \ufb01xed image\nsize without progressive learning.\nTraining Speed Comparison: Figure 3 compares the train-\ning step time for our new Ef\ufb01cientNetV2, where all models\nare trained with \ufb01xed image size without progressive learn-\ning. For Ef\ufb01cientNet (Tan & Le, 2019a), we show t",
  "lnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao\nLiu, Ekin D Cubuk, and Quoc V Le. Rethinking pre-training\nand self-training. In Advances in Neural Information Process-\ning Systems, 2020. 1, 8\n[90] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\nLe. Learning transferable architectures for scalable image\nrecognition. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 2018. 7\n12\n\n--------------------------------------------------\nA. Derivation of the Teacher\u2019s",
  "Xiv preprint arXiv:1711.05101, 2017. 3\n[24] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll\u00b4ar. Designing network design\nspaces.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 10428\u2013\n10436, 2020. 4\n[25] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\n\n--------------------------------------------------\nAditya Khosla, Michael Bernstein, et al.\nImagenet l",
  " and -101 [25]. Note that MDEQ\nhas only one layer of residual blocks followed by multi-resolution fusion. Therefore, to match the\ncapacity of standard explicit models, we need to increase the feature dimensionality within MDEQ.\nThis is accomplished mainly by adjusting the width of the convolutional \ufb01lter within the residual\nblock (see Figure 2).\nTable 2 shows the accuracy of two MDEQs (of different sizes) in comparison to well-known reference\nmodels in computer vision. MDEQs are remarkably compe",
  " (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7\n22\n\n--------------------------------------------------\n",
  "Training BEIT: Masked Image Modeling\nWe propose a masked image modeling (MIM) task to pretrain BEIT. We randomly mask some\npercentage of image patches, and then predict the visual tokens that are corresponding to the masked\npatches.\nFigure 1 shows the overview of our method. As presented in Section 2.1, given an input image\nx, we split it into N image patches ({xp\ni }N\ni=1), and tokenize it to N visual tokens ({zi}N\ni=1). We\nrandomly mask approximately 40% image patches, where the masked positio",
  ", green\nborder indicates humans are not contained.\n13\n\n--------------------------------------------------\nReferences\n[1] Yikang Zhang, Jian Zhang, and Zhao Zhong. Autobss: An ef\ufb01cient algorithm for block stacking\nstyle search. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,\nAdvances in Neural Information Processing Systems, volume 33, pages 10259\u201310270. Curran\nAssociates, Inc., 2020.\n[2] Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions o",
  "d and self-supervised settings. We also distinguish our work from other data\naugmentation strategies like Auto-Augment (Cubuk et al., 2018) which uses Reinforcement Learn-\ning to automatically search for improved data augmentation policies and introduces Patch Gaussian\nAugmentation, which allows the network to interpolate between robustness and accuracy (Lopes\net al., 2019). The motivation behind our proposed approach is to suppress the reliance of CNNs on\nlow-level cues and encourage CNNs to le",
  "d as performing 1D DFT on the two dimensions alternatively. Similar to 1D\nDFT, 2D DFT of real input x[m, n] satis\ufb01ed the conjugate symmetry property X[M \u2212u, N \u2212v] =\nX\u2217[u, v]. The FFT algorithms can also be applied to 2D DFT to improve computational ef\ufb01ciency.\n3.2\nGlobal Filter Networks\nOverall architecture. Recent advances in vision transformers [10, 43] demonstrate that models\nbased on self-attention can achieve competitive performance even without the inductive biases\nassociated with the convo",
  "in an attention-free network whose top-1 accuracy on Imagenet-\nval is 79.2%, which is comparable to a ResNet-50 trained with a modern training strategy. This\nnetwork has served as our baseline for subsequent ablations. Note that, at this stage, we still include\nLayerScale, a class embedding (in the class-MLP stage) and positional encodings.\nDistillation.\nThe same model trained with distillation inspired by Touvron et al. [56] achieves\n81.5%. The distillation variant we choose corresponds to the ",
  "n, the\ncropped region is resized to 224 \u00d7 224 and \ufb02ipped horizon-\ntally with a random probability of 0.5 followed by the RGB\nchannel normalization. During validation, shorter dimen-\nsion of each image is resized to 256 pixels while the aspect\nratio is maintained. Next, the image is center-cropped to\n224 \u00d7 224, and the RGB channels are normalized.\nHyperparameter We use 1,024 batch size for training\nwhich is close to the maximum size that can be received\nby a single machine with 8 P40 GPUs. Stocha",
  "urrent neural\nnetworks. arXiv preprint arXiv:1308.0850, 2013. 5\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017. 9, 14\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 1, 5, 6, 14\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In Eu",
  "Hengel.\nWider or deeper: Revisiting the resnet model for visual\nrecognition. arXiv preprint arXiv:1611.10080, 2016. 8\n[45] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. In ICCV, 2021. 2\n[46] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. In BMVC, 2016. 8\n[47] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,\nXin Jiang, and Qun Liu.\nTernarybe",
  "d Nick Barnes. Deep texture and structure aware \ufb01ltering network for\nimage smoothing, 2018. 4\nS. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classi\ufb01cation of\naircraft. Technical report, 2013. 2, 5\nIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representa-\ntions, 2019. 3\nT. Nathan Mundhenk, Daniel Ho, and Barry Y. Chen. Improvements to context based self-supervised\nlearning. 2018 IEEE/CVF Conference on Computer Vision and Pat",
  "ls uses UDA in training the teacher. Both of these base-\nlines use the same experimental protocols and hence ensure\na fair comparison. We follow [48]\u2019s train/eval/test splitting,\nand we use the same amount of resources to tune hyper-\nparameters for our baselines as well as for Meta Pseudo\nLabels. More details are in Appendix C.\nAdditional baselines.\nIn addition to these two baselines,\nwe also include a range of other semi-supervised baselines\nin two categories: Label Propagation and Self-Supervi",
  "eleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30(4):838\u2013855, 1992.\n[37] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and J. Shlens. Stand-alone\nself-attention in vision models. In NeurIPS, 2019.\n[38] M. Sandler, J. Baccash, A. Zhmoginov, and Howard. Non-discriminative data or weak model?\nOn the relative importance of data and model resolution. In ICCV Workshop on Real-World\nRecognition from Low-Quality Images and Videos, 2019.\n[39",
  "D relative encoding into\nhorizontal and vertical directions, such that each direction\ncan by modeled by a 1D encoding. The method formula-\ntion is given as follows\neij =\n(xiWQ)(xjWK+ concat(pK\n\u03b4\u02dcx, pK\n\u03b4\u02dcy))T\n\u221adz\n,\n(12)\nwhere \u03b4\u02dcx = \u02dcxi \u2212\u02dcxj and \u03b4\u02dcy = \u02dcyi \u2212\u02dcyj denote the relative\nposition offsets on x-axis and y-axis of the image coordi-\nnate respectively, pK\n\u03b4\u02dcx and pK\n\u03b4\u02dcy are learnable vectors with\nlength 1\n2dz, the concat operation concatenates the two en-\ncodings to form a \ufb01nal relative encodi",
  "age which generated the anchor image.\n\u2022 Views of a sample image different from that which generated the anchor image but having\nthe same label as that of the anchor.\nThe loss for self-supervised contrastive learning (Eq. 1 in the paper) is a special case of SupCon\nwhen P(i) is restricted to contain only a view of the same source image as that of the anchor (i.e.,\nthe \ufb01rst category above). In this case, P(i) = j(i), where j(i) is the index of view, and Eq. 25\nreadily takes on the self-supervised ",
  " se-\nmantic segmentation examples in Figure 14.\nC.2. Generalization and Robustness\nTo further evaluate the generalization performance of our pro-\nposed models, we compare on a recently proposed benchmark\ndataset, ImageNetV2 [32], complementary to the original Im-\nageNet 2012.\nWe use the MatchedFrequency version of the\nImageNet-V2.\nFigure 15a reports the top-5 accuracy compari-\n\n--------------------------------------------------\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n2\n4\n6\n\u001b\n10\n0.5\n1\n1.5\n2\nLHaYH\u00adRXW\u0003UaWLR\nGURXS\u0003I",
  "\nimages, ViT-B/16-SAM outperforms ViT-B/16-AUG by 7.6%. Apart from the improved validation\naccuracy, we also observe that both SAM and strong augmentations increase the training error (see\nFigure 2 (Middle) and Table 4), indicating their regularization effects. However, they have distinct\ntraining dynamics as the loss curve for ViT-B/16-AUG is much nosier than ViT-B/16-SAM.\nTable 4:\nComparison between ViT-B/16-\nSAM and ViT-B/16-AUG. We average the\nresults for 1,000 random noises when calcu-\nlati",
  "hape\n(3, 224, 224) into (256, 14, 14) with 184 MFLOPs. For\ncomparison, the \ufb01rst 10 layers of a ResNet-18 perform\nthe same dimensionality reduction with 1042 MFLOPs.\nNo classi\ufb01cation token.\nTo use the BCHW tensor for-\nmat, we remove the classi\ufb01cation token. Similar to convo-\nlutional networks, we replace it by average pooling on the\nlast activation map, which produces an embedding used\nin the classi\ufb01er. For distillation during training, we train\nseparate heads for the classi\ufb01cation and distillati",
  "d shed light on the dif\ufb01culties of comparing optimizers for\nneural network training more generally.\n1\nIntroduction\nIn recent years, hardware systems employing GPUs and TPUs have enabled neural network training\nprograms to process dramatically more data in parallel than ever before. The most popular way to\nexploit these systems is to increase the batch size in the optimization algorithm (i.e. the number\nof training examples processed per training step). On many workloads, modern systems can scale",
  "osed method achieves better performance than supervised pre-\ntraining, although BEIT does not require manual annotations for pre-training. Moreover, we employ\nintermediate \ufb01ne-tuning for BEIT on ImageNet, i.e., we \ufb01rst \ufb01ne-tune pretrained BEIT on ImageNet,\nand then \ufb01ne-tune the model on ADE20K. The results indicate that intermediate \ufb01ne-tuning further\nimproves BEIT on semantic segmentation.\n3.3\nAblation Studies\nWe conduct ablation studies to analyze the contributions of each component in BEIT. T",
  "ith softmax normalization.\nThe size and number of convolutional \ufb01lters, and the num-\nber of fully connected neurons were determined via meta-\nparameter search. See Section C of the supplementary ma-\nterial for further details about said parameters.\nWe use a single SSAL branch (i.e. an auxiliary classi\ufb01er)\nwith either 2, 4, 10 or 20 groups, and a grouping criterion\nthat either splits or joins visually similar classes following\nthe computation outlined in Section 3.1. The model pre-\ndiction is don",
  "es multiply-add cumulation and Params\nmeans the number of parameters in the model.\n4\nRELATED WORKS\nFast Transformers\nA line of recent works have been\ndeveloped to approximate self-attention layers to improve\nover the O(N 2) complexity. These methods keep the ba-\nsic design of self-attention, mapping N inputs to N out-\nputs, and is hence different in purpose with our method,\nwhich compresses N inputs to a smaller number M of out-\nputs. Among these works, Reformer (Kitaev et al., 2019)\nuses Locall",
  " Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\nexperiments.\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\nparameters \u2013 weight decay, dropout, and label smoothing. Figure 3 shows the results after \ufb01ne-\ntuning to ImageNe",
  "-------------------\nTable 2: Evaluation on ImageNet classi\ufb01cation\nwith top-1 and top-5 accuracies reported. MDEQs\nwere trained for 100 epochs.\nModel Size\ntop1 Acc.\ntop5 Acc.\nAlexNet [31]\n238M\n57.0%\n80.3%\nResNet-18 [25]\n13M\n70.2%\n89.9%\nResNet-34 [25]\n21M\n74.8%\n91.1%\nInception-V2 [28]\n12M\n74.8%\n92.2%\nResNet-50 [25]\n26M\n75.1%\n92.5%\nHRNet-W18-C [55]\n21M\n76.8%\n93.4%\nSingle-stream DEQ + global pool [5]\n18M\n72.9%\n91.0%\nMDEQ-small (ours) [Implicit]\n18M\n75.5%\n92.7%\nResNet-101 [25]\n52M\n77.1%\n93.5%\nW-ResNe",
  "\n8,2,3,192\n1,2,6,384\nSmall 1-10-1\n1,8,3,192\n10,2,6,384\n1,2,12,768\nSmall 2-9-1\n2,8,3,192\n9,2,6,384\n1,2,12,768\nSmall 1-9-2\n1,8,3,192\n9,2,6,384\n2,2,12,768\nSmall 2-8-2\n2,8,3,192\n8,2,6,384\n2,2,12,768\nSmall 1-1-9-1 1,4,3,96\n1,2,3,192\n9,2,6,384\n1,2,12,768\nSmall 1-2-8-1 1,4,3,96\n2,2,3,192\n8,2,6,384\n1,2,12,768\nTable 9. Model architecture for multi-scale stacked ViTs. Archi-\ntecture parameters for each E-ViT module E-ViT(a \u00d7 n/p ; h, d):\nnumber of attention blocks n, input patch size p, number of heads\nh ",
  "e-excitation networks and beyond,\u201d in ICCV\nWorkshops, 2019, pp. 1971\u20131980.\n[64]\nA. Radford, L. Metz, and S. Chintala, \u201cUnsupervised represen-\ntation learning with deep convolutional generative adversarial\nnetworks,\u201d in ICLR (Poster), 2016.\n[65]\nX. Mao, Q. Li, H. Xie, R. Y. K. Lau, Z. Wang, and S. P. Smolley,\n\u201cLeast squares generative adversarial networks,\u201d in ICCV, 2017,\npp. 2813\u20132821.\n[66]\nX. Wei, B. Gong, Z. Liu, W. Lu, and L. Wang, \u201cImproving the\nimproved training of wasserstein gans: A consi",
  "s the resolution of each image patch, and N = HW/P 2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\n\ufb02atten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT\u2019s [class] token, we prepend a learnable embedding to the sequence of embed-\nded",
  " (CNNs) can learn powerful visual features that have resulted\nin signi\ufb01cant improvements on many computer vision tasks such as semantic segmentation (Shel-\nhamer et al., 2017), object recognition (Krizhevsky et al., 2012), and object detection (Ren et al.,\n2015). However, CNNs often fail to generalize well across datasets under domain-shift due to var-\nied lighting, sensor resolution, spectral-response etc. One of the reasons for this poor generalization\nis CNNs\u2019 over reliance on low-level cues ",
  "ccuracy of Swin Transformer using different methods\nfor self-attention computation on three benchmarks.\nsentation and has linear computational complexity with re-\nspect to input image size. Swin Transformer achieves the\nstate-of-the-art performance on COCO object detection and\nADE20K semantic segmentation, signi\ufb01cantly surpassing\nprevious best methods. We hope that Swin Transformer\u2019s\nstrong performance on various vision problems will encour-\nage uni\ufb01ed modeling of vision and language signals.\nAs",
  "arison, where a 1D FFT on feature dimension and a 2D FFT on spatial\ndimensions are used to mix tokens. As shown in Table 6, our method outperforms all baseline\nmethods except DeiT-S that has 64% higher FLOPs.\nRobustness & generalization ability. Inspired by the [32], we further conduct experiments to\nevaluate the robustness and the generalization ability of the GFNet. For robustness, we consider\nImageNet-A, ImageNet-C, FGSM and PGD. ImageNet-A [16] (IN-A) is a challenging dataset that\ncontains n",
  "en observe\nthat the gradient descent update yields a generalization\nof self-attention layer and use it to motivate the design of\nour new module.\nUsing our new modules, we build centroid transformers,\nin which we insert our centroid attention modules between\ntypical self-attention modules. We apply centroid trans-\nformers on several challenging scenarios, ranging from\nnatural language processing to computer vision. On ab-\n1\narXiv:2102.08606v2  [cs.LG]  8 Mar 2021\n\n--------------------------------",
  "l., 2016; Zhang et al., 2020; Bello, 2021).\nWide ResNets (Zagoruyko & Komodakis, 2016) and Mo-\nbileNets (Howard et al., 2017) instead scale the width. In-\ncreasing image resolutions has also been a reliable source\nof progress. Thus as training budgets have grown, so have\nthe image resolutions: Ef\ufb01cientNet uses 600 image reso-\nlutions (Tan & Le, 2019) and both ResNeSt (Zhang et al.,\n2020) and TResNet (Ridnik et al., 2020) use 448 image\nresolutions for their largest model. In an attempt to sys-\nte",
  "better since more discriminative\nparts\u2019 features will be extracted; then, attention regions pro-\nvide accurate location of object, which ensures our model\nto look at the object closer and further improve the per-\nformance.\nComprehensive experiments in common \ufb01ne-\ngrained visual classi\ufb01cation datasets also prove the effec-\ntiveness of WS-DAN.\nDCL is also a data augmentation method for \ufb01ne-grained\nrecognition, it enhances the dif\ufb01culty of \ufb01ne-grained recog-\nnition and exercises the classi\ufb01cation m",
  "f MUXNets on three tasks; im-\nage classi\ufb01cation, object detection, and transfer learning.\n5.1. Hyperparameter Search Details\nSearch Space: To compensate for the extra hyperparam-\neters introduced by spatial and channel multiplexing, we\nconstrain the commonly adopted layer-wise search space [1,\n40, 13] to a stage-wise search space, where layers within\nthe same stage share the same hyperparameters. MUXNets\nconsist of four stages, where each stage begins with a reduc-\ntion block and is followed by ",
  "e\nhave considered the Cait-S24 model for faster iterations. This network consists of 24-layer with a\nworking dimension of 384. All our experiments below were carried out with images in resolution\n224\u00d7224 and N = 16 \u00d7 16 patches. Trained with regular supervision, Cait-S24 attains 82.7% top-1\nacc. on Imagenet.\nSA \u2192MLP.\nThe self-attention can be seen a weight generator for a linear transformation on the\nvalues. Therefore, our \ufb01rst design modi\ufb01cation was to get rid of the self-attention by replacing",
  "reported by the original UDA paper [76]. In their work, UDA [76] use a much\nlarger batch size for their UDA objective. In our implementation of UDA, we keep these batch sizes the same. This leads to a\nmuch easier implementation of data parallelism in our framework, TensorFlow [1] running on TPU big pods. To compensate\nfor the difference, we train all UDA baselines for much longer than the UDA paper [76]. During the training process, we also\nmask out the supervised examples with high con\ufb01dence. E",
  "5\n77.5\n80\n82.5\n85\n87.5\n90\n92.5\n95\n97.5\n100\nCCT\u22c6\nProxyless-G\nCCT\nViT-Lite\nMobileNetV2\nResNet164\nResNet1001\nResNet18\n# Parameters (M)\nTop-1 validation accuracy\nFigure 4: CIFAR-10 accuracy vs model size (sizes < 12M).\nCCT\u22c6was trained longer with extra augmentations.)\nWe also present ImageNet training results in Table 3. We\nshould note that ResNet152 [7] used 10-crop testing, and\nViT has more than 3 times as many parameters than our\nvariants. We used the timm package [40] to train our models\non Imag",
  "URL http://arxiv.org/abs/1805.\n07836.\n13\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\nA\nAPPENDIX\nA.1\nPAC BAYESIAN GENERALIZATION BOUND\nBelow, we state a generalization bound based on sharpness.\nTheorem 2. For any \u03c1 > 0 and any distribution D, with probability 1 \u2212\u03b4 over the choice of the\ntraining set S \u223cD,\nLD(w) \u2264max\n\u2225\u03f5\u22252\u2264\u03c1 LS(w + \u03f5) +\nv\nu\nu\nu\nu\nt\nk log\n \n1 + \u2225w\u22252\n2\n\u03c12\n\u0012\n1 +\nq\nlog(n)\nk\n\u00132!\n+ 4 log n\n\u03b4 + \u02dcO(1)\nn \u22121\n(4)\nwhere n = |S|, k is the num",
  ".8M\n311M\n1372.5\n94.17\n77.01\nSwin-B [35]\n86.7M\n497M\n868.3\n94.55\n78.45\nNesT-T\n6.2M\n187M\n1616.9\n96.04\n78.69\nNesT-S\n23.4M\n411M\n627.9\n96.97\n81.70\nNesT-B\n90.1M\n984M\n189.8\n97.20\n82.56\n3.2\nVisual Interpretability via Tree Traversal\nDifferently from existing methods, the nested hierarchy with independent block process in NesT\nresembles a decision tree that each block is encouraged to learn non-overlapping features and be\nselected by the block aggregation. This unique behavior motivates us to explore a ne",
  "ientNet. We trained for 100 epochs, using RMSProp (Tieleman and Hinton 2012) as implemented in the timm\npackage8 with a learning rate of 0.0016, a weight decay of 1e \u22125 and a momentum of 0.9. The learning rate is decayed by a\nfactor of 0.9875 with every epoch. As a warmup, the learning rate is linearly scaled from 0 to 0.0016 over the \ufb01rst 3 epochs.\nFollowing (Wightman 2019), we evaluate with an exponential moving average of the weights of the model, with a decay of\n0.99985. We use the random er",
  "periments with Re\ufb01ner, we shared the attention maps for 1\nadjacent transformer block.\nC\nDataset description for NLP experiments\nGLUE benchmark [52] is a collection of nine natural language understanding tasks where the labels\nof the testing set is hidden and the researchers need to submit their predictions to the evaluation\nserver3 to obtain results on testing sets. In this work, we only present results of single-task setting for\nfair comparison. The nine tasks included in GLUE benchmark are des",
  "1,000 classes of ImageNet. This selection results in\n12.8 million images. We also make sure that none of the\n12.8 million images that we use overlaps with the ILSVRC\n2012 validation set of ImageNet. This procedure of \ufb01ltering\nextra unlabeled data has been used by UDA [76] and Noisy\nStudent [77].\nImplementation details.\nWe implement Meta Pseudo La-\nbels the same as in Section 3.2 but we use a larger batch size\nand more training steps, as the datasets are much larger for\nthis experiment. Speci\ufb01cal",
  "sults for image\nclassi\ufb01cation on ImageNet reported in this paper, we did not use any external dataset for pre-training.\nViP vs. CNNs. Table 2 compares ViP family with some of the state-of-the-art CNN models on\nImageNet. The RegNet is also better tuned using training tricks in [59]. ViP is both cost-ef\ufb01cient\nand parameter-ef\ufb01cient compared to these state-of-the-art models. For example, ViP-M can achieve\na competitive 83.3% with only 49.6M parameters and 8.0G FLOPS. The counterpart BOTNet-T5\nneeds",
  "ang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nProceedings of the European conference on computer vision\n(ECCV), pages 801\u2013818, 2018. 7\n[12] Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen\nLin, and Han Hu. Reppoints v2: Veri\ufb01cation meets regres-\nsion for object detection. In NeurIPS, 2020. 6, 7, 9\n[13] Cheng Chi, Fangyun Wei, and Han Hu.\nRelationnet++:\nBridging visual re",
  "r-parameter S \u2208(0,1). We will process a fraction\nS of the patches, and directly skip the remaining 1 \u2212S fraction. As before, we rank the N patches\n25\n\n--------------------------------------------------\naccording to some scoring function s(\u22c5). Then, we directly discard the bottom (1 \u2212S)% of the\npatches, and proceed like in Algorithm 2 over the selected M = SN patches. Algorithm 3 formally\ndescribes the idea. Going back to our previous example with two patches, if we set S = 0.5 there, we\nwill dis",
  "c assumptions about grid structure are to the perfor-\nmance of the benchmark methods, we evaluate all methods\non permuted ImageNet. To generate permutations, we use a\nsingle, shared permutation pattern for all images. The per-\nmutation is performed after position features are generated.\nWe make this choice because it still allows each network\nto infer the spatial relationship between points (using the\nposition encoding), but prevents the network from using an\n\n-----------------------------------",
  " Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 1, 2, 3, 8, 9\n[61] Ross\nWightman.\nPytorch\nimage\nmodels.\nhttps://github.com/rwightman/\npytorch-image-models, 2019. 10\n[62] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation system: Bridging\nthe gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2",
  "rate the probability\nscores of correct class. After this we calculated the mean of correct probability scores on both the\nmodels. As we can see from Table 10 that Anistropic ImageNet has larger mean which means that\nAnistropic ImageNet has better con\ufb01dence as compared to Standard ImageNet. We also calculate\nthe entropy of output probability distribution from both the models. We can see from Table 10\nAnistropic ImageNet has lower entropy scores as compared to Standard ImageNet.\n[t!]\n15\nSALIENCY M",
  "er of epochs on large-scale datasets (e.g. \u223c40\nepochs on 300M images), consistent with our experimen-\ntal \ufb01ndings that scaling the width is preferable in shorter\nepoch regimes. In particular, Kolesnikov et al. (2019) train\na ResNet-152 with 4x \ufb01lter multiplier while Brock et al.\n(2021) scales the width with \u223c1.5x \ufb01lter multiplier.\n6.2. Strategy #2 - Slow Image Resolution Scaling\nIn Figure 2, we also observe that larger image resolutions\nyield diminishing returns. We therefore propose to increase",
  "\n384\n84.6\n88.9\nRe\ufb01ned-ViT-M\n55M\n13.5B\n224\n224\n84.6\n88.9\nRe\ufb01ned-ViT-M\u2191384\n55M\n49.2B\n224\n384\n85.6\n89.3\nRe\ufb01ned-ViT-L\n81M\n19.1B\n224\n224\n84.9\n89.1\nRe\ufb01ned-ViT-L\u2191384\n81M\n69.1B\n224\n384\n85.7\n89.7\nRe\ufb01ned-ViT-L\u2191448\n81M\n98.0B\n224\n448\n85.9\n90.1\ngood results. It is worthy future studies on the effects of such subtraction attention (with negative\nattention weights) for feature aggregation. Second, the local distributed attention indeed imposes a\nlocality inductive bias which is proven to be effective in enhanc",
  "\n0.03\n83.5\n72.7\n88.3\n3B\n20K\n10K\n0.03\n67.5\n54.5\n74.9\n3B\n30K\n10K\n0.03\n72.3\n60.0\n79.4\n3B\n60K\n10K\n0.03\n76.7\n64.8\n83.3\n3B\n120K\n50K\n0.03\n79.1\n67.9\n85.4\n3B\n400K\n50K\n0.03\n81.9\n71.1\n87.3\n3B\n1.2M\n50K\n0.01\n82.8\n72.7\n87.9\n3B\n2M\n50K\n0.03\n83.2\n73.2\n88.1\n3B\n4M\n50K\n0.01\n84.0\n73.8\n88.5\n27\n\n--------------------------------------------------\nTable 18: Tabular representation of the \ufb01netune results (%) for model ViT-Ti/16 on ImageNet,\nImageNet V2 test set and ImageNet ReaL test set.\nData Size\nSteps\nCooldown\nLR\nImage",
  "tNet-4\n3842\n275M\n189.5B\n87.9\n+ PT-RA\n3842\n275M\n189.5B\n88.3\n+ PT-RA-E150\n3842\n275M\n189.5B\n88.4\nCoAtNet-2\n5122\n75M\n96.7B\n87.3\nCoAtNet-3\n5122\n168M\n203.1B\n87.9\nCoAtNet-4\n5122\n275M\n360.9B\n88.1\n+ PT-RA\n5122\n275M\n360.9B\n88.4\n+ PT-RA-E150\n5122\n275M\n360.9B\n88.56\n18\n\n--------------------------------------------------\n",
  "ng ef\ufb01ciency is thus boosted since the training space\nhas been greedily shrunk from all paths to those potentially-\ngood ones. Moreover, we further adopt an exploration and\nexploitation policy by introducing an empirical candidate\npath pool. Our proposed method GreedyNAS is easy-to-\nfollow, and experimental results on ImageNet dataset indi-\ncate that it can achieve better Top-1 accuracy under same\nsearch space and FLOPs or latency level, but with only\n\u223c60% of supernet training cost. By searching",
  "ene\ufb01cial once features contain semantic information, i.e. after having been processed by\na few operations, in which case using global contexts in the early layers would be wasteful. In\nthe Appendix 5.3, we study hybrid designs that use standard convolutions to capture local contexts\nand lambda layers to capture global contexts. We \ufb01nd that such convolution-lambda hybrids have\nincreased representational power at a negligible decrease in throughput compared to their purely\nconvolutional counterpar",
  " knowledge they did not use it with CutMix or Mixup as we propose to do.\n4\n\n--------------------------------------------------\nIn our experiments, even when using BCE, setting all mixed concepts with a target to\n1 (or 1 \u2212\u03b5) is more effective than considering a distribution of concepts that sum to 1.\nConceptually we believe it is more aligned with what Mixup and CutMix are actually\ndoing: it is likely that a human could recognize each of two mixed concepts.\nData-Augmentation.\nWe adopt the followi",
  "ple and classic TwoM-\noon dataset [7]. The 2D nature of the TwoMoon dataset\nallows us to visualize how Meta Pseudo Labels behaves\ncompared to Supervised Learning and Pseudo Labels.\nDataset.\nFor this experiment, we generate our own version\nof the TwoMoon dataset. In our version, there are 2,000 ex-\namples forming two clusters each with 1,000 examples. Only\n6 examples are labeled, 3 examples for each cluster, while the\nremaining examples are unlabeled. Semi-supervised learn-\ning algorithms are ask",
  "eep convolutional neural net-\nworks. In NIPS, 2012. 2\n[28] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, 86(11):2278\u20132324, 1998. 4\n[29] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyramid\nnetworks for object detection. In CVPR, 2017. 6\n[30] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Dollar. Focal loss for dense object detection. In ",
  " for each image\nis L \u2208R18\u00d718\u00d71000. During training, the target labels for the tokens are generated by applying\nRoIAlign [18] on the corresponding score map. In practice, we only store the top-5 score maps for\neach position in half-precision to save space as storing the entire score maps for all the images results\nin 2TB storage. In our experiment, we only need 10GB of storage to store all the score maps.\n5\n\n--------------------------------------------------\nTable 1: Performance of the proposed L",
  "tersection over Union) is better.\nBackbone\nModel Size\nmIoU\nResNet-18-A [39]\nResNet-18\n3.8M\n55.4\nResNet-18-B [39]\nResNet-18\n15.24M\n69.1\nMobileNetV2Plus [46]\nMobileNetV2\n8.3M\n74.5\nGSCNN [53]\nResNet-50\n-\n73.0\nHRNetV2-W18-Small-v2* [55]\nHRNet\n4.0M\n76.0\nMDEQ-small (ours) [Implicit]\nMDEQ\n7.8M\n75.1\nU-Net++ [64]\nResNet-101\n59.5M\n75.5\nDilated-ResNet [60]\nD-ResNet-101\n52.1M\n75.7\nPSPNet [62]\nD-ResNet-101\n65.9M\n78.4\nDeepLabv3 [9]\nD-ResNet-101\n58.0M\n78.5\nPSANet [63]\nResNet-101\n-\n78.6\nHRNetV2-W48* [55]\nHRNet\n",
  "alysis (face detection, face alignment,\nface recognition and face generation). During\nhis PhD studies, he has organised the Menpo\n2D Challenge (CVPR 2017), the Menpo 3D Challenge (ICCV 2017)\nand Lightweight Face Recognition Challenge (ICCV 2019). He also\nwon many academic challenges, such as ILSVRC Object Detection\nand Tracking 2017, Activity-Net Untrimmed Video Classi\ufb01cation 2017,\niQIYI Celebrity Video Identi\ufb01cation Challenge 2018, Disguised Face\nRecognition Challenge 2019. He is a reviewer in ",
  ".\nThis block uses our implementation of the baselines. The Perceiver\nis competitive with standard baselines on ImageNet without relying\non domain-speci\ufb01c architectural assumptions.\nnecessary details from the input signal. To hedge against\nthis effect, the Perceiver may be structured with multiple\ncross-attend layers, which allow the latent array to iter-\natively extract information from the input image as it is\nneeded. This allows us to tune the model to balance ex-\npensive, but informative cros",
  "lution 224 unless mentioned otherwise. We follow the setup of [14].\nThe only differences are: (1) We exclude lr = 0.001 from the grid search and instead include\nlr = 0.06 for CIFAR-10, CIFAR-100, Flowers, and Pets. (2) We perform a grid search over\nlr \u2208{0.003, 0.01, 0.03} for VTAB-1k. (3) We try two different ways of pre-processing during\nevaluation: (i) \u201cresize-crop\u201d: \ufb01rst resize the image to 256 \u00d7 256 pixels and then take a 224 \u00d7 224\npixel sized central crop. (ii) \u201cresmall-crop\u201d: \ufb01rst resize t",
  "\ufb01ers. We democratise the use\nof Transformers by providing a framework that allows for\nthese SOTA results on small datasets and minimal hard-\nware. Our method is \ufb02exible in size, and the smallest of\nour variants can be easily loaded on even a minimal GPU,\nor even a CPU. While the trend of research has been bigger\nand bigger transformers, we show that there is still much\nresearch to be done to make ef\ufb01cient networks that work on\nsmall datasets and less ef\ufb01cient hardware. This kind of re-\nsearch is",
  "onal CNNs and the\nconcurrently proposed all-MLP models, e.g., MLP-Mixer\n[25], ResMLP [26], gMLP [19], AS-MLP [18], etc. In this\npaper, we use RepMLP in ResNet for most of our experi-\nments because this work was \ufb01nished before the publicity\nof all the above-mentioned all-MLP models. The applica-\ntion of RepMLP on the all-MLP models is scheduled as our\nfuture work.\nIn order to use RepMLP in ResNet, we follow the bottle-\nneck [14] design principle of ResNet-50 to reduce the chan-\nnels by 4\u00d7 via 1\u00d71",
  "nition 3. (Data local smoothness for the multi labels)\n\u03c6i,j(zi, zj) := zizj\u03c6b\u2032\ni,j(1, 1) + zi(1 \u2212zj)\u03c6b\u2032\ni,j(1, 0) + (1 \u2212\nzi)zj\u03c6b\u2032\ni,j(0, 1) + (1 \u2212zi)(1 \u2212zj)\u03c6b\u2032\ni,j(0, 0), \u2200zi, zj \u2208L.\nProposition 2. With \u03c6i,j de\ufb01ned as De\ufb01nition 3, ei,j satis-\n\ufb01es pairwise submodularity.\nProof. We can represent \u03c6i,j as follows:\n\u03c6i,j(zi, zj)\n= f(zi, zj)\u03c6b\u2032\ni,j(1, 0) + \u03c6b\u2032\ni,j(0, 1) \u2212\u03c6b\u2032\ni,j(0, 0) \u2212\u03c6b\u2032\ni,j(1, 1)\n2\n+ zi\n\u03c6b\u2032\ni,j(1, 0) + \u03c6b\u2032\ni,j(1, 1) \u2212\u03c6b\u2032\ni,j(0, 0) \u2212\u03c6b\u2032\ni,j(0, 1)\n2\n+ zj\n\u03c6b\u2032\ni,j(0, 1) + \u03c6b\u2032\ni,j(1, 1) ",
  "riginal image is clean and there is no doubt to use the ground-truth label, while the\naugmented counterparts look more like other classes which the annotation is not aware\nof. This phenomenon is called augment ambiguity. Right: We leverage the idea of\nknowledge distillation to provide softened signals to avoid ambiguity.\ncareful control (e.g., in an expanded search space or with an increased distortion\nmagnitude), these methods are not guaranteed to perform well \u2013 as we shall see\nin Section 3.2,",
  "ing, most convolutional neural net-\nworks crop a subimage with a given ratio, typically extracting a 224\u00d7224 center\ncrop from a 256\u00d7256 resized image, leading to the typical ratio of 0.875. Wight-\nman et al. [68] notice that setting this crop ratio to 1.0 for transformer models\nhas a positive impact: the distilled DeiT-B\u2191384 reaches a top1-accuracy on\nImagenet1k-val of 85.42% in this setting, which is a gain of +0.2% compared to\nthe accuracy of 85.2% reported by Touvron et al. [63].\nOur measurem",
  "his gives rise to the question if a network can dy-\nnamically and adaptively fuse the received features in\na contextual scale-aware way.\nMotivated by the above observations, we present the at-\ntentional feature fusion (AFF) module, trying to answer the\nquestion of how a uni\ufb01ed approach for all kinds of feature\nfusion scenarios should be and address the problems of con-\ntextual aggregation and initial integration. The AFF frame-\nwork generalizes the attention-based feature fusion from the\nsame-la",
  "s are conducted to utilize the transformer archi-\ntecture to the backbone network for computer vision tasks.\nNon-local network [37] adds a few self-attention layers to\nCNN backbone, and it shows that the self-attention mecha-\nnism can be used in CNN. [28] replaced 3 \u00d7 3 convolution\nof ResNet to local self-attention layer. [36] used an atten-\ntion layer for each spatial axis. [2] enables self-attention of\nthe entire spatial map by reducing the computation of the\nattention mechanism. Most of these",
  "atial re-\nduction layer in a convolutional architecture. ResNet50, one\nof the most widely used networks in ImageNet, is used for\narchitecture and is trained over 100 epochs without complex\ntraining techniques. For ResNet with ViT style dimension,\nwe use the stem layer of ViT to reduce the feature to 14\u00d714\nspatial dimensions while reducing the spatial information\nloss in the stem layer. We also remove the spatial reduction\nlayers of ResNet to maintain the initial feature dimensions\nfor all layers",
  "ugmentation can be dif-\nferent between the stages of training teachers and distilla-\n6\n\n--------------------------------------------------\n\"# = %. '\n\"# = %. %'\n\"# = %. %%'\n\"# = %. '\n\"# = %. %' \"# = %. %%'\nFigure 6. A comparison of training from random initialized parameters, inferior parameters and superior parameters. Left are the training\naccuracy curves and middle are validation accuracy curves. The results indicate that a good initialization is crucial for the \ufb01nal performance.\nLeft is the c",
  " centroid transformers is signi\ufb01cantly re-\nduced against their full-length counterparts. Experiments\nconducted on text summarization, 3D vision and image\nprocessing demonstrates centroid transformers can yield\nsimilar / better performance over the original transformers\nwith high ef\ufb01ciency.\nREFERENCES\nAljalbout, E., Golkov, V., Siddiqui, Y., Strobel, M., and\nCremers, D. Clustering with deep learning: Taxonomy\nand new methods. arXiv preprint arXiv:1801.07648,\n2018.\nBeltagy, I., Peters, M. E., and ",
  "se building block for multi-head context aggregation. A CONTAINER block contains both\nstatic af\ufb01nity as well as dynamic af\ufb01nity based aggregation, which are combined using learnable\nmixing coef\ufb01cients. This enables the CONTAINER block to process long range information while\nstill exploiting the inductive bias of the local convolution operation. CONTAINER blocks are easy to\nimplement, can easily be substituted into many present day neural architectures and lead to highly\nperformant networks whils",
  ")\nCPVT-S\n(80.5%)\nRPE methods on DeiT-S\n0\n20\n40\nExtra Rate (%)\n(Ours)\n(Ours)\n(Ours)\n39\n18\n38\n12\n17\n18\n29\n11\n37\n10\n39\n 5\n20\n10\n27\n14\nextra inference-time rate (%)\nextra memory rate (%)\nFigure 10: The extra cost brought by RPEs. The reference\nmodel is DeiT-S [22] without RPE, taking 1,096 images/s\nand 8,930 Mb memory.\nuct method (de\ufb01ned in Sec. 3.2 in the main manuscript) to\nadapt 1D encoding methods for 2D images. We replace the\npiecewise function g(x) with the clip function h(x), which\nis matched",
  " and thus increase the representation power of contextual\naggregation compared with a single-head version. Note that only spatial information is propagated\nduring contextual aggregation using the af\ufb01nity matrices; cross-channel information exchange does\nnot occur within the af\ufb01nity matrix multiplication, and that there is no non-linear activation function.\n3.2\nThe Transformer, Depthwise Convolution and MLP-Mixer\nTransformer [56], depthwise convolution [30] and the recently proposed MLP-Mixer [52",
  " datasets either through few-shot or \ufb01ne-tuning accuracy.\nFine-tuning accuracies capture the performance of each model after \ufb01ne-tuning it on the respective\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\nthat maps the (frozen) representation of a subset of training images to {\u22121, 1}K target vectors. This\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\n\ufb01ne-tuning performance, we sometimes use linear",
  "es, we \ufb01rst train a separate\nhigh-quality image embedding model following (Wang et al.,\n2014) with a large-scale labeled dataset as in (Juan et al.,\n2020), and then generate 4K clusters via k-means based on\nall training images of the embedding model. For each query\nimage (from the ALIGN dataset) and index image (from\ntest sets of downstream tasks), we \ufb01nd their top-10 nearest\nclusters based on the embedding distance. Each image is\nthen assigned to\n\u000010\n3\n\u0001\nbuckets (all possible combinations of\n3 ",
  " binary matrix with one-hot row vectors, then get Wm\u00d7n via smoothing and normalizing. For\nexample, Aj,k = 1 means the delegator assigns the j-th sample to the k-th expert, while Wj,k should\nbe larger. Given that the main concern of this paper is not the optimization of network architectures,\nwe can suppose they have similar accuracy/FLOPs trade-off, thus make an assumption about the\nexperts:\nNo Superiority Assumption\nThe overall accuracy/FLOPs trade-off for each expert is same, only\nthe one spec",
  "pace. For our implementation in Table 2 for\nM = 12 blocks, ((N+1)V1V2V3V4)M \u22481.3\u00d71030, which\nis about 1012 times the search space of DARTS [22] and\n1018 times the search space of SPOS [13]. The main-stream\nfast Neural Architecture Search methods, such as differen-\ntial [22] and one-shot [13] method, can not work well on\nthis huge search space. For DARTS, the parameters of all\ncandidate networks are trained for each iteration, leading\nto an unacceptable memory requirements. One-shot NAS\nmethod do",
  "crease the number of\nparameters in the model body, We increase the base width\nrw to 2.0 (Making the base width 128, twice the normal\nwidth), and we also change rb from 3.0 to the default 4.0.\nWe remove the \ufb01nal extra 1\u00d71 convolution, so that the label\nembeddings have a large number of \ufb01lters to account for the\nlarger number of labels. Finally, we increase the number of\nlayers in the second stage from 3 to 4. For the hybrid model,\nwe use convolutions in the \ufb01rst two stages.\nFor pretraining on 256",
  "onference on Computer Vision\nand Pattern Recognition, pages 9308\u20139316, 2019. 8, 12\n[77] Yi Zhu, Karan Sapra, Fitsum A. Reda, Kevin J. Shih, Shawn\nNewsam, Andrew Tao, and Bryan Catanzaro. Improving Se-\nmantic Segmentation via Video Propagation and Label Re-\nlaxation. 2019. 8\n[78] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\nLe. Learning transferable architectures for scalable image\nrecognition.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 86",
  "gure 7: Saliency maps on three different set of images. The text on the left of the row indicates\nwhether Anisotropic model or ImageNet model was used. The \ufb01rst two rows show the saliency maps\nwhere both model gave correct predictions. We can see from saliency maps that the Anisotropic\nmodel has more diffused saliency maps. The second two rows show the saliency maps where\nAnisotropic model gave correct predictions and ImageNet model gave wrong predictions. The fail-\nure of ImageNet model might b",
  " strategy. However, given a transformer model with \ufb01xed embedding\ndimension, directly increasing the number of heads will reduce the number of embeddings allocated\nto each head, making the computed attention map less comprehensive and accurate as shown in\nTab. 9 in [49]. To address this dilemma, we explore attention expansion that linearly projects the\nmulti-head attention maps to a higher-dimensional space spanned by a larger number of attention\nmaps. As such, the number of attention heads (use",
  "., 2017) has emerged as a popular approach to do\nso, but the costly memory requirement of self-attention hinders its application to long sequences\nand multidimensional data such as images2. Linear attention mechanisms (Katharopoulos et al.,\n2020; Choromanski et al., 2020) offer a scalable remedy for high memory usage but fail to model\ninternal data structure, such as relative distances between pixels or edge relations between nodes in\na graph.\nThis work addresses both issues. We propose lambda l",
  "erNorm \u2192X.\nWe have tried different activations on top of the aforementioned\nMLP-based baseline, and kept GeLU for its accuracy and to be consistent with the transformer choice.\nActivation\ntop-1 acc.\nGeLU (baseline)\n79.2%\nSILU\n78.7%\nHard Swish\n78.8%\nReLU\n79.1%\nI\n\n--------------------------------------------------\nAblation on the size of the communication MLP.\nFor the MLP that replaced the class-attention,\nwe have explored different sizes of the latent layer, by adjusting the expansion factor e in",
  "r of the polynomial. To\nthat end, we cast polynomial parameters estimation as a coupled\ntensor factorization [11] that jointly factorizes all the polynomial\nparameters tensors. We introduce three joint decompositions with\nshared factors and exhibit the resulting hierarchical structures (i.e.,\narchitectures of neural networks).\nIn our preliminary works [12], [13], [14], we introduced\nthe concept of higher-order expansions for both generative and\ndiscriminative networks. In this work, our improvem",
  "image\u2019s spatial dimension by 4\u00d7, 2\u00d7, and 2\u00d7, respectively. The feature\nmaps generated by the last RC are of a size of [H/16, W/16, D], which are then \ufb02attened into visual\ntokens and fed into the following NCs.\n3.4\nNormal cell\nAs shown in the bottom right part of Figure 2, NCs share a similar structure with the reduction cell\nexcept for the absence of the PRM. Due to the relatively small ( 1\n16\u00d7) spatial size of feature maps after\nRCs, it is unnecessary to use PRM in NCs. Given f3 from the third ",
  "), the query chunk (dark\nbrown) attends to itself and the i\u2019th neighbor chunk.\nFigure 10. The random-shifting strategy does not harm the model\nperformance (Left), an accelerates the Vision Longformer train-\ning signi\ufb01cantly (Right).\nWhen zooming in, the performance\nof \u201crandom-shift and switch at 75%\u201d is slightly better than the\n\u201cSliding-chunk attention with 8 neighbor chunks\u201d.\nthis, we de\ufb01ne 10 modes of the sliding-chunk local atten-\ntion:\n\u2022 0 (default): attend to itself and all 8 neighbor chunk",
  " of N \u22641024, which makes\nour latent Transformer comparable in input size to models\nin wide-spread use in language. The latent array itself is\ninitialized using a learned position encoding (Gehring et al.,\n2017) (see Appendix Sec. C for details).\nIterative cross-attention & weight sharing. The size of\nthe latent array allows us to directly model pixels and to\nbuild deeper Transformers, but the severity of the bottle-\nneck may restrict the network\u2019s ability to capture all of the\n1We ignore the con",
  "ter Vision and\nPattern Recognition, pages 248\u2013255, 2009. 8\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding.\narXiv\npreprint arXiv:1810.04805, 2018. 22\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\n23\n\n--------------------------------------------------\nHeigold, Sylvain Gelly, et al. An",
  "04.\n20\n\n--------------------------------------------------\n[32] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for \ufb01ne-grained categorization. In\n4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia,\n2013.\n[33] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01cation with deep convolutional neural\nnetworks. Advances ",
  "hese MLP-like models can not be used in other downstream\ntasks, e.g., object detection and semantic segmentation due\nto the two challenges:\n(1) Such models are composed of blocks with the same\narchitectures, resulting in features with a single scale in low\nresolution. Thus, the non-hierarchical architectures make\nthe model infeasible to provide pyramid feature representa-\ntions.\n(2) They can not deal with \ufb02exible input scales. They\nutilize the Spatial FC (Figure 1b), which is connected to\nall sp",
  "themes have emerged. The \ufb01rst studies\nthe origin of the bene\ufb01ts of batch normalization during train-\ning (Balduzzi et al., 2017; Santurkar et al., 2018; Bjorck\net al., 2018; Luo et al., 2018; Yang et al., 2019; Jacot et al.,\n2019; De & Smith, 2020), while the second seeks to train\ndeep ResNets to competitive accuracies without normaliza-\ntion layers (Hanin & Rolnick, 2018; Zhang et al., 2019a; De\n& Smith, 2020; Shao et al., 2020; Brock et al., 2021).\nA key theme in many of these works is that it",
  "alable and ef\ufb01cient\nthan existing one-shot approaches since the supernet training now\ncan focus on the promising task-speci\ufb01c subnets recommended by\nthe search algorithm, instead of on all subnets globally.\nTo visualize the difference between the existing approach\nof disentangling supernet training from architecture search, and\nour approach that use architecture search to guide the supernet\ntraining, let us consider the following problem of minimizing a\ntwo-variable Rosenbrock function [89]:\nmin",
  "E\nwith comparable model size with T2T-ViT-14, the ghost op-\neration can reduce the number of parameters and MACs of\nmodels, so the T2T-ViT-Ghost only has 80% parameters and\nof T2T-ViT-14.\nFor fair comparisons, the above variants of T2T-ViT\nare designed with comparable size with T2T-ViT-14 and\nResNet50 except for T2T-ViT-Ghost. It is noted that our\ndesign of each transferring is not the only choice, and we\nwish the transfers can motivate the model designs of Trans-\nformers in vision tasks.\nA.2. D",
  "es the convnet\u2019s large third stage. Note that in\nthis experiment, the training process is similar to DeiT:\n300 epochs, we measure the top-1 validation accuracy on\nImageNet, and the speed as the number of images that one\nGPU can process per second.\nOne interesting observation that we show Figure 3 is\nthat the convergence of grafted models during training\nseems to be similar to a convnet during the early epochs\nand then switch to a convergence rate similar to DeiT-S. A\nhypothesis is that the convo",
  "nstance, 8 or 4 bits, the set of possible values\n*These authors contributed equally.\nCopyright 2021 to the authors. Preprint version.\nCorrespondence to defossez at fb.com.\nis no longer a good approximation of R, hence preventing the\nuse of \ufb01rst-order optimization methods. Speci\ufb01cally, uniform\nquantization requires using the round function, which has\nzero gradients wherever it is differentiable.\nQuantization can be done as a post-processing step to\nregular training. However, errors accumulate in ",
  "ccuracy in Tab. 8. The ac-\ncuracies of the CNN branch, the transformer branch, and\nthe Conformer-S respectively reach 83.3%, 83.1%, and\n83.4%. In contrast, the ensemble model (DeiT-S+ResNet-\n2Refer to Appendix for detailed attention-based sampling.\n0\u00b0\n60\u00b0\n120\u00b0\n180\u00b0\n240\u00b0\n300\u00b0\nConformer-S\nDeiT-S\nResNet-101\nDeiT-B\n50\n55\n60\n65\n70\n75\n80\n85\n112\n160\n224\n336\n448\nTop-1  Accuracy(%)\nImage Resolution\nConformer-S\nDeiT-S\nResNet-50\nResNet-152\n(a)\n(b)\nFigure 5: Generalization capability. (a) Comparison of ro-\n",
  " observations\nso that they are easily described by its external memory (or\nschemata), while during accommodation, PAWS updates its\nexternal memory to account for the new observations.\nThe use of a supervised support set has some practical\nadvantages as well, since it enables the model to learn ef\ufb01-\nciently. However, it remains an interesting question to see if\none can learn competitive representations in this framework\nusing only instance supervision and more \ufb02exible memory\nrepresentations. We p",
  " Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistill-\ning the knowledge in a neural network.\narX",
  "in a window size. After all, there\nare four components in this \u201dlocal attention + global mem-\nory\u201d mechanism, namely global-to-global, local-to-global,\nglobal-to-local, and local-to-local, as illustrated in Figure 2\n(Left). In Equation (2), a Multi-head Self-Attention (MSA)\nblock with the Vision Longformer attention mechanism is\ndenoted as MSAViL, i.e., a = ViL in Equation (2).\nRelative positional bias for Vision Longformer. Follow-\ning [33, 2, 26], we add a relative positional bias B to each\nhe",
  "] dataset as labeled data and the JFT-300M\ndataset [26, 60] as unlabeled data.\nWe train a pair of\nEf\ufb01cientNet-L2 networks, one as a teacher and one as a\nstudent, using Meta Pseudo Labels. The resulting student\nnetwork achieves the top-1 accuracy of 90.2% on the Im-\nageNet ILSVRC 2012 validation set [56], which is 1.6%\nbetter than the previous record of 88.6% [16]. This student\nmodel also generalizes to the ImageNet-ReaL test set [6], as\nsummarized in Table 1. Small scale semi-supervised learn-\ni",
  "nd Machine Intelligence. 6\nNakkiran, P.; Kaplun, G.; Bansal, Y.; Yang, T.; Barak, B.;\nand Sutskever, I. 2020. Deep Double Descent: Where Bigger\nModels and More Data Hurt. In International Conference on\nLearning Representations. 1\nPark, D. S.; Chan, W.; Zhang, Y.; Chiu, C.-C.; Zoph, B.;\nCubuk, E. D.; and Le, Q. V. 2019. SpecAugment: A Simple\nData Augmentation Method for Automatic Speech Recogni-\ntion. In Interspeech. 2, 3\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Kill",
  "s one of the earliest convolutional networks in com-\nputer vision. AlexNet uses three max-pooling layers. In the\nmax-pooling layer, the spatial size of the feature is reduced\nby half, and the channel size is increased by the convolu-\ntion after the max-pooling. VGGnet [30] uses 5 spatial res-\nolutions using 5 max-pooling. In the pooling layer, the spa-\ntial size is reduced by half and the channel size is doubled.\nGoogLeNet [31] also used the pooling layer. ResNet [13]\nperformed spatial size redu",
  "the\nInternational Conference on Learning Representations.\nFan, A.; Stock, P.; Graham, B.; Grave, E.; Gribonval, R.;\nJegou, H.; and Joulin, A. 2020. Training with Quantization\nNoise for Extreme Model Compression. In ICLR 2021.\nGong, R.; Liu, X.; Jiang, S.; Li, T.; Hu, P.; Lin, J.; Yu, F.;\nand Yan, J. 2019. Differentiable soft quantization: Bridging\nfull-precision and low-bit neural networks. In Proceedings\nof the IEEE International Conference on Computer Vision.\nHe, K.; Zhang, X.; Ren, S.; and Su",
  " information \ufb02ow than dep-sep+1 \u00d7 1 conv. This\nis more apparent in small models which have less channels,\nso 1 \u00d7 1 conv cannot effectively mix channel information.\nChannel Multiplexing: To make models more ef\ufb01cient,\nmethods such as scaling down the number of channels by\na factor (named width multiplier), or scaling down the in-\nput resolution have been proposed. Here we investigate the\nimpact of channel multiplexing as an alternative to reduce\nmodel complexity. To be consistent with the main exp",
  "nd Song Han. Proxylessnas: Direct\nneural architecture search on target task and hardware. In In-\nternational Conference on Learning Representations, 2018.\n7\n[39] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted\nresiduals and linear bottlenecks.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4510\u20134520, 2018. 8\n[40] Ross\nWightman.\nPytorch\nimage\nmod-\nels.\nhttps://github.com/rwightman/\npytorch-image-",
  "tzer et al., 2011) and\nFashion-MNIST datasets (Xiao et al., 2017). Once again, SAM enables a simple WideResNet to\nachieve accuracy at or above the state-of-the-art for these datasets: 0.99% error for SVHN, and\n3.59% for Fashion-MNIST. Details are available in appendix B.1.\nTo assess SAM\u2019s performance at larger scale, we apply it to ResNets (He et al., 2015) of different\ndepths (50, 101, 152) trained on ImageNet (Deng et al., 2009). In this setting, following prior work\n(He et al., 2015; Szegedy ",
  " Cosine LR Decay\n79.3\n+0.3\n+ Increase training epochs\n78.8 \u2020\n-0.5\n+ EMA of weights\n79.1\n+0.3\n+ Label Smoothing\n80.4\n+1.3\n+ Stochastic Depth\n80.6\n+0.2\n+ RandAugment\n81.0\n+0.4\n+ Dropout on FC\n80.7 \u2021\n-0.3\n+ Decrease weight decay\n82.2\n+1.5\n+ Squeeze-and-Excitation\n82.9\n+0.7\n+ ResNet-D\n83.4\n+0.5\nTable 1. Additive study of the ResNet-RS training recipe. The\ncolors refer to Training Methods , Regularization Methods\nand Architecture Improvements .\nThe baseline ResNet-200\nwas trained for the standard 90 ",
  "jecture that this maybe due to the use of\nshifted windows in Swin, which might not work well with CPE.\nArchitecture settings\nWe report the detailed settings of Twins-PCPVT in Table 9 (in supplemen-\ntary), which are similar to PVT [8]. Therefore, Twins-PCPVT has similar FLOPs and number of\nparameters to [8].\n3.2\nTwins-SVT\nVision transformers suffer severely from the heavy computational complexity in dense prediction\ntasks due to high-resolution inputs. Given an input of H \u00d7 W resolution, the comp",
  "d task cascade for instance\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4974\u2013\n4983, 2019. 6, 9\n[10] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, et al. Mmdetection: Open mmlab detection tool-\nbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\n6, 9\n[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder wit",
  "lbox and bench-\nmark.\nhttps://github.com/open-mmlab/\nmmsegmentation, 2020. 8, 10\n[17] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe.\nRandaugment:\nPractical automated data augmenta-\ntion with a reduced search space.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702\u2013703, 2020. 9\n[18] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE Inter",
  "Therefore, we only need to perform\ninterpolation to shift from one resolution to another.\n5\n\n--------------------------------------------------\nTable 2: Detailed con\ufb01gurations of different variants of GFNet. For hierarchical models, we provide\nthe number of channels and blocks in 4 stages. The FLOPs are calculated with 224 \u00d7 224 input.\nModel\n#Blocks\n#Channels\nParams (M)\nFLOPs (G)\nGFNet-Ti\n12\n256\n7\n1.3\nGFNet-XS\n12\n384\n16\n2.9\nGFNet-S\n19\n384\n25\n4.5\nGFNet-B\n19\n512\n43\n7.9\nGFNet-H-Ti\n[3, 3, 10, 3]\n[64",
  "Net-50 + DropBlock (kp=0.9) [10] + label smoothing (0.1)\n224\n25.6M\n78.35\n94.15\nResNet-50 + MEAL [42]\n224\n25.6M\n78.21\n94.01\nResNet-50 + Ours (MEAL V2)\n224\n25.6M\n80.67\n95.09\nResNet-50 + FixRes [49]\n384\n25.6M\n79.0\n94.6\nResNet-50 + FixRes (*) [49]\n384\n25.6M\n79.1\n94.6\nResNet-50 + Ours (MEAL V2)\n380\n25.6M\n81.72\n95.81\nResNet-50 + FixRes [49] + CutMix\n320\n25.6M\n79.7\n94.9\nResNet-50 + FixRes [49] + CutMix (*)\n320\n25.6M\n79.8\n94.9\nResNet-50 + Ours (MEAL V2) + CutMix\n224\n25.6M\n80.98\n95.35\nTable 4. Comparison",
  "softplus provides a smooth approximation to the ReLU, but has slope\n1 \u2212\n1\n1+exp(\u03b2z) \u21921\n2 around z = 0 (where \u03b2 controls the curvature).\nConvolution and Convergence to Equilibrium.\nWhereas the original DEQ model focused pri-\nmarily on self-attention transformations [54], where all hidden units communicate globally, MDEQ\nmodels face additional challenges due to the nature of typical vision models. Speci\ufb01cally, our MDEQ\nmodels employ convolutions with small receptive \ufb01elds (e.g., the two 3 \u00d7 3 conv",
  "els to initialize the backbone models and jointly train the\nwhole model for 30 epochs. We set the learning rate of the prediction module to batch size\n1024\n\u00d7 0.001 and\nuse 0.01\u00d7 learning rate for the backbone model. We \ufb01x the weights of the backbone models in the\n\ufb01rst 5 epochs. All of our models are trained on a single machine with 8 GPUs. Other training setups\nand details can be found in the supplementary material.\n6\n\n--------------------------------------------------\n4\n6\n8\n10\n12\nGFLOPs\n80\n81\n8",
  "ith positional or content attention masked off at test time.\n\n--------------------------------------------------\n",
  "of probing other promising paths that\nare yet to be sampled and get trained, which can be easily\nful\ufb01lled by uniform sampling from A. For achieve a bal-\nanced trade-off of exploration and exploitation, we adopt a\ntypical \u03f5-sampling policy, i.e., implementing uniform sam-\npling both from A and pool P (line 4 of Algorithm 1),\na \u223c(1 \u2212\u03f5) \u00b7 U(A) + \u03f5 \u00b7 U(P),\n(8)\nwhere \u03f5 \u2208[0, 1] indicates the probability of sampling from\nthe pool P. Note that candidate pool runs through the train-\ning process of supern",
  "Y-8GF [40]\u22c6\n39M\n2242\n591.6\n81.7\n87.4\n70.8\nRegNetY-16GF [40]\u22c6\n84M\n2242\n334.7\n82.9\n88.1\n72.4\nEf\ufb01cientNet-B0 [48]\n5M\n2242\n2694.3\n77.1\n83.5\n64.3\nEf\ufb01cientNet-B1 [48]\n8M\n2402\n1662.5\n79.1\n84.9\n66.9\nEf\ufb01cientNet-B2 [48]\n9M\n2602\n1255.7\n80.1\n85.9\n68.8\nEf\ufb01cientNet-B3 [48]\n12M\n3002\n732.1\n81.6\n86.8\n70.6\nEf\ufb01cientNet-B4 [48]\n19M\n3802\n349.4\n82.9\n88.0\n72.3\nEf\ufb01cientNet-B5 [48]\n30M\n4562\n169.1\n83.6\n88.3\n73.6\nEf\ufb01cientNet-B6 [48]\n43M\n5282\n96.9\n84.0\n88.8\n73.9\nEf\ufb01cientNet-B7 [48]\n66M\n6002\n55.1\n84.3\nEf\ufb01cientNet-B5 RA [12",
  "\nis still under the original transformers. These smaller mod-\nels are constructed by compressing the original model along\nwith depth so all transformer blocks share the same knowl-\nedge. Such structure induces the unavoidable loss of model\ncapacity.\nIn this paper, we present a parameter deployment frame-\nwork that deploys trainable parameters more effectively: go-\ning wider instead of deeper. We then implement it on the\ntransformer and named it as WideNet. Specially, we \ufb01rst em-\nploys parameter ",
  "tion to enhance the\nperformance of ViT on the ImageNet benchmark and obtain an about 81.8% ImageNet top-1 accuracy,\nwhich is comparable to that of the state-of-the-art convolutional networks. Chen et al. further treat the\nimage processing tasks (e.g., denosing and super-resolution) as a series of translations and develop\nthe IPT model for handling multiple low-level computer vision problems [4]. Nowadays, transformer\narchitectures have been used in a growing number of computer vision tasks [11] ",
  "m ran-\ndom crop (with additional random horizontal \ufb02ip) in training\nand central crop in evaluation. For BERT we use wordpiece\nsequence of maximum 64 tokens since the input texts are\nno longer than 20 unigrams. The softmax temperature vari-\nable is initialized as 1.0 (this temperature variable is shared\nbetween image-to-text loss and text-to-image loss) and we\nuse 0.1 as label smoothing parameter in the softmax losses.\nWe use LAMB optimizer (You et al., 2020)1 with weight\ndecay ratio 1e-5. The le",
  "experimental results on \u201cCNN to ViT\u201d in\nTab. 6, we can \ufb01nd both SE (ViT-SE) and Deep-Narrow\nstructure (ViT-DN) bene\ufb01t the ViT but the most effective\nstructure is deep-narrow structure, which decreases model\nsize and MACs nearly 2x and brings 0.9% improvement on\nthe baseline model ViT-S/16.\nWe further apply these structures from CNN to our T2T-\nViT, and conduct experiments on ImageNet under the same\ntraining scheme.\nWe take ResNet50 as the baseline for\nCNN, ViT-S/16 for ViT, and T2T-ViT-14 for T2",
  " to a much larger dataset with higher-resolution images:\nImageNet [16]. As with CIFAR-10 classi\ufb01cation, we add a shallow classi\ufb01cation layer after the\nMDEQ module to fuse the equilibrium outputs from different scales, and train on a combined loss.\nWe benchmark both a small MDEQ model and a large MDEQ to provide appropriate comparisons\nwith a number of reference models, such as ResNet-18, -34, -50, and -101 [25]. Note that MDEQ\nhas only one layer of residual blocks followed by multi-resolution fu",
  "ayer 4\nLayer 7\nLayer 10\n(b) DeiT\nLayer 1\n( ) = 0.01\nHead 1\n( ) = 0.52\nHead 2\n( ) = 0.01\nHead 3\n( ) = 0.00\nHead 4\n( ) = 0.04\nHead 5\n( ) = 0.92\nHead 6\n( ) = 0.07\nHead 7\n( ) = 0.01\nHead 8\n( ) = 0.07\nHead 9\nLayer 4\n( ) = 0.00\n( ) = 0.82\n( ) = 0.03\n( ) = 0.82\n( ) = 0.14\n( ) = 0.84\n( ) = 0.75\n( ) = 0.00\n( ) = 0.34\nLayer 7\n( ) = 0.15\n( ) = 0.00\n( ) = 0.59\n( ) = 0.00\n( ) = 0.04\n( ) = 0.02\n( ) = 0.04\n( ) = 0.00\n( ) = 0.00\nLayer 10\n( ) = 0.00\n( ) = 0.00\n( ) = 0.00\n( ) = 0.00\n( ) = 0.00\n( ) = 0.00\n( ) = 0.",
  "-------------------------------------------\nthan existing models. In addition, on three commonly used\ndownstream single-label classi\ufb01cation datasets it reaches\nnew state-of-the-art accuracies. We also show that TResNet\ngeneralizes well to other computer vision tasks, reaching\ntop scores on multi-label classi\ufb01cation and object detection\ndatasets.\nReferences\n[1] Mart\u00b4\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-\nmawat, Geoffrey Irving, Mic",
  "he\nhead decreases the spatial resolution of the feature maps from 38\u00d738 to 1\u00d71. All the output feature\nmaps from the PyConvs in the head are used for detections.\nFor each of the six output feature maps selected for detection {S3F M, HF M1, HF M2, HF M3,\nHF M4, HF M5} the framework performs the detection using a coresponding number of default boxes\n(anchor boxes) {4, 6, 6, 6, 4, 4} for each spatial location. For instance, for (S3F M) output feature\nmaps with the spatial resolution 38\u00d738, using th",
  "cknowledgment\nThis work was supported in part by the National Key Research and Development Program of China\nunder Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under\nGrant 62125603, Grant 61822603, Grant U1813218, Grant U1713214, in part by Beijing Academy\nof Arti\ufb01cial Intelligence (BAAI), and in part by a grant from the Institute for Guo Qiang, Tsinghua\nUniversity.\n10\n\n--------------------------------------------------\nReferences\n[1] Gregory A Baxes. Digital ",
  "rove.\nTraining ef\ufb01ciency has gained signi\ufb01cant interests recently.\nFor instance, NFNets (Brock et al., 2021) aim to improve\ntraining ef\ufb01ciency by removing the expensive batch nor-\nmalization; Several recent works (Srinivas et al., 2021) fo-\ncus on improving training speed by adding attention layers\ninto convolutional networks (ConvNets); Vision Transform-\ners (Dosovitskiy et al., 2021) improves training ef\ufb01ciency\non large-scale datasets by using Transformer blocks. How-\never, these methods often",
  " net-\nworks (CNNs) to the ViT architecture (i.e. shift, scale,\nand distortion invariance) while maintaining the merits of\nTransformers (i.e. dynamic attention, global context, and\nbetter generalization). We validate CvT by conducting ex-\ntensive experiments, showing that this approach achieves\nstate-of-the-art performance over other Vision Transform-\ners and ResNets on ImageNet-1k, with fewer parame-\nters and lower FLOPs.\nIn addition, performance gains\nare maintained when pretrained on larger da",
  "subsumption in description logics. In IJCAI (1), 1995. 3\n[18] Cynthia Rudin. Stop explaining black box machine learning\nmodels for high stakes decisions and use interpretable mod-\nels instead. Nature Machine Intelligence, 2019. 1, 8\n[19] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nPar-\nallel distributed processing: Explorations in the microstruc-\nture of cognition, vol. 1. In David E. Rumelhart, James L.\nMcClelland, and CORPORATE PDP Research Group, ed-\nitors, Parallel Distributed Process",
  "\n60%\nLinear 5-shot ImageNet accuracy\nToken (ICLR'21)\nToken (linear)\nToken + head-wd\nGAP + head-wd\nMAP + head-wd\nFigure 4: Left and middle: The dependence of 5-shot ImageNet accuracy and upstream performance\ndepends on the weight decay strength. Normally, a single weight decay value is applied to all weights\n(corresponds to the diagonal on the heatmaps). We show that by using weight decay values for the\n\u201chead\u201d and the rest of the weights one signi\ufb01cantly improves few-shot transfer performance. Ri",
  "omputational cost and\nthe accuracy for 14 \u00d7 14 feature map in DeiT-S [22].\nComponent-wise analysis.\nWe perform a component-\nwise analysis to study the effects of different position en-\nModel\n#Param.\nInput\nMACs\nTop-1\n(M)\nAcc (%)\nConvnets\nResNet-50 [10]\n25M\n2242\n4121\n79.0\nRegNetY-4.0GF [15]\n21M\n2242\n4012\n79.4\nEf\ufb01cientNet-B1 [21]\n8M\n2402\n712\n79.1\nEf\ufb01cientNet-B5 [21]\n30M\n4562\n10392\n83.6\nTransformers\nViT-B/16 [6]\n86M\n3842\n55630\n77.9\nViT-L/16 [6]\n307M\n3842\n191452\n76.5\nDeiT-Ti [22]\n5M\n2242\n1261\n72.2\nCP",
  "--------------------\nModel\nCIFAR-10\nCIFAR-100\nFashion-MNIST\nMNIST\n# Params\nMACs\nConvolutional Networks (Designed for ImageNet)\nResNet18\n90.27%\n63.41%\n93.51%\n99.18%\n11.18 M\n0.04 G\nResNet34\n90.51%\n64.52%\n93.47%\n99.24%\n21.29 M\n0.08 G\nResNet50\n90.60%\n61.68%\n93.17%\n99.18%\n23.53 M\n0.08 G\nMobileNetV2/0.5\n84.78%\n56.32%\n93.49%\n99.08%\n0.70 M\n< 0.01 G\nMobileNetV2/1.0\n89.07%\n63.69%\n93.62%\n99.28%\n2.24 M\n0.01 G\nMobileNetV2/1.25\n90.60%\n65.24%\n93.83%\n99.25%\n3.47 M\n0.01 G\nMobileNetV2/2.0\n91.02%\n67.44%\n94.07%\n99.",
  "the training settings (e.g., learning rate, training epochs, etc.) of [10].\n3.2\nImage classi\ufb01cation datasets\nStem\nImage\nObject class\nTokenLearner\nTransformer\n...\nClassification head\n8 tokens\n...\nTransformer\n1024 tokens\n8 tokens\nTransformer\n8 tokens\n8 tokens\n8 tokens\nStem\nImage\nObject class\n...\nClassification head\n1024 tokens\nTokenLearner\nTokenFuser\nTransformer\nRepeat\n1024 tokens\n(a)\n(b)\nTransformer\nFigure 3: Our models following the ViT archi-\ntecture. (a) with TokenLearner and (b) with\nboth Tok",
  "ize on the upper layers of the pyramid. In the \ufb01rst stage of the\nnetwork, our PyConv with four layers contains kernel sizes of: 7\u00d79\u00d79, 5\u00d77\u00d77, 3\u00d75\u00d75 and 3\u00d73\u00d73\n(the temporal dimension comes \ufb01rst).\nFor video classi\ufb01cation, we perform the experiments on Kinetics-400 [50], which is a large-scale\nvideo recognition dataset that contains \u223c246k training videos and 20k validation videos, with 400\naction classes. Similar to image recognition, use the SGD optimizer with a standard momentum of\n0.9 and weight",
  "s are\nconsistent with [22].\nTable A.1: ImageNet Top-1 accuracy of ef\ufb01cient self-attention variants (after 300 epochs of training).\nModel\nComplexity\nTop-1\nDeiT-S [65]\nO(N 2)\n79.9\nSRA (Average Pool) [71]\nO(N 2/R2)\n73.5\nSRA (Convolutional) [71]\nO(N 2/R2)\n74.0\nLinformer (k=\u221an) [70]\nO(kN)\n75.7\nEf\ufb01cient Transformer [56]\nO(N)\n76.3\nAxial [30]\nO(N\n\u221a\nN)\n78.4\nI\n\n--------------------------------------------------\nA.3\nTraining and testing with varying resolution\nAs discussed in the main manuscript, for sever",
  "de\n2242 \u00d7 3\n3 \u00d7 3 conv\n32\n1\n2\n1122 \u00d7 32\nMB1 K3\n16\n1\n1\n1122 \u00d7 16\nChoice Block\n32\n4\n2\n562 \u00d7 32\nChoice Block\n40\n4\n2\n282 \u00d7 40\nChoice Block\n80\n4\n2\n142 \u00d7 80\nChoice Block\n96\n4\n1\n142 \u00d7 96\nChoice Block\n192\n4\n2\n72 \u00d7 192\nChoice Block\n320\n1\n1\n72 \u00d7 320\n1 \u00d7 1 conv\n1280\n1\n1\n72 \u00d7 1280\nglobal avgpool\n-\n1\n-\n1280\nFC\n1000\n1\n-\n11\n\n--------------------------------------------------\nTable 6: Operation choices for each MobileNetV2-based Choice Block in Table 5, where ID means for an identity mapping.\nblock type\nexpansi",
  "ach visual token.\n2.2\nVision transformers with learned IB\nViT [19] is the pioneering work that applies a pure transformer to vision tasks and achieves promising\nresults. However, since ViT lacks intrinsic inductive bias in modeling local visual structures, it indeed\nlearns the IB from amounts of data implicitly. Following works along this direction are to simplify\nthe model structures with fewer intrinsic IBs and directly learn them from large scale data [46, 70,\n71, 21, 18], which have achieved",
  "24\n82.5\n-\n87.2\nConViT-S+ [15]\n48.0\n20.0\n224\n82.2\n-\n-\nConViT-B [15]\n86.0\n34.0\n224\n82.4\n-\n-\nConViT-B+ [15]\n152.0\n60.0\n224\n82.5\n-\n-\nPiT-B [26]\n73.8\n25.0\n224\n82.0\n-\n-\nTNT-B [22]\n65.6\n28.2\n224\n82.8\n96.3\n-\nT2T-ViT-19 [88]\n39.2\n8.9\n224\n81.9\n95.7\n86.9\nViTAE-B-Stage\n48.5\n13.8\n224\n83.6\n96.4\n87.9\n12\n\n--------------------------------------------------\nTable 6: Model details of ViTAE variants.\nModel\nReduction Cell\nNormal Cell\nNC\nParams Macs\ndilation\ncells heads embed cells Arrangement\n(M)\n(G)\nViTAE-T\n[1, 2, ",
  "ons. In Conference on\nComputer Vision and Pattern Recognition, 2015. 22\n[62] Mingxing Tan and Quoc V. Le. Ef\ufb01cientnet: Rethinking model scaling for convo-\nlutional neural networks. arXiv preprint arXiv:1905.11946, 2019. 1, 14, 15, 21\n[63] Hugo Touvron, M. Cord, M. Douze, F. Massa, Alexandre Sablayrolles, and H.\nJ\u00b4egou. Training data-ef\ufb01cient image transformers & distillation through attention.\narXiv preprint arXiv:2012.12877, 2020. 3, 7, 8, 12, 13, 14, 15, 16, 17, 18, 21, III\n[64] Hugo Touvron, ",
  "onstructs\nsuch a hierarchy with transformers. ViP divides visual representations into two\nlevels, the part level and the whole level. Information of each part represents a\ncombination of several independent vectors within the whole. To model the repre-\nsentations of the two levels, we \ufb01rst encode the information from the whole into\npart vectors through an attention mechanism, then decode the global information\nwithin the part vectors back into the whole representation. By iteratively parsing the",
  "en Ma, and Shilei Wen. Multi-label classi\ufb01cation with\nlabel graph superimposing. ArXiv, abs/1911.09243, 2019.\n[41] Ross\nWightman.\npytorch-image-models,\n2019.\nhttps://github.com/rwightman/pytorch-image-models.\n[42] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines\nfor human pose estimation and tracking. In Proceedings of\nthe European conference on computer vision (ECCV), pages\n466\u2013481, 2018.\n[43] Saining Xie, Ross Girshick, Piotr Doll\u00b4ar, Zhuowen Tu, and\nKaiming He. Aggregated residual trans",
  "hyper-parameters:\nsize, stride, share_t, and share_c. size indicates\nhow many tokens does a pattern affects; stride indicates\nthe number of tokens to be skipped by the pattern; share_t\nis a binary deciding whether all the tokens covered by size\nare set to zero using the same noise mask or independent\nnoise masks; and share_c is a binary deciding whether\na the dropout pattern shared along the channel dimension\nC. Once the values of size, stride, share_t, and\nshare_c are decided, at each training ",
  " Table 4).\nIn Figure 7,\nwe show the running time (includ-\ning forward and backward) and memory usage of our\n\u201cSCw/Handgrad\u201d implementation of conv-like local atten-\ntion (sliding chunk attention without padding mode) with\ndifferent window sizes. We can see that the speed is not\nsensitive to the window size for small window sizes (\u226417)\nand the memory usage monotonically increases.\nFinally, both the \u201cunfold/nn.F\u201d and the \u201ccuda kernel\u201d\nimplementations support dilated conv-like attention. The\ncustomi",
  "------------------------\n\ufb01nd gMLPs can achieve appealing results on challenging tasks such as SQuAD without self-attention,\nand can signi\ufb01cantly outperform Transformers in certain cases. We also \ufb01nd the inductive bias in\nTransformer\u2019s multi-head self-attention useful on downstream tasks that require cross-sentence align-\nment. However in those cases, making gMLP substantially larger closes the gap with Transformers.\nMore practically, blending a small single-head self-attention into gMLP allows f",
  "hes. For CIFAR-10 and CIFAR-100, we \ufb01ne-tune the models for 64 epochs, and for\n\ufb01ne-grained datasets, we \ufb01ne-tune the models for 300 epochs. Table 9 compares the transfer learning\nresults of TNT to those of ViT, DeiT and other convolutional networks. We \ufb01nd that TNT outperforms\nDeiT in most datasets with less parameters, which shows the superiority of modeling pixel-level\nrelations to get better feature representation.\nTable 9: Results on downstream image classi\ufb01cation tasks with ImageNet pre-tra",
  "rformer\n71.12\n73.09\n78.81\n78.72\n\u0013\nPar-Global\n75.32\n75.4\n81.6\n81.46\n\u2013\nPar-Linformer\n75.56\n75.33\n81.66\n81.79\n\u0017\nPar-SRA/32\n75.2\n75.26\n81.62\n81.61\n\u2013\nPar-Performer\n75.34\n75.93\n81.72\n81.72\n\u2013\nTable 3. Overall comparison of different attention mechanisms on\nImageNet classi\ufb01cation top-1 accuracy (%), with input size 224.\nTiny-4stage / 4 means that the model has a comparable size with\nDeiT-Tiny, has 4 stages and uses patch size 4x4 in the initial pixel\nspace. \u201c1,2,8,1\u201d are the numbers of attention blocks ",
  "pansion (shrink) rate for the SE is always 0.25.\nStages\nSize\nCoAtNet-0\nCoAtNet-1\nCoAtNet-2\nCoAtNet-3\nCoAtNet-4\nS0-Conv\n1/2\nL=2 D=64\nL=2\nD=64\nL=2\nD=128\nL=2\nD=192\nL=2\nD=192\nS1-MbConv\n1/4\nL=2 D=96\nL=2\nD=96\nL=2\nD=128\nL=2\nD=192\nL=2\nD=192\nS2-MBConv\n1/8\nL=3 D=192\nL=6\nD=192\nL=6\nD=256\nL=6\nD=384\nL=12 D=384\nS3-TFMRel\n1/16\nL=5 D=384\nL=14 D=384\nL=14 D=512\nL=14 D=768\nL=28 D=768\nS4-TFMRel\n1/32\nL=2 D=768\nL=2\nD=768\nL=2\nD=1024\nL=2\nD=1536\nL=2\nD=1536\nresolutions for 30 epochs and obtain the corresponding evaluation",
  " 800 epochs)\nthat must happen sequentially (not in parallel) with the student updates.\nPAWS-NN refers to performing nearest-neighbour classi\ufb01cation directly us-\ning the PAWS-pretrained representations, with the labeled training samples\nas support, while PAWS refers to \ufb01ne-tuning a classi\ufb01er using the available\nlabeled data after PAWS-pretraining.\n6\nMain Results\nIn this section we analyze the features learned by PAWS on\nImageNet [41]. The standard procedure for evaluating semi-\nsupervised methods",
  "e visualization results are provided in Figure 4. Compared with the baseline T2T-ViT,\nour ViTAE covers the single or multiple targets in the images more precisely and attends less to the\nbackground. Moreover, ViTAE can better handle the scale variance issue as shown in Figure 4(b).\nNamely, it can precisely cover the birds no matter they are in small, middle, or large size. Such\nobservations demonstrate that introducing the intrinsic IBs of locality and scale-invariance from\nconvolutions to trans",
  "of modules are abstracted in our framework, and such modules (classes and functions)\nin user\u2019s installed torchvision are registered. In this example, \u2019resnet34\u2019 function4 is\nused to instantiate an object of type ResNet by using a dictionary of keyword arguments\n(**params). i.e. num classess = 1000 and pretrained = True are given as arguments\nof \u2019resnet34\u2019 function. For image classi\ufb01cation models implemented in torchvision or\nthose users add to the registry in our framework, users can easily try ",
  "ageNet\n77.91\n73.38\n76.53\n71.16\n-\nImageNet ReaL\n83.57\n79.56\n82.19\n77.83\n-\nOxford Flowers-102\n89.49\n85.43\n89.66\n86.36\n-\nOxford-IIIT-Pets\n93.81\n92.04\n93.64\n91.35\n-\nImageNet-21k\nCIFAR-10\n98.95\n98.79\n99.16\n99.13\n99.27\nCIFAR-100\n91.67\n91.97\n93.44\n93.04\n93.82\nImageNet\n83.97\n81.28\n85.15\n80.99\n85.13\nImageNet ReaL\n88.35\n86.63\n88.40\n85.65\n88.70\nOxford Flowers-102\n99.38\n99.11\n99.61\n99.19\n99.51\nOxford-IIIT-Pets\n94.43\n93.02\n94.73\n93.09\n94.82\nJFT-300M\nCIFAR-10\n99.00\n98.61\n99.38\n99.19\n99.50\nCIFAR-100\n91.87\n90.4",
  "]. The learning rate\nis initialized to be 0.001 and decayed to zero within 300 epochs following the cosine strategy. We use\na linear warm-up in the \ufb01rst \ufb01ve epochs and the same regularization setting as in [2]. Note that we\ndo not utilize extra tricks in [26, 28] to make fair comparisons although it may further improve the\n5\n\n--------------------------------------------------\nperformance of our method. We use increasing stochastic depth [38] augmentation of 0.2, 0.3, 0.5 for\nsmall, base and larg",
  "--------------------------------------\ntorchdistill: A Modular, Con\ufb01guration-Driven Framework for Knowledge Distillation\n15\nTable 6: Validation mAP of bottleneck-injected R-CNN models for split computing\n(student) trained on COCO 2017 dataset by GHND with original Faster/Mask R-CNN\nmodels (teacher). Reproduced results match those reported in the original work [25].\nBackbone: ResNet-50 and FPN\nmAP\n# Epochs Training time\nBBox\nMask\nFaster R-CNN w/ Bottleneck\n0.359\nN/A\n20\n24hr 13min\nMask R-CNN w/ Bo",
  "ethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105\u20136114, 2019.\n1, 2, 6\n[48] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nCon-\ntrastive representation distillation. In ICLR, 2020. 7\n[49] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00b4e\nJ\u00b4egou. Fixing the train-test resolution discrepancy. In Ad-\nvances in Neural Information Processing Systems, pages\n8252\u20138262, 2019. 1, 6\n[50] C. Wah, S. Branson, P. Welinder, P. Peron",
  " involved, the candidate pool can be maintained by a\nmin-heap structure in real time.\nAs a result, we can conservatively implement local\nsearch by sampling from the candidate pool since it consists\nof a smaller number (but promising) of paths. However, this\ngreedy exploitation brings in the risks of losing path diver-\nsity for the training. In this way, we also favor a global\nsearch with the hope of probing other promising paths that\nare yet to be sampled and get trained, which can be easily\nful",
  "ructure of\nthe network, consisting of a stem, multiple stages and a tail (see\nFig. 2a). The stem and tail are common to all networks and not\nsearched. Each stage in turn comprises of multiple layers, and\neach layer itself is an inverted residual bottleneck structure [56].\n-Network: We search for the input image resolution and the width\nmultiplier (a factor that scales the # of output channels of each\nlayer uniformly [57]). Following previous work [27], [28], [31],\nwe segment the CNN architecture",
  "e on computer vision, pages 2961\u20132969, 2017.\n[24] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual\nrecognition. IEEE transactions on pattern analysis and machine intelligence, 37(9):1904\u20131916, 2015.\n[25] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n[26] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J.",
  "s, RVT has surpassed most of CNN architectures with fewer\nparameters and FLOPs. RVT-Ti\u2217achieves 79.2% Top-1 accuracy on ImageNet-1K validation set,\nwhich is competitive with currently popular ResNet and RegNet series, but only has 1.3G FLOPs and\n10.9M parameters (around 60% lower than CNNs). With the same computation cost, RVT-S\u2217obtains\n81.9% test accuracy, 2.9% higher than ResNet-50. This result is closed to Ef\ufb01cientNet-B4, however\nEf\ufb01cientNet-B4 requires larger 380\u00d7380 input size and has much ",
  "), 1.1% worse on Stylized-ImageNet (16.3% vs. 17.4%),\nand 7.0% worse on defending against FGSM (20.4% vs. 27.4%). Moreover, due to model ensem-\nble, this strategy is 2\u00d7 expensive at the inference stage. These evidences clearly demonstrate the\neffectiveness and ef\ufb01ciency of the proposed shape-texture debiased training.\nDoes our method help models to learn debiased shape-texture representations? Here we take\na close look at whether our method indeed prevents models from being biased toward shape o",
  "OCO\nval2017, 2.6 AP higher than the one with Swin-T [21],\n5.7 AP higher than the one with ResNet50 [12]. We hope\nthese improved baselines will provide a reference for future\nresearch in vision Transformer.\n1PVTv2 has 6 different size variants, from B0 to B5 according to the\nparameter number.\narXiv:2106.13797v4  [cs.CV]  17 Jul 2021\n\n--------------------------------------------------\n2. Related Work\nTransformer Backbones. ViT [7] treats each image as a\nsequence of tokens (patches) with \ufb01xed lengt",
  "gure 13, left), the naive implementation is extremely slow\non TPUs (Figure 13, right).\nD.7\nATTENTION DISTANCE\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed\nthe average distance spanned by attention weights at different layers (Figure 11). This \u201cattention\ndistance\u201d is analogous to receptive \ufb01eld size in CNNs. Average attention distance is highly variable\nacross heads in lower layers, with some heads attending to much of the image, while others at",
  "hlights the hares and\na little bit of the person (near the face) but predicts the in-\ncorrect label of car wheel. Lastly, ConvMLP also pays at-\ntention to the hares but incorrectly classi\ufb01es the images as\nhare. This presents an interesting phenomena when analyz-\ning results. We have a clue that MLP-Mixer and ResMLP\nmight be over\ufb01tting the data. While ConvMLP makes the\nmisclassi\ufb01cation, we note that it is at least paying attention\nto the part of the image that closely corresponds to the label\ntha",
  "sion transformers. arXiv preprint\narXiv:2104.14294, 2021.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of\nthe 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine\nLearning Research, pages 1691\u20131703. PMLR, 13\u201318 Jul 2020a. URL http://proceedings.\nmlr.press/v119/chen20s.html.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and ",
  "] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So\nKweon.\nBAM: bottleneck attention module.\nIn British\nMachine Vision Conference (BMVC) 2018, Newcastle, UK,\npages 1\u201314, 2018.\n[29] Shaoqing Ren, Kaiming He, Ross B. Girshick, Xiangyu\nZhang, and Jian Sun. Object detection networks on convolu-\ntional feature maps. IEEE Trans. Pattern Anal. Mach. Intell.,\n39(7):1476\u20131481, 2017.\n[30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentati",
  "rmed on the NVIDIA 2080Ti GPUs. We\nimplement RVT in three sizes named by RVT-Ti, RVT-S, RVT-B respectively. All of them adopt the\nbest settings investigated in section 2. For RVT\u2217, we add PAAS on multiple transformer blocks. The\npatch-wise augmentation uses the combination of base augmentation introduced in section 6.4. Other\ntraining hyperparameters are same with DeiT [4].\nEvaluation Benchmarks We adopt the ImageNet-1K [45] dataset for training and standard perfor-\nmance evaluation. No other la",
  "omputer\nVision, 2016.\nKuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin,\nI., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M.,\nKolesnikov, A., Duerig, T., and Ferrari, V. The open\nimages dataset v4: Uni\ufb01ed image classi\ufb01cation, object\ndetection, and visual relationship detection at scale. In-\nternational Journal of Computer Vision, 2020.\nLi, A., Jabri, A., Joulin, A., and van der Maaten, L. Learning\nvisual n-grams from web data. In Proceedings of IEEE\nInternational Conference on Comput",
  "Conference on Ma-\nchine Learning (ICML), Lille, France, pages 448\u2013456, 2015.\n[19] Alex Krizhevsky. Learning multiple layers of features from\ntiny images. Technical report, University of Toronto, 2009.\n[20] Hanchao Li, Pengfei Xiong, Jie An, and Lingxue Wang.\nPyramid attention network for semantic segmentation.\nIn\nBritish Machine Vision Conference (BMVC) 2018, Newcas-\ntle, UK, pages 1\u201313, 2018.\n[21] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Se-\nlective kernel networks. In 2019 IEEE Confer",
  "adk =\n\u001aH-Attentionk(X)\nk = 1, . . . , K/2\nV-Attentionk(X)\nk = K/2 + 1, . . . , K\nWhere W O \u2208RC\u00d7C is the commonly used projection matrix that projects the self-attention results\ninto the target output dimension (set as C by default). As described above, one key insight in\nour self-attention mechanism design is splitting the multi-heads into different groups and applying\ndifferent self-attention operations accordingly. In other words, the attention area of each token within\none Transformer block i",
  "ization, vol. 8, no. 3, p. 631\u2013657, Mar.\n1998. [Online]. Available: https://doi.org/10.1137/S1052623496307510\n[62] B. Wu, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda,\nY. Jia, and K. Keutzer, \u201cFbnet: Hardware-aware ef\ufb01cient convnet design\nvia differentiable neural architecture search,\u201d in CVPR, 2019.\n[63] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, \u201cRandaugment: Prac-\ntical automated data augmentation with a reduced search space,\u201d arXiv\npreprint arXiv:1909.13719, 2019.\n[64] J. ",
  "NETX-200MF\n0.2\n2.7\n2.2\n1024\n10\n2.8\n31.1\u00b10.09\nREGNETX-400MF\n0.4\n5.2\n3.1\n1024\n15\n3.9\n27.3\u00b10.15\nREGNETX-600MF\n0.6\n6.2\n4.0\n1024\n17\n4.4\n25.9\u00b10.03\nREGNETX-800MF\n0.8\n7.3\n5.1\n1024\n21\n5.7\n24.8\u00b10.09\nREGNETX-1.6GF\n1.6\n9.2\n7.9\n1024\n33\n8.7\n23.0\u00b10.13\nREGNETX-3.2GF\n3.2\n15.3\n11.4\n512\n57\n14.3\n21.7\u00b10.08\nREGNETX-4.0GF\n4.0\n22.1\n12.2\n512\n69\n17.1\n21.4\u00b10.19\nREGNETX-6.4GF\n6.5\n26.2\n16.4\n512\n92\n23.5\n20.8\u00b10.07\nREGNETX-8.0GF\n8.0\n39.6\n14.1\n512\n94\n22.6\n20.7\u00b10.07\nREGNETX-12GF\n12.1\n46.1\n21.4\n512\n137\n32.9\n20.3\u00b10.04\nREGNETX-16GF",
  "\n0.90\n0.95\n1.00\nTrue Positive Rate\nProdpoly-ResNet50\nResNet50\n(a) ROC for IJB-B\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nFalse Positive Rate\n0.85\n0.88\n0.91\n0.94\n0.97\n1.00\nTrue Positive Rate\nProdpoly-ResNet50\nResNet50\n(b) ROC for IJB-C\nFig. 11: ROC curves of ResNet50 and Prodpoly-ResNet50 under 1:1\nveri\ufb01cation protocol on the IJB-B and IJB-C dataset.\nResults on MegaFace. The MegaFace dataset [85] includes 1M\nimages of 690K different individuals as the gallery set and 100K\nphotos of 530 unique individuals fr",
  ": Deep residual learning for image recognition. In: Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770\u2013778\n(2016)\n12. Heo, B., Lee, M., Yun, S., Choi, J.Y.: Knowledge transfer via distillation of activation bound-\naries formed by hidden neurons. In: Proceedings of the AAAI Conference on Arti\ufb01cial In-\ntelligence. vol. 33, pp. 3779\u20133787 (2019)\n13. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In: Deep\nLearning and Representat",
  " 1), \u22062 =\n(\u22121, \u22121), \u22063 = (1, 1), \u22064 = (1, \u22121).\n\u2022 The locality strength \u03b1h > 0 determines how focused\nthe attention is around its center \u2206h (it can also by un-\nderstood as the \u201ctemperature\u201d of the softmax in Eq. 1).\nWhen \u03b1h is large, the attention is focused only on the\npatch(es) located at \u2206h, as in Fig. 3(d); when \u03b1h is\nsmall, the attention is spread out into a larger area, as\nin Fig. 3(c).\nThus, the PSA layer can achieve a strictly convolutional\nattention map by setting the centers of attentio",
  "rn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig\nAdam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2018 dataset.\narXiv preprint arXiv:1707.06642, 2018. 15\n[31] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig\nAdam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2019 dataset.\narXiv preprint arXiv:1707.06642, 2019. 15\n[32] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. arXiv preprint\narXiv:1709.01507, 2017. 21\n[3",
  " is\nrelated to a speci\ufb01c family of data augmentation, called Mixup (Zhang et al., 2018), which blends\npairs of images and their labels in a convex manner, either at pixel-level (Zhang et al., 2018; Yun\net al., 2019) or feature-level (Verma et al., 2019; Li et al., 2021). Our method can be interpreted\nas a special instantiation of Mixup which blends pairs of images at the abstraction level\u2014images\u2019\ntexture information and shape information are mixed. Our method successfully guides CNNs to\nlearn be",
  "ub.com/facebookresearch/detectron2, 2019. 6\n[38] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR:\ndeformable transformers for end-to-end object detection. CoRR, abs/2010.04159, 2020. 2, 3, 5\n13\n\n--------------------------------------------------\n",
  "es the similarity\nto a mini-batch of labeled support samples, and outputs a soft class distri-\nbution. Positive views are created using data-augmentations of the anchor\nview. Since the trivial collapse of all representations to a single vector would\nlead to high-entropy predictions by the similarity classi\ufb01er, sharpening the\ntarget pseudo-labels is suf\ufb01cient to eliminate all trivial solutions.\n3\nMethodology\nWe consider a large dataset of unlabeled images D\n=\n(xi)i\u2208[1,N] and a small support datas",
  " of token\nembedding, type of projection, and Transformer structure in\nthe backbone, between the above representative concurrent\nworks and ours.\nIntroducing Self-attentions to CNNs.\nSelf-attention\nmechanisms have been widely applied to CNNs in vision\ntasks. Among these works, the non-local networks [35] are\ndesigned for capturing long range dependencies via global\nattention. The local relation networks [17] adapts its weight\naggregation based on the compositional relations (similar-\nity) between ",
  "ptor is prede\ufb01ned (subset of the input elements), which\ndoes not allow the method to scale to high-dimensional data with\ncomplex correlations.\nShin et al. [36] introduce the pi-sigma network, which is\na neural network with a single hidden layer. Multiple af\ufb01ne\ntransformations of the data are learned; a product unit multiplies\nall the features to obtain the output. Improvements in the pi-sigma\nnetwork include regularization for training in [37] or using multiple\nproduct units to obtain the output",
  "ter on TPUs (2.1x - 3.3x faster\non GPUs) than the popular Ef\ufb01cientNets on the speed-\naccuracy Pareto curve. In a large-scale semi-supervised\nlearning setup, ResNet-RS obtains a 4.7x training speed-\nup on TPUs (5.5x on GPUs) over Ef\ufb01cientNet-B5 when\nco-trained on ImageNet and an additional 130M pseudo-\nlabeled images.\nFinally, we conclude with a suite of experiments test-\ning the generality of the improved training and scaling\nstrategies.\nWe \ufb01rst design a faster version of Ef\ufb01cient-\nNet using our",
  "e ratio of similar attention maps in M after\nthe 17th block is larger than 90% . This indicates that the\nlearned attention maps afterwards are similar and the trans-\nformer block may degenerate to an MLP. As a result, fur-\nther stacking such degenerated MHSA may introduce the\nmodel rank degeneration issue (i.e., the rank of the model\nparameter tensor from multiplying the layer-wise parame-\nters together will decrease) and limits the model learning\ncapacity. This is also validated by our analysis",
  "st architectures against adversarial attacks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 631\u2013640, 2020.\n[21] Minjing Dong, Yanxi Li, Yunhe Wang, and Chang Xu. Adversarially robust neural architectures. arXiv\npreprint arXiv:2009.00902, 2020.\n[22] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards\ndeep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n[23] Yingwei ",
  "the training of very deep\nnetworks. In Feature Pyramid Networks (FPN) [23] and\nU-Net [30], low-level features and high-level features are\nfused via long skip connections to obtain high-resolution\nand semantically strong features, which are vital for seman-\ntic segmentation and object detection. However, despite its\nprevalence in modern networks, most works on feature fu-\nsion focus on constructing sophisticated pathways to com-\nbine features in different kernels, groups, or layers. The\nfeature f",
  "dford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding with unsupervised learning. Technical Report, 2018.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. Technical Report, 2019.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. In NeurIPS, 2019.\nChen Sun, Abhinav Shrivastava, Sau",
  "through the\nnetwork. This built-in property allows dropping the position\nembedding from the network without hurting performance,\nas evidenced by our experiments (Section 4.4), simplifying\ndesign for vision tasks with variable input resolution.\nRelations to Concurrent Work:\nRecently, two more re-\nlated concurrent works also propose to improve ViT by in-\ncorporating elements of CNNs to Transformers. Tokens-\nto-Token ViT [41] implements a progressive tokenization,\nand then uses a Transformer-based ",
  "g experimental results and few\nempirical theoretical results already. We expect in the following\nyears new works that improve our results and extend our formula-\ntion. To that end, we summarize below several fundamental topics\nthat are open for interested practitioners.\nThe generalization of the \u03a0\u00b4nets is a crucial topic. In our\nevaluation without activation functions, we noticed that polynomi-\nals might be prone to over\ufb01tting (e.g., in the classi\ufb01cation setting\nwithout activation functions in t",
  "ck, the transformer block, and the Feature\nCoupling Unit (FCU). (c) Thumbnail of Conformer.\nDespite of the progress, existing solutions that introduce\nglobal cues to CNNs have obvious disadvantages. For the\n\ufb01rst solution, larger receptive \ufb01elds require more intensive\npooling operations, which implies lower spatial resolution.\nFor the second solution, if convolutional operations are not\nproperly fused with attention mechanisms, local feature\ndetails could deteriorate.\nVisual Transformers. As a pi",
  "peci\ufb01cally, our MDEQ\nmodels employ convolutions with small receptive \ufb01elds (e.g., the two 3 \u00d7 3 convolutional \ufb01lters\nin f\u03b8\u2019s residual block) on potentially very large images: for instance, we eventually evaluate our\nsemantic segmentation model on megapixel-scale images. In consequence, we typically need a\nhigher number of root-\ufb01nding iterations to converge to an exact equilibrium. While this does pose a\nchallenge, we \ufb01nd that using the aforementioned strategies of 1) multiscale simultaneous up- ",
  "e \ufb01nd that position-based inter-\nactions are crucial to reach high accuracies, while content-based interactions only bring marginal\nimprovements over position-based interactions15.\nContent\nPosition\nParams (M)\nFLOPS (B)\ntop-1\n\u2713\n\u00d7\n14.9\n5.0\n68.8\n\u00d7\n\u2713\n14.9\n11.9\n78.1\n\u2713\n\u2713\n14.9\n12.0\n78.4\nTable 10: Contributions of content and positional interactions. As expected, positional interac-\ntions are crucial to perform well on the image classi\ufb01cation task.\n15This observation is challenged by concurrent work (Do",
  "he compositional features directly from the network. Ideal modeling of the\nvisual representation should be able to model the part-whole hierarchy as humans do so that we can\nleverage representations of all levels directly from one backbone model.\nBuilding up a framework that includes different levels of representations in the part-whole hierarchy\nis dif\ufb01cult for conventional neural networks as it requires neurons to dynamically respond to the\ninput, while neural networks with \ufb01xed weights cannot",
  " low-level fea-\nture map, and Y is the high-level semantic feature map\nin a feature pyramid.\nBased on the multi-scale channel attention module M, At-\ntentional Feature Fusion (AFF) can be expressed as\nZ = M(X \u228eY) \u2297X + (1 \u2212M(X \u228eY)) \u2297Y,\n(4)\nwhere Z \u2208RC\u00d7H\u00d7W is the fused feature, and \u228edenotes\nthe initial feature integration. In this subsection, for the\nsake of simplicity, we choose the element-wise summation\nas initial integration. The AFF is illustrated in Fig. 2(a),\nwhere the dashed line denotes 1",
  "and Quoc V. Le.\nEf\ufb01cientnet: Rethinking\nmodel scaling for convolutional neural networks. In ICML,\npages 6105\u20136114. PMLR, 2019. 11\n[41] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training\ndata-ef\ufb01cient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 3, 4,\n6, 7, 8, 11\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nl",
  "es encountered in a realistic\nsetting, such as the \ufb01ne-grained similarity between classes,\nsigni\ufb01cant class imbalance, and domain mismatch between\nthe labeled and unlabeled data. This challenge is part of the\nFGVC7 workshop in CVPR 2020.\nThe data set is divided into the following three parts, the\n\ufb01rst is labeled data, a total of 5959 pictures, the second is\nunlabeled data whose categories belong to the labeled data,\na total of 26640 pictures, and the third is unlabeled data but\nits category is o",
  "cks. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 1911\u20131920, 2019. 2, 4\n[10] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,\nGuiguang Ding, and Jian Sun. Repvgg: Making vgg-style\nconvnets great again.\narXiv preprint arXiv:2101.03697,\n2021. 2, 4\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 wor",
  "ision Transformer [47] uses this SRA\nto build multi-scale vision transformer backbones, with dif-\nferent spatial reduction ratios (R1 = 8, R2 = 4, R3 =\n2, R4 = 1) for each stage. With this PVT\u2019s setting, the\nkey and value feature maps at all stages are essentially with\nresolution H\n32 \u00d7 W\n32 . This choice is able to scale up to image\nresolution 600\u00d71000, but the memory usage is much larger\nthan ResNet counterparts for 800 \u00d7 1333.\nIn this paper, we benchmarked the performance of\nSRA/32 with SR ra",
  "isual data, resulting in exces-\nsively sharp loss landscapes and poor generalization, as shown in the previous section. We hypothe-\nsize that smoothing the loss landscapes at convergence can signi\ufb01cantly improve the generalization\nability of those convolution-free architectures, leading us to the recently proposed sharpness-aware\nminimizer (SAM) (Foret et al., 2021) that explicitly avoids sharp minima.\n4.1\nSAM: OVERVIEW\nIntuitively, SAM (Foret et al., 2021) seeks to \ufb01nd the parameter w whose ent",
  "sts the\nresults. These networks serve as teachers when we use our distillation strategy.\n12\n\n--------------------------------------------------\n\u2697\u2191\n\u2697\nFigure 3: Distillation on ImageNet [42] with DeiT-B: performance as a func-\ntion of the number of training epochs. We provide the performance without\ndistillation (horizontal dotted line) as it saturates after 400 epochs.\n5.4\nTransfer learning: Performance on downstream tasks\nAlthough DeiT perform very well on ImageNet it is important to evaluate th",
  "is needed. Placing the SE layers after the reduction\nphase of the Bottleneck layer further reduces the computa-\ntional cost. The complete blocks design, with SE layers and\nInplace-ABN, is presented in Figure 3.\n2.2. Code Optimizations\nIn this section we will describe code optimizations we\ndid to enhance the GPU throughput and reduce the mem-\nory footprint of TResNet models. While code optimizations\nare sometimes overlooked and seen as \u2019implementation de-\ntails\u2019, we claim that they are crucial fo",
  "to match this result with a standard\noptimizer. In Section 3, contradicting the claims in You et al. [2019], we show that Adam obtains\nbetter BERT pre-training results than LAMB at the largest batch sizes, resulting in better downstream\nperformance metrics after \ufb01ne-tuning.\nIn addition, we establish a new state-of-the-art for BERT pretraining speed, reaching an F1 score of\n90.46 in 7,818 steps using Adam at batch size 65,536 (we report training speed in steps because our\nfocus is algorithmic ef\ufb01",
  ")\nx = x.mean(dim=1).reshape(B,-1) #average pooling\nreturn self.linear_classifier(x)\nD\nAdditional Ablations\nTraining recipe.\nDeiT [56] proposes a training strategy which allows for data-ef\ufb01cient vision\ntransformers on ImageNet only. In Table D.1 we ablate each component of the DeiT training to go\nback to the initial ResNet50 training. As to be expected, the training used in the ResNet-50 paper [23]\ndegrades the performance.\nTraining schedule.\nTable D.2 compares the performance of ResMLP-S36 accor",
  " number of classes. In In-\ndian Conference on Computer Vision, Graphics and Image\nProcessing, Dec 2008. 6\n[23] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nC. V. Jawahar. Cats and dogs. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2012. 6\n[24] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In International Conference on Machine\nLearning, pages 4055\u20134064. PMLR, 2018. 2\n[25] Alec Radford, ",
  "ientNet-B5 [69]\n30\n98.1\n91.1\n-\n-\n98.5\n-\nViT-B/16 [19]\n86.5\n98.1\n87.1\n-\n-\n89.5\n93.8\nViT-L/16 [19]\n304.3\n97.9\n86.4\n-\n-\n89.7\n93.6\nDeiT-B [72]\n86.6\n99.1\n90.8\n77.7\n92.1\n98.4\n-\nT2T-ViT-14 [88]\n21.5\n98.3\n88.4\n-\n-\n-\n-\nViTAE-T\n4.8\n97.3\n86.0\n73.3\n89.5\n97.5\n92.6\nViTAE-S\n23.6\n98.8\n90.8\n76.0\n91.4\n97.8\n94.2\nWe further investigate the generalization of the proposed ViTAE models on downstream tasks by \ufb01ne-\ntuning them on the training sets of several \ufb01ne-grained classi\ufb01cation tasks, including Flowers [49],\nCars ",
  "10.41% in classi\ufb01cation which suggests that we\nare indeed able to learn texture agnostic feature representations.\nOther Datasets - Aircraft, Birds, Dogs, and Cars. We further evaluate our method on image clas-\nsi\ufb01cation task on four different \ufb01ne-grained classi\ufb01cation datasets. We also observe improvement\non image classi\ufb01cation across \ufb01ve datasets in Table 2. These results suggest that in case of domain\nshift, higher level semantics are more important and capturing them helps in better transfer ",
  "ingle scale and multi-scale testing (we use\nscales from 0.5 to 1.75 with step 0.25) in the right columns of Table 2. Both with multi-scale\ntesting, Twins-SVT-S outperforms Swin-T by 1.3% mIoU. Moreover, Twins-SVT-L achieves new\nstate of the art result 50.2% mIoU under comparable FLOPs and outperforms Swin-B by 0.5% mIoU.\nTwins-PCPVT also achieves comparable performance to Swin [4].\n4.3\nObject Detection and Segmentation on COCO\nWe evaluate the performance of our method using two representative fr",
  "images of 690K different individuals as the gallery set and 100K\nphotos of 530 unique individuals from FaceScrub [87] as the\nprobe set. On MegaFace, there are two testing protocols (e.g.,\nidenti\ufb01cation and veri\ufb01cation). Table 12 show the identi\ufb01cation\nand veri\ufb01cation results on MegaFace dataset. In particular, the\nproposed Prodpoly-ResNet50 achieve 0.50% improvement at the\nRank-1@1e6 identi\ufb01cation rate and 0.31% improvement at the\nveri\ufb01cation TPR@FAR=1e-6 rate over the baseline ResNet50. In\nFigu",
  "------------------------------------------------\nXue, F.; Sun, A.; Zhang, H.; and Chng, E. S. 2020a. An\nEmbarrassingly Simple Model for Dialogue Relation Ex-\ntraction. arXiv preprint arXiv:2012.13873.\nXue, F.; Sun, A.; Zhang, H.; and Chng, E. S. 2020b. GDP-\nNet: Re\ufb01ning Latent Multi-View Graph for Relation Extrac-\ntion. arXiv preprint arXiv:2012.06780.\nYang, A.; Lin, J.; Men, R.; Zhou, C.; Jiang, L.; Jia, X.;\nWang, A.; Zhang, J.; Wang, J.; Li, Y.; et al. 2021.\nEx-\nploring Sparse Expert Models an",
  "2)\nViT-24B [7]\n11\n36.3\n79.4\nDeepViT-24B\n0\n36.3\n80.1 (+0.7)\nViT-32B [7]\n16\n48.1\n79.3\nDeepViT-32B\n0\n48.1\n80.9 (+1.6)\nComparison to adding temperature in self-attention:\nThe most intuitive way to mitigate the over-smoothing phe-\nnomenon is to sharpen the distribution of the elements in\nthe attention map of MHSA. We could achieve this by as-\nsigning a temperature \u03c4 to the Softmax layer of MHSA:\nAttention(Q, K, V ) = Softmax\n\u0012QK\u22a4\n\u03c4\n\u221a\nd\n\u0013\nV.\n(4)\nAs the attention collapse is observed to be severe on de",
  "respect to the unquantized weights and number of bits used.\n(ii) We provide a differentiable model size estimate, so that\ngiven a single penalty level \u03bb, DIFFQ optimizes the number\nof bits per weight or group of weights to achieve a given\ntrade-off between model size and accuracy.\narXiv:2104.09987v2  [stat.ML]  19 Oct 2021\n\n--------------------------------------------------\n(iii) We provide extensive experimental validation using vari-\nous models (ConvNets and Transformers) and domains (im-\nage ",
  "\nconvolution,\nillustrated\nin\nFig. 2(a), contains a single type of kernel:\nwith a single spatial size K1\n2 (in the case of\nsquare kernels, e.g., height\u00d7width: 3\u00d73 = 32,\nK1 = 3) and the depth equal to the number\nof input feature maps FMi.\nThe result of\napplying a number of FMo kernels (all hav-\ning the same spatial resolution and the same\ndepth) over FMi input feature maps is a num-\nber of FMo output feature maps (with spa-\ntial height H and width W). Thus, the num-\nber of parameters and FLOPs (\ufb02o",
  "e(Error)\nAnistropic Model(Error)\n0.2\n0.05\n29.61\n28.75\n0.2\n0.1\n28.31\n27.84\n0.4\n0.05\n31.92\n30.5\n0.4\n0.1\n32.59\n31.48\n0.6\n0.05\n39.04\n38.52\n0.6\n0.1\n36.75\n34.98\n0.8\n0.05\n54.23\n52.6\n0.8\n0.1\n44.31\n43\n1.0\n0.05\n75.05\n71.21\n1.0\n0.1\n51.19\n45.51\nfollow the details of (Ren et al., 2015) to train a model; 10 epochs with an initial learning rate of\n0.001.\nFor semantic segmentation, we report our results on FCN(Shelhamer et al., 2017) using our pre-\ntrained model as backbone. We train the FCN model for 30 epochs",
  "hat we\nhave not extensively tuned the RandAugment magnitudes.\nC.4. Detection and instance segmentation hyperpa-\nrameters\nWe use Mask-RCNN [16] for all detection and instance\nsegmentation experiments. We pretrain the backbone on\nImageNet, mostly reusing the same hyperparameters as in\nSection C.3. Backbones are pretrained for 350 epochs using\nan image size of 512, which was chosen to be closer to the\n14\n\n--------------------------------------------------\nHaloNet\nModel\nb\nh\nrv\nrb\nTotal\nLayers\nl3\ns\nd",
  "sed on the standard residual bottleneck\nblock with group convolution [31]. (a) Each X block consists of a\n1\u00d71 conv, a 3\u00d73 group conv, and a \ufb01nal 1\u00d71 conv, where the 1\u00d71\nconvs alter the channel width. BatchNorm [12] and ReLU follow\neach conv. The block has 3 parameters: the width wi, bottleneck\nratio bi, and group width gi. (b) The stride-two (s = 2) version.\nThe AnyNetX design space has 16 degrees of freedom as\neach network consists of 4 stages and each stage i has 4 pa-\nrameters: the number of ",
  "t image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 1, 3, 6, 7\n[52] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv\u00b4e J\u00b4egou. Going deeper with im-\nage transformers. arXiv preprint arXiv:2103.17239, 2021.\n3\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, page",
  "attention operator.\nWe decouple the usage of K, V and Q pooling, with\nQ pooling being used in the \ufb01rst layer of each stage and\nK, V pooling being employed in all other layers. Since the\nsequence length of key and value tensors need to be identical\nto allow attention weight calculation, the pooling stride used\non K and value V tensors needs to be identical. In our\ndefault setting, we constrain all pooling parameters (k; p; s)\nto be identical i.e. \u0398K \u2261\u0398V within a stage, but vary s\nadaptively w.r.t",
  "khorn attention.\nIn International\nConference on Machine Learning, pages 9438\u20139447. PMLR,\n2020. 2\n[41] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian\nRuder, and Donald Metzler. Long range arena: A benchmark\nfor ef\ufb01cient transformers. arXiv preprint arXiv:2011.04006,\n2020. 2\n[42] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler.\nEf\ufb01cient transformers:\nA survey.\narXiv preprint\narXiv:2009.06732, 2020. 2\n[43] Hugo Touvron, Matt",
  "we follow [35]. Therefore, different hierarchy has a gradually increased\nnumber of head, hidden dimensions, and number of repeated MSANesT layers. Table A1 speci\ufb01es\ndetails.\nData Augmentation. We apply the commonly used data augmentation and regularization techniques\nas [49]. Repeated augmentation [25] in DeiT is not used. In addition, for ImageNet models, we\nalso add color jittering similar to [8, 9] which seems to reduce dependency on local texture cues and\nslightly improves generalization (\u223c0",
  "eption-resnet-v2\nResNeXt-101\nAmoebaNet-A\nFig. 1. Improvement brought by FixRes (in bold) to several\npopular architectures from the literature. Our FixEf\ufb01cientNet\n(orange curve) surpasses all Ef\ufb01cientNet models, including\nthe models trained with Noisy student (red curve) and adver-\nsarial examples (blue curve). The sws models are from [2].\nTables 1 and 2 report results on larger models.\nwhich jointly optimizes the choice of resolutions and scales at\ntraining and test time, while keeping the same ",
  "0\n300M\n1.2M\n50K\n0.03\n79.0\n67.9\n85.1\n300M\n2M\n50K\n0.03\n79.6\n67.8\n85.6\n300M\n4M\n50K\n0.03\n79.9\n68.5\n85.8\n1B\n20K\n10K\n0.03\n59.0\n46.2\n66.2\n1B\n30K\n10K\n0.03\n64.0\n51.4\n71.4\n1B\n60K\n10K\n0.03\n70.5\n57.7\n77.7\n1B\n120K\n50K\n0.03\n73.6\n60.8\n80.7\n1B\n400K\n50K\n0.03\n77.6\n65.7\n84.0\n1B\n1.2M\n50K\n0.03\n79.5\n68.0\n85.5\n1B\n2M\n50K\n0.03\n79.7\n68.2\n85.5\n1B\n4M\n50K\n0.03\n80.2\n68.1\n85.9\n3B\n20K\n10K\n0.03\n59.3\n47.3\n66.4\n3B\n30K\n10K\n0.03\n64.3\n51.5\n71.6\n3B\n60K\n10K\n0.03\n70.2\n57.2\n77.6\n3B\n120K\n50K\n0.03\n73.5\n61.3\n80.7\n3B\n400K\n50K\n0.03\n77.6\n65.7",
  "ing network tweaks and regu-\nlarizations is also demonstrated in mobile-oriented mod-\nels. Based on this, we reduced the reduction ratio r of the\nSE block from 16 to 2 to maximize synergy between net-\nwork tweaks and regularization. By doing so, we could im-\nprove MobileNet-V1\u2019s top-1 accuracy by 1% with minimal\nthroughput loss. However, unlike ResNet, the top-1 accu-\nracy of SE-MobileNet-V1 decreased when DropBlock was\napplied. As the network capacity of MobileNet-V1 back-\nbone is smaller than ",
  "\nneural networks from over\ufb01tting. The journal of machine\nlearning research, 15(1):1929\u20131958, 2014.\nVerma, V., Lamb, A., Beckham, C., Naja\ufb01, A., Mitliagkas,\nI., Courville, A., Lopez-Paz, D., and Bengio, Y. Manifold\nmixup: Better representations by interpolating hidden\nstates. ICML, 2019.\nVillani, C. Optimal transport: old and new, volume 338.\nSpringer Science & Business Media, 2008.\nWang, L., Lu, H., Ruan, X., and Yang, M.-H. Deep networks\nfor saliency detection via local estimation and global\nse",
  "Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint\narXiv:1906.05849, 2019. 2, 3, 4, 5, 19\n[49] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola.\nWhat makes for good views for contrastive learning. arXiv preprint arXiv:2005.10243, 2019.\n6, 7, 8, 21, 22\n[50] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural networks for machine",
  ": Supervised Learn-\ning, Pseudo Labels, and Meta Pseudo Labels. We observe\nthat Meta Pseudo Labels has a much higher success rate of\n\ufb01nding the correct classi\ufb01er than Supervised Learning and\nPseudo Labels. Figure 2 presents a typical outcome of our\nexperiment, where the red and green regions correspond to\nthe classi\ufb01ers\u2019 decisions. As can be seen from the \ufb01gure, Su-\npervised Learning \ufb01nds a bad classi\ufb01er which classi\ufb01es the\nlabeled instances correctly but fails to take advantage of the\nclusterin",
  "ages smaller than 224). All models use a label smoothing of 0.1 and a weight decay of 4e-5. These values were set\nbased on the preliminary experiments across various model scales on the ImageNet minival-set.\n16\n\n--------------------------------------------------\nRevisiting ResNets: Improved Training and Scaling Strategies\nFilter Scaling\nDropout Rate\n0.25\n0.0\n0.5\n0.1\n1.0\n0.25\n1.5\n0.6\n2.0\n0.75\nTable 12. Dropout values for \ufb01lter scaling. Filter scaling refers to the \ufb01lter scaling multiplier based o",
  ".2 \u00b1 0.07\n78.3 / 94.3\n76.3 / 92.8\n\u2212\n\u2212\n\u2212\nStochastic Depth (Huang et al. 2016)\n96.2 \u00b1 0.07\n77.5 / 93.7\n76.8 / 93.1\n80.2 / 95.0\n82.5 / 96.2\n84.1 / 96.9\nAutoDropout\n96.8 \u00b1 0.09\n78.7 / 94.3\n77.5 / 93.8\n80.9 / 95.6\n83.1 / 96.5\n84.7 / 97.1\nTable 1: Performance of AutoDropout and the baselines on supervised image classi\ufb01cation (higher is better). This is a control experiment and\nall models are implemented by us.\nMethods\nCIFAR-10\nImageNet\n(WRN-28-10)\n(ResNet-50)\nStochastic Depth (2016)\n96.2 \u00b1 0.07\u2020\n77.5 ",
  "ness\nTest error\nEstimated sharpness\n1\n2.77\u00b10.03\n0.17\u00b10.03\n16.72\u00b10.08\n0.82\u00b10.05\n2\n2.76\u00b10.03\n0.82\u00b10.03\n16.59\u00b10.08\n1.83\u00b10.05\n3\n2.73\u00b10.04\n1.49\u00b10.05\n16.62\u00b10.09\n2.36\u00b10.03\n5\n2.77\u00b10.03\n2.26\u00b10.05\n16.60\u00b10.06\n2.82\u00b10.04\nTable 11: Test error rate and estimated sharpness (max\u03f5 L(w +\u03f5)\u2212L(w)) at the end of the training.\n19\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\n10 6\n10 5\n10 4\n10 3\n10 2\n10 1\n100\n101\n102\n103\n0.034\n0.036\n0.038\n0.040\n0.042\n0.044\n0.046\n0.048\n",
  " error rates on ImageNet on ResNet-50\nfollowing the training protocol in (Yun et al., 2019) (300 epochs).\n6.2. Robustness Against Corruption\nHendrycks et al. (2020) proposed AugMix which performs\nInput mixup between clean and augmented images to im-\nprove robustness against corrupted datasets as well as the\ngeneralization performance. AugMix uses Jensen-Shannon\ndivergence (JSD) between network outputs of a clean image\nand two AugMix images as a consistency loss. However,\ncomputing the JSD term r",
  "size is doubled.\nGoogLeNet [31] also used the pooling layer. ResNet [13]\nperformed spatial size reduction using the convolution layer\nof stride 2 instead of max pooling. It is an improvement\nin the spatial reduction method. The convolution layer of\nstride 2 is also used as a pooling method in recent archi-\ntectures (Ef\ufb01cietNet [32], MobileNet [29, 19]). Pyramid-\nNet [11] pointed out that the channel increase occurs only\nin the pooling layer and proposed a method to gradually\nincrease the channel",
  "MoEx (Li et al., 2021)), the SoTA on ImageNet-C (Deep-\nAugment + AugMix (Hendrycks et al., 2020)), and the SoTA on Stylized-ImageNet (SIN (Geirhos\net al., 2019)). Interestingly, we note the improvements of all these SoTAs are not consistent across\ndifferent benchmarks. For example, as shown in Table 3, SIN signi\ufb01cantly improves the results on\nStylized-ImageNet, but at the cost of huge performance drop on ImageNet (-16.2%) and ImageNet-\nC (-2.3%). Our shape-texture debiased training stands as the",
  "gns to verify these curiosities.\nFrom the results, it is important to \ufb01nd out that: 1) when performing these spatial operations, it is\nalways important to apply on the holistic image plane versus the block plane although both reasonably\ncan introduce spatial priors. 2) small kernel convolution is suf\ufb01cient and has to be applied ahead of\npooling; 3) Max pooling is far better than other options, such as stride-2 sub-sampling and average\npooling; 4) sub-sampling the query sequence length (similar t",
  "e in dealing with image corruptions. Because ViTs are good at long-range\ndependencies modeling, which makes it easier to learn the shape feature than CNNs. Such a shape\nfeature is more robust and less likely to be destroyed by image corruptions.\nOut-of-distribution Robustness. We test the model generalization on out-of-distribution data by\nreporting the accuracy on ImageNet-R and ImageNet-Sketch in Table 5. The performance gap is\nnot obvious on ImageNet-R, as the task is hard for most compared m",
  "1 outperform all prior work using\nResNet-101.\n6.3. Semantic Segmentation\nIn transfer learning for semantic segmentation, we use\nthe GluonCV [19] implementation of DeepLabV3 [9] as a\nbaseline approach. Here a dilated network strategy [8, 65]\nis applied to the backbone network, resulting in a stride-\n8 model. Synchronized Batch Normalization [68] is used\nduring training, along with a polynomial-like learning rate\nschedule (with initial learning rate = 0.1).\nFor evalua-\ntion, the network prediction",
  " A. Vedaldi, A. Zisserman, and C. Jawahar, \u201cCats and dogs,\u201d\nin CVPR, 2012.\n[19] M. Nilsback and A. Zisserman, \u201cAutomated \ufb02ower classi\ufb01cation over a\nlarge number of classes,\u201d in 2008 Sixth Indian Conference on Computer\nVision, Graphics Image Processing, 2008.\n[20] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,\nU. Franke, S. Roth, and B. Schiele, \u201cThe cityscapes dataset for semantic\nurban scene understanding,\u201d in CVPR, 2016.\n[21] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. H",
  " patch and feature dimensions\nof the image tokens. The architecture is identical to that of\nViT [4] with the attention layer replaced by a feed-forward\nlayer.\nthe model a global receptive \ufb01eld. This layer may be seen\nas a data-dependent linear layer, and when applied on im-\nage patches it resembles (but is not exactly equivalent to)\na convolution. Indeed, a signi\ufb01cant amount of recent work\nhas gone into improving the ef\ufb01ciency and ef\ufb01cacy of the\nattention layer.\nIn this short report, we conduct ",
  "ko, W. Wang, T. Weyand, M. Andreetto, and H. Adam.\nMobilenets: Ef\ufb01cient convolutional neural networks for mobile vision applications. arXiv preprint\narXiv:1704.04861, 2017.\n[29] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708,\n2017.\n[30] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate",
  "on-preserving) skip-\nconnections that operate on the un-normalized signal.\n3.3. Network instantiation details\nTable 3 shows concrete instantiations of the base mod-\nels for Vision Transformers [25] and our Multiscale Vision\nTransformers. ViT-Base [25] (Table 3b) initially projects\nthe input to patches of shape 1\u00d716\u00d716 with dimension\nD = 768, followed by stacking N = 12 transformer\nblocks. With an 8\u00d7224\u00d7224 input the resolution is \ufb01xed to\n768\u00d78\u00d714\u00d714 throughout all layers. The sequence length\n(sp",
  "omated data augmentation with a reduced search space. arXiv preprint arXiv:1909.13719,\n2019. 3, 5, 18, 20\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Conference on Computer Vision and Pattern Recognition, pages\n248\u2013255, 2009. 3\n[10] Piotr Doll\u00b4ar, Mannat Singh, and Ross B. Girshick. Fast and accurate model scaling. In\nConference on Computer Vision and Pattern Recognition, 2021. 3, 6\n[11] Alexey Dosovitskiy, Lu",
  "d recipes. From the results in Tab. 6, Re\ufb01ner boosts the model\nperformance across all the tasks signi\ufb01cantly and increases the average score by 1%, demonstrating\nRe\ufb01ner is well generalizable to transformer-based NLP models to improve their attentions and \ufb01nal\nperformance.\nTable 6: Comparison of BERT-small w/o and w/ re\ufb01ner on the GLUE development set.\nModel\nParams\nMNLI\nQNLI\nQQP\nRTE\nSST\nMRPC\nCoLA\nSTS-B\nAvg.\nBERT-small [11]\n14M\n75.8\n83.7\n86.8\n57.4\n88.4\n83.8\n41.6\n83.6\n75.1\n+ Re\ufb01ner\n14M\n78.1\n86.4\n88",
  "e datasets, meta-parameters,\nbaselines and performance experiments to support and\nquantify the bene\ufb01ts of SSAL models.\n4.1. Datasets\nWe conduct experiments on three different image classi-\n\ufb01cation datasets with varying degrees of complexity:\nCIFAR100 [12]: extension of CIFAR10 where 60 000\ncolor images of size 32x32 belong to 100 different classes\nof \ufb01ne-grained objects or animals. The training and test set\ncontain 50 000 and 10 000 images respectively.\nTinyImagenet [16]: 110 000 color images of",
  " the case with the attention operation which produces per-example attention\nmaps. In practice, the hyperparameter |k| is set to a small value (such as |k|=16) and we can process\nlarge batches of large inputs in cases where attention cannot (see Table 4). Additionally, position\nembeddings can be shared across lambda layers to keep their \u0398(knm) memory footprint constant -\nwhereas the memory footprint of attention maps scales with the number of layers5.\nMulti-query lambda layers reduce time and spa",
  "ral networks, triggering the surge of deep learning in vi-\nsion. In this work, we rethink the inherent principles of\nstandard convolution for vision tasks, speci\ufb01cally spatial-\nagnostic and channel-speci\ufb01c. Instead, we present a novel\natomic operation for deep neural networks by inverting\nthe aforementioned design principles of convolution, coined\nas involution. We additionally demystify the recent pop-\nular self-attention operator and subsume it into our invo-\nlution family as an over-complicat",
  "e aggregation of\nlocal descriptors, like in VLAD [36]. Most of the modern methods now rely on convolutional\nTable D.2: Evaluation on transfer learning.\nArchitecture\nCIFAR10\nCIFAR100\nFlowers102\nCars\niNat18\niNat19\nEf\ufb01cientNet-B7 [58]\n98.9\n91.7\n98.8\n94.7\n_\n_\nViT-B/16 [22]\n98.1\n87.1\n89.5\n_\n_\n_\nViT-L/16 [22]\n97.9\n86.4\n89.7\n_\n_\n_\nDeit-B/16 [65] \u03a5\n99.1\n91.3\n98.8\n92.9\n73.7\n78.4\nXCiT-S24/16 \u03a5\n99.1\n91.2\n97.4\n92.8\n68.8\n76.1\nXCiT-M24/16 \u03a5\n99.1\n91.4\n98.2\n93.4\n72.6\n78.1\nXCiT-L24/16 \u03a5\n99.1\n91.3\n98.3\n93.7\n75.6\n",
  "g, Mason Liu, and Weilong Yang. Beyond Synthetic Noise: Deep Learning on\nControlled Noisy Labels. arXiv e-prints, art. arXiv:1911.09781, November 2019.\nYiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic\ngeneralization measures and where to \ufb01nd them. arXiv preprint arXiv:1912.02178, 2019.\nDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv e-prints,\nart. arXiv:1412.6980, December 2014.\nAlexander Kolesnikov, Lucas Beyer, Xiaohu",
  "ge problems, attaining 90.2% top-1 accu-\nracy on ImageNet, which is 1.6% better than the previous\nstate-of-the-art [16]. The consistent gains con\ufb01rm the bene\ufb01t\nof the student\u2019s feedback to the teacher.\nAcknowledgements\nThe authors wish to thank Rohan Anil, Frank Chen, Wang\nTao for their help with many technical issues in running\nour experiments. We also thank David Berthelot, Nicholas\nCarlini, Sylvain Gelly, Geoff Hinton, Mohammad Norouzi,\nand Colin Raffel for their comments on earlier drafts of",
  "ave almost no effect on FLOPs. In\nthis way, CeiT-T outperforms DeiT-T by a large margin of\n4.2% for the Top-1 accuracy. And CeiT-S obtains higher re-\nsults than that of DeiT-S and DeiT-B by 2.1% and 0.2% re-\nspectively. We also compare CeiT with concurrent work of\nT2T-ViT and PVT. CeiT-S achieves slightly higher accuracy\nthan T2T-ViT-19 with much fewer FLOPs and parameters.\nFor PVT models, CeiT-T obtains higher accuracy of 1.3%\nthan PVT-T with fewer FLOPs and parameters. CeiT-S also\noutperforms ",
  "r, 2020a.\nZoph, B., Ghiasi, G., Lin, T.-Y., Cui, Y., Liu, H., Cubuk,\nE. D., and Le, Q. V. Rethinking pre-training and self-\ntraining. arXiv preprint arXiv:2006.06882, 2020b.\n13\n\n--------------------------------------------------\nRevisiting ResNets: Improved Training and Scaling Strategies\nA. Author Contributions\nIB, BZ: led the research, designed and ran the scaling experiments, designed and experimented with the training strate-\ngies. JS, TL, EC, AS, WF, XD: advised the research, proposed exper",
  "P\nFigure 3: ImageNet acc. vs model complexity.\nTable 6: Comparisons among the GFNet and other\nvariants based on the transformer-like architecture\non ImageNet. We show that GFNet outperforms the\nResMLP [42], FNet [25] and models with local depth-\nwise convolutions. We also report the number of pa-\nrameters and theoretical complexity in FLOPs.\nModel\nAcc\nParam\nFLOPs\n(%)\n(M)\n(G)\nDeiT-S [43]\n79.8\n22\n4.6\nLocal Conv (3 \u00d7 3)\n77.7\n15\n2.8\nLocal Conv (5 \u00d7 5)\n78.1\n15\n2.9\nLocal Conv (7 \u00d7 7)\n78.2\n15\n2.9\nResML",
  "ages 448\u2013456. PMLR, 2015. 2\n[25] Md Amirul Islam, Matthew Kowal, Sen Jia, Konstantinos G\nDerpanis, and Neil DB Bruce. Position, padding and predic-\ntions: A deeper look at position information in cnns. arXiv\npreprint arXiv:2101.12322, 2021. 5\n[26] Anil K. Jain and Farshid Farrokhnia.\nUnsupervised tex-\nture segmentation using gabor \ufb01lters.\nPattern Recognit.,\n24(12):1167\u20131186, 1991. 3\n[27] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:\nTwo transformers can make one strong gan. arXiv prepr",
  "ed\nposition-aware attention scaling and patch-wise data augmentation, RVT-Ti\u2217can further improve\n0.8% on RVT-Ti with little additional computation cost. For other scales of the model, RVT-S\u2217and\nRVT-B\u2217also achieve a good promotion compared with DeiT. Although the improvement becomes\nsmaller with the increase of model capacity, we think the advance of our model is still obvious as it\nstrengthen the model ability in various views such as robustness and out-of-domain generalization.\n6.3\nRobustness E",
  " have *Partial* version because its window size is set as 15 (comparable with\nthe ViT(DeiT)/16 feature map size 14), and its attention mechanism in the last two stages is equivalent to full attention.\n\n--------------------------------------------------\ntion!\n2. Using a customized CUDA kernel,\ndenoted as\n\u201ccuda kernel\u201d. We make use of the TVM, like what\nhas done in Longformer [3], to write a customized\nCUDA kernel for Vision Longformer. As shown in\nFigure 5, the \u201ccuda kernel\u201d (green line) achieves",
  "74.74\n80.76\n80.99\n81.19\n80.98\nPartial Linformer\n75.64\n75.82\n75.56\n75.33\n81.66\n81.63\n81.66\n81.79\nSRA/64 [47]\n68.71\n68.84\n69.08\n68.78\n75.9\n76.18\n76.35\n76.37\nSRA/32 [47]\n73.16\n73.46\n73.22\n73.2\n79.82\n79.8\n79.96\n79.9\nPartial SRA/32\n75.17\n75.8\n75.2\n75.26\n81.63\n81.59\n81.62\n81.61\nGlobal\n70.93\n71.62\n71.52\n72.00\n79.04\n79.08\n79.17\n78.97\nPartial Global\n75.55\n75.61\n75.32\n75.4\n81.39\n81.42\n81.6\n81.45\nPerformer\n71.28\n71.87\n71.12\n73.09\n78.17\n78.58\n78.81\n78.72\nPartial Performer\n75.65\n75.74\n75.34\n75.93\n81.59\n81.86",
  "different model sizes, namely, TNT-Ti, TNT-S and TNT-B. They consist of 6.1M,\n23.8M and 65.6M parameters respectively. The corresponding FLOPs for processing a 224\u00d7224\nimage are 1.4B, 5.2B and 14.1B respectively.\nTable 1: Variants of our TNT architecture. \u2018Ti\u2019 means tiny, \u2018S\u2019 means small, and \u2018B\u2019 means base.\nThe FLOPs are calculated for images at resolution 224\u00d7224.\nModel\nDepth\nInner transformer\nOuter transformer\nParams\nFLOPs\ndim c\n#heads\nMLP r\ndim d\n#heads\nMLP r\n(M)\n(B)\nTNT-Ti\n12\n12\n2\n4\n192\n3\n4",
  " 79.35% top-1, and trained on Imagenet-1k with a 8xV100 GPU machine.\n14\n\n--------------------------------------------------\ntop-1 accuracy\nAblation on \u2193\nPre-training\nFine-tuning\nRand-Augment\nAutoAug\nMixup\nCutMix\nErasing\nStoch. Depth\nRepeated Aug.\nDropout\nExp. Moving Avg.\npre-trained 2242\n\ufb01ne-tuned 3842\nnone: DeiT-B\nadamw\nadamw\n\u0013\n\u0017\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017\n81.8 \u00b10.2\n83.1 \u00b10.1\noptimizer\nSGD\nadamw\n\u0013\n\u0017\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017\n74.5\n77.3\nadamw\nSGD\n\u0013\n\u0017\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017\n81.8\n83.1\ndata\naugmentation\nadamw\nadamw\n\u0017\n\u0017\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0017\n\u0017",
  "depthwise conv1d(values, embeddings)\n# Transpose from shape [b, n, v, k] to shape [b, n, k, v]\nposition lambdas = transpose(position lambdas, [0, 1, 3, 2])\nreturn position lambdas\ndef lambda layer(queries, keys, embeddings, values, impl=\u2019einsum\u2019):\n\"\"\"Multi\u2212query lambda layer.\"\"\"\ncontent lambda = einsum(softmax(keys), values, \u2019bmk,bmv\u2212>bkv\u2019)\nposition lambdas = compute position lambdas(embeddings, values, impl=impl)\ncontent output = einsum(queries, content lambda, \u2019bhnk,bkv\u2212>bnhv\u2019)\nposition output",
  " make ConvMLP scalable, we extend\nConvMLP model by scaling both the depth and width of\nboth convolution and Conv-MLP stages. It achieves com-\npetitive performances on ImageNet-1k with fewer param-\neters compared to recent MLP-based models. On object\ndetection and semantic segmentation, we conduct experi-\nments on MS COCO and ADE20K benchmarks. It shows\nthat using ConvMLP as a backbone achieves better trade-off\nbetween performance and model size.\nIn conclusion, our contributions are as follows:\n\u2022",
  " [30]) and/or additional data used for training (as in [29], [31]), however, this\nrequires signi\ufb01cantly more computational and data resources, which are not easily available.\n8\n\n--------------------------------------------------\nTable 4: Validation error rates comparison results of PyConvResNet on ImageNet with different\ntraining settings, for network depth 50 and 101 (\u2020on the already trained model with 224\u00d7224 crop, just perform the test on 320\u00d7320).\nNetwork\ntest crop: 224\u00d7224\ntest crop: 320\u00d732",
  "e 2: Masked Image Modeling\n\u0001\n(3)\nwhere the second term is our BEIT pre-training objective.\n2.5\nPre-Training Setup\nThe network architecture of BEIT follows that of ViT-Base (Dosovitskiy et al., 2020) for a fair\ncomparison. We use a 12-layer Transformer with 768 hidden size, and 12 attention heads. The\nintermediate size of feed-forward networks is 3072. We employ the default 16 \u00d7 16 input patch size.\nWe directly borrow the image tokenizer trained by Ramesh et al. (2021). The vocabulary size of\nvis",
  "\nImageNet data (13 examples per class), as commonly\nused in self-supervised and semi-supervised learn-\ning [42]. For reference, Figure 3 shows two state-\nof-the-art self-supervised learning models, SimCLR\nv2 [6] and BYOL [12], using 1% of ImageNet data.\nNote, however, that these approaches a quite differ-\nent: ViT-G/14 uses large source of weakly-supervised\ndata, and is pre-trained only once and transferred\nto different tasks. Meanwhile, the self-supervised\nlearning models use unlabeled but in-d",
  "ality, we can add terms in region A whose coef\ufb01cient is zero. By design,\nnon-local region is also within the receptive \ufb01eld of the boundary pixels of the local region. Thus, we\ncan transform Eq. 4 into below formulation:\nX\nxi\u2208(\u2126\\A)\nX\nxk\u2208\u02da\nA\n\u03b1i\u03b2k \u27e8xk, xi\u27e9=\nX\nxi\u2208\u2126\nX\nxk\u2208\u02da\nA\n\u03b1i\u03b2k \u27e8xk, xi\u27e9.\n(5)\nAccording to the Markovian property of the image, we can assume that for xk \u2208A, the interaction\nbetween xi (far away from xk) and xk is weak. Thus, the Eq. 5 can be further simpli\ufb01ed:\nX\nxi\u2208\u2126\nX\nxk\u2208\u02da\nA\n\u03b1i\u03b2k \u27e8xk,",
  "s. Here, sw is the stripe\nwidth and can be adjusted to balance the learning capacity and computation complexity. Formally,\nsuppose the projected queries, keys and values of the kth head all have dimension dk, then the output\nof the horizontal stripes self-attention for kth head is de\ufb01ned as:\nX = [X1, X2, . . . , XM], where Xi \u2208R(sw\u00d7W )\u00d7C and M = H/sw\nY i\nk = Attention(XiW Q\nk , XiW K\nk , XiW V\nk ), where i = 1, . . . , M\nH-Attentionk(X) = [Y 1\nk , Y 2\nk , . . . , Y M\nk ]\n(1)\nWhere W Q\nk \u2208RC\u00d7dk, ",
  "4.8\n62.6\n68.4\n90.2\n92.1\n63.6\n70.3\n30M\n1.2M\n64.2\n67.4\n63.9\n69.3\n91.0\n92.2\n66.4\n73.9\n30M\n2M\n64.4\n67.5\n64.0\n69.0\n91.6\n92.2\n68.4\n73.7\n300M\n20K\n33.6\n37.5\n44.7\n51.1\n58.3\n67.2\n38.5\n46.1\n300M\n30K\n40.0\n44.6\n51.7\n57.0\n70.4\n75.1\n44.2\n52.0\n300M\n60K\n48.2\n52.8\n55.9\n61.3\n80.2\n83.8\n52.0\n59.7\n300M\n120K\n54.4\n58.3\n60.9\n65.9\n84.2\n88.6\n57.6\n65.5\n300M\n400K\n63.1\n66.1\n65.8\n70.8\n90.2\n91.4\n64.5\n71.9\n300M\n1.2M\n66.5\n69.6\n68.2\n72.1\n92.3\n92.9\n68.4\n74.7\n300M\n2M\n67.9\n70.9\n68.1\n72.7\n92.7\n92.8\n70.0\n76.2\n1B\n20K\n33.6\n37.9\n45.6\n51.",
  "42] requires an ef\ufb01cient strategy to retrieve a large number of negative samples\nin the training session, that requires the dataset module to return an additional object\n(e.g., negative sample indices). To support such extensions, we design dataset wrappers\nto return input batch, targets, and a supplementary dictionary, that can be empty when\nnot used. For the above case, the additional object can be stored in the supplementary\ndictionary, and used when computing the contrastive loss. This desig",
  "s of different losses in Table 3. We see the improve-\nment brought by the distillation loss and the KL loss is not very signi\ufb01cant, but it can consistently\nfurther boost the performance of various models.\nComparisons of different sparsi\ufb01cation strategies.\nAs illustrated in Figure 2, the dynamic token\nsparsi\ufb01cation is unstructured. To discuss whether the dynamic sparsi\ufb01cation is better than other\nstrategies, we perform ablation experiments and the results are shown in Table 4. For the structural\n",
  "nstrated that jointly learning disparate tasks\ndoes not harm performance [11], measuring the net ben-\ne\ufb01t for all tasks still depends on a perceived ad-hoc sim-\nilarity between them. The symbiotic interaction between\nthe two objectives is thereby obscured, making it dif\ufb01cult\nto establish if one or both tasks bene\ufb01t from one another\n(i.e. if the relation is mutualistic, commensalistic or para-\nsitic). Some surprising and rather unexpected relationships\nbetween tasks have been reported in the lite",
  "experimented with hyperparameter stability by changing augmentations, optimizers and learning\nrates one at a time from the best combination for each of the methodologies. In Fig. 4(a), we\ncompare the top-1 accuracy of SupCon loss against cross-entropy across changes in augmentations\n(RandAugment [6], AutoAugment [5], SimAugment [3], Stacked RandAugment [49]); optimizers\n(LARS, SGD with Momentum and RMSProp); and learning rates. We observe signi\ufb01cantly lower\nvariance in the output of the contrast",
  "nd\nHong-\nYuan Mark Liao. Yolov4: Optimal speed and accuracy of\nobject detection. arXiv preprint arXiv:2004.10934, 2020. 7\n[5] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and\nLarry S. Davis. Soft-nms \u2013 improving object detection with\none line of code. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), Oct 2017. 6, 9\n[6] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-\ning into high quality object detection. In Proceedings of the\nIEEE Conference on Computer Vis",
  " and model that were\nused in the development of both of those algorithms. LAMB is a variant of Adam and performs about\nthe same as Adam with the same value of \u03f5; LARS is more analogous to Momentum and indeed\nNesterov momentum and LARS have similar performance.\n5\nDiscussion\nOur results show that standard, generic optimizers suf\ufb01ce for achieving strong results across batch\nsizes. Therefore, any research program to create new optimizers for training at larger batch sizes\nmust start from the fact th",
  "acy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at training time (negligible w.r.t. the total\nnumber of weights).\n\u2022 Our architecture with speci\ufb01c class-attention offers a more effective pro-\ncessing of the class embedding.\n2\n\n--------------------------------------------------\n\u2022 Our best CaiT models establish the new state of the art on Imagenet-\nReal [6] and Imagenet V2 matched frequency [52] with no additional\ntraining data. On ImageNet1k-val [54",
  "ore interpretable\nand can lead to insights that we can learn from. We analyze\nthe RegNet design space and arrive at interesting \ufb01ndings\nthat do not match the current practice of network design.\nFor example, we \ufb01nd that the depth of the best models is sta-\nble across compute regimes (\u223c20 blocks) and that the best\nmodels do not use either a bottleneck or inverted bottleneck.\nWe compare top REGNET models to existing networks\nin various settings. First, REGNET models are surprisingly\neffective in th",
  "utional neural networks. In\nICML, 2019. 9\n[33] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9\nJ\u00e9gou.\nTraining data-ef\ufb01cient image transformers & distillation through attention.\narXiv preprint\narXiv:2012.12877, 2020. 8\n[34] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution\ndiscrepancy. arXiv preprint arXiv:1906.06423, 2020. 12\n[35] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blak",
  "ani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998\u20136008, 2017. 1,\n2, 4\n[65] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\nTan, Xinggang Wang, et al. Deep high-resolution represen-\ntation learning for visual recognition. IEEE transactions on\npattern analysis and machine intelligence,",
  "r moder-\nate augmentation. Furthermore, this transition can slightly\nimprove the elite performance.\n3.3.2\nReplacing patch \ufb02attening with step-wise patch\nembedding\nDeiT and ViT models directly encode the image pixels with\na patch embedding layer which is equivalent to a convolu-\ntion with large kernel size and stride (e.g., 16). This oper-\nation \ufb02attens the image patches to a sequence of tokens so\nthat Transformers can handle images. However, patch \ufb02at-\ntening impairs the position information wit",
  " conversely how it is dif\ufb01cult to compare different pro-\ncedures with a single architecture. We choose ResNet-50 and DeiT-S. The latter [45] is\nessentially a ViT parameterized so that it has approximately the same number of pa-\nrameters as a ResNet-50. For each architecture, we have put a signi\ufb01cant effort in opti-\nmizing the procedure to maximize the performance on Imagenet-val with the same 300\nepochs training schedule and same batch size. Under this constraint, the best training\nprocedure tha",
  "last blocks of c3,c4) by RepMLP Bottlenecks with\nh = w = 6, r = 2, g = 4.\nFor training, we use a batch size of 512, momentum of\n0.9, AM-Softmax loss [28], and weight decay following\n[3]. All the models are trained for 420k iterations with a\nlearning rate beginning with 0.1 and divided by 10 at 252k,\n364k and 406k iterations. For evaluation, we report the top-\n1 accuracy on MegaFace. Table. 5 shows that FaceRes-\n\n--------------------------------------------------\nTable 5: Results of face recognit",
  "/5shot IN/10shot IN/25shot\nV-MoE-S/32\nNo\n0\n37.53\n51.75\n54.97\n57.44\nV-MoE-S/32\nYes\n0\n34.07\n49.34\n52.21\n55.11\nV-MoE-S/32\nYes\n1\n35.63\n51.95\n55.79\n58.19\nV-MoE-S/32\nYes\n2\n36.72\n53.09\n56.50\n58.84\n20\n\n--------------------------------------------------\n1\n2\n3\n4\n5\n6\n7\n8\n9\nK (Selected Experts per Token)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nImageNet/1shot\n1\n2\n3\n4\n5\n6\n7\n8\n9\nK (Selected Experts per Token)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nImageNet/5shot\n1\n2\n3\n4\n5\n6\n7\n8\n9\nK (Selected Experts per Token)\n0.0\n0.1\n0.2\n0.",
  " image patches based on encoding vectors. For\ndownstream tasks (such as image classi\ufb01cation, and semantic segmentation), we append task layers\nupon pretrained BEIT and \ufb01ne-tune the parameters on the speci\ufb01c datasets.\n2.1\nImage Representations\nThe images have two views of representations in our method, namely, image patch, and visual tokens.\nThe two types serve as input and output representations during pre-training, respectively.\n2.1.1\nImage Patch\nThe 2D image is split into a sequence of patches",
  " with the 4000 labeled training samples as\nsupport. We report the mean top-1 accuracy and standard deviation across 5 seeds for the 4000 label split.\nsharpening temperature of T = 0.25. To construct the different image views, we use the multi-crop strategy, generating two large\ncrops (32 \u00d7 32), and six small crops (18 \u00d7 18) of each unlabeled image. We use the RandomResizedCrop method from the\ntorchvision.transforms module in PyTorch. The two large-crops (global views) are generated with scale (0",
  "o the ImageNet ILSVRC 2012 validation set, we test it on\nthe ImageNet-ReaL test set [6]. On this test set, our model\nalso works well and achieves 91.02% Precision@1 which is\n0.4% better than Vision Transformer [14]. This gap is also\nbigger than the gap between Vision Transformer and Noisy\nStudent which is only 0.17%.\nA lite version of Meta Pseudo Labels.\nGiven the expen-\nsive training cost of Meta Pseudo Labels, we design a lite ver-\nsion of Meta Pseudo Labels, termed Reduced Meta Pseudo\nLabels.",
  "ion.\nIn this work, we present new base-\nlines by improving the original Pyramid Vision Transformer\n(abbreviated as PVTv1) by adding three designs, including\n(1) overlapping patch embedding, (2) convolutional feed-\nforward networks, and (3) linear complexity attention lay-\ners.\nWith these modi\ufb01cations, our PVTv2 signi\ufb01cantly im-\nproves PVTv1 on three tasks e.g., classi\ufb01cation, detection,\nand segmentation.\nMoreover, PVTv2 achieves compara-\nble or better performances than recent works such as Swin\n",
  " we proposed a Feature Coupling Unit to in-\nteract the local features and global representations in each\nblock to progressively align the features to \ufb01ll the seman-\ntic gap. To validate whether fusion should be done in each\nblock, we conduct experiments on fusion intervals and re-\nport the performance on ImageNet in Tab. 11. From Tab. 11,\none can see that smaller fusion intervals report higher per-\nformance, implying that frequent interaction facilities the\nrepresentation learning.\nInterval #Par",
  "he worst case to do\nbackpropagation for each given sentence. In this section,\nwe set a small m = 2 since the models are already well-\nregularized by other regularization techniques.\nImplement Details\nThe PTB corpus (Marcus et al.,\n1993) is a standard dataset for benchmarking language\nmodels. It consists of 923k training, 73k validation and\n82k test words. We use the processed version provided by\nMikolov et al. (2010) that is widely used for PTB.\nThe WT2 dataset is introduced in Merity et al. (20",
  "ntly\nprevents many researchers from being able to provide in-\nsight. This not only limits the ability to apply models in\ndifferent domains, but also limits reproducibility, making\nour \ufb01eld vulnerable to the reproducibility crisis. Veri\ufb01ca-\ntion of state of the art machine learning algorithms should\nnot be limited to those with large infrastructures and com-\nputational resources. Throughout the history of research,\nwe have learned that the speed of scienti\ufb01c advancement is\ndirectly proportional t",
  " in describing the mechanisms by which assimilation and accommodation occur to\nproduce mental representations of semantic concepts in sensorimotor observations [51]. It is perhaps for this reason that Piaget\nwas especially interested in the emerging \ufb01eld of cybernetics (a precursor to arti\ufb01cial intelligence developed in the 40\u2019s by Norbert\nWiener) and has gone so far as to say that \u201cLife is essentially auto-regulation,\u201d and \u201ccybernetic models are, so far, the only ones\nthrowing any light on the ",
  "laced with the LeFF.\nThe structure is given in Figure 3.\nA LeFF module performs following procedures. First,\ngiven tokens xh\nt \u2208R(N+1)\u00d7C generated from the preced-\ning MSA module, we split them into patch tokens xh\np \u2208\nR(N+1)\u00d7C and a class token xh\nc \u2208RC accordingly. A lin-\near projection is conducted to expand the embeddings of\npatch tokens to a higher dimension of xl1\np \u2208RN\u00d7(e\u00d7C),\nwhere e is the expand ratio. Second, the patch tokens are\nrestored to \u201cimages\u201d of xs\np \u2208R\n\u221a\nN\u00d7\n\u221a\nN\u00d7(e\u00d7C) on spatia",
  " F. Massa, A. Sablayrolles, and H. J\u00e9gou. Training data-ef\ufb01cient\nimage transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.\n[50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and\nI. Polosukhin. Attention is all you need. In NeurIPS, 2017.\n[51] A. Vaswani, P. Ramachandran, A. Srinivas, N. Parmar, B. Hechtman, and J. Shlens. Scaling\nlocal self-attention for parameter ef\ufb01cient visual backbones. arXiv preprint arXiv:2103.12731,\n20",
  "t [93] is a popular benchmark for object\ndetection and instance segmentation. It contains more than\n200,000 images with over 500,000 annotated object instances\nfrom 80 categories.\nMMDetection [47] is a widely-used toolkit for object\ndetection and instance segmentation. We conducted our\nobject detection and instance segmentation experiments\nusing MMDetection with a RestNet-50 backbone, applied to\nthe COCO dataset. We only added our external attention at\nthe end of Resnet stage 4. Results in Tabs.",
  " (or 1\n2\nof an ISAB-like block) whose output size is relatively large\n(e.g. 512) and task-independent rather than determined by\nthe task; it would be 1 (for classi\ufb01cation) if used as pro-\nposed in the Set Transformer. This is followed by a very\ndeep stack of (latent) self-attention blocks and a \ufb01nal aver-\nage and project. In other words, Perceivers exploit similar\nprimitives to the Set Transformer, but compose them differ-\nently, in service of building an architecture with improved\nscaling prope",
  "e, and Steven W. Su. Differentiable neural architecture\nsearch in equivalent space with exploration enhancement. In\nNeurIPS, 2020. 12\n[83] Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, and\nSteven W. Su. Overcoming multi-model forgetting in one-shot\nNAS with diversity maximization. In CVPR, 2020. 12\n[84] Mingyang Zhang and Linlin Ou. Stage-wise channel pruning\nfor model compression. arXiv:2011.04908, 2020. 2, 3\n[85] Man Zhang, Yong Zhou, Jiaqi Zhao, Shixiong Xia, Jiaqi\nWang, and Zizheng Huang.",
  "rence time (see Sec. 6, supplementary)\nand parameter count.\nFinally, in Fig. 14 we assess how the order of the polynomial\nqualitatively re\ufb02ects in the reconstruction of an exemplary mesh.\nIn particular, we color code the per vertex reconstruction error on\nthe reconstructed meshes (right) and compare them with the input\n(left). Notice that the overall shape resembles the input more as we\nincrease the order of the polynomial (especially in the head), while\nbody parts with strong articulations (e.g",
  " and position-based\ninteractions in global, local or masked contexts. The resulting neural networks, LambdaNetworks,\nare computationally ef\ufb01cient, model long-range dependencies at a small memory cost and can there-\nfore be applied to large structured inputs such as high resolution images.\nWe evaluate LambdaNetworks on computer vision tasks where works using self-attention are hin-\ndered by large memory costs (Wang et al., 2018; Bello et al., 2019), suffer impractical implemen-\ntations (Ramachand",
  "oved. Additionally, we show the differ-\nence between training a model with these parameters and\ninferring on a model that was trained on 32\u00d732 sized im-\nages, denoting the generalizability of the models. We note\nthat the number of parameters for these competing models\nis roughly the same, see Table 9.\nB. Hyperparameter tuning\nWe tuned the hyperparamters per experiment and ar-\nrived at the following for each table in Sec. 4. We deter-\nmined these hyper parameters through a parameter sweep.\nFor Re",
  "split the multi-heads into parallel groups and apply\ndifferent self-attention operations onto different groups. This parallel strategy introduces no extra\ncomputation cost while enlarging the area for computing self-attention within each Transformer block.\nThis strategy is fundamentally different from existing self-attention mechanisms [57, 39, 70, 26]\nthat apply the same attention operation across multi-heads((Figure 1 b,c,d,e), and perform different\nattention operations sequentially(Figure 1 c",
  " CeiT architecture\nthat combines the advantages of CNNs in extracting low-\nlevel features, strengthening locality, and the advantages\nof Transformers in establishing long-range dependencies.\nCeiT obtains state-of-the-art performances on ImageNet\n\n--------------------------------------------------\nand various downstream tasks, without requiring a large\namount of training data and extra CNN teachers.\nBe-\nsides, CeiT models demonstrate better convergence than\npure Transformer with 3\u00d7 fewer training",
  " [19, 59] as explained in experiments. Based on our experiments, it is\ninteresting to \ufb01nd that, similar to our observations in image classi\ufb01cation, a carefully-designed block\nde-aggregation makes the model signi\ufb01cantly more effective in performance.\nAlgorithm 1 Gradient-based class-aware tree traversal (GradGAT).\nDe\ufb01ne: Al denotes the feature maps at hierarchy l. Yc is the logit of predicted class c. [\u00b7]2\u00d72 indexes one of\n2 \u00d7 2 partitions of input maps.\nInput: {Al|l = 2, ..., Td}, \u03b1Td = ATd, P =",
  "eeps\nthe same number of spatial tokens overall layer of the net-\nwork. Although the self-attention operation is not limited\nby spatial distance, the size of the spatial area participat-\ning in attention is affected by the spatial size of the feature.\nTherefore, in order to adjust the dimension con\ufb01guration\nlike ResNet, a spatial reduction layer is also required in ViT.\nTo utilize the advantages of the dimension con\ufb01gura-\ntion to ViT, we propose a new architecture called Pooling-\nbased Vision Tra",
  "s\nCIFAR-10\nSVHN\nImageNet\nSupervised\n97.18 \u00b1 0.08\n98.17 \u00b1 0.03\n84.49/97.18\nNoisyStudent\n98.22 \u00b1 0.05\n98.71 \u00b1 0.11\n85.81/97.53\nReduced Meta Pseudo Labels\n98.56 \u00b1 0.07\n98.78 \u00b1 0.07\n86.87/98.11\nTable 11: Image classi\ufb01cation accuracy of Ef\ufb01cientNet-B0 on CIFAR-10 and SVHN, and Ef\ufb01cientNet-B7 on ImageNet. Higher is better.\nCIFAR-10 results are mean \u00b1 std over 5 runs, and ImageNet results are top-1/top-5 accuracy of a single run. All numbers are produced in\nour codebase and are controlled experiments.\n",
  "g the Re\ufb01ned-ViT model.\n4\nExperiments\n4.1\nExperiment Setup\nWe mainly evaluate Re\ufb01ner for the image classi\ufb01cation task. Besides, we also conduct experiments\non natural language processing tasks to investigate its generalizability to NLP transformer models.\nComputer vision\nWe evaluate the effectiveness of the re\ufb01ner on ImageNet [15]. For a fair com-\nparison with other methods, we \ufb01rst replace the SA module with re\ufb01ner on ViT-Base [19] model\nas it is the most frequently used one [19, 48, 61, 49]. W",
  "mputer Vision (ECCV).\nSpringer, 2016, pp. 87\u2013102. 8\n[75] J. Deng, J. Guo, D. Zhang, Y. Deng, X. Lu, and S. Shi, \u201cLightweight face\nrecognition challenge,\u201d in CVPRW, 2019, pp. 0\u20130. 8\n[76] J. Deng, J. Guo, Y. Zhou, J. Yu, I. Kotsia, and S. Zafeiriou, \u201cRetinaface:\nSingle-stage dense face localisation in the wild,\u201d Conference on Computer\nVision and Pattern Recognition (CVPR), 2020. 8\n\n--------------------------------------------------\n13\n[77] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller, \u201cL",
  "l self-attention,\nwe abstain from enforcing translation equivariance in lieu\nof better hardware utilization, which improves the speed-\naccuracy tradeoff (Section 2.2). Also note that while we use\nlocal attention, our receptive \ufb01elds per pixel are quite large\n(up to 18 \u00d7 18) and we show in Section 4.2.2 that larger\nreceptive \ufb01elds help with larger images. In the remainder\nof this section, we will motivate self-attention for vision\ntasks and describe how we relax translational equivariance\nto ef\ufb01c",
  "ter and FLOPs ef\ufb01ciency.\nTop-1 Acc.\nParams\nFLOPs\nEf\ufb01cientNet-B6 (Tan & Le, 2019a)\n84.6%\n43M\n19B\nResNet-RS-420 (Bello et al., 2021)\n84.4%\n192M\n64B\nNFNet-F1 (Brock et al., 2021)\n84.7%\n133M\n36B\n3.2. Understanding Training Ef\ufb01ciency\nWe study the training bottlenecks of Ef\ufb01cientNet (Tan & Le,\n2019a), henceforth is also called Ef\ufb01cientNetV1, and a few\nsimple techniques to improve training speed.\nTraining with very large image sizes is slow: As pointed\nout by previous works (Radosavovic et al., 2020), ",
  "ed the design of BoTNet for detection and\nsegmentation, it is a natural question to ask whether the\nBoTNet architecture design also helps improve the image\nclassi\ufb01cation performance on the ImageNet [52] benchmark.\nPrior work [65] has shown that adding Non-Local blocks\nto ResNets and training them using canonical settings does\nnot provide substantial gains. We observe a similar \ufb01nd-\ning for BoTNet-50 when contrasted with ResNet-50, with\nboth models trained with the canonical hyperparameters for\nI",
  "accuracy, compactness and ef\ufb01ciency of MUXNet\nand other baselines. Overall, MUXNet signi\ufb01cantly out-\nperforms previous methods on both CIFAR-10 and -100\ndatasets, indicating that our models also transfer well to\nother similar tasks. In particular, MUXNet-m achieves 1%\nhigher accuracy than NASNet-A mobile with 3\u00d7 fewer pa-\nrameters while being 2\u00d7 more ef\ufb01cient in MAdds.\n5.4.2\nChestX-Ray14\nThe ChestX-Ray14 benchmark was recently introduced\nin [43]. The dataset consists of 112,120 high resolution\nf",
  ".\nThey can be broadly classi\ufb01ed into two categories depending on\nwhether the supernet training is coupled with architecture search\nor decoupled into a two-stage process. Approaches of the former\nkind [24], [26], [46] are computationally ef\ufb01cient but return sub-\noptimal models. Numerous studies [47], [48], [49] allude to weak\ncorrelation between performance at the search and \ufb01nal evaluation\nstages. Methods of the latter kind [10], [31], [50] use performance\nof subnets (obtained by sampling the tr",
  "Twins: Revisiting the Design of Spatial Attention in\nVision Transformers\nXiangxiang Chu1,\nZhi Tian2,\nYuqing Wang1,\nBo Zhang1,\nHaibing Ren1,\nXiaolin Wei1,\nHuaxia Xia1,\nChunhua Shen2\u2217\n1 Meituan Inc.\n2 The University of Adelaide, Australia\n1 {chuxiangxiang,wangyuqing06,zhangbo97,renhaibing,weixiaolin02,xiahuaxia}@meituan.com\n2 zhi.tian@outlook.com, chunhua@me.com\nAbstract\nVery recently, a variety of vision transformer architectures for dense prediction\ntasks have been proposed and they show that th",
  "], and in Figure 37 we compare directly to their results. V-MoE\nseems initially fairly robust to reduced data, but after reducing to 9M pre-training samples (3% of the\ndataset), it becomes slightly preferable to instead train a dense model.\nTraining on ImageNet21k.\nImageNet21k [16] is a large public dataset with approximately 14M\nimages and 21k classes. Previous works [20, 36] have successfully pre-trained on it to achieve\nstrong results in downstream tasks. In particular, dense ViT models train",
  "he order of the polynomial (especially in the head), while\nbody parts with strong articulations (e.g. hands) are reconstructed\nwith higher \ufb01delity.\nFig. 13: ProdPoly vs 1st order graph learnable operators for mesh\nautoencoding. Note that even without using activation functions the\nproposed methods signi\ufb01cantly improve upon the state-of-the-art.\n5\nFUTURE DIRECTIONS\nThe new class of \u03a0\u00b4nets has strong experimental results and few\nempirical theoretical results already. We expect in the following\nyea",
  "s to learn suf\ufb01cient domain-\nrelated information but gives disordered ranking of paths.\nThis smaller correlation coef\ufb01cient implies that it might be\nnot sensible to implement greedy sampling from a random\nsupernet since the ranking evaluated by 1000 validation im-\nages will be rather noisy. In this way, we record the trend of\nrank correlation coef\ufb01cients with uniform sampling in right\nFigure 4. It shows that with more iterations, the correla-\ntion coef\ufb01cients increase and at 10K iteration, they ",
  "an be either shared\nor unshared across different heads. We show the effects of\nthese two schemes in bias and contextual modes in Tab.\n2, respectively. For bias mode, the accuracy drops signif-\nicantly when sharing encoding across the heads. By con-\ntrast, in contextual mode, the performance gap between two\nschemes is negligible. Both of them achieve an average\ntop-1 accuracy of 80.9%. We conjecture that different head\nMode\nShared\n#Param.\nMACs\nTop-1\n(M)\n(M)\nAcc(%)\nBias\n\u00d7\n22.05\n4613\n80.54 \u00b1 0.06\n\u2713",
  "evel features like\nedges or textures compared with ResNet and more seman-\ntics compared with Pure-MLP Baseline .\n5. Conclusion\nIn this paper, we analyze the constraints of current MLP-\nbased models for visual representation learning: 1. Spa-\ntial MLPs only take inputs with \ufb01xed resolutions, making\ntransfer to downstream tasks, such as object detection and\nsegmentation, dif\ufb01cult. 2. Single-stage design and fully\nconnected layers further constrain usage due to the added\ncomplexity. To tackle these",
  "s), and skip\nconnections [20]. More recently, building models solely on\nMLPs and skip connections without the self-attention lay-\ners has been a new trend for visual recognition tasks [49,\n50, 35].\n\ud835\udc3b\u00d7\ud835\udc4a\n\ud835\udc36\n(a) Channel FC\n\ud835\udc3b\u00d7\ud835\udc4a\n\ud835\udc36\n(b) Spatial FC\n\ud835\udc3b\u00d7\ud835\udc4a\n\ud835\udc36\n(c) Cycle FC\nFC\nO(HW)\nScale\nVariable\nImgNet\nTop-1\nCOCO\nAP\nADE20K\nmIoU\nChannel\nHW\n\u0013\n79.4\n35.0\n36.3\nSpatial\nH2W 2\n\u0017\n80.9\n\u0017\n\u0017\nCycle\nHW\n\u0013\n81.6\n41.7\n42.4\nFigure 1: Motivation of Cycle Fully-Connected Layer\n(FC) compared to Channel FC and Spatial FC. (a) Chan-",
  "iv:1912.02781, 2020. 18\n[16] Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Youngjung\nUh, and Jung-Woo Ha. Adamp: Slowing down the weight norm increase in momentum-\nbased optimizers. arXiv preprint arXiv:2006.08217, 2020. 21\n[17] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoe\ufb02er, and Daniel Soudry.\nAugment your batch: Improving generalization through instance repetition. In Conference\non Computer Vision and Pattern Recognition, 2020. 5\n[18] Grant Van Horn,",
  "\u00f8\n\u00f8\n(b) Image-relative coordinates\nFigure 4. For ImageNet experiments, we generate position encod-\nings using [-1, 1]-normalized (x, y)-coordinates drawn from (a)\ncrops rather than from (b) the raw images, as we \ufb01nd the latter\nleads to over\ufb01tting.\nB. Ablations\nTo illustrate the effect of various network hyperparameters,\nwe considered a small Perceiver model and swept a num-\nber of options around it. Unlike ConvNets, each module\nin a Perceiver-based architecture takes as input the full in-\nput by",
  "(xu, byu) to update its parameter \u03b8S. In expectation, the student\u2019s new parameter is\nEbyu\u223cT (xu;\u03b8T )\n\u0002\n\u03b8S\u2212\u03b7S\u2207\u03b7SCE(byu, S(xu; \u03b8S))\n\u0003\n. We will update the teacher\u2019s parameter to minimize the student\u2019s cross-entropy\non a batch of labeled data a this expected parameter. To this end, we need to compute the Jacobian:\n\u2202R\n\u2202\u03b8T\n|{z}\n1\u00d7|T |\n=\n\u2202\n\u2202\u03b8T\nCE\n\u0010\nyl, S\n\u0010\nxl; Ebyu\u223cT (xu;\u03b8T )\n\u0002\n\u03b8S \u2212\u03b7S\u2207\u03b7SCE(byu, S(xu; \u03b8S))\n\u0003\u0011\u0011\n(4)\nTo simplify our notation, let us de\ufb01ne\n\u00af\u03b8\u2032\nS\n|{z}\n|S|\u00d71\n= Ebyu\u223cT (xu;\u03b8T )\n\u0002\n\u03b8S \u2212\u03b7S\u2207\u03b7SCE(b",
  "eed-accuracy Pareto curve of LambdaResNets compared to Ef\ufb01cientNets (Tan & Le, 2019) on\nTPUv3 hardware. In order to isolate the bene\ufb01ts of lambda layers, we additionally compare against\nthe same architectures when replacing lambda layers by (1) standard 3x3 convolutions (denoted\nResNet-RS wo/ SE) and (2) 3x3 convolutions with squeeze-and-excitation (denoted ResNet-RS w/\nSE). All architectures are trained for 350 epochs using the same regularization methods and evalu-\nated at the same resolution ",
  "r visual representation.\nHowever, transformer-based vision models, including\nAutoFormer, now are still inferior to the models based on\ninverted residual blocks [44], such as MobileNetV3 [21]\nand Ef\ufb01cientNet [49]. The reason is that inverted residu-\nals are optimized for edge devices, so the model sizes and\nFLOPs are much smaller than vision transformers.\n4.4. Transfer Learning Results\nClassi\ufb01cation.\nWe transfer Autoformer to a list of\ncommonly used recognition datasets: 1) general classi\ufb01ca-\ntio",
  "re\nfar below their true performances of training from scratch\n(see the right part of Fig. 4). This limits the ranking ca-\npacities of supernet. Furthermore, after the search, it is still\nnecessary to perform additional retraining for the searched\narchitectures since the weights are not fully optimized. In-\nspried by BigNAS [65] and slimmable networks [66, 64],\nwe propose the weight entanglement training strategy ded-\nicated to vision transformer architecture search. The cen-\ntral idea is to enab",
  " approach, which let\nCNNs automatically \ufb01gure out how to avoid being biased towards either shape or texture from their\ntraining samples. Speci\ufb01cally, we apply style transfer to generate cue con\ufb02ict images, which breaks\nthe correlation between shape and texture, for augmenting the original training data. The most\nimportant recipe of training a successful shape-texture debiased model is that we need to provide\nsupervision from both shape and texture on these generated cue con\ufb02ict images, otherwise",
  "ween different tasks.\nThus, directly applying the vanilla transformer architecture\nto other tasks may be sub-optimal. It is natural that there\nexists better transformer architectures for image recogni-\ntion.\nHowever, hand-designing such an architecture is\ntime consuming since there are too many influential fac-\ntors to be considered. On one hand, Neural Architecture\nSearch (NAS) has achieved great progress in computer vi-\n\n--------------------------------------------------\nsion tasks [13, 22, 2]",
  "un. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 39:1137\u20131149, 2015. 1, 5, 6, 7, 14, 15\nSam Ringer, Will Williams, Tom Ash, Remi Francis, and David MacLeod. Texture bias of cnns\nlimits few-shot classi\ufb01cation performance, 2019. 1, 3\nRamprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,\nand Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradie",
  "Malik 2000). When\ninputs grow very large, this may introduce a bandwidth bot-\ntleneck. By using multiple cross-attentions, the Perceiver\ncan use a form of re-entrant processing to mitigate this ef-\nfect, by allowing \ufb01rst-pass processing of an input to feed\nback and in\ufb02uence how the input is processed in subsequent\npasses. Re-entrant processing of this kind (sometimes re-\nferred to as top-down processing) has a long history in com-\nputer vision (Borenstein et al., 2004; Kumar et al., 2005;\nCarrei",
  "t of frozen patch embeddings\nxpatches. We discuss why we include xclass in the keys in Appendix B.\nConsidering a network with h heads and p patches, and denoting by d the\nembedding size, we parametrize the multi-head class-attention with several\nprojection matrices, Wq, Wk, Wv, Wo \u2208Rd\u00d7d, and the corresponding biases\nbq, bk, bv, bo \u2208Rd. With this notation, the computation of the CA residual block\nproceeds as follows. We \ufb01rst augment the patch embeddings (in matrix form)\nas z = [xclass, xpatches] ",
  "9.17\n94.08\n95.95\n96\nResNet152x1\n7\n81.88\n87.96\n98.82\n90.22\n94.17\n96.94\n141\nResNet152x2\n7\n84.97\n89.69\n99.06\n92.05\n95.37\n98.62\n563\nResNet152x2\n14\n85.56\n89.89\n99.24\n91.92\n95.75\n98.75\n1126\nResNet200x3\n14\n87.22\n90.15\n99.34\n93.53\n96.32\n99.04\n3306\nR50x1+ViT-B/32\n7\n84.90\n89.15\n99.01\n92.24\n95.75\n99.46\n106\nR50x1+ViT-B/16\n7\n85.58\n89.65\n99.14\n92.63\n96.65\n99.40\n274\nR50x1+ViT-L/32\n7\n85.68\n89.04\n99.24\n92.93\n96.97\n99.43\n246\nR50x1+ViT-L/16\n7\n86.60\n89.72\n99.18\n93.64\n97.03\n99.40\n859\nR50x1+ViT-L/16\n14\n87.12\n89.76\n99",
  " (Left). Its intermediate performance in all\nobjectives indicate that this best trade-off solution makes a good\ncompromise on all 12 objectives among all 45 obtained solutions.\nIn Fig. 26 (Right), we compare this solution with different baseline\nmodels that are \ufb01ne-tuned to each dataset separately. Notably,\nour NATNet achieves better accuracy on all datasets with similar\nor less #MAdds than Ef\ufb01cientNet-B0 [28], MobileNetV2 [56],\nNASNet-A [8], and ResNet-50 [3], making our highest trade-off\nsolut",
  "es, each of which is denoted as Siloj with\na supply of one, as well as n demand sources, each of which is denoted as Millk with a demand\nof m\nn . Cj,k is the per-unit transportation cost from Siloj to Millk. Speci\ufb01cally, Cj,k = \u2212Pj,k in\nWGM and Cj,k = \u2212Sj,k in LGM. To make it clear, we illustrate this algorithm with a toy example,\nwhere the problem is simpli\ufb01ed as Fig.6 (a) with m = 4, n = 2. In the \ufb01rst step, we calculate the\npenalty cost pcrowj for each row and pccolk for each column of the ta",
  "rs (e.g. number of heads\nand number of MSA layers). We mainly follow previous work to obtain right architectures that has\nsimilar capacity (e.g. number of parameters and throughput).\nRecall that the overall hierarchy can be determined by two key hyper-parameters: patch size S \u00d7 S\nand hierarchy depth Td. Just like how ResNet [23] adapts to small and large input sizes, NesT also\nhas different con\ufb01guration for small input size and large input size. We follow [35, 49] to con\ufb01gure\nthe number of head,",
  "s change\nby increasing the mixing weight \u03bb, and in Figure 10, we\nvisualize how the results change as we increase the coef\ufb01-\ncients of the smoothness terms, \u03b2 and \u03b3.\nE.2. More Samples\nIn this section, we provide Puzzle Mix results with various\nresolutions of the optimal mask and transport. Figure 11\nvisualizes the Puzzle Mix results along with the given inputs.\n\n--------------------------------------------------\nPuzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup\nReferences\nBen",
  "getting the routing\nright is the most important. The model is robust to mis-routing at most intermediate layers\u2014layer 9 is\nan exception here. This observation motivated us into trying to train sparse models with MoE layers\nonly at the very end\u201421 and 23, for example\u2014with excellent results (and computational savings).\n1 3 5 7 9\n11131517192123\nOriginal\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nImageNet 1-shot Acc\n1 3 5 7 9\n11131517192123\nOriginal\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nImageNet 5-shot Acc\n1 3 5 7 9\n11131517192123\n",
  "nsidered applying\nstraddled Squeeze & Excite layers, where the input to the\nS&E is the input to the convolution, but the output of the\nS&E multiplies the output of the convolution. This is in con-\nstrast to the normal Squeeze & Excite formulation, which\nsimply operates directly on an activation (i.e. it takes in a\nvalue, and its output is used to multiply that same value).\nBoth forms of S&E block help to restore cross-channel con-\nnectivity (albeit not as strongly as using a fully connected\nconv",
  "ration for small input size and large input size. We follow [35, 49] to con\ufb01gure\nthe number of head, hidden dimensions for the tiny, small and base versions. For 32 \u00d7 32 image size,\nwe follow [49]. Speci\ufb01cally, we setup the same number of repeated MSANesT per block hierarchy.\nIn each hierarchy, the number of hidden dimensions and the number of heads are the same as well.\nFor 224 \u00d7 224 image size, we follow [35]. Therefore, different hierarchy has a gradually increased\nnumber of head, hidden dime",
  "the reshaped tokens along channels. By reaching\na good combination of local and global features, our GLiT\nfocuses on more object regions than DeiT.\n5. Conclusion\nWe exploit better architectures for vision transformers\nin this paper through carefully designing the searching\nspace with local information and the hierarchical searching\nmethod. Transformer is applied to vision community not\nlong ago. Its architecture is not well exploited for image\nrecognition. Our method provides a feasible and auto",
  " images\nfor training and 5k images for evaluating performances\nof object detectors. We follow standard practice of Reti-\nnaNet [20] and Mask R-CNN [8] with ResNet as backbones\nin mmdetection [1]. We replace ResNet backbones with\nConvMLP and adjust the dimension of convolution layers\nin feature pyramids accordingly. We also replace SGD op-\ntimizer with AdamW and adjust learning rate to 0.0001\nwith weight decay at 0.0001, which follows the con\ufb01gs in\nPVT [37]. We train both RetinaNet and Mask R-CNN",
  "further iterate by adding\nconvolutional blocks to the tokenization step and thus creat-\ning the Compact Convolutional Transformer (CCT). Both of\nthese additions add to signi\ufb01cant increases in performance,\nleading to a top-1%accuracy of 94.72% on CIFAR-10. This\nis only slightly behind ResNet1001-v2, which obtains an\naccuracy of 95.38% but with 2.7\u00d7 model size and 1.6\u00d7\nGMACs comparing to ours. Our model outperforms all pre-\nvious transformer based models within this domain. Addi-\ntionally, we show",
  "ntal, as the same weights are used for two\ndifferent purposes: helping the attention process, and preparing the vector to be fed\nto the classi\ufb01er. We put this problem in evidence by showing that inserting CLS later\nimproves performance (middle). In the CaiT architecture (right), we further propose to\nfreeze the patch embeddings when inserting CLS to save compute, so that the last part\nof the network (typically 2 layers) is fully devoted to summarizing the information to\nbe fed to the linear clas",
  "ible to adopt another way for acceleration rather than the\nmethods used for CNN acceleration like \ufb01lter pruning [13], which removes non-critical or redundant\nneurons from a deep model. Our method aims at pruning the tokens of less importance instead of the\nneurons by exploiting the sparsity of informative image patches.\n3\nDynamic Vision Transformers\n3.1\nOverview\nThe overall framework of our DynamicViT is illustrated in Figure 2. Our DynamicViT consists of a\nnormal vision transformer as the backb",
  " category include\nShuf\ufb02eNet [27], MobileNetV2 [34], MnasNet [40] and Mo-\nbileNetV3 [13]. Such solutions sought to improve compu-\ntational ef\ufb01ciency by progressively replacing the parameter\nand compute intensive standard convolutions by a combina-\ntion of 1\u00d71 convolutions and depth-wise separable 3\u00d73 con-\nvolutions. Figure 2 depicts the trend in the relative contribu-\ntions of different layers in terms of parameters and MAdds.\nDepth-wise separable convolutions [36, 4] offer signif-\nicant computat",
  "otr Dollar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In CVPR, 2017. 2\n[55] Saining Xie, Alexander Kirillov, Ross Girshick, and Kaim-\ning He. Exploring randomly wired neural networks for im-\nage recognition. In ICCV, 2019. 9\n[56] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan\nNgiam. Condconv: Conditionally parameterized convolu-\ntions for ef\ufb01cient inference. In NeurIPS, 2019. 4\n[57] Yibo Yang, Zhisheng Zhong, Tiancheng Shen, and Zhouchen\nLin. ",
  "we now propose a training-aware NAS.\nNAS Search: Our training-aware NAS framework is\nlargely based on previous NAS works (Tan et al., 2019;\nTan & Le, 2019a), but aims to jointly optimize accuracy,\nparameter ef\ufb01ciency, and training ef\ufb01ciency on modern ac-\ncelerators. Speci\ufb01cally, we use Ef\ufb01cientNet as our backbone.\nOur search space is a stage-based factorized space similar\nto (Tan et al., 2019), which consists of the design choices for\nconvolutional operation types {MBConv, Fused-MBConv},\nnumber ",
  "f TNT block\nare\nFLOPsT NT = 2nmc(6c + m) + nmcd + 2nd(6d + n).\n(17)\nSimilarly, the parameter complexity of TNT block is calculated as\nParamsT NT = 12cc + mcd + 12dd.\n(18)\nAlthough we add two more components in our TNT block, the increase of FLOPs is small since\nc \u226ad and O(m) \u2248O(n) in practice. For example, in the DeiT-S con\ufb01guration, we have d = 384\nand n = 196. We set c = 24 and m = 16 in our structure of TNT-S correspondingly. From Eq. 15\nand Eq. 17, we can obtain that FLOPsT = 376M and FLOPsT",
  ", 51, 57].\nSome works have modernized the ResNet training procedure and obtained some\nimprovement over the original model (e.g. Dollar et al. [10]). This allows a more di-\nrect comparison when considering new models or methods involving more elaborate\ntraining procedures than the one initially used. Nevertheless, improving the ResNet-50\nbaseline [6, 10, 14, 47, 48, 54] was not the main objective of these works. As a conse-\nquence and as we will see, the best performance reported so far with a Re",
  "luated the effect of the latent array\u2019s ini-\ntialization scale and the parameters of the Fourier frequency\nposition encoding on ImageNet performance. The results of\nthis experiment are shown in Fig. 6. These experiments use\nthe full-sized ImageNet architecture, but were trained with a\nsmaller batch size (256) and fewer TPUs (16) (for reasons of\ncompute availability). These experiments suggest that stan-\ndard and relatively small values for the initialization scale\nare best (values \u22651 may lead to",
  "e\u201d\nbut with pdecay, twarmup, \u03b70, \u03c1, \u03f5 set to their values in the LARS pipeline. Finally, \u201cL2 variables\u201d is\nthe same point as \u201cBase\u201d but where the L2 regularization is applied to all variables. The only ablation\n20 https://github.com/google-research/bert\n15\n\n--------------------------------------------------\n32k\n65k-32k\n65k\n90.25\n90.50\n90.75\n91.00\n91.25\n91.50\nF1 score\nFigure 3: 6 \ufb01netuning runs starting from the same pretraining checkpoint to show the stability of our\nresults, at each of the 32,7",
  "te choice modelling economet-\nrics literature and theory. However unlike Collier et al. [9]\nwe use a low-rank approximation to a full (non-diagonal)\ncovariance matrix in the latent distribution. In the below\nexperiments, we demonstrate that our combination of (a)\nrecognizing the importance of the softmax temperature in\ncontrolling a bias-variance trade-off and (b) modelling the\ninter-class noise correlations yields signi\ufb01cantly improved\nperformance compared with individually using (a) or (b).\nOu",
  "arameters and the script provided by (Fan et al. 2020) in the\nFairseq repository5, however, and unlike what they mention in their paper, this script does not include layer drop (Fan, Grave,\nand Joulin 2019). For DIFFQ, we tried the penalty levels \u03bb in {1, 5, 10}, with group size 8, as well as \u03bb = 10 and g = 16. For\nLSQ, we used the same training hyper-parameters as DIFFQ, except we initialized the model to a pre-trained model and used a\nlearning rate 10 time smaller for \ufb01ne tuning. Without this ",
  "ave more tradi-\ntional blocks while preserving the number of FLOPs. We\nTable 6:\nTimings for the components of the LeViT ar-\nchitecture on an Intel Xeon E5-2698 CPU core with batch\nsize 1.\nModel\nDeiT-tiny\nLeViT-256\nDimensions\nC = 192\nN = 3\nD = 64\nC = 256\nN = 4\nD = 32\nComponent\nRuntime (\u00b5s)\nRuntime (\u00b5s)\nLayerNorm\n49\nn/a\nKeys Q, K\n299\n275\nValues V\n172\n275\nProduct QKT\n228\n159\nProduct Attention AV\n161\n206\nAttention projection\n175\n310\nMLP\n1390\n1140\nTotal\n2474\n2365\ntherefore take Q, K, V to all have di",
  "vironments of ImageNet training.\nAnd, we extended the ImageNet comparison to architec-\ntures other than Transformer. In particular, we focus on the\ncomparison of the performance of ResNet and PiT, and in-\nvestigate whether PiT can beat ResNet. We also applied PiT\nto an object detector based on deformable DETR [44], and\ncompared the performance as a backbone architecture for\nobject detection. To analyze PiT in various views, we eval-\nuated the performance of PiT on robustness benchmarks.\n4.1. Ima",
  " have also tried applying a smaller keeping ratio (larger acceleration\nrate). The results based on DeiT-S [25] and LV-ViT-S [16] models are presented in the following\ntables. We see that using \u03c1 < 0.7 will lead to a signi\ufb01cant accuracy drop while reducing fewer\nFLOPs. Since only 22% and 13% tokens are remaining in the last stage when we set \u03c1 to 0.6 and\n0.5 respectively, small \u03c1 may cause a signi\ufb01cant information loss. Therefore, we use \u03c1 \u22650.7 in our\nmain experiments. Jointly scaling \u03c1 and the m",
  "8, 34] in current T2T-ViT-7 and T2T-\nViT-12, and we only reduce model size by reducing the hid-\nden dimension, MLP ratio and depth of layers, indicating\nT2T-ViT is also very promising as a lite model. We also ap-\nply knowledge distillation on our T2T-ViT as the concurrent\nwork DeiT [38] and \ufb01nd that our T2T-ViT-7 and T2T-ViT-\n12 can be further improved by distillation. Overall, the ex-\nperimental results show, our T2T-ViT can achieve superior\n\n--------------------------------------------------\nT",
  "te their implemented modules/func-\ntions with this framework. Similar to AllenNLP [6] and Catalyst [17], this can be done\neven outside the framework by using a Python decorator. The following example shows\nthat a new model class, MyModel, is added to the framework by simply using @regis-\nter model (de\ufb01ned in the framework), and the new class can be instantiated by de\ufb01ning\n\u201cMyModel\u201d with required parameters at designated places in a con\ufb01guration \ufb01le.\n2 https://pytorch.org/hub/\n3 https://github.co",
  "Our\nresults show that existing networks\u2019 potential has not been\nfully exploited and there is still room to boost and enhance\nthrough our framework. We hope the proposed method can\ninspire more studies along this direction of boosting tiny\nand compact models through knowledge distillation.\n8\n\n--------------------------------------------------\nAppendix\nA. Implementation Details of Transferring\nFine-tuning backbone. On classi\ufb01cation task, we \ufb01ne-tune\nthe entire network of ResNet-50 using the parame",
  "on datasets\nwith larger training samples per class (e.g. 4,000 in CIFAR-10).\nOn datasets with fewer samples per class (e.g. 20 in Flowers\n102), there is still a large performance gap between supernet\nadaptation and additional \ufb01ne-tuning. Overall the results suggest\nthat supernet adaptation is more effective on tasks with limited\ntraining samples.\n5.6\nTowards Quantifying Architectural Advancement\nComparing the architectural contribution to the success of different\nNAS algorithms can be dif\ufb01cult a",
  "peed-\naccuracy trade-off among these methods on image classi-\n\ufb01cation, even though our work focuses on general-purpose\nperformance rather than speci\ufb01cally on classi\ufb01cation. An-\nother concurrent work [66] explores a similar line of think-\ning to build multi-resolution feature maps on Transform-\ners. Its complexity is still quadratic to image size, while\nours is linear and also operates locally which has proven\nbene\ufb01cial in modeling the high correlation in visual sig-\nnals [36, 25, 41]. Our approa",
  " ResNeXt-50 ResNeXt-101\nCE(baseline)\n41.6\n44.4\n44.8\nDecouple-cRT\n47.3\n49.6\n49.4\nDecouple-\u03c4-norm\n46.7\n49.4\n49.6\nDe-confound-TDE\n51.7\n51.8\n53.3\nResLT\n-\n52.9\n54.1\nMiSLAS\n52.7\n-\n-\nDecouple-\u03c4-norm \u2020\n54.5\n56.0\n57.9\nBalanced Softmax \u2020\n55.0\n56.2\n58.0\nPaCo\u2020\n57.0\n58.2\n60.0\nstate-of-the-arts of Decouple [28], Balanced Softmax [38]\nand RIDE [49] as mentioned in Section 4.2.\nFor Places-LT, following previous setting [35, 14], we\nchoose ResNet-152 as the backbone network, pre-train it on\nthe full ImageNet-201",
  "0B\n86.1@224\n87.1@512\nD2 to VOLO-D5 yields nearly another 1% accuracy gain.\nIn addition, for all the \ufb01ve models, increasing the resolution\nduring \ufb01netuning brings around 1% performance gain.\nNumber of Outlookers: We observe that the number of\nOutlookers used in our VOLO has an impact on the classi-\n\ufb01cation performance. Here, we investigate the in\ufb02uence of\nusing different numbers of Outlookers in our VOLO. Note\nthat all Outlookers act on \ufb01ner-level token representations\n(28 \u00d7 28). The results have",
  "trained models with token labeling are also bene\ufb01cial to downstream tasks with dense prediction,\nsuch as semantic segmentation.\n2\nRelated Work\nTransformers [39] refer to the models that entirely rely on the self-attention mechanism to build\nglobal dependencies, which are originally designed for natural language processing tasks. Due to their\nstrong capability of capturing spatial information, transformers have also been successfully applied\nto a variety of vision problems, including low-level vi",
  "on of \u03c9 and we can obtain a sequence of X[k] by sampling X(ej\u03c9) at frequencies\n\u03c9k = 2\u03c0k/N:\nX[k] = X(ej\u03c9)|\u03c9=2\u03c0k/N =\nN\u22121\nX\nn=0\nx[n]e\u2212j(2\u03c0/N)kn,\n(A.9)\nwhich is exactly the formulation of DFT. The extension from 1D DFT to 2D DFT is straightforward.\nIn fact, The 2D DFT can be viewed as performing 1D DFT on the two dimensions alternatively, i.e.,\nthe 2D DFT of x[m, n] is given by:\nX[u, v] =\nM\u22121\nX\nm=0\nN\u22121\nX\nn=0\nx[m, n]e\u2212j2\u03c0( um\nM + vn\nN ).\n(A.10)\nA.2\nSome properties of DFT\nDFT of real signals.\nGiven a ",
  "-MSA denote window based multi-head\nself-attention using regular and shifted window partitioning\ncon\ufb01gurations, respectively.\nThe shifted window partitioning approach introduces\nconnections between neighboring non-overlapping win-\ndows in the previous layer and is found to be effective in im-\nage classi\ufb01cation, object detection, and semantic segmenta-\ntion, as shown in Table 4.\nEf\ufb01cient batch computation for shifted con\ufb01guration\nAn issue with shifted window partitioning is that it will re-\nsult ",
  "Santoro et al., 2018; Goyal et al., 2021), all while keep-\ning the cost of each operation linear in the input size. We\nuse cross-attention to induce a latent space for deep pro-\ncessing. This can be viewed as a fully attentional, domain-\nagnostic analogue of models that stacks self-attention on\ntop of convolutional feature maps to perform cheap but\nglobal processing on top or in conjunction with otherwise\nspatially localized convolutional feature maps (e.g. Carion\net al. 2020; Locatello et al. 2",
  "ation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In Proc. Eur. Conf. Comp.\nVis., pages 213\u2013229. Springer, 2020.\n[4] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\n[5] ",
  "ion [30]. We do not compare to\nthe distilled version of DeiT since it\u2019s an orthogonal axis of improvement\napplicable to all models.\n\n--------------------------------------------------\nHoulsby, Alexey Dosovitskiy for feedback. We thank Zak\nStone for extensive compute support throughout this project\nthe through TFRC program providing Google Cloud TPUs\n(https://www.tensorflow.org/tfrc).\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv",
  "than simply\nstacking it). For Broyden\u2019s method (see Eq. (7)), the Broyden matrix B[i] is a full matrix that mixes\nall locations of the feature map (which is represented by g\u03b8(z[i]; x)); whereas typical convolutional\n\ufb01lters only mix the signals locally. On the other hand, multiscale up- and downsamplings broaden the\neffective receptive \ufb01eld on the high-resolution stream by direct interpolation from lower-resolution\nfeature maps.\nC\nQualitative Segmentation Results on Cityscapes\nWe demonstrate in F",
  "ate\nmulti-scale attention maps, while the scale of feature con-\ntext aggregation inside the attention module remains single\n[2, 4, 45, 6, 35, 40]. The second type, which is also re-\nferred to as multi-scale spatial attention, aggregates feature\ncontexts by convolutional kernels of different sizes [20] or\nfrom a pyramid [20, 43] inside the attention module .\nThe proposed MS-CAM follows the idea of ParseNet\n[25] with combining local and global features in CNNs and\nthe idea of spatial attention wit",
  "-18\nDIFFQ (OURS)\n69.7\n4.1\nRESNET-50\nUNCOMPRESSED\n77.1\n97.5\nRESNET-50\nLSQ 8 BITS (ESSER ET AL. 2020)\n76.8\n24.5\nRESNET-50\nLSQ 4 BITS (ESSER ET AL. 2020)\n76.2\n12.3\nRESNET-50\nLSQ 3 BITS (ESSER ET AL. 2020)\n75.6\n9.3\nRESNET-50\nLSQ* 8 BITS (ESSER ET AL. 2020)\n76.8\n24.5\nRESNET-50\nLSQ* 4 BITS (ESSER ET AL. 2020)\n76.7\n12.3\nRESNET-50\nLSQ* 3 BITS (ESSER ET AL. 2020)\n75.8\n9.3\nRESNET-50\nDIFFQ (OURS)\n76.9\n14\nRESNET-50\nDIFFQ (OURS)\n76.6\n10.5\nRESNET-50\nDIFFQ (OURS)\n76.3\n8.8\nmust be smaller than 1 + 1/100 times t",
  "rXiv, 2017. 3\n[27] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection.\nIn CVPR, 2018. 3\n[28] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018. 3\n[29] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\nCriss-cross attention for semantic segmentation. In ICCV, 2019. 3\n[30] Lukasz Kaiser, Aidan N Gomez, and Francois Chollet. Depthwise separable convolutions for neural\nmachine transl",
  "ff\nthe expressivity and ef\ufb01ciency of convolutional layers. We\nintroduce MUXConv, a layer that leverages the ef\ufb01ciency of\ndepth-wise or group-wise convolutional layers along with a\nmechanism to enhance the \ufb02ow of information in the net-\nwork. MUXConv achieves this through two components,\nspatial multiplexing and channel multiplexing. Spatial mul-\ntiplexing extracts feature information at multiple scales via\nspatial shuf\ufb02ing, processes such information through depth-\nwise or group-wise convolution",
  "NetV2: Smaller Models and Faster Training\nTable 8. Transfer Learning Performance Comparison \u2013 All models are pretrained on ImageNet ILSVRC2012 and \ufb01netuned on\ndownstream datasets. Transfer learning accuracy is averaged over \ufb01ve runs.\nModel\nParams\nImageNet Acc.\nCIFAR-10\nCIFAR-100\nFlowers\nCars\nConvNets\nGPipe (Huang et al., 2019)\n556M\n84.4\n99.0\n91.3\n98.8\n94.7\nEf\ufb01cientNet-B7 (Tan & Le, 2019a)\n66M\n84.7\n98.9\n91.7\n98.8\n94.7\nVision\nTransformers\nViT-B/32 (Dosovitskiy et al., 2021)\n88M\n73.4\n97.8\n86.3\n85.4",
  " apply a center crop on\nthe validation set to benchmark, where a 224\u00d7 224 patch is\ncropped to evaluate the classi\ufb01cation accuracy.\nResults. In Table 2, we see that PVTv2 is the state-of-\nthe-art method on ImageNet-1K classi\ufb01cation. Compared\nto PVT, PVTv2 has similar \ufb02ops and parameters, but the\nimage classi\ufb01cation accuracy is greatly improved. For ex-\nample, PVTv2-B1 is 3.6% higher than PVTv1-Tiny, and\nPVTv2-B4 is 1.9% higher than PVT-Large.\nCompared to other recent counterparts, PVTv2 series\nal",
  "in\nthe supplemental document, we introduce the left settings\nthat not mentioned before.\nFor the experiments on the CIFAR-100 dataset, the\nweight decay is 1e-4, and we decay the learning rate by a\nfactor of 0.1 at epoch 300 and 350.\nFor the experiments on the ImageNet, we use the label\nsmoothing trick and a cosine annealing schedule for the\nlearning rate without weight decay.\nFor the semantic segmentation experiment, the StopSign\ndataset is a subset of the COCO dataset [24], which has\na large sca",
  "b)\nmethods which only model the diagonal of the covariance\nmatrix and (c) methods from the noisy labels literature.\nWe evaluate the effect of our probabilistic label noise\nmodel on the representations learned by the network. We\nshow that our method, when pre-trained on JFT, learns image\n1\narXiv:2105.10305v1  [cs.LG]  19 May 2021\n\n--------------------------------------------------\nrepresentations which transfer better to the 19 datasets from\nthe Visual Task Adaptation Benchmark (VTAB) [47].\nContr",
  "that models of this size over\ufb01t without\nweight sharing, so we use a model that shares weights for\nall but the \ufb01rst cross-attend and latent Transformer mod-\nules. The resulting model has \u223c45 million parameters,\nmaking it comparable in size to convolutional models used\non ImageNet.\nStandard ImageNet. As shown in Table 1, the Perceiver\nmodel we trained on ImageNet obtains results that are com-\npetitive with models speci\ufb01cally designed for processing\nimages. We include ResNet-50 results from (Cubuk ",
  " 4, 68, 15] are feasible for input resolutions\n(224 \u00d7 224 (for classi\ufb01cation) and 640 \u00d7 640 (for detec-\ntion experiments in SASA [49])) considered in these papers.\nOur goal is to use attention in more realistic settings of high\nperformance instance segmentation models, where typically\nimages of larger resolution (1024 \u00d7 1024) are used. Con-\nsidering that self-attention when performed globally across\nn entities requires O(n2d) memory and computation [61],\nwe believe that the simplest setting that",
  "nstructured sparsi\ufb01cation can lead to better performance for vision\ntransformers compared to structural downsampling). The basic idea of our method is illustrated in\nFigure 1.\nIn this work, we propose to employ a lightweight prediction module to determine which tokens to be\npruned in a dynamic way, dubbed as DynamicViT. In particular, for each input instance, the prediction\nmodule produces a customized binary decision mask to decide which tokens are uninformative and\nneed to be abandoned. This m",
  "insert blocks\nto the 8 \u00d7 8, 16 \u00d7 16 and 32 \u00d7 32 patch embedding stages,\nwhich correspond to 28 \u00d7 28, 14 \u00d7 14 and 7 \u00d7 7 feature\nresolutions respectively for 224 \u00d7 224 inputs. Additionally,\nwe halve the head dimension and feature dimension before\nself-attention in 28 \u00d7 28 stage to ensure that the blocks in\ndifferent stages utilize similar FLOPs.\nThis transition leads to interesting results. The base per-\nformance is further improved.\nIt is conjectured that the\nstage-wise design leverages the image",
  " mlps. arXiv preprint\narXiv:2105.08050, 2021. 2, 3, 7, 8\n[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021. 1, 3, 6, 7, 8, 16, 17\n[30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017. 16\n[31] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vlad",
  "h SS. The NATS-Bench size search space\nSS [17] is a channel con\ufb01guration search space built\n12\n\n--------------------------------------------------\nupon a \ufb01xed cell-based architecture with 5 layers, where\nthe 2-nd and 4-th layers have a down-sample rate of\n2. Number of channels in each layer is chosen from\n{8, 16, 24, 32, 40, 48, 56, 64}. SS has 85 = 32768\narchitecture candidates in total. Candidates of different\nchannel numbers in our supernet share the weights in\na slimmable manner [78, 77, 76,",
  "f different models for image\nclassi\ufb01cation tasks on ImageNet, centroid @ means re-\nplacing the self-attention block with the centroid attention\nblock at speci\ufb01c layer.\nFigure 7: Comparing DeiT and centroid transformer for\nimage inputs. Upper panel: DeiT partitions the input im-\nage into non-lapping patches and keeps the same number\nof patches throughout the layers, which may lose infor-\nmation of the image because the patches do not overlap.\nIn comparison, thanks to the ability of down-sampling,",
  "follow most of the training techniques used in DeiT [25].\nWe use the pre-trained vision transformer models to initialize the backbone models and jointly train\nthe backbone model as well as the prediction modules for 30 epochs. We set the learning rate of the\nprediction module to batch size\n1024\n\u00d7 0.001 and use 0.01\u00d7 learning rate for the backbone model. The batch\nsize is adjusted adaptively for different models according to the GPU memory. We \ufb01x the weights of\nthe backbone models in the \ufb01rst 5 e",
  "-------\n",
  "oned techniques such that the reduc-\ntion in throughput is minimized. Therefore, among network\ntweaks, only SE was applied to MobileNet-V1 and boosted\nthe accuracy by 1.69 % (M0, M1). The top-1 accuracy gain\nof using SE-MobileNet-V1 together with LS+Mixup+KD\nwas 2.05% more than that of vanilla MobileNet-V1 with\nthe same regularizations applied (M5, M6). In other words,\nthe synergistic effect of using network tweaks and regu-\nlarizations is also demonstrated in mobile-oriented mod-\nels. Based on ",
  "1\nN4 = 8\nE4 = 4\nL4 = 3\nTable 1: Detailed settings of PVTv2 series. \u201c-Li\u201d denotes PVTv2 with linear SRA.\n\u2022 Si: the stride of the overlapping patch embedding in\nStage i;\n\u2022 Ci: the channel number of the output of Stage i;\n\u2022 Li: the number of encoder layers in Stage i;\n\u2022 Ri: the reduction ratio of the SRA in Stage i;\n\u2022 Pi: the adaptive average pooling size of the linear SRA\nin Stage i;\n\u2022 Ni: the head number of the Ef\ufb01cient Self-Attention in\nStage i;\n\u2022 Ei: the expansion ratio of the feed-forward laye",
  "Knowledge distillation meets self-supervision. In: The\nEuropean Conference on Computer Vision (ECCV) (2020)\n50. Yim, J., Joo, D., Bae, J., Kim, J.: A gift from knowledge distillation: Fast optimization,\nnetwork minimization and transfer learning. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. pp. 4133\u20134141 (2017)\n51. Yuan, L., Tay, F.E., Li, G., Wang, T., Feng, J.: Revisiting knowledge distillation via label\nsmoothing regularization. In: Proceedings of the IEE",
  "50\n66.4\n1894.5\n78.2\nRedNet-50\n56.4\n1825.6\n80.6 (+2.4)\nPanoptic-DeepLab [7]\nAxial-DeepLab-S\n12.1\n220.8\n80.5\nAxial-DeepLab-M\n25.9\n419.6\n80.3\nAxial-DeepLab-XL\n173.0\n2446.8\n80.6\nTable 5: Performance comparison on Cityscapes segmentation\nbased on UPerNet. The ef\ufb01ciency of UPerNet is greatly boosted\nby the RedNet backbone, showing competitive performance to\nAxial-DeepLab-XL with only 32.6% parameter counts and 75.7%\ncomputational cost.\nder the single-scale mode and adopt the Intersection-over-\nUnion (",
  "r an implementation and Table 8 for\nthe complexities. Experiments (see Appendix D.1) demonstrate that this variant results in accuracy\nimprovements but we \ufb01nd that using |u|=1 (i.e. the default case) is optimal when controlling for\nspeed on modern machine learning accelerators.\n20\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\ndef compute position lambdas(embeddings, values, impl=\u2019einsum\u2019):\n\"\"\"Compute position lambdas with intra\u2212depth u.\"\"\"\nif im",
  "creases as training accuracy increases. The big difference in\ngeneralization performance causes the performance differ-\nence between PiT and ViT as shown in Figure 3 (c). The\nphenomenon that ViT does not increase performance even\nwhen FLOPs increase in ImageNet is reported in ViT pa-\nper [9]. In the training data of ImageNet scale, ViT shows\npoor generalization performance, and PiT alleviates this.\nSo, we believe that the spatial reduction layer is also nec-\nessary for the generalization of ViT.",
  "ion, Mixer achieves an overall strong\nperformance (84.15% top-1 on ImageNet), although slightly inferior to other models2. Regularization\nin this scenario is necessary and Mixer over\ufb01ts without it, which is consistent with similar observations\nfor ViT [14]. The same conclusion holds when training Mixer from random initialization on ImageNet\n(see Section 3.2): Mixer-B/16 attains a reasonable score of 76.4% at resolution 224, but tends to\nover\ufb01t. This score is similar to a vanilla ResNet50, but be",
  "0\nImageNet Top-1 Accuracy (%)\nprogressive resize\nprogressive resize + adaptive reg\nFigure 6. Training curve comparison \u2013 Our adaptive regulariza-\ntion converges faster and achieves better \ufb01nal accuracy.\n7. Conclusion\nThis paper presents Ef\ufb01cientNetV2, a new family of smaller\nand faster neural networks for image recognition. Optimized\nwith training-aware NAS and model scaling, our Ef\ufb01cient-\nNetV2 signi\ufb01cantly outperforms previous models, while\nbeing much faster and more ef\ufb01cient in parameters. To",
  "ble 2. The position encoding used here is initialized\nrandomly and trained end-to-end along with the network\n(using the same initialization type used for the latent array\u2019s\nposition encoding, see Appendix Sec. C). Because the posi-\ntion encodings used here are unaware of the structure of the\ninput, it makes no difference whether inputs are permuted\nbefore or after the position encoding is constructed. We\nfound that the network with 8 cross-attends had stability is-\nsues when learned position enc",
  "s that our gating is computed based on a projection over the spatial (cross-token)\ndimension rather than the channel (hidden) dimension. SGU is also related to Squeeze-and-Excite\n(SE) blocks [30] in terms of element-wise multiplication. However, different from SE blocks, SGU\ndoes not contain cross-channel projections at all, nor does it enforce permutation invariance (a\nkey feature for content-based attentive modules) due to its static parameterization for the spatial\ntransformation. The spatial",
  "e image recognition. Based on this observation, we propose a\ndynamic token sparsi\ufb01cation framework to prune redundant tokens progressively\nand dynamically based on the input. Speci\ufb01cally, we devise a lightweight prediction\nmodule to estimate the importance score of each token given the current features.\nThe module is added to different layers to prune redundant tokens hierarchically. To\noptimize the prediction module in an end-to-end manner, we propose an attention\nmasking strategy to differenti",
  "and weight decay (wd) for each model.\nThese results use a batch size of 128 and are trained on 1 GPU.\nTop: Distribution of model error versus lr, wd, and also lr\u00b7wd\n(at 10 epochs and 400MF). Applying an empirical bootstrap, we\nsee that clear trends emerge, especially for lr and lr\u00b7wd. Middle:\nWe repeat this experiment but across various \ufb02op regimes (trained\nfor 10 epochs each); the trends are stable. Bottom: Similarly, we\nrepeat the above while training for various number of epochs (in\nthe 400MF",
  "rchitecture [34] signi\ufb01cantly increased the\nperformance of the NLP task with the self-attention mecha-\nnism. Funnel Transformer [7] improves the transformer ar-\nchitecture by reducing tokens by a pooling layer and skip-\nconnection. However, because of the basic difference be-\ntween the architecture of NLP and computer vision, the\nmethod of applying to pool is different from our method.\nSome studies are conducted to utilize the transformer archi-\ntecture to the backbone network for computer visio",
  " p\u03c8(xi|zi)] with an uniform prior as described in Equation (2). In the second stage,\nwe learn the prior p\u03b8 while keeping q\u03c6 and p\u03c8 \ufb01xed. We simplify q\u03c6(z|xi) to a one-point distribution\nwith the most likely visual tokens \u02c6zi = arg maxz q\u03c6(z|xi). Then Equation (2) can be rewritten as:\nX\n(xi,\u02dcxi)\u2208D\n\u0000Ezi\u223cq\u03c6(z|xi)[log p\u03c8(xi|zi)]\n|\n{z\n}\nStage 1: Visual Token Reconstruction\n+\nlog p\u03b8(\u02c6zi|\u02dcxi)\n|\n{z\n}\nStage 2: Masked Image Modeling\n\u0001\n(3)\nwhere the second term is our BEIT pre-training objective.\n2.5\nPre-T",
  "G. Hinton, \u201cA simple\nframework for contrastive learning of visual representations,\u201d\npreprint arXiv:2002.05709, 2020. 1, 2, 5, 6, 7, 13\n[3] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski,\nand A. Joulin,\n\u201cUnsupervised learning of visual fea-\ntures by contrasting cluster assignments,\u201d arXiv preprint\narXiv:2006.09882, 2020. 1, 2, 5, 6, 7, 12\n[4] J.-B. Grill, F. Strub, F. Altch\u00b4e, C. Tallec, P. H. Richemond,\nE. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G.\nAzar, et al., \u201cBootstrap yo",
  " ViTs are\nmore adversarially robust than CNNs [30] and the transferability of adversarial examples between\nCNNs and ViTs is remarkably low[31]. Follow up works [32, 33] extend the robustness study on\nViTs to much common image corruption and distribution shift, and indicate ViTs are more robust\nlearners. Although some \ufb01nds are consistent with above works, in this paper, we do not make simple\ncomparison of robustness between ViTs and CNNs, but analyze the detailed robust component in\nViT and its v",
  "eedy path \ufb01ltering w.t/w.o candaidate pool.\nInput: supernet N with parameter \u2126,validation data Dval,\nnumber of sampled multiple paths m, number of kept\npaths k, candidate pool P with sampling probability \u03f5.\n1: if without candidate pool P then\n2:\nsample m paths {ai}m\ni=1 i.i.d. w.r.t. ai \u223cU(A)\n3: else\n4:\nsample m paths {ai}m\ni=1 i.i.d. w.r.t. ai \u223c(1 \u2212\u03f5) \u00b7\nU(A) + \u03f5 \u00b7 U(P)\n5: end if\n6: randomly sample a batch \u02c6Dval in Dval\n7: evaluate the loss \u2113i of each path ai on \u02c6Dval\n8: rank the paths by \u2113i, an",
  "e processing [5, 58], image classi\ufb01cation [54, 67, 65, 14, 21,\n59, 12, 61, 12, 66, 31, 55, 19, 24], object detection [3, 74] and semantic segmentation [60, 71, 49].\nRather than concentrating on one special task, some recent works [59, 70, 39] try to design a general\nvision Transformer backbone for general-purpose vision tasks. They all follow the hierarchical\n2\n\n--------------------------------------------------\n\u210e1\nConcat\n(a) Cross-Shaped Window Self-Attention\nSplit\nCross-Shaped Window\n\ud835\udc60\ud835\udc64\n\ud835\udc60\ud835\udc64\n\ud835\udc60\ud835\udc64\n",
  " denoted as {Vj|j = 1, ..., m}, which are the weights for LossCE over\ndifferent samples. To obtain these values, VCM \ufb01rstly calculates the suitability matrix Sm\u00d7n via\nEquation. 5. Then variance for elements of each row vector Sj,: can re\ufb02ect the diversity for the\nexperts. A small diversity means xj can be assigned to an arbitrary expert with little to no drop of\naccuracy, thus Vj can be smaller. Therefore, VCM determines the value of Vj based on the normalized\nstandard deviation of Sj,::\nVj =\nSt",
  "ation. We explicitly analyze the differential of performance\nbetween the ImageNet-val and the distinct ImageNet-V2 test set. The relative offsets between curves\nre\ufb02ect to which extent models are over\ufb01tted to ImageNet-val w.r.t. hyper-parameter selection. The\ndegree of over\ufb01tting of our MLP-based model is overall neutral or slightly higher to that of other\ntransformer-based architectures or convnets with same training procedure.\nNormalization & activation. Our network con\ufb01guration does not contai",
  "egate context information due to its limited\nreceptive \ufb01eld.\nThe motivation of Cycle FC is to enjoy channel FC\u2019s\nmerit of taking input with arbitrary resolution and linear\ncomputational complexity while enlarging its receptive \ufb01eld\nfor context aggregation. To achieve this goal, our Cycle\nFC samples points in a cyclical style along the channel di-\nmension (Figure 1c). In this way, Cycle FC has the same\ncomplexity (both the number of parameters and FLOPs) as\nchannel FC, while increasing the recept",
  " object detection and segmentation, multi-scale Vision\n\n--------------------------------------------------\nLongformer pretrained on ImageNet can be transferred to\nsuch high-resolution tasks, as we will show in Section 4.3.\nHowever, Linformer is not transferable because the\nweights of the linear projection layer is speci\ufb01c to a reso-\nlution. The Partial X-formers and Multi-scale ViT with full\nattention are not transferable due to its prohibitively large\nmemory usage after transferred to high-reso",
  "s hold\nagainst all the other solutions aj (with j \u0338= i) in the entire search\nspace of a.\nWith the above de\ufb01nition, we can sort solutions to different\nranks of domination, where solutions in the same rank are non-\ndominated to each other, and there exists at least one solution in\nlower rank that dominates any solution in the higher rank. Thus, a\nlower non-dominated ranked set is lexicographically better than a\nhigher ranked set. This process is referred as non dominated sort,\nand it is the \ufb01rst s",
  " tend to bias toward either shape or\ntexture, depending on the training dataset.\nWe verify that such biased representation learning (towards either shape or texture) weakens CNNs\u2019\nperformance.1 Nonetheless, surprisingly, we also \ufb01nd (1) the model with shape-biased representa-\ntions and the model with texture-biased representations are highly complementary to each other, e.g.,\nthey focus on completely different cues for predictions (an example is provided in Figure 1); and\n(2) being biased toward",
  ".7B\n85.3\nMulti-Stage TFM Only\nHaloNet-H4\n3842\n85M\n-\n85.6\nHaloNet-H4\n5122\n85M\n-\n85.8\nSwin-B\n3842\n88M\n47.0B\n86.0\nSwin-L\n3842\n197M\n103.9B\n86.4\nMulti-Stage Conv+TFM\nHaloNet-Conv-H4\n3842\n87M\n-\n85.5\nHaloNet-Conv-H4\n5122\n87M\n-\n85.8\nCvT-13\n3842\n20M\n16B\n83.3\nCvT-21\n3842\n32M\n25B\n84.9\nCvT-W24\n3842\n277M\n193.2B\n87.7\nProposed\nMulti-Stage Conv+TFM\nCoAtNet-2\n3842\n75M\n49.8B\n87.1\nCoAtNet-3\n3842\n168M\n107.4B\n87.6\nCoAtNet-4\n3842\n275M\n189.5B\n87.9\n+ PT-RA\n3842\n275M\n189.5B\n88.3\n+ PT-RA-E150\n3842\n275M\n189.5B\n88.4\nCoAtNe",
  " neural\nnetwork architectures that trade-off multiple objectives for a\ngiven image classi\ufb01cation task. We introduced Neural Architec-\nture Transfer (NAT), a practical and effective approach for this\npurpose. We described our efforts to harness the concept of a\nTABLE 9: Effect of different training setups. Details of the standard and\nadvanced settings under Random Initialization are provided in Table 7.\nTraining\nSettings\nRandom Initialization\nInherited from Supernet\nstandard\nadvanced\nw/o \ufb01ne-tune",
  "J.: Revisiting knowledge distillation via label\nsmoothing regularization. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. pp. 3903\u20133911 (2020)\n52. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving the perfor-\nmance of convolutional neural networks via attention transfer. In: Fifth International Confer-\nence on Learning Representations (2017)\n53. Zhang, Y., Lan, Z., Dai, Y., Zeng, F., Bai, Y., Chang, J., Wei, Y.: Prime-aware adaptiv",
  " the gap between training, when the mask is used, and\ninference, when the mask is not used, we scale the values of\nthe non-drop neurons properly during training. Speci\ufb01cally,\nto apply a dropout pattern to a layer h of a neural network,\nwe randomly generate a binary mask m of the same shape\nwith h. We then scale the values in the mask m, and replace\nh with:\nDrop(h, m)\n\u25b3= h \u2297\n\u0012 Size(m)\nSum(m) \u00b7 m\n\u0013\n(1)\nDimensional notations.\nIn modern deep learning frame-\nworks (Abadi et al. 2016; Paszke et al. 20",
  "19.\nDeVries, T. and Taylor, G. W.\nImproved regularization\nof convolutional neural networks with cutout.\narXiv\npreprint arXiv:1708.04552, 2017.\nGhiasi, G., Lin, T.-Y., and Le, Q. V. Dropblock: A regular-\nization method for convolutional networks. In NeurIPS,\npp. 10727\u201310737, 2018.\nGong, C., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.-\nY. Frage: Frequency-agnostic word representation. In\nNeurIPS, pp. 1334\u20131345, 2018.\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining\nand harnessing adve",
  "n as x. See Figure 5 for a zoomed-in version on the largest models (versus inference FLOPs).\n23\n\n--------------------------------------------------\nTable 10: Time and FLOPs unmatched inference results for JFT prec@1 and ImageNet 5shot.\nModel\nExperts\nRouting\nJFT prec@1\nINet/5shot\nTime[%]\nFLOPs[%]\nVIT-H/14\n-\n-\n56.68\n76.95\n100.00\n100.00\nVIT-L/16\n-\n-\n53.40\n74.36\n27.58\n36.83\nV-MoE-L/16\nLast-2\nVanilla\n56.76\n76.53\n32.56\n39.02\nV-MoE-L/16\nEvery-2\nVanilla\n57.64\n77.10\n57.40\n49.95\nV-MoE-H/14\nLast-5\nVanilla\n",
  " are 0.9\nbetter than K-means.\nVisualization We visualize our second centroid attention\nblocks\u2019 feature cluster in Figure 4. We plot the sampled\npoint in white star and its K-Nearest-Neighbour (KNNs)\nin red point. We set K = 40 and use L2 distance in\nKNNs. From Figure 4 we can see that rather than gather-\ning around the sampled point in the 3D space, the KNNs\nin our features space tend to distribute within the same se-\nmantic part of the query part, which indicates our feature\ncaptures high-level",
  "es & Taylor, 2017).\nRecently, a line of research called mixup has been proposed.\nThese methods mainly focus on creating previously unseen\nvirtual mixup examples via convex combination or local\nreplacement of data for training (Zhang et al., 2018; Verma\net al., 2019; Yun et al., 2019; Guo et al., 2019).\nHowever, the underlying data domains contain rich re-\ngional saliency information (i.e. foreground objects in vi-\nsion, prominent syllables in speech, informative textual units\nin language) (Simon",
  "der of this section, we\nprovide an overview of NSGA-III procedure and refer readers to\nthe original publication for more details.\nDomination is a widely-used partial ordering concept for\ncomparing two objective vectors. For a generic many-objective\noptimization problem: mina {f1(a), . . . , fm(a)}, where fi(\u00b7)\nare the objectives (say, loss functions) to be optimized and a is\nthe representation of a neural network architecture. For two given\nsolutions a1 and a2, solution a1 is said to dominate a2",
  " a large number\nof classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing, pp.\n722\u2013729, 2008. doi: 10.1109/ICVGIP.2008.47.\n12\n\n--------------------------------------------------\nPreprint\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar.\nCats and dogs.\nIn\n2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3498\u20133505, 2012. doi:\n10.1109/CVPR.2012.6248092.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improvin",
  "[46], 300 millions images). The paper concluded that\ntransformers \u201cdo not generalize well when trained on insuf\ufb01cient amounts of data\u201d,\nand the training of these models involved extensive computing resources.\nIn this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of \ufb01ne-tuning)\nthat is competitive with convnets having a similar number of parameters and\nef\ufb01ciency. It uses Imagenet as the sole training set. We bui",
  "e 2 do not conduct experiments on dense prediction\ntasks due to the incapability of dealing with variable input\nscales, which is discussed in Sec 1. However, CycleMLP\nsolved this issue by adopting Cycle FC. The experimental\nresults on dense prediction tasks are presented in Sec 4.3\nand Sec 4.4.\nComparison with SOTA Models.\nTable 3 further\ncompares CycleMLP with previous state-of-the-art CNN,\nTransformer and Hybrid architectures.\nIt is interesting\nto see that CycleMLP models achieve comparable pe",
  ", and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88(2):303\u2013338, 2010. 5, 9\n[9] Jonathan Frankle and Michael Carbin. The lottery ticket hy-\npothesis: Finding sparse, trainable neural networks. arXiv\npreprint arXiv:1803.03635, 2018. 10\n[10] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock:\nA regularization method for convolutional networks.\nIn\nAdvances in Neural Information Processing Systems, pages\n10727\u201310737, 2018. 6\n[11] R",
  "Large setting with four experts.\nConsidering many works [9, 25] select branches based on the category, we \ufb01rstly experiment to\nobserve the relationship between selection pattern and rough prediction of the delegator. We count\nthe probabilities for each expert to be selected within each predicted class on the ImageNet validation\nset. For better visualization, we cluster the 1000 probability vectors and then plot them on Fig. 8. It\ncan be seen that samples with the same predicted class are assigne",
  "jective optimization using evolutionary\nalgorithms \u2014 a comparative case study,\u201d in Parallel Problem Solving\nfrom Nature \u2014 PPSN V, A. E. Eiben, T. B\u00a8ack, M. Schoenauer, and H.-P.\nSchwefel, Eds.\nBerlin, Heidelberg: Springer Berlin Heidelberg, 1998,\npp. 292\u2013301.\n[79] F. Hutter, H. H. Hoos, and K. Leyton-Brown, \u201cSequential model-based\noptimization for general algorithm con\ufb01guration,\u201d in International Con-\nference on Learning and Intelligent Optimization.\nSpringer, 2011, pp.\n507\u2013523.\n[80] P. Goyal, P",
  ", indicating that incorporating these S&E layers\nhelps but does not fully recover the expressivity of dense\n1 \u00d7 1 convolutions.\nFinally, we did not experiment with any attention variants\n(Bello, 2021; Srinivas et al., 2021), and we expect that our\nresults could likely be improved by adopting these strategies\ninto our models.\n\n--------------------------------------------------\n",
  "context and\ng(X) =\n1\nH\u00d7W\nPH\ni=1\nPW\nj=1 X[:,i,j] is the global average\npooling (GAP). \u03b4 denotes the Recti\ufb01ed Linear Unit (ReLU)\n[27], and B denotes the Batch Normalization (BN) [18]. \u03c3\nis the Sigmoid function. This is achieved by a bottleneck\nwith two fully connected (FC) layers, where W1 \u2208R\nC\nr \u00d7C\nis a dimension reduction layer, and W2 \u2208RC\u00d7 C\nr is a di-\nmension increasing layer. r is the channel reduction ratio.\nWe can see that the channel attention squeezes each fea-\nture map of size H \u00d7 W into",
  "[22].\nTo overcome these limitations, Transformer backbones are recently explored for their ability to\ncapture long-distance information [5, 32, 25, 18]. Unlike CNN backbones, the Transformer ones \ufb01rst\n\u2217This work is funded by the Natural Science Foundation of China (No. 62176119).\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2105.13677v5  [cs.CV]  14 Oct 2021\n\n--------------------------------------------------\nFigure 1: Examples of backbone blo",
  "128, G=8\n 5x5, 128, G=4\n 3x3, 128, G=1\n1x1, 512\nBN\nReLU\n1x1, 512\nBN\nReLU\nBN\nReLU\nUpsample\n(a) Local PyConv\n(b) Global PyConv\nFigure 6: PyConv blocks.\n5\nPyConv Network on Semantic Segmentation\nOur proposed framework for scene parsing (image segmentation) is illustrated in Fig. 5. To build an\neffective pipeline for scene parsing, it is necessary to create a head that can parse the feature maps\nprovided by the backbone and obtain not only local but also global information. The head should be\nable t",
  " The gain of Meta Pseudo Labels over UDA,\nalbeit smaller than the gain of UDA over RandAugment, is signi\ufb01cant as UDA is already very strong.\nmaking labeled data the same with unlabeled data in Figure 1. In this case, Meta Pseudo Labels can be seen as an adaptive\nform of Label Smoothing: the teacher generates soft labels on labeled data for the student, just like the way Label Smoothing\nsmooths the hard labels to regularize the model. The main difference is that the policy in Label Smoothing is \ufb01",
  "y outperforms existing transformer-based visual models, being comparable to CNN models.\u2020: reported\nby [50], \u22c6: reported by [63].\nModels\nTop-1 Acc.\nTop-5 Acc.\n#Parameters\nFLOPs\nResolution\nModel Type\nDesign Type\nMobileNetV3Large1.0 [21]\n75.2%\n-\n5.4M\n0.22G\n2242\nCNN\nAuto\nEf\ufb01cietNet-B0[49]\n77.1%\n93.3%\n5.4M\n0.39G\n2242\nCNN\nAuto\nDeiT-tiny [50]\n72.2%\n91.1%\n5.7M\n1.2G\n2242\nTransformer\nManual\nAutoFormer-tiny (Ours)\n74.7%\n92.6%\n5.7M\n1.3G\n2242\nTransformer\nAuto\nResNet50\u22c6[18]\n79.1%\n-\n25.5M\n4.1G\n2242\nCNN\nManual\n",
  "at the convolution-induced translation equivariance and locality help ResNets es-\ncape from bad local minima when trained on visual data. However, we need improved learning\nalgorithms to prevent them from happening to the convolution-free ViTs and and MLP-Mixers. The\n\ufb01rst-order optimizers (e.g., SGD and Adam (Kingma & Ba, 2015)) only seek the model parameters\nthat minimize the training error. They dismiss the higher-order information such as \ufb02atness that cor-\nrelates with the generalization (Kes",
  "R101+NL\nX3D-XL\nX3D-M\nMViT-B 16x4\nMViT-B 32x2\nX3D-S\nViViT-L ImageNet-21K\nTimeSformer ImageNet-21K\nVTN ImageNet-1K / 21K\nFigure A.4. Accuracy/complexity trade-off on K400-val for varying # of inference clips per video. The top-1 accuracy (vertical axis) is\nobtained by K-Center clip testing where the number of temporal clips K \u2208{1, 3, 5, 7, 10} is shown in each curve. The horizontal axis\nmeasures the full inference cost per video. The left-sided plots show a linear and the right plots a logarithmic",
  "nter-biased receptive \ufb01eld [35], leading the model to\nignore the features at the image boundary. In contrast, self-attention based feature aggregation can\neffectively model the global-range relation among features but suffer the over-smoothing issue. DLA\ncan effectively overcome their limitations and better model the local and global context jointly.\n5\n\n--------------------------------------------------\n3.4\nRe\ufb01ner and Re\ufb01ned-ViT\nThe attention expansion and distributed local attention can be easi",
  ". This explicit separation avoids the contradictory objective of guiding\nthe attention process while processing the class embedding. We refer to this\nnew architecture as CaiT (Class-Attention in Image Transformers).\nIn the experimental Section 4, we empirically show the effectiveness and\ncomplementary of our approaches:\n\u2022 LayerScale signi\ufb01cantly facilitates the convergence and improves the ac-\ncuracy of image transformers at larger depths. It adds a few thousands\nof parameters to the network at ",
  "ansformer to image clas-\nsi\ufb01cation.\nThey propose ViT [5] that reshapes image\nto patches for feature extraction by transformer encoder,\nwhich achieves comparable results to CNN-based mod-\nels.\nDeiT [35] further employs more data augmentation\nand makes ViT comparable to CNN-based model with-\nout ImageNet-22k or JFT-300M pretraining.\nDeiT also\nproposes an attention-based distillation method, which is\nused for student-teacher training, leading to even better\nperformance. PVT [37] proposes feature py",
  "recognition. In Computer Vision and Pattern Recognition, 2016.\n[29] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In International\nConference on Computer Vision, 2017.\n[30] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv preprint arXiv:1912.12180, 2019.\n[31] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig Adam, Pietro Perona,\nand Serge J. Belongie. The iNaturalist spec",
  "(Net1) is\nshown in Table 2. As can be seen, this transition can sub-\nstantially improve the base performance. Our further ex-\nperiments show that adding global pooling itself can im-\nprove the base performance from 64.17% to 69.44%. In\nother words, the global average pooling operation which is\nwidely used in convolution-based models since NIN [24],\nenables the network to learn more ef\ufb01ciently under moder-\nate augmentation. Furthermore, this transition can slightly\nimprove the elite performance.\n",
  "019. 3, 6\n[6] K. Deb. Multi-objective optimization using evolutionary al-\ngorithms. Chichester: Wiley, 2001. 4\n[7] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Ef-\n\ufb01cient multi-objective neural architecture search via lamar-\nckian evolution. In International Conference on Learning\nRepresentations (ICLR), 2019. 3\n[8] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\nWilliams, J. Winn, and A. Zisserman. The pascal visual ob-\nject classes challenge: A retrospective. International Journa",
  "e of visual rep-\nresentation.\nIn this context, the desiderata of com-\nposing pixel pairs for relation modeling is challenged.\nFurthermore, we unify the view of self-attention and\nconvolution through the lens of our involution.\n3. The involution-powered architectures work universally\nwell across a wide array of vision tasks, including im-\nage classi\ufb01cation, object detection, instance and se-\nmantic segmentation, offering signi\ufb01cantly better per-\nformance than the convolution-based counterparts.\n2",
  "\u22121), and\nvalues in B are taken from \u02c6B.\n4To make the window size (M, M) divisible by the feature map size of\n(h, w), bottom-right padding is employed on the feature map if needed.\nWe observe signi\ufb01cant improvements over counterparts\nwithout this bias term or that use absolute position embed-\nding, as shown in Table 4. Further adding absolute posi-\ntion embedding to the input as in [20] drops performance\nslightly, thus it is not adopted in our implementation.\nThe learnt relative position bias in ",
  "popular way to fur-\nther improve accuracy is by training on additional sources\nof data (either labeled, weakly labeled, or unlabeled). Pre-\ntraining on large-scale datasets (Sun et al., 2017; Maha-\njan et al., 2018; Kolesnikov et al., 2019) has signi\ufb01cantly\npushed the state-of-the-art, with ViT (Dosovitskiy et al.,\n2020) and NFNets (Brock et al., 2021) recently achiev-\ning 88.6% and 89.2% ImageNet accuracy respectively.\nNoisy Student, a semi-supervised learning method, ob-\ntained 88.4% ImageNet ",
  "g Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122, 2021.\n[72] Ross\nWightman.\nPytorch\nimage\nmodels.\nhttps://github.com/rwightman/\npytorch-image-models, 2019.\n[73] Yuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision, 2018.\n[74] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uni\ufb01ed perceptual ",
  "75.1\n44.6\n52.6\n1B\n60K\n47.2\n51.3\n50.6\n55.8\n78.2\n84.5\n53.9\n60.8\n1B\n120K\n51.5\n55.8\n53.6\n59.4\n83.3\n86.8\n59.0\n66.8\n1B\n400K\n58.9\n62.6\n56.7\n62.3\n88.0\n90.6\n65.7\n72.2\n1B\n1.2M\n61.5\n65.2\n59.6\n64.4\n90.7\n92.1\n67.3\n75.0\n1B\n2M\n62.8\n66.6\n60.8\n66.0\n90.7\n92.0\n69.1\n75.5\n1B\n4M\n64.0\n67.4\n61.4\n66.2\n91.2\n92.1\n69.5\n74.9\n3B\n20K\n32.3\n36.5\n38.4\n43.8\n56.2\n59.7\n37.7\n45.4\n3B\n30K\n38.8\n43.1\n43.8\n50.7\n68.8\n75.1\n45.9\n53.9\n3B\n120K\n52.6\n56.3\n53.5\n58.8\n83.8\n87.6\n58.5\n66.0\n3B\n400K\n59.1\n62.7\n56.9\n62.2\n88.7\n90.8\n65.8\n72.1\n3B\n1.2M\n62.1",
  "thinking batch normalization in trans-\nformers. In International Conference on Machine Learn-\ning, pp. 8741\u20138751. PMLR, 2020.\nSimonyan, K. and Zisserman, A. Very deep convolutional\nnetworks for large-scale image recognition. In 3rd Inter-\nnational Conference on Learning Representations, ICLR,\n2015.\nSingh, S. and Shrivastava, A. Evalnorm: Estimating batch\nnormalization statistics for evaluation. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pp. 3633\u20133641, 2019.\nSmith",
  "arger datasets, ImageNet21-K and JFT. Although such regularization can\nharm the pre-training metrics, this allows more versatile regularization and augmentation during\n\ufb01netuning, leading to improved down-stream performances.\n4.2\nMain Results\n0\n5\n10\n15\n20\n25\n30\n35\nFLOPs (Billions)\n80\n81\n82\n83\n84\nImageNet Top-1 Accuracy (%)\nDeiT\nCaiT\nT2T-ViT\nDeepViT\nCvT\nSwinTFM\nCoAtNet-0\nCoAtNet-1\nCoAtNet-2\nCoAtNet-3\nFigure 2: Accuracy-to-FLOPs scaling curve un-\nder ImageNet-1K only setting at 224x224.\n0\n50\n100\n15",
  "nd d\ufb00n=3072. Each gMLP model has L=36\nlayers with dmodel=512 and d\ufb00n = 3072. No positional encodings are used for Mixer or gMLPs.\nModel\nPerplexity\u2217\nParams (M)\nBERTbase\n4.37\n110\nBERTbase + rel pos\n4.26\n110\nBERTbase + rel pos - attn\n5.64\n96\nMLP-Mixer\n5.34\n112\nLinear gMLP, s(Z) = f(Z)\n5.14\n92\nAdditive gMLP, s(Z) = Z + f(Z)\n4.97\n92\nMultiplicative gMLP, s(Z) = Z \u2299f(Z)\n4.53\n92\nMultiplicative, Split gMLP, s(Z) = Z1 \u2299f(Z2), Z = Z1\u2225Z2\n4.35\n102\n* Standard deviation across multiple independent runs is arou",
  "-10 and ResNet-50, we compare\nAutoDropout against DropBlock (Ghiasi, Lin, and Le 2018),\nsince DropBlock has been well-tuned for these models.\nFor Ef\ufb01cientNet, we compare AutoDropout with Stochastic\nDepth (Huang et al. 2016) since it is the default noise-based\nregularization scheme of this architecture. We implement\nthese baselines in our environment for fair comparison. Note\nthat large Ef\ufb01cientNet models, such as B3, B5, B7 in our ex-\nperiments, enlarge the spatial dimensions of the input images",
  "aining cycle and complex\ntricks [31, 20, 38, 2]. Witnessing this, several works [1, 36] propose that convolution should be\nintroduced into the self-attention mechanism to improve its robustness and performance. In short,\ndifferent model assumptions are adopted to make convolution and self-attention complement each\nother in terms of optimization characteristics (i.e., well-condition/ill-condition), attention scope (i.e.,\nlocal/long-term), and content dependence (content-dependent/independent) e.t",
  " are trained for\n350 epochs with a batch size B of 4096 or 2048 distributed across 32 or 64 TPUv3 cores, depending\non memory constraints.\nWe tuned our models using a held-out validation set comprising \u223c2% of the ImageNet training set\n(20 shards out of 1024). We perform early stopping on the held-out validation set for the largest\nmodels, starting with LambdaResNet-350 at resolution 288x288, and simply report the \ufb01nal accura-\ncies for the smaller models.\nSemi-supervised learning with pseudo-label",
  "er-S\nDeiT-S\nResNet-50\nResNet-152\n(a)\n(b)\nFigure 5: Generalization capability. (a) Comparison of ro-\ntation invariance. The compared models are trained under\nthe same data augmentation settings and directly evaluated\non rotated images without model \ufb01ntuning. (b) Comparison\nof scale invariance. The models are trained on images with\nthe resolution of 224\u00d7224, and tested on different image\nresolutions without model \ufb01netuning.\n101) archives 81.8%, which is 1.6% lower than that of\nConformer-S (83.4%),",
  "tic depth [19]. This set of techniques was inspired by the timm library [54]\nand Touvron et al. [48]. More details on these hyperparameters are provided in Supplementary B.\nFine-tuning We \ufb01ne-tune using momentum SGD, batch size 512, gradient clipping at global norm 1,\nand a cosine learning rate schedule with a linear warmup. We do not use weight decay when \ufb01ne-\ntuning. Following common practice [22, 48], we also \ufb01ne-tune at higher resolutions with respect to\nthose used during pre-training. Since",
  "tNet models.\n3.1. Basic Training\nOur main benchmark for evaluating TResNet models is\nthe popular ImageNet dataset [18]. We trained the mod-\nels on input resolution 224, for 300 epochs, using a SGD\noptimizer and 1-cycle policy [34]. For regularization, we\nused AutoAugment [6], Cutout [7], Label-smoothing [35]\nand True-weight-decay [25]. We found that the common\nImageNet statistics normalization [20, 6, 36] does not im-\nprove the training accuracy, and instead normalized all the\nRGB channels to be",
  "8.0 \n77.9\n77.7\n77.5\n2\n5\n10\n25\n50\n(a) The impact of different \ud835\udf06values\n(b) The impact of different \ud835\udc3evalues\nFig. 4. Training E\ufb03cientNet-B0 with di\ufb00erent knowledge distillation parameters. All\nnumbers in the \ufb01gure are top-1 accuracy (%). Left: The testing accuracy of di\ufb00erent \u03bb\nvalues, while K is set as 10. Right: The testing accuracy of di\ufb00erent K values, while\n\u03bb is set to be 0.5.\n1.3M training and 50K validation images. The number of images in each class is\napproximately the same for training data",
  "s\n94.43\n93.02\n94.73\n93.09\n94.82\nJFT-300M\nCIFAR-10\n99.00\n98.61\n99.38\n99.19\n99.50\nCIFAR-100\n91.87\n90.49\n94.04\n92.52\n94.55\nImageNet\n84.15\n80.73\n87.12\n84.37\n88.04\nImageNet ReaL\n88.85\n86.27\n89.99\n88.28\n90.33\nOxford Flowers-102\n99.56\n99.27\n99.56\n99.45\n99.68\nOxford-IIIT-Pets\n95.80\n93.40\n97.11\n95.83\n97.56\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\nare \ufb01ne-tun",
  "wed to model the\nsemantic correlation among local regions, experimental re-\nsults also show its effectiveness.\nWe further optimize WSDAN and DCL by the following\nmethods:\n(1) About tricks for weakly supervised learning, we \ufb01nd\nthat label smooth, warmup + Learning rate with cosine de-\ncay and 144-crop prediction can improved the results. Be-\nsides, larger batch size and image crop size can increase by\nabout 1.5 points;\n(2) With the increase of iterations of inclass data, the\nnumber of trusted pse",
  "ing.\narXiv preprint arXiv:1312.5602, 2013.\n[25] W Pirie. S pearman rank correlation coef\ufb01cient. Encyclope-\ndia of statistical sciences, 2004.\n[26] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V\nLe. Regularized evolution for image classi\ufb01er architecture\nsearch. In Proceedings of the AAAI Conference on Arti\ufb01cial\nIntelligence, volume 33, pages 4780\u20134789, 2019.\n[27] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla",
  "cale vision longformer:\nA new vision transformer for high-resolution image encoding. arXiv preprint arXiv:2103.15358,\n2021. 3, 9\n12\n\n--------------------------------------------------\n[63] X. Zhang, Y. Wei, J. Feng, Y. Yang, and T. S. Huang. Adversarial complementary learning for\nweakly supervised object localization. In CVPR, 2018. 9\n[64] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang. Random erasing data augmentation. In AAAI,\n2020. 5\n[65] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. To",
  "zes\nthe multi-level representations to improve the \ufb01nal repre-\nsentation. In summary, our contributions are as follows:\n\u2022 We design a new visual Transformer architecture\nnamely Convolution-enhanced image Transformer\n(CeiT). It combines the advantages of CNNs in extract-\ning low-level features, strengthening locality, and the\nadvantages of Transformers in establishing long-range\ndependencies.\n\u2022 Experimental results on ImageNet and seven down-\nstream tasks show the effectiveness and generaliza-\nti",
  "that Transformers can handle images. However, patch \ufb02at-\ntening impairs the position information within each patch\nand makes it more dif\ufb01cult to extract the patterns within\npatches. To solve this problem, existing methods usually\nattach a preprocessing module before patch embedding.\nThe preprocessing module can be a feature extraction con-\nvnet [14] or a specially designed Transformer [40].\nWe found that there is a rather simple solution, which\nis factorizing the large patch embedding to step-wi",
  "rance, 6-11 July 2015, volume 37 of JMLR Workshop\nand Conference Proceedings, pages 448\u2013456. JMLR.org, 2015.\n[14] Md. Amirul Islam, Sen Jia, and Neil D. B. Bruce. How much position information do convolutional neural\nnetworks encode? In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net, 2020.\n[15] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In IEEE Conference on\nComputer Vision and Patter",
  "e\n(ms)\n(hr)\n(top-1)\nREGNETY-200MF\n0.2\n3.2\n2.2\n1024\n11\n3.1\n29.6\u00b10.11\nREGNETY-400MF\n0.4\n4.3\n3.9\n1024\n19\n5.1\n25.9\u00b10.16\nREGNETY-600MF\n0.6\n6.1\n4.3\n1024\n19\n5.2\n24.5\u00b10.07\nREGNETY-800MF\n0.8\n6.3\n5.2\n1024\n22\n6.0\n23.7\u00b10.03\nREGNETY-1.6GF\n1.6\n11.2\n8.0\n1024\n39\n10.1\n22.0\u00b10.08\nREGNETY-3.2GF\n3.2\n19.4\n11.3\n512\n67\n16.5\n21.0\u00b10.05\nREGNETY-4.0GF\n4.0\n20.6\n12.3\n512\n68\n16.8\n20.6\u00b10.08\nREGNETY-6.4GF\n6.4\n30.6\n16.4\n512\n104\n26.1\n20.1\u00b10.04\nREGNETY-8.0GF\n8.0\n39.2\n18.0\n512\n113\n28.1\n20.1\u00b10.09\nREGNETY-12GF\n12.1\n51.8\n21.4\n512\n150\n",
  "ts used for\neach group b \u2208Rg\n\u2217+ is obtained from a logit parameter\nl \u2208Rg, so that we have\nb = bmin + \u03c3(l)(bmax \u2212bmin),\n(11)\nwith \u03c3 is the sigmoid function, and bmin and bmax the minimal\nand maximal number of bits to use. The trainable parameter\nl is initialized so that b = binit. We set binit = 8.\nEvaluation and noise distribution. At evaluation time, we\nround the value b obtained from (10) as \u02dcb = round(b) and\nquantize w as Q(w,\u02dcb). Thus, the amount of quantization\nnoise at evaluation can be la",
  "time and memory complexity with respect to the se-\nquence length. There are also some NLP literatures that\ntend to reduce the sequence length during processing. For\nexample, Goyal et al. [13] propose PoWER-BERT, which\nprogressively eliminates word tokens during the forward\npass. Funnel-Transformer [8] presents a pool-query-only\nstrategy, pooling the query vector within each self-attention\nlayer. However, there are few literatures targeting improv-\ning the ef\ufb01ciency of the ViT models.\nTo compromi",
  "e to refactor/build on the frame-\nworks for introducing new methods, models, datasets and designing experiments.\nIn this paper, we present our developed open-source framework built on PyTorch\nand dedicated for knowledge distillation studies. The framework is designed to\nenable users to design experiments by declarative PyYAML con\ufb01guration \ufb01les,\nand helps researchers complete the recently proposed ML Code Completeness\nChecklist. Using the developed framework, we demonstrate its various ef\ufb01cient\nt",
  "iv:2011.00186,\n2020. 3\n[70] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing\nJia, and Kurt Keutzer. Fbnet: Hardware-aware ef\ufb01cient con-\nvnet design via differentiable neural architecture search. In\nCVPR, 2019. 3, 7, 12\n[71] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. In CVPR, 2018. 3\n[72] Shen Yan, Yu Zheng, Wei Ao, Xiao Zeng, and Mi Zhang.\nDoes unsu",
  "T and concur-\nrent ViT [25] video variants: VTN [78], TimeSformer [6],\nViViT [1]. To achieve similar accuracy level as MViT, these\nmodels require signi\ufb01cant more computation and parameters\n(e.g. ViViT-L [1] has 6.8\u00d7 higher FLOPs and 8.5\u00d7 more pa-\nrameters at equal accuracy, more analysis in \u00a7A.1) and need\nlarge-scale external pre-training on ImageNet-21K (which\ncontains around 60\u00d7 more labels than Kinetics-400).\nIN-1K\nIN-21K\nIN-21K\nIN-21K\n+4.6% acc\nat 1/5 FLOPs\nat 1/3 Params \nwithout ImageNet \nM",
  "per, we have presented a Hierarchical Visual\nTransformer, termed HVT, for image classi\ufb01cation. In par-\nticular, the proposed hierarchical pooling can signi\ufb01cantly\ncompress the sequential resolution to save computational\ncost in a simple yet effective form. More importantly, this\nstrategy greatly improves the scalability of visual Trans-\nformers, making it possible to scale various dimensions -\ndepth, width, resolution and patch size. By re-allocating\nthe saved computational cost, we can scale up",
  "ngs: multi-scale training (resizing the input such that the shorter\nside is between 480 and 800 while the longer side is at most 1333), AdamW [19] optimizer (initial\nlearning rate of 1e-4, weight decay of 0.05, and batch size of 16), and 1\u00d7 schedule (12 epochs).\nUnlike CNN backbones, which adopt post normalization and can directly apply to downstream tasks.\nResT employs the pre-normalization strategy to accelerate network convergence, which means the\noutput of each stage is not normalized before",
  "tworks\noutperform more complex architectures, like SE-ResNet [17], despite that it uses an additional\nsqueeze-and-excitation block, which increases model complexity.\nThe above results mainly aim to show the advantages of our PyConv over the standard convolution\nby running all the networks with the same standard training settings for a fair comparison. Note\nthat there are other works which report better results on ImageNet, such as [29]\u2013[31]. However, the\nimprovements are mainly due to the traini",
  "formance of shallow\narXiv:2103.11886v4  [cs.CV]  19 Apr 2021\n\n--------------------------------------------------\nViTs, a natural question arises: can we further improve\nperformance of ViTs by making it deeper, just like CNNs?\nThough it seems to be straightforward at the \ufb01rst glance,\nthe answer may not be trivial since ViT is essentially dif-\nferent from CNNs in its heavy reliance on the self-attention\nmechanism. To settle the question, we investigate in detail\nthe scalability of ViTs along depth",
  "ptimization.\narXiv preprint arXiv:1412.6980,\n2014. 5, 9\n[38] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\narXiv preprint arXiv:1912.11370, 6(2):8, 2019. 6\n[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classi\ufb01cation with deep convolutional neural net-\nworks. In Advances in neural information processing sys-\ntems, pages 1097\u20131105, 2012. 1, 2\n[40] Y",
  "020.\n[65] Daquan Zhou, Xiaojie Jin, Qibin Hou, Kaixin Wang, Jianchao Yang, and Jiashi Feng. Neural epitome search\nfor architecture-agnostic network compression. In International Conference on Learning Representations,\n2019.\n[66] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi Feng. Deepvit:\nTowards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\n[67] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end d",
  "\u2212Mean(TCP:,k)\nStd(TCP:,k)\n.\n(5)\nWe can denote the output of LGM as a binary matrix Lm\u00d7n, thus each row of Lm\u00d7n is a selection\nlabel. Because we aim at maximizing the total suitability, Lm\u00d7n can be determined by solving the\nfollowing problem:\nmin\nX\nj,k\n\u2212Sj,k \u2217Lj,k\ns.t.\nLj,k \u2208{0, 1}\nP\nk\nLj,k = 1\nP\nj\nLj,k = m\nn\n(6)\nThis problem can also be modeled as BTP, and solved via VAM as described in section 3.1.\n3.3\nVariance Calculation Module (VCM)\nThe expert selection output by the delegator involves proba",
  "dAugment (Cubuk et al., 2020): a per-image data\naugmentation, with adjustable magnitude \u03f5.\n\u2022 Mixup (Zhang et al., 2018): a cross-image data aug-\nmentation.\nGiven two images with labels (xi, yi)\nand (xj, yj), it combines them with mixup ratio \u03bb:\n\u02dcxi = \u03bbxj + (1 \u2212\u03bb)xi and \u02dcyi = \u03bbyj + (1 \u2212\u03bb)yi. We\nwould adjust mixup ratio \u03bb during training.\n5. Main Results\nThis section presents our experimental setups, the main\nresults on ImageNet, and the transfer learning results on\nCIFAR-10, CIFAR-100, Cars, and ",
  " sampled class, sharpening the target predic-\ntions is equivalent to using a lower temperature in the cosine\nsimilarity between the unlabeled representation and support\nrepresentations. However, when the sampled support set con-\ntains more than one instance per sampled class, then sharpen-\ning the target predictions is actually different from adjusting\nthe cosine temperature. In this case, it is preferable to sharpen\nthe target predictions rather than use a different temperature\nin the cosine si",
  "essing) has a long history in com-\nputer vision (Borenstein et al., 2004; Kumar et al., 2005;\nCarreira et al., 2016; Hu & Ramanan, 2016; Yang et al.,\n2018; Lin et al., 2020). There is widespread evidence that\nit plays an important role in human vision (e.g. Felleman\n& Essen 1991; Olshausen et al. 1993; Lollo et al. 2000),\nwhich is characterized by limited bandwidth input streams\n(Wolfe et al., 2006). In the Perceiver, attention to the full\nset of inputs can be in\ufb02uenced by a latent array produce",
  "he local channel\ncontext aggregator, which only exploits point-wise channel\ninteractions for each spatial position. To save parameters,\nthe local channel context L(X) \u2208RC\u00d7H\u00d7W is computed\nvia a bottleneck structure as follows:\nL(X) = B (PWConv2 (\u03b4 (B (PWConv1(X)))))\n(2)\nThe kernel sizes of PWConv1 and PWConv2 are C\nr \u00d7C \u00d7\n1 \u00d7 1 and PWConv2 is C \u00d7 C\nr \u00d7 1 \u00d7 1, respectively. It\nis noteworthy that L(X) has the same shape as the input\nfeature, which can preserve and highlight the subtle details\nin th",
  ", those straightforward scaling strategies\nonly work well with larger datasets and stronger augmen-\ntation policies [51, 46, 43] to alleviate the brought training\n\n--------------------------------------------------\nSelf-Attention\nNorm\nFeed Foward\nNorm\nAdd\nAdd\nPos. Embed\nPatch Embed\nInput\nLinear + Loss\n\u00d7N\nRe-Attention \nNorm\nFeed Foward\nNorm\nAdd\nAdd\nPos. Embed\nPatch Embed\nInput\nLinear + Loss\n\u00d7N\n(a)\n(b)\nTransformer block with self-attention\nTransformer block with Re-attention\nFigure 2: Comparison b",
  "0 case for Adam updates. We note that the \u03b22 = 0\ncase where the bound depends on Lavg instead of ||L||1 can be very similarly derived for Adam, but\nis also a very unrealistic condition in practice.\nProof. Under the assumption \u03b21 = 0, \u03bb = 0, one could write the Adam update rule as follows:\nx(i)\nt+1 = x(i)\nt\n\u2212\u03b7t\nq\n1 \u2212\u03b2t\n2\ng(i)\nt\nq\nv(i)\nt\n,\nwhere vt = \u03b22vt\u22121 + (1 \u2212\u03b22)g2\nt for all i \u2208[h].\n12\n\n--------------------------------------------------\nSince the function f is L-smooth, we have the following:\n",
  "nd exper-\niments to reproduce the reported results on ImageNet and COCO datasets.\n\n--------------------------------------------------\ntorchdistill: A Modular, Con\ufb01guration-Driven Framework for Knowledge Distillation\n11\nTable 3: Epoch-level training speed improvement by redesigning teacher and student\n(ResNet-18) models with required modules only for hint-training shown in Figure 3.\nTeacher ResNet-34 ResNet-50 ResNet-101 ResNet-152\nOriginal\n934 sec\n1,175 sec\n1,468 sec\n1,779 sec\nMinimal\n786 sec\n92",
  "low previous\nwork [17] to generate three architectures that has comparable capacity (e.g. number of parameters\nand FLOPS), notated as tiny (NesT-T), small (NesT-S), and base (NesT-B). Most recent ViT based\nmethods follow the training techniques of DeiT [49], which include a mixture of data augmentation\nlike MixUp [58], CutMix [56], RandAugment [14], RandomErasing [64], and regularization like\nStochastic Depth [26]. We follow major settings with minor rationale modi\ufb01cations that is we \ufb01nd\nuseful ",
  "8748. IEEE\nComputer Society, 2018. 2\n[53] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In ICLR, 2021. 1, 2\n\n--------------------------------------------------\n",
  "rrelations in the\nJFT label noise, improves the VTAB1K score by 0.88%\nover the homoscedastic baseline and by 0.22% over the\nheteroscedastic diagonal model. We stress that the down-\nstream models are trained without a heteroscedastic output\nlayer, so our experiment demonstrates that a model trained\nupstream on JFT with a heteroscedastic output layer learns\nrepresentations which transfer better than a homoscedastic\nor a heteroscedastic diagonal model.\n4.3. Deep Ensembles for Full Predictive Uncert",
  " split out and\nbypasses the feed-forward network. The derived new fea-\nture map is converted back to image tokens and concate-\nnated with the bypassed class token. The concatenated se-\nquence is processed by the next transformer layer.\nThe effectiveness of the introduced locality mechanism\nis validated in two ways. Firstly, its properties are investi-\ngated experimentally. We draw four basic conclusions. i.\nDepth-wise convolution alone can already improve the per-\nformance of the baseline transf",
  "reprint arXiv:2010.03019, 2020. 7\n[59] Byeongho Heo, Sangdoo Yun, Dongyoon Han,\nSanghyuk Chun, Junsuk Choe, and Seong Joon Oh,\n\u201cRethinking spatial dimensions of vision transform-\ners,\u201d 2021. 9, 10\n[60] Haiping Wu, Bin Xiao, Noel Codella, Mengchen\nLiu, Xiyang Dai, Lu Yuan, and Lei Zhang, \u201cCvt:\nIntroducing convolutions to vision transformers,\u201d\n2021. 9, 10\n[61] A. Brock,\nSoham De,\nS. L. Smith,\nand K.\nSimonyan, \u201cHigh-performance large-scale image\nrecognition without normalization,\u201d arXiv preprint\nar",
  " the self-attention stage, which dominates the complexity, we increase the\nperformance signi\ufb01cantly by adding two blocks of CA+FFN.\n4.3\nOur CaiT models\nOur CaiT models are built upon ViT: the only difference is that we incorporate\nLayerScale in each residual block (see Section 2) and the two-stages architec-\nture with class-attention layers described in Section 3. Table 3 describes our\ndifferent models. The design parameters governing the capacity are the depth\nand the working dimensionality d. ",
  "\u00d78\u00d78\n\u2713\n70.5\n6.8\n77.2\n2\u00d78\u00d78\n\u2713\n63.7\n6.3\n75.8\nTable 14. Key-Value pooling: Vary stride s = sT \u00d7 sH \u00d7 sW , for\npooling K and V . \u201cadaptive\u201d reduces stride w.r.t. stage resolution.\nKey-Value pooling. The ablation in Table 14 analyzes the\npooling stride s = sT \u00d7 sH \u00d7 sW , for pooling K and V\ntensors. Here, we compare an \u201cadaptive\u201d pooling that uses a\nstride w.r.t. stage resolution, and keeps the K, V resolution\n\ufb01xed across all stages, against a non-adaptive version that\nuses the same stride at every b",
  "the number of buckets. The accuracy increase\n6\n\n--------------------------------------------------\n0\n10 26 50 82 122\n226\n362\n730\nNumber of buckets\n80.0\n80.5\n81.0\nTop-1 Accuracy (%)\n79.9\n80.7\n80.8\n80.9\n80.9\n80.7\n81.0\n80.8\n80.9\nFigure 3: Ablation for the number of buckets in contextual\nproduct model with shared relative position encodings on\nImageNet [4].\n112\n224\n384\n448\n512\n800\nResolution\n10\n0\n10\n20\n30\nExtra MACC Ratio (%)\n1.1\n3.9\n9.9\n12.6\n15.3\n25.9\n1.1\n1.0\n0.9\n0.9\n0.9\n0.7\nInefficient\nEfficient\nF",
  "gregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492\u20131500,\n2017. 4\n[32] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017. 6\n[33] Hengshuang Zhao. Of\ufb01cial pspnet. https://github.\ncom/hszhao/semseg, 2020. 9\n[34] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene ",
  "es signi\ufb01cantly by 3.95%. It reveals that position em-\nbedding is less important in the transition model than that\nin the pure Transformer-based models. It is because that\nthe position prior inter tokens is preserved by the feature\nmaps and convolutions with spatial kernels can encode and\nleverage it. Consequently, the harm of removing position\nembedding is remarkably reduced in the transition network.\nIt also explains why convolution-based models do not need\nposition embedding.\n3.3.7\nReplacing ",
  "mutiplexing in stages two and three. For the kernel size\noptions in case of reduction blocks, we allow multiple parallel kernels to\ndown-sample the resolution, for example, \u201c[3, 5, 7]\u201d means three parallel\nconvolutions with kernel size of 3, 5, and 7.\nThe Mobilenet block consist of a 1 \u00d7 1 convolution to\nexpand the input channels, followed by a 3 \u00d7 3 depth-wise\nseparable convolution and another 1\u00d71 convolution to com-\npress the channels (see Figure 12a). We use E to denote\nexpansion rate. Then t",
  "RPE methods into DeiT. We elab-\norate how to inject previous relative position encoding\nmethods into DeiT [22] in Tab. 5 of Section 4.3.\n\u2022 Training and test settings of DETR. We provide the\ndetails of training and test settings of DETR [1] in Sec-\ntion 4.4.\n\u2022 The effectiveness on other vision transformers. We\nshow the effectiveness of the proposed iRPE on the\nrecent Swin transformer [13].\n\u2022 Transfer learning on \ufb01ne-grained datasets.\nTo\nverify the generalizability, we evaluate our models\non \ufb01ne-g",
  "o the original single-layer patch\nembedding. However, when further increasing the number of convolutional layer in patch embedding\nto 6, we do not observe any performance gain. This indicates that using 4-layer convolutions in patch\nembedding is enough. Meanwhile, if we use a larger stride to reduce the size of the feature map,\nwe can largely reduce the computation cost, but the performance also drops. Thus, we only apply a\n13\n\n--------------------------------------------------\nconvolution of st",
  "nt network by balancing the resolu-\ntion, height, and width of the network. The performance of\nEf\ufb01cientNet for ILSVRC2012 top-1 accuracy was greatly\nimproved relative to AlexNet.\nUnlike these studies which focus on designing new net-\nwork architecture, He et al. [9] proposes different ap-\nproaches to improve model performance. They noted that\nperformance can be improved not only through changes in\nthe model structure, but also through other aspects of net-\nwork training such as data preprocessin",
  "like data augmentations, which are domain-\nspeci\ufb01c, our dropout patterns for the hidden states have the\nsame design philosophy on ConvNets for image recogni-\ntion models and Transformer for text understanding models.\nCutMix (Yun et al. 2019) and ManifoldMixup (Verma et al.\n2We will release the datasets consisting of the dropout patterns\nthat our search algorithm has sampled and run. Like the similar\ndatasets collected from benchmarking various model architectures\n(Ying et al. 2019; Dong and Yang",
  "were used to \ufb01lter the\nimages from the unlabelled set [8]. Therefore the pre-trained\nmodels may bene\ufb01t from more over\ufb01tting on the validation\nset. We quantify this in the experiments presented below.\nSince we use pre-trained Ef\ufb01cientNet for our initializa-\ntion, our results are comparable to those from the Noisy Stu-\ndent [8], which uses the same degree of over\ufb01tting, but not\ndirectly with other semi-supervised approaches like that of\nYalniz et al. [2].\n3.4. Evaluation on ImageNet-V2\nThe ImageNe",
  "ed to play a central\nrole in the deep learning models [22, 29, 31, 15] for visual\nrecognition. This situation starts to change when the Trans-\nformer [35], a module that originates from natural language\nprocessing [35, 13, 25], is transplanted to the vision scenar-\nios. It was shown in the ViT model [14] that an image can\nbe partitioned into a grid of patches and the Transformer is\ndirectly applied upon the grid as if each patch is a visual\nword. ViT requires a large amount of training data (e.g",
  "r attention variant which only has a\nlinear complexity w.r.t. the spatial size [12, 32, 33].\nWe brie\ufb02y experimented with option (C) without getting a reasonably good result. For option (B), we\nfound that implementing local attention involves many non-trivial shape formatting operations that\nrequires intensive memory access. On our accelerator of choice (TPU), such operation turns out to be\nextremely slow [34], which not only defeats the original purpose of speeding up global attention, but\nalso ",
  "nd that self-attentions in lower\nresolutions play more important roles than those in higher\nresolutions. Additionally, replacing the self-attentions in\nthe \ufb01rst stage almost has no effect on the network perfor-\nmance. Larger resolutions contain much more tokens and\n6\n\n--------------------------------------------------\nNetwork\nbase perf.(%)\nelite perf.(%)\nNet5\n77.37\n80.15\nNet5-DS1\n77.29 (-0.08)\n80.13 (-0.02)\nNet5-DS2\n77.34 (-0.02)\n79.75 (-0.40)\nNet5-DS3\n77.05 (-0.32)\n79.59 (-0.56)\nTable 3. Impact",
  "ern Recognition,\npp. 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby.\nAn image is worth 16x16 words: Tra",
  "mostly focus on improving FLOPs\nef\ufb01ciency (Tan & Le, 2019b;a) or inference ef\ufb01ciency (Tan\net al., 2019; Cai et al., 2019; Wu et al., 2019; Li et al., 2021).\n\n--------------------------------------------------\nEf\ufb01cientNetV2: Smaller Models and Faster Training\nUnlike prior works, this paper uses NAS to optimize training\nand parameter ef\ufb01ciency.\n3. Ef\ufb01cientNetV2 Architecture Design\nIn this section, we study the training bottlenecks of Ef\ufb01cient-\nNet (Tan & Le, 2019a), and introduce our training-awar",
  "a labeled data. We use the same\nhyper-parameters with Noisy Student, except that we use the\ntraining image resolution of 512x512 instead of 475x475.\nWe increase the input image resolution to be compatible with\nour model parallelism implementation which we discuss in\nthe next paragraph. In addition to Ef\ufb01cientNet-L2, we also\nexperiment with a smaller model, which has the same depth\nwith Ef\ufb01cientNet-B6 [63] but has the width factor increased\nfrom 2.1 to 5.0. This model, termed Ef\ufb01cientNet-B6-Wide,",
  ".\nHyper-parameter\nImageNet-1K\nImageNet-21K\nJFT\nPre-Training Finetuning\nPre-Training Finetuning\nPre-Training\nFinetuning\n(CoAtNet-0/1/2/3)\n(CoAtNet-2/3/4)\n(CoAtNet-3/4/5)\nStochastic depth rate\n0.2 / 0.3 / 0.5 / 0.7\n0.3 / 0.5 / 0.7\n0.0 / 0.1 / 0.0\n0.1 / 0.3 / 0.2\nCenter crop\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nRandAugment\n2, 15\n2, 15\u22c4\nNone / None / 2, 5\u2020\n2, 5\n2, 5\nMixup alpha\n0.8\n0.8\nNone\nNone\nNone\nNone\nLoss type\nSoftmax\nSoftmax\nSigmoid\nSoftmax\nSigmoid\nSoftmax\nLabel smoothing\n0.1\n0.1\n0.0001\n0.1\n0.0001",
  "er-term) using a search engine [27].\nBoth the crawled data and the original dataset are passed\nthrough a network with two corresponding branches and\ntrained jointly. Note that data for the \u201cauxiliary\u201d task (the\nbranch for super-classes) is disjoint to the one used for the\n\ufb01ne-grained task. Moreover, there is no explicit correspon-\ndence between the classi\ufb01cation of the super-class and the\noriginal class. A different approach starts by assigning the\noriginal labels to visually similar groups and ",
  "1.9\n55.8\n61.2\n66.0\n82.3\n86.2\n53.5\n61.2\n300M\n400K\n60.1\n64.1\n65.4\n70.6\n88.9\n90.7\n61.5\n68.1\n300M\n1.2M\n64.0\n67.7\n66.3\n71.4\n90.9\n91.9\n65.1\n71.2\n300M\n2M\n66.4\n69.3\n67.7\n73.1\n91.6\n92.1\n67.5\n73.7\n300M\n4M\n67.5\n70.1\n68.4\n73.1\n91.7\n92.2\n68.3\n74.1\n1B\n20K\n30.6\n35.2\n43.9\n50.4\n57.1\n61.9\n35.1\n41.9\n1B\n30K\n37.1\n41.9\n49.5\n55.4\n65.3\n71.8\n41.6\n48.5\n1B\n60K\n46.3\n50.2\n56.0\n61.3\n76.2\n80.3\n48.3\n56.7\n1B\n120K\n51.9\n55.9\n60.8\n65.2\n81.5\n85.7\n54.6\n62.3\n1B\n400K\n60.8\n64.5\n65.7\n70.3\n88.6\n90.7\n62.5\n69.1\n1B\n1.2M\n65.1\n68.1\n66.6\n72.3\n",
  " NAACL, 2018.\n[49] H. Pham, Z. Dai, Q. Xie, M.-T. Luong, and Q. V. Le. Meta pseudo labels. arXiv preprint\narXiv:2003.10580, 2020.\n[50] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a uni\ufb01ed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019.\n[51] P. Ramachandran and Q. V. Le. Diversity and depth in per-example routing models. In ICLR,\n2018.\n[52] C. Rosenbaum, I. Cases, M. Riemer, and T. ",
  "exander Ku, and\nDustin Tran, \u201cImage transformer,\u201d in International\nConference on Machine Learning.\nPMLR, 2018,\npp. 4055\u20134064. 2, 3\n[23] Rewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever, \u201cGenerating long sequences with sparse\ntransformers,\u201d arXiv preprint arXiv:1904.10509,\n2019. 2, 3\n[24] Jean-Baptiste Cordonnier, Andreas Loukas, and\nMartin Jaggi, \u201cOn the relationship between self-\nattention and convolutional layers,\u201d arXiv preprint\narXiv:1911.03584, 2020. 2\n[25] Patryk Chrabaszcz, Ilya ",
  " and general patch-wise aug-\nmentation method for patch sequences which\nadds rich af\ufb01nity and diversity to training data.\nPatch-wise augmentation also contributes to the\nmodel generalization by reducing the risk of\nover-\ufb01tting. With the above proposed methods,\nwe can build an augmented Robust Vision Transformer\u2217. Contributions of this paper are three-folds:\n\u2022 We give systematic robustness analysis of ViTs and \ufb01nd some components are shown to be harmful.\nInformed by this, we propose a Robust Visi",
  "\n64.0\n51.1\n71.5\n300M\n400K\n50K\n0.03\n68.6\n55.5\n76.0\n300M\n1.2M\n50K\n0.03\n70.1\n57.1\n77.6\n300M\n2M\n50K\n0.03\n70.5\n57.1\n77.9\n1B\n20K\n10K\n0.03\n49.9\n37.8\n56.5\n1B\n30K\n10K\n0.03\n55.2\n42.8\n62.3\n1B\n60K\n10K\n0.03\n61.0\n47.9\n68.4\n1B\n120K\n50K\n0.03\n64.0\n51.1\n71.5\n1B\n400K\n50K\n0.03\n68.5\n55.7\n75.9\n1B\n1.2M\n50K\n0.03\n70.0\n57.3\n77.3\n3B\n20K\n10K\n0.03\n49.9\n38.0\n56.3\n3B\n30K\n10K\n0.03\n55.4\n43.5\n62.4\n31\n\n--------------------------------------------------\n",
  "tt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697,\n2021.\n11\n\n--------------------------------------------------\n[41] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? arXiv preprint\narXiv:1905.10650, 2019.\n[42] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking the positional encoding in language pre-training. arXiv\npreprint arXiv:2006.15595, 2020.\n[43] Hon",
  " tactfully decoupled in\nour RedNet towards a favorable accuracy-ef\ufb01ciency trade-\noff, as empirically evidenced in Figure 2. To be speci\ufb01c,\nthe information encoded in the channel dimension of one\npixel is implicitly scattered to its spatial vicinity in the ker-\nnel generation step, after which the information in an en-\nriched receptive \ufb01eld is gathered thanks to the vast and dy-\nnamic involution kernels. Indispensably, linear transforma-\ntions (realized by 1 \u00d7 1 convolutions) are interspersed for",
  "ly be deployed in other research\ndomains where self-attention has shown success.\nReferences\n[1] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham,\nAnirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured\ninputs in transformers. In Conference on Empirical Methods in Natural Language Processing, 2020.\n[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u02c7ci\u00b4c, and Cordelia Schmid.\nVivit: A video vision trans",
  ", Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. CoRR,\nabs/2012.00364, 2020. 1, 2\n[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In ICML, volume 119 of Proceedings\nof Machine Learning Research, pages 1691\u20131703. PMLR,\n2020. 1, 2\n[8] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\nYan, and Jiashi Feng. A\u02c62-nets: Double attention networks.\nIn NeurIPS, pages 350\u20133",
  "]. Due to the usage of absolution positional embeddings, standard\nTransformer (ViT) lacks this property. This partially explains why ConvNets are usually better than\nTransformers when the dataset is not enormously large.\n\u2022 Finally, the size of the receptive \ufb01eld is one of the most crucial differences between self-attention\nand convolution. Generally speaking, a larger receptive \ufb01eld provides more contextual information,\nwhich could lead to higher model capacity. Hence, the global receptive \ufb01eld ",
  "hildon, Karan Grewal, Adam Trischler, and\nYoshua Bengio. Learning deep representations by mutual information estimation and maximiza-\ntion. ArXiv, abs/1808.06670, 2018. 3\nOlivier J. H\u00b4enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\nand Aaron van den Oord. Data-ef\ufb01cient image recognition with contrastive predictive coding,\n2019. 3\nA. Khosla, Nityananda Jayadevaprakash, B. Yao, and Li Fei-Fei. Novel dataset for \ufb01ne-grained\nimage categorization : Stanford dogs.",
  "strained to+ 1 or-1.\narXiv preprint arXiv:1602.02830,\n2016. 2\n[3] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmentation\npolicies from data. arXiv preprint arXiv:1805.09501, 2018.\n1, 5\n[4] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe.\nRandaugment:\nPractical automated data augmenta-\ntion with a reduced search space.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702\u2013703, 20",
  "th-wise Conv2d. In addition, we compensate short-length limitations\nof the input token for each head by projecting the interaction across the attention-heads dimension\nwhile keeping the diversity ability of multi-heads.\nWe comprehensively validate the effectiveness of the proposed ResT on the commonly used bench-\nmarks, including image classi\ufb01cation on ImageNet-1k and downstream tasks, such as object detection,\nand instance segmentation on MS COCO2017. Experimental results demonstrate the effect",
  " consistent settings\nfor future methods. Speci\ufb01cally, we report standard 1\u00d7-schedule (12 epochs) detection results on\nthe COCO 2017 dataset [48] in Tables 3 and 4. As for the evaluation based on RetinaNet, we train\n6\n\n--------------------------------------------------\nTable 1 \u2013 Comparisons with state-of-the-art methods for ImageNet-1K classi\ufb01cation. Throughput is tested on\nthe batch size of 192 on a single V100 GPU. All models are trained and evaluated on 224\u00d7224 resolution on\nImageNet-1K datase",
  "samples. Yet, it obtains a 0.3% accuracy\ngain, implying that these samples, though rarely seen, can make the training\n1 https://github.com/tensor\ufb02ow/tpu/tree/master/models/o\ufb03cial/e\ufb03cientnet\n\n--------------------------------------------------\nCircumventing Outliers of AutoAugment with Knowledge Distillation\n13\nprocess unstable. On the other hand, when \u03bb is overly large, e.g., knowledge\ndistillation can dominate the training process and force the student model to\nhave a very similar behavior to th",
  "4.15%). For reference, the current state\nof the art of 88.55% achieved with extra training data was obtained by the ViT-\nH model (600M parameters) trained on JFT-300M at resolution 512. Hereafter\nwe provide several analysis and observations.\nConvnets teachers.\nWe have observed that using a convnet teacher gives bet-\nter performance than using a transformer. Table 2 compares distillation results\nwith different teacher architectures. The fact that the convnet is a better teacher\nis probably due to",
  "to ViT&T2T-\nViT as ViT-Dense&T2T-ViT-Dense. Similar to DenseNet,\nif each block in ViT-Dense&T2T-ViT-Dense has L Trans-\nformer layers, there are L(L + 1)/2 connections in this\nblock and l-th layer has l input from the early layers. Specif-\nically, we set the hidden dimension of the \ufb01rst layer in ViT-\nDense&T2T-ViT-Dense as 128 and it increases 64 channels\n(\u2018\u2019growth rate\u201d as DenseNets) in each layer after concate-\nnating with the early layers channels. The ViT-Dense&T2T-\nViT-Dense has 4 blocks as ",
  "pixel-level representation. Tokens-to-Token (T2T) [41]\nmainly improves tokenization in ViT by concatenating mul-\ntiple tokens within a sliding window into one token. How-\never, this operation fundamentally differs from convolutions\nespecially in normalization details, and the concatenation\nof multiple tokens greatly increases complexity in compu-\ntation and memory. PVT [34] incorporates a multi-stage\ndesign (without convolutions) for Transformer similar to\nmulti-scales in CNNs, favoring dense pr",
  " mechanism, our\nmulti-branch design of X-volution obtains a better performance, showing the complementarity and\nnecessity of convolution and self-attention.\n4.3\nAnalysis Study\n4.3.1\nEffect of Different Kernel Size\nTo study the in\ufb02uence of convolution branch in our X-volution, we conduct experiments of different\nconvolutional kernel size (i.e., ranges from 1 \u00d7 1 to 9 \u00d7 9). We show the results in Fig. 3 (a). When\nreducing the kernel size to 1 \u00d7 1, obvious performance decay occurs, which is possibl",
  "use the CNN and Transformer architectures [36, 64, 40,\n24, 55, 69, 64, 47], validating our approach. We hope that our uni\ufb01ed view helps place these different\nconcurrent proposals in context and leads to a better understanding of the landscape of these methods.\n2\n\n--------------------------------------------------\n2\nRelated Work\nVisual Backbones. Since AlexNet [33] revolutionized computer vision, a host of CNN based archi-\ntectures have provided further improvements in terms of accuracy including",
  "-q3\n42.3\n46.2\n47.6\nconc-q4\n40.2\n60.7\n57.8\nhard\n27.2\n28.8\n31.7\nThe results are listed in the Table 12 compared to Picture-\nbook (Kiros et al., 2018) and GloVe (Pennington et al., 2014)\nembeddings. Overall the learned ALIGN perform better\nthan Picturebook but slightly worse than GloVe embeddings.\nWhat is interesting is that the ALIGN word embeddings\nhas a similar trend of Picturebook embeddings, with bet-\nter performance on nouns and most concrete categories but\nworse on adjs and less concrete cat",
  "fferent\nportions of the dataset.\n\u2022 We update the state-of-the-art on ImageNet for mobile setting, achieving 80.0% top-1\naccuracy with only 100M FLOPs for the \ufb01rst time.\n2\n\n--------------------------------------------------\n2\nRelated Work\n2.1\nEnsemble Learning\nEnsemble learning [2] aims at combining the predictions from several models to get a more robust\none. Some recently proposed literatures [3, 4] demonstrate that signi\ufb01cant gains can be achieved\nwith negligible additional parameters compared",
  "\nDeiT-B\nViT-B\n768\n12\n12\n86M\n224\n292\nTable 2: We compare on ImageNet [42] the performance (top-1 acc., %) of the\nstudent as a function of the teacher model used for distillation.\nTeacher\nStudent: DeiT-B \u2697\nModels\nacc.\npretrain\n\u2191384\nDeiT-B\n81.8\n81.9\n83.1\nRegNetY-4GF\n80.0\n82.7\n83.6\nRegNetY-8GF\n81.7\n82.7\n83.8\nRegNetY-12GF\n82.4\n83.1\n84.1\nRegNetY-16GF\n82.9\n83.1\n84.2\n5.2\nDistillation\nOur distillation method produces a vision transformer that becomes on par\nwith the best convnets in terms of the trade-of",
  "ions in Table 4 exceed the MLPerf\ntarget accuracy of 75.9% at batch size 32,768 with minimal tuning. Training with larger batch\nsizes is not fundamentally unstable: stringent step budgets make hyperparameter tuning trickier.\nWeights\nOptimizer\nBias/BN\nOptimizer\nTop-1\nNesterov\nNesterov\n76.7\nLARS\nMomentum\n76.9\nLARS\nLARS\n76.9\nAdam (\u03f5 = 10\u22128)\nAdam (\u03f5 = 10\u22128)\n76.2\nAdam (\u03f5 = 10\u22126)\nAdam (\u03f5 = 10\u22126)\n76.4\nLAMB\nLAMB\n27.3\nLAMB\nAdam (\u03f5 = 10\u22128)\n76.3\nLAMB\nAdam (\u03f5 = 10\u22126)\n76.3\nTable 4: Validation accuracy of Res",
  "model variants, termed Conformer-\nTi, -S, and -B, respectively. The details of Conformer-S\nare described in Tab. 1, and those of Conformer-Ti/B are\nin the Appendix. Conformer-S/32 splits the feature maps to\n7\u00d77 patches, i.e., the patch size is 32\u00d732 in the transformer\nbranch.\n4.2. Image Classi\ufb01cation\nExperimental Setting.\nConformer is trained on the\nImageNet-1k [14] training set with 1.3M images and tested\nupon the validation set.\nThe Top-1 accuracy is reported\nin Tab. 2. To make the transformer",
  " Z. Han, Y. Liu, and M. Zwicker, \u201cPoint2sequence: Learning\nthe shape representation of 3d point clouds with an attention-\nbased sequence to sequence network,\u201d in AAAI, 2019, pp. 8778\u2013\n8785.\n[75]\nW. Wu, Z. Qi, and F. Li, \u201cPointconv: Deep convolutional networks\non 3d point clouds,\u201d in CVPR, 2019, pp. 9621\u20139630.\n[76]\nY. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, \u201cPointcnn:\nConvolution on x-transformed points,\u201d in NeurIPS, 2018, pp. 828\u2013\n838.\n[77]\nX. Yan, C. Zheng, Z. Li, S. Wang, and S. Cui, \u201cPo",
  "of comparison experiments\nto show the advantages of our proposed Re-attention.\nRe-attention v.s. Self-attention: We \ufb01rst evaluate the ef-\nfectiveness of Re-attention by comparing to the pure ViT\nmodels using the same set of training hyper-parameters. We\ndirectly replace the self-attention module in ViT with Re-\nattention and show the results in Tab. 4 with different num-\nber of transformer blocks. As can be seen, the vanilla ViT\narchitecture suffers performance saturation when adding\nmore transf",
  "e, all experimental results\nin this section report ImageNet accuracies obtained by training a LambdaNetwork architecture that\nreplaces the spatial convolutions in the ResNet-50 with lambda layers.\nVarying query depth, number of heads and intra-depth.\nTable 9 presents the impact of the\nquery depth |k|, number of heads |h| and intra depth |u| on performance (See Appendix B.4 for\na presentation of the intra-depth |u|). Our experiments indicate that the lambda layer outperforms\nconvolutional and att",
  "in GluonCV [19].\nAs comparison we replace the backbone with ResNeSt50\nand ResNeSt101 respectively while keeping other settings\nunchanged. The input image size is \ufb01xed to 256x192 for all\nruns. We use Adam optimizer with batch size 32 and ini-\ntial learning rate 0.001 with no weight decay. The learning\nrate is divided by 10 at the 90th and 120th epoch. The ex-\nperiments are conducted on COCO Keypoints dataset, and\nwe report the OKS AP for results without and with \ufb02ip test.\nFlip test \ufb01rst makes pre",
  " of architectures\nfrom its accuracy in the \ufb01rst 25% of training iterations. PNAS\n[23] uses a multilayer perceptron (MLP) and a recurrent neural\nnetwork to estimate the expected improvement in accuracy if the\ncurrent modular structure (which is later stacked together to form\na network) is expanded with a new branch. Conceptually, both\nof these methods seek to learn a prediction model that extrapolate\n(rather than interpolate), resulting in poor correlation in prediction.\nOnceForAll [31] also uses",
  "on\n(ECA) module [45], and their combinations. A thorough\nanalysis of the activation function is discussed in the exper-\niments section.\n3.3. Class token\nTo apply vision transformers to image classi\ufb01cation, a\ntrainable class token is added and inserted into the token\nembedding, i.e.\n\u02c6X \u2190Concat(Xcls, \u02c6X),\n(7)\n4\n\n--------------------------------------------------\nNetwork\n\u03b3\nDW\nParams\nFLOPs\nTop-1\n(M)\n(G)\nAcc. (%)\nDeiT-T [41]\n4\nNo\n5.7\n1.3\n72.2\nLocalViT-T\n4\nNo\n5.7\n1.3\n72.5 (0.3\u2191)\nLocalViT-T*\n4\nYes\n5.8\n",
  "ch bounding box, the network\nshould output the con\ufb01dences for each class category (in total C class categories). For providing the\ndetections the framework uses a classi\ufb01er which is represented by a 3\u00d73 convolution, that outputs for\neach bounding box the con\ufb01dences for all class categories (C). For localization the framework uses\nalso a 3\u00d73 convolution to output the four localization values for each regressed default bounding box.\nIn total, the framework outputs 8732 detections (for 300\u00d7300 inpu",
  " . . .\n6\n3.3\nMaking lambda layers translation equivariant. . . . . . . . . . . . . . . . . . . . .\n7\n3.4\nLambda convolution: modeling longer range interactions in local contexts. . . . . .\n7\n4\nRelated Work\n8\n5\nExperiments\n9\n5.1\nLambda layers outperform convolutions and attention layers. . . . . . . . . . . . .\n9\n5.2\nComputational bene\ufb01ts of lambda layers over self-attention.\n. . . . . . . . . . . .\n9\n5.3\nHybrids improve the speed-accuracy tradeoff of image classi\ufb01cation. . . . . . . . .\n10\n5.4\nO",
  "and have a two stage evaluation process we also\ncompared against using models trained with cross-entropy loss for representation learning. We\ndo this by \ufb01rst training the model with cross entropy and then re-initializing the \ufb01nal layer of the\nnetwork randomly. In this second stage of training we again train with cross entropy but keep the\nweights of the network \ufb01xed. Table 7 shows that the representations learnt by cross-entropy for a\nResNet-50 network are not robust and just the re-initializati",
  "ikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\nD.10\nVTAB BREAKDOWN\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\n20\n\n--------------------------------------------------\nPublished as a conference paper at ICLR 2021\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82",
  "39.5\n61.9\n42.5\nCycleMLP-B4\n61.5M\n43.2\n63.9\n46.2\n26.6\n46.5\n57.4\n71.5M\n44.1\n65.7\n48.1\n40.2\n62.7\n43.5\nResNeXt101-64x4d [61]\n95.5M\n41.0\n60.9\n44.0\n23.9\n45.2\n54.0\n101.9M\n42.8\n63.8\n47.3\n38.4\n60.6\n41.3\nCycleMLP-B5\n85.9M\n42.7\n63.3\n45.3\n24.1\n46.3\n57.4\n95.3M\n44.1\n65.5\n48.4\n40.1\n62.8\n43.0\nTable 5: Object detection and instance segmentation on COCO val2017 [34].\nare initialized with ImageNet pre-trained weights and other\nnewly added layers are initialized via Xavier [17]. We use\nthe AdamW [37] optimizer with",
  "object detection with Mask R-CNN. We notice that\nthe window size plays a crucial role and the default window\nsize 15 gives the best performance. Smaller window sizes\nlead to serious performance drop. As shown in Figure 3\nFigure 3. Effects of window size (Left) and number of global to-\nkens (Right) in Vision Longformer for object detection with Mask\nR-CNN. All use the same ImageNet1K pre-trained checkpoint\n(ViL-Small-RPB in Table 4).\n(Right), as long as there is one global token, adding more\nglob",
  "rk (for re-entrant processing), while in \u201cat\nstart\u201d all cross-attends are placed at the start of the network fol-\nlowed by all latent self-attend layers. All cross-attention layers\nexcept the initial one are shared, and self-attends are shared as\nusual (using 8 blocks of 6 self-attention modules). Results are\ntop-1 validation accuracy (in %) on ImageNet (higher is better).\ninitializing the position encoding: this value is used for the\nmodel reported in Tab. 2.\nFor all FLOPs numbers reported here",
  " 1097\u20131105, 2012.\n[20] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks\nwithout residuals. arXiv preprint arXiv:1605.07648, 2016.\n[21] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature\npyramid networks for object detection. In CVPR, 2017.\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects i",
  "e later transformer encoder. More formally, the output\nzi of this fusion module can be represented as\nzi =\n\uf8ee\n\uf8f0gi(\nX\nj\u2208{l,s}\nf j(xj\ncls)) || xi\npatch\n\uf8f9\n\uf8fb,\n(3)\nwhere f i(\u00b7) and gi(\u00b7) play the same role as Eq. 2.\nPairwise Fusion. Figure 3(c) shows how both branches are\nfused in pairwise fusion. Since patch tokens are located at\nits own spatial location of an image, a simple heuristic way\nfor fusion is to combine them based on their spatial loca-\ntion. However, the two branches process patches of di",
  "al if we are to understand neural network training,\nespecially at larger batch sizes where we lose some of the regularization effect of gradient noise.\nHypothetically, if the primary bene\ufb01t of a training procedure is regularization, then it would be better\nto compare the method with other regularization baselines than other optimizers.\nUltimately, we only care about batch size to the extent that higher degrees of data parallelism lead\nto faster training. Training with a larger batch size is a me",
  "output layer of the network is removed and\nreplaced with a untrained homoscedastic output layer for\n\ufb01ne-tuning. For downstream \ufb01ne-tuning we use the standard\nhyperparameters and data augmentation settings speci\ufb01ed\nby Kolesnikov et al. [26]. The VTAB1K score is an average\nof the accuracy on all 19 datasets.\nTable 6 shows VTAB1K scores. Our parameter-ef\ufb01cient\nheteroscedastic model, which captures correlations in the\nJFT label noise, improves the VTAB1K score by 0.88%\nover the homoscedastic baselin",
  "iT [22] as the baseline for most experiments. The rela-\ntive position encoding is added into all self-attention layers.\nIf not speci\ufb01ed, the relative position encoding is only added\non keys. We set \u03b1:\u03b2:\u03b3 = 1:2:8 for the piecewise function\ng(x), and adjust the number of buckets by changing \u03b2. An\n5\n\n--------------------------------------------------\nMethod\nIs\nMode\nTop-1\n\u2206\nbased on DeiT-S [22]\nDirected\nAcc(%)\nAcc(%)\nOriginal [22]\n-\n-\n79.9\n\u2013\nEuclidean\n\u00d7\nbias\n80.1\n+0.2\ncontextual\n80.4\n+0.5\nQuantizati",
  "d implementation details that are not often discussed. It was not\nobvious to us a priori which ones would prove crucial. These details do not involve changes to the\noptimizer, but they interact with the optimizer in a regime where all hyperparameters need to be well\ntuned to stay competitive, making it necessary to re-tune everything for a new optimizer.\nIn neural network optimization research, training loss is rarely discussed in detail and evaluation\ncenters on validation/test performance sinc",
  ".2\n86.5\n152\n256\n0.49\n83.8\n-\n152\n288\n0.63\n-\n86.7\n270\n256\n0.91\n84.2\n-\n350\n256\n1.16\n84.4\n-\n350\n288\n1.48\n84.5\n-\n350\n320\n1.91\n84.7\n-\n420\n320\n2.25\n84.9\n-\nTable 19: Detailed LambdaResNets results. Latency refers to the time per training step for a batch\nsize of 1024 on 8 TPU-v3 cores using bfloat16 activations.\nSupervised ImageNet 90 epochs training setup with vanilla ResNet.\nIn the 90 epoch setup, we\nuse the vanilla ResNet for fair comparison with prior works. We used the default hyperparameters\nas fo",
  "ortantly, XCA has a linear\ncomplexity in the number of patches. To construct our Cross-Covariance Image Transformers\n(XCiT), we combine XCA with local patch interaction modules that rely on ef\ufb01cient depth-\nwise convolutions and point-wise feedforward networks commonly used in transformers, see\nFigure 1. XCA can be regarded as a form of a dynamic 1\u00d71 convolution, which multiplies\nall tokens with the same data-dependent weight matrix. We \ufb01nd that the performance of\nour XCA layer can be further imp",
  "f semi-supervised methods related\nto self-training [5] that explicitly generate pseudo-labels for\nthe unlabeled samples and that optimize prediction accuracy\non both the ground truth labels (for the labeled samples) and\nthe pseudo-labels (for the unlabeled samples). For example\nPseudo-Label [18] and earlier related methods [19, 20, 21]\n\ufb01rst train a model on the labeled samples, use this model\nto assign pseudo-labels to unlabeled samples, and then re-\ntrain the model using both the labeled and un",
  "meterized supernet model,\nand then share the weights across subnets. Among them,\nSPOS is simple and representative [16]. In each iteration, it\nonly samples one random path and trains the path using one\nbatch data. Once the training process is \ufb01nished, the subnets\ncan be ranked by inheriting the shared weights. However,\nmost weight-sharing methods need an additional retraining\nstep after the best architecture is identi\ufb01ed [16, 31, 59].\nRecent works, OFA [6], BigNAS [65] and slimmable\nnetworks [66",
  "markable 0.8% improvement on ImageNet compared to\nN=16. However, the improvement comes to be saturated when adding more parts into the network.\nEffects of the part-wise linear. Different from the original design in Transformer [62] that uses a\nself-attention module for part-wise communication, we replace it with a simple linear operation to\nsave the computational cost. Introducing such a simple linear operation into ViP-S can lead to a 0.4%\ngain on ImageNet with only a fractional increase in par",
  "ets.\nDataset\nTrain Size\nTest Size\n# Classes\nFood-101\n75,750\n25,250\n101\nCARS196\n8,144\n8,041\n196\nOxford-Flowers\n2,040\n6,149\n102\nFGVC Aircraft\n6,667\n3,333\n100\nOxford-IIIT Pets\n3,680\n3,669\n37\nTable 16. Statistics of FGVC datasets.\nC. IR Task Con\ufb01guration\nIn this section, we will describe experimental con\ufb01g-\nurations for three public \ufb01ne-grained image retrieval (IR)\ndatasets: Stanford Online Products (SOP) [28], CUB200\n[33] and CARS196 [17]. The basic experimental setup and\nhyperparameters are descri",
  ", so we measure model performance via precision@1 (see Appendix B.6 for\ndetails). Note that as in previous works [20], hyperparameters were tuned for transfer performance,\nand JFT precision could be improved at the expense of downstream tasks e.g. by reducing weight\ndecay. Figure 2a shows the quality of different V-MoE and ViT variants with respect to total training\ncompute and time. It shows models that select k = 2 experts and place MoEs in the last n even blocks\n(n = 5 for V-MoE-H, n = 2 othe",
  "in, M., et al.: ImageNet large scale visual recognition challenge. Interna-\ntional Journal of Computer Vision 115(3), 211\u2013252 (2015)\n38. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: MobileNetV2: Inverted resid-\nuals and linear bottlenecks. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. pp. 4510\u20134520 (2018)\n39. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distilled version of bert: smaller,\nfaster, cheaper and lighter. The 5th Work",
  "to computational complexity that\nis less than the Mobilenet block (ratio \u22641, i.e. red color in Fig.13).\n\n--------------------------------------------------\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\n\u2193\nSTEM\n\u2193\nDSConv_K3\n\u2193\nIB_E4_K3.5\n\u2193\nE6_K3_G1_L0.5\n\u2193\nIB_E4_K3.5.7\n\u2193\nE6_K5_G1_L0.5\n\u2193\nIB_E4_K3.5.7\n\u2193\nE6_K5_G2_L0.25\n\u2193\nIB_E6_K3\n\u2193\n3\u00d7224\u00d7224\n16\u00d7112\u00d7112\n16\u00d756\u00d756\n32\u00d728\u00d728\n32\u00d728\u00d728\n48\u00d714\u00d714\n48\u00d714\u00d714\n96\u00d77\u00d77\n96\u00d77\u00d77\n112\u00d77\u00d77\n\u00d72\n\u00d72\n\u00d72\nPrimitive layers\n(not s",
  "y cross entropy for each concept selected by these augmentations,\nassuming that all the mixed concepts are present in the synthetized image.\n\u2022 We measure the stability of the accuracy over a large number of runs with differ-\nent seeds, and discuss the over\ufb01tting issue by jointly comparing the performance\non ImageNet-val versus the one obtained in ImageNet-V2 [34].\n\u2022 We train popular architectures and re-evaluate their performance. We also dis-\ncuss the necessity to optimize jointly the architect",
  "udy on the global \ufb01lter. To more clearly show the effectiveness of the proposed global\n\ufb01lters, we compare GFNet-XS with several baseline models that are equipped with different token\nmixing operations. The results are presented in Table 6. All models have a similar building block (\ntoken mixing layer + FFN ) and the same feature dimension of D = 384. We also implement the\nrecent FNet [25] for comparison, where a 1D FFT on feature dimension and a 2D FFT on spatial\ndimensions are used to mix token",
  "32.1\n36.4\n39.2\n44.5\n56.1\n62.3\n38.4\n45.8\n30M\n30K\n39.1\n43.3\n46.3\n51.4\n67.2\n74.1\n46.8\n53.7\n30M\n60K\n47.1\n51.2\n50.7\n55.7\n78.4\n83.1\n54.0\n61.6\n30M\n120K\n51.8\n55.4\n53.8\n59.1\n82.2\n86.1\n58.8\n66.3\n30M\n400K\n58.4\n61.8\n57.2\n62.6\n88.3\n90.6\n65.0\n71.6\n30M\n1.2M\n61.1\n64.4\n57.1\n63.2\n90.5\n91.3\n68.4\n73.7\n30M\n2M\n61.9\n65.6\n58.8\n64.1\n91.0\n92.0\n68.0\n74.0\n30M\n4M\n63.2\n66.5\n58.7\n65.2\n91.3\n92.1\n69.7\n74.7\n300M\n20K\n31.3\n36.4\n38.9\n43.9\n57.4\n62.2\n37.4\n45.4\n300M\n60K\n47.4\n51.1\n50.8\n56.6\n78.3\n83.8\n53.4\n62.0\n300M\n120K\n52.6\n56.4\n53.0\n",
  "Net-m achieves\n75.3% accuracy with 3.4M parameters and 218M MAdds,\nwhich is 1.4\u00d7 more ef\ufb01cient and 1.6\u00d7 more compact when\ncompared to MnasNet-A1 [40] and MobileNetV3 [13], re-\nspectively.\nFigures 1 and 6 visualize the trade-off ob-\ntained by MUXNet and previous models. In terms of ac-\ncuracy and compactness, MUXNet clearly dominates all\nprevious models including MnasNet [40], FBNet [44], Mo-\nbileNetV3 [13], and MixNet [42]. In terms of accuracy and\nef\ufb01ciency, MUXNets are on par with current stat",
  "gmentation, 2020. 8\n[4] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe.\nRandaugment:\nPractical automated data augmenta-\ntion with a reduced search space.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702\u2013703, 2020. 6\n[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words",
  "------------------------------------------\nAnalysis\nBits Histogram Figure 3 presents the weight bitwidth as-\nsignment over layer groups for the Ef\ufb01cientNet-B3 (Tan and\nLe 2019) and DeiT (Touvron et al. 2020) models trained\non ImageNet. The capacity distribution over depth for Con-\nvNets (Ef\ufb01cientNet-B3) and Transformers (DeiT) are dif-\nferent (fp32 shows uncompressed capacity). Notice, that the\nquantization trends are different too: for the ConvNet, smaller\nbitwidths are used for deeper layers o",
  "egy to accelerate network convergence, which means the\noutput of each stage is not normalized before feeding to FPN [16]. Here, we add a layer normalization\n(LN [1]) for the output of each stage (before FPN [16]), similar to Swin [18]. Results are reported on\nthe validation split.\n7\n\n--------------------------------------------------\nObject Detection Results. Table 3 lists the results of RetinaNet with different backbones. From\nthese results, it can be seen that for smaller models, ResT-Small is",
  "ing similar accuracies on ImageNet.\nIn a large-scale semi-supervised learning setup,\nResNet-RS achieves 86.2% top-1 ImageNet ac-\ncuracy, while being 4.7x faster than Ef\ufb01cientNet-\nNoisyStudent. The training techniques improve\ntransfer performance on a suite of downstream\ntasks (rivaling state-of-the-art self-supervised al-\ngorithms) and extend to video classi\ufb01cation on\nKinetics-400. We recommend practitioners use\nthese simple revised ResNets as baselines for fu-\nture research.\n1. Introduction\nThe",
  " Previous token\npruning works (Rao et al. 2021; Tang et al. 2021) present\na gradual shrinking architecture, in which more tokens are\nrecognized as placeholder tokens in deeper layers. They are\nrestricted in this type of architecture due to direct pruning.\nOur method allows more \ufb02exible token selection owing to\nthe structure preserving slow-fast token evolution. As shown\nin Fig. 6, we maintain the sum of the number of placeholder\ntokens in all layers and adjust the keeping ratio in each layer.\nRe",
  " class to the least one is rebalanced from\n1\nKyh\n\u2212\u2192\n1\nKyt\nto\n1\n1\n\u03b1 + Kyh\n\u2212\u2192\n1\n1\n\u03b1 + Kyt\n. The smaller \u03b1 is, the more uni-\nform the optimal value from the most frequent class to the\nleast one is, friendly to low-frequency classes learning.\nHowever, when \u03b1 decreases, the intensity of contrast\namong samples becomes weaker, the intensity of contrast\nbetween samples and centers is stronger. The whole loss\nbecomes closer to supervised cross-entropy. To make good\nuse of contrastive learning and rebalan",
  "offer\ntwo new scaling strategies: (1) scale model depth\nin regimes where over\ufb01tting can occur (width\nscaling is preferable otherwise); (2) increase im-\nage resolution more slowly than previously rec-\nommended (Tan & Le, 2019). Using improved\ntraining and scaling strategies, we design a fam-\nily of ResNet architectures, ResNet-RS, which\nare 1.7x - 2.7x faster than Ef\ufb01cientNets on TPUs,\nwhile achieving similar accuracies on ImageNet.\nIn a large-scale semi-supervised learning setup,\nResNet-RS achie",
  "\u223c32 random models is likely to yield good models.\nTable 1 shows a summary of the design space sizes (for\nRegNet we estimate the size by quantizing its continuous\nparameters). In designing RegNetX, we reduced the dimen-\nsion of the original AnyNetX design space from 16 to 6 di-\nmensions, and the size nearly 10 orders of magnitude. We\nnote, however, that RegNet still contains a good diversity\nof models that can be tuned for a variety of settings.\n3.4. Design Space Generalization\nWe designed the Re",
  "d in\nthe past two years. A small MDEQ (7.8M parameters) achieves a mean IoU of 75.1. This improves\nupon a MobileNetV2Plus [46] of the same size and is close to the SOTA for models on this scale. A\nlarge MDEQ (53.5M parameters) reaches 77.8 mIoU, which is within 1 percentage point of highly\nregarded recent semantic segmentation models such as DeepLabv3 [9] and PSPNet [62], whereas a\nlarger version (70.9M parameters) surpasses them. It is surprising that such levels of accuracy can\nbe achieved by ",
  "ke around 5\u00d7 more\nFLOPs than LeViT-384 and more parameters for compa-\nrable accuracies than LeViT. Bottleneck transformers [46]\nand \u201cVisual Transformers\u201d [47] (not to be confused with\nViT) are both generic architectures that can also be used\nfor detection and object segmentation. Both are about 5\u00d7\nslower than LeViT-192 at a comparable accuracy. The\nsame holds for the pyramid vision transformer [48] (not\nreported in the table) but its design objectives are differ-\nent. The advantage of LeViT comp",
  "ssing Systems. 2\nPham, H.; Guan, M. Y.; Barret, Z.; Le, Q. V.; and Dean, J.\n2018. Ef\ufb01cient Neural Architecture Search via Parameter\nSharing. In International Conference on Machine Learning.\n2, 7\nReal, E.; Aggarwal, A.; Huang, Y.; and Le, Q. V. 2018. Reg-\nularized Evolution for Image Classi\ufb01er Architecture Search.\nArxiv, 1802.01548 . 1, 2, 5\nReal, E.; Moore, S.; Selle, A.; Saxena, S.; Leon, Y. S.; Tan,\nJ.; Le, Q.; and Kurakin, A. 2017. Large-Scale Evolution of\nImage Classi\ufb01ers. In International C",
  "sformer-Ti is still the best overall\nmodel, and the advantage is slightly enlarged because the\nrisk of over-\ufb01tting has been reduced.\n5. Conclusions\nThis paper presents Visformer, a Transformer-based\nmodel that is friendly to visual recognition. We propose\nto use two protocols, the base and elite setting, to evaluate\nthe performance of each model. To study the reason why\nTransformer-based models and convolution-based models\nbehave differently, we decompose the gap between these\nmodels and design ",
  ". IEEE Computer Society,\n2015.\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In 2016 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), Las Vegas, NV, USA, pages 770\u2013778, 2016.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In 14th Euro-\npean Conference on Computer Vision (ECCV), Amsterdam,\nThe Netherlands, pages 630\u2013645, 2016.\n[14] Tong He, Zhi Zhang, Hang Zhang,",
  "0.24 G\nCVT-7/4\nSP\n\u0017\n\u0017\n\u0013\n\u0013\n92.43%\n73.01%\n3.72 M\n0.24 G\nCVT-7/2\nSP\n\u0017\n\u0017\n\u0017\n\u0017\n84.8%\n57.98%\n3.76 M\n0.94 G\nCCT-7/7\u00d71\nSP\n1\n7 \u00d7 7\n\u0017\n\u0017\n87.81%\n62.83%\n3.74 M\n0.25 G\nCCT-7/7\u00d71\nSP\n1\n7 \u00d7 7\n\u0013\n\u0017\n91.85%\n69.43%\n3.74 M\n0.25 G\nCCT-7/7\u00d71\nSP\n1\n7 \u00d7 7\n\u0013\n\u0013\n92.29%\n72.46%\n3.74 M\n0.25 G\nCCT-7/3\u00d72\nSP\n2\n3 \u00d7 3\n\u0013\n\u0013\n93.65%\n74.77%\n3.85 M\n0.28 G\nCCT-7/3\u00d71\nSP\n1\n3 \u00d7 3\n\u0013\n\u0013\n94.47%\n75.59%\n3.76 M\n0.95 G\nTable 9: CIFAR Top-1 validation accuracy when transforming ViT into CCT step by step.\n15\n\n---------------------------------------------",
  "roceeds as follows. We \ufb01rst augment the patch embeddings (in matrix form)\nas z = [xclass, xpatches] (see Appendix B for results when z = xpatches). We then\nperform the projections:\nQ = Wq xclass + bq,\n(5)\nK = Wk z + bk,\n(6)\nV = Wv z + bv.\n(7)\n6\n\n--------------------------------------------------\nThe class-attention weights are given by\nA = Softmax(Q.KT /\np\nd/h)\n(8)\nwhere Q.KT \u2208Rh\u00d71\u00d7p. This attention is involved in the weighted sum A \u00d7 V\nto produce the residual output vector\noutCA = Wo A V + bo,\n",
  "odel remains constant. As one\nmight expect, the top-1 accuracy of both the DeiT-S and its\nConViT-S counterpart drops as f decreases. However, the\nConViT suffers much less: while training on only 10% of\nthe data, the ConVit reaches 59.5% top-1 accuracy, com-\npared to 46.5% for its DeiT counterpart.\nThis result can be directly compared to (Zhai et al., 2019),\nwhich after testing several thousand convolutional models\n\n--------------------------------------------------\nConViT: Improving Vision Trans",
  "he tokenizer is also ap-\npended with a max pooling layer.\n3.3. Convolution Stage\nIn order to augment spatial connections, we adopt a\nfully-convolutional \ufb01rst stage.\nIt consists of multiple\nblocks, where each block is comprised of two 1x1 convo-\nlution layers with a 3x3 convolution in between.\n3.4. Conv-MLP Stage\nTo reduce constraints on input dimension, we replace all\nspatial MLPs with channel MLPs. Since channel MLP only\nshare weights across channels which lacks spatial interac-\ntions, we make ",
  "ations of the Transformer\narchitectures to computer vision, have been slow, although some optimizations, have been successful\ne.g., for image classi\ufb01cation, [5, 48, 7, 29] and for video generation [43].\nApplying attention-based architectures to video presents a de\ufb01nite challenge as the model needs\nto learn dependencies across both the spatial and temporal domains. The Vision Transformer [10]\ndemonstrated how the NLP-speci\ufb01c Transformer architecture can elegantly work for images, by\nsubdividing t",
  "ues in MLP-like models. The proposed GFNet enjoys log-linear complexity and can be\neasily scaled up to any resolution.\nApplications of Fourier transform in vision.\nFourier transform has been an important tool in\ndigital image processing for decades [35, 1]. With the breakthroughs of CNNs in vision [14, 13],\nthere are a variety of works that start to incorporate Fourier transform in some deep learning\nmethod [26, 50, 9, 24, 6] for vision tasks. Some of these works employ discrete Fourier transfor",
  "ImageNet 5-shot accuracy\n(Relative) TokenLearner loc.\n0\n0.25\n0.5\n0.75\nBase\n0\n20\n40\n60\n80\n8 tokens\n16 tokens\nGLOPS\n(Relative) TokenLearner loc.\n0\n0.25\n0.5\n0.75\nBase\n0\n5\n10\n15\n20\n8 tokens\n16 tokens\nFigure 4: ImageNet 5-shot accuracy (left) and FLOPS (right) per different TokenLearner location\nwithin the model. \u20180\u2019 means that the TokenLearner is at the very beginning of the model (before any\ntransformer), \u20180.5\u2019 means the middle of the model, and \u2018Base\u2019 means that there is no token learning.\n5\n\n----",
  ": A new vision transformer for high-resolution image encoding. arXiv, 2021. 2, 7,\n8, 9\n[70] Qinglong Zhang and Yubin Yang. Rest: An ef\ufb01cient transformer for visual recognition. arXiv preprint\narXiv:2105.13677, 2021. 3\n[71] Guangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, and Liangchen Luo. Muse: Parallel multi-scale\nattention for sequence to sequence learning. arXiv preprint arXiv:1911.09483, 2019. 3\n[72] Zelin Zhao, Karan Samel, Binghong Chen, and Le Song. Proto: Program-guided transformer ",
  "estigations with the simple attention-convolutional hybrids\non detection and instance segmentation, using the Mask R-\nCNN [16] framework. These hybrids are also faster and\nconsume less memory than pure attention models, enabling\nfaster experimental cycles. We only replace the last 3 convo-\nlutional layers in the ResNet-50 and ResNet-101 backbones\nwith two halo layers with block size, b = 8 and halo size\nh = 3 (Rows 3 and 6 in Table 5). For ResNet-50, we\nalso examine using b = 32 and halo size h ",
  " when it has a small model size as MobileNets.\nTransfer learning\nWe transfer our pretrained T2T-ViT to\ndownstream datasets such as CIFAR10 and CIFAR100. We\n\ufb01netune the pretrained T2T-ViT-14/19 with 60 epochs by\nusing SGD optimizer and cosine learning rate decay.The\nresults are given in Tab. 5.\nWe \ufb01nd that our T2T-ViT\ncan achieve higher performance than the original ViT with\nsmaller model sizes on the downstream datasets.\n4.2. From CNN to ViT\nTo \ufb01nd an ef\ufb01cient backbone for vision transformers,\nw",
  "arning\nthe shape of convolution for image classi\ufb01cation. In CVPR,\n2017. 4\n[25] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V\nGool. Dynamic \ufb01lter networks. In NIPS, 2016. 4\n[26] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDollar. Panoptic feature pyramid networks. In CVPR, 2019.\n6, 7, 9\n[27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classi\ufb01cation with deep convolutional neural net-\nworks. In NIPS, 2012. 2\n[28] Y. Lecun, L. Bottou, Y. Bengio, and P. Ha",
  "in. We again con\ufb01rm that the relative positional bias\n(RPB) outperforms the absolute 2-D positional embedding\n(APE) on Vision Longformer. When compared with Swin\nTransformers [26], our models still performs better with\nfewer parameters.\n4.2. ImageNet-21K pretraining and ImageNet-1K\n\ufb01netuning\nWhen trained purely on ImageNet-1K, the performance\ngain from ViL-Medium to ViL-Base is very marginal. This\nis consistent with the observation in ViT [12]: large pure\ntransformer based models can be trained ",
  "S methods by plotting the\ncorrelation of the architecture rating and the true accuracy\nof 3000 randomly sampled architectures from NATS-Bench\nsize search space SS [17]. Architectures with BossNAS form\ndenser and more spindly scatter pattern than CE on both of\nthe two datasets. Moreover, as measured quantitatively in\nTab. 6, BossNAS outperforms CE by a large margin (0.11\nand 0.16 \u03c4) in both datasets.\n13\n\n--------------------------------------------------\n0\n20\n0.0\n0.2\n0.4\n0\n20\n0.00\n0.25\n0.50\n0.75\n",
  " Lambdanetworks: Modeling long-range interactions without attention. arXiv preprint\narXiv:2102.08602, 2021.\n[2] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The \ufb01fth pascal recognizing textual\nentailment challenge. In TAC, 2009.\n[3] Lucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. Are we\ndone with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n[4] Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-sc",
  "------\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei.\nImagenet: A large-scale hierarchical im-\nage database. In Computer Vision and Pattern Recognition,\n2009. CVPR 2009. IEEE Conference on, pages 248\u2013255.\nIEEE, 2009. 2\n[9] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong\nHan. Acnet: Strengthening the kernel skeletons for power-\nful cnn via asymmetric convolution blocks. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 1911\u20131920, 2019. ",
  "nd\n1% classes should be much higher if epochs are extended).\nIn comparison, Visformer remains robust in these scenarios,\nshowing its potential of being used for visual recognition\nwith limited data.\nIn tiny level, ResNet-50-55% is obtained by reducing the\nchannel numbers (like other tiny models) to 55% (so that the\nFLOPs, 1.3G, is similar to Visformer-Ti and Deit-Ti). The\nconclusion is similar: Visformer-Ti is still the best overall\nmodel, and the advantage is slightly enlarged because the\nrisk ",
  " \ufb01nd\nsome components can be harmful to robustness. By using and combining robust\ncomponents as building blocks of ViTs, we propose Robust Vision Transformer\n(RVT), which is a new vision transformer and has superior performance with strong\nrobustness. We further propose two new plug-and-play techniques called position-\naware attention scaling and patch-wise augmentation to augment our RVT, which\nwe abbreviate as RVT\u2217. The experimental results on ImageNet and six robustness\nbenchmarks show the adv",
  "s for semantic segmentation. arXiv\npreprint arXiv:1909.11065, 2019. 8\n[67] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017. 5\n[68] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang,\nXiaogang Wang, Ambrish Tyagi, and Amit Agrawal. Con-\ntext encoding for semantic segmentation.\nIn The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), June 2018. 7, 8\n[69] Hang Zhang, Han Zhang",
  "rch: An\nimperative style, high-performance deep learning library. In\nAdvances in neural information processing systems, pages\n8026\u20138037, 2019. 5\n[38] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,\nand Ali Farhadi. Xnor-net: Imagenet classi\ufb01cation using bi-\nnary convolutional neural networks. In European conference\non computer vision, pages 525\u2013542. Springer, 2016. 2\n[39] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\npropo",
  "arning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n[23] Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, and Cihang Xie.\nShape-texture debiased neural network training. arXiv preprint arXiv:2010.05981, 2020.\n[24] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves\nimagenet classi\ufb01cation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 106",
  "\ntrade-off than ResNet50.\n3.2.2\nCode Optimizations\nIn Table 5 we evaluate the contribution of the different code\noptimizations. We can see from Table 5 that among the\noptimizations, dedicated inplace operations give the great-\nest boost - not only it improves the GPU throughput, but it\nalso signi\ufb01cantly increases the maximal possible batch size,\nsince it avoids the creation of unneeded activation maps for\nbackward propagation.\nRe\ufb01nement\nInference\nSpeed\n[img/sec]\nTraining\nSpeed\n[img/sec]\nMaximal\n",
  "arisons to standard optimizers, it remains an open question\nwhether LARS and LAMB have any bene\ufb01t over traditional, generic algorithms. In\nthis work we demonstrate that standard optimization algorithms such as Nesterov\nmomentum and Adam can match or exceed the results of LARS and LAMB at large\nbatch sizes. Our results establish new, stronger baselines for future comparisons\nat these batch sizes and shed light on the dif\ufb01culties of comparing optimizers for\nneural network training more generally.\n",
  "ikhin et al., 2021), followed by the\nsize of each image patch. For instance, \u201cB/16\u201d means the model of base scale with non-overlapping\nimage patches of resolution 16 \u00d7 16. We use the input resolution 224 \u00d7 224 throughout the paper.\nFollowing Tolstikhin et al. (2021), we sweep the batch sizes in {32, 64, . . . , 8192} on TPU-v3 and\nreport the highest throughput for each model.\nTable 6: Comparison under the adversarial training framework on ImageNet (numbers in the paren-\ntheses denote the improve",
  "he similarity scores. In practice, the similarity func-\ntion is often de\ufb01ned as\nsim\u2113(xi, xj) =\nexp(Q\u22a4\n\u2113(xi)K\u2113(xj))\nPN\nk=1 exp(Q\u22a4\n\u2113(xi)K\u2113(xk))\n,\n(2)\nwhere Q\u2113(x) and K\u2113(x) are the \u201cquery\u201d and \u201ckey\u201d func-\ntions of the \u2113-th head. Eq. (1) and (2) illustrate how the\nfeatures are processed by one head. A complete trans-\nformer encoder with T layers is the composition of self-\nattention layers and multi-layer perceptrons (MLPs); see\nFigure 1. Obviously, the time and memory complexities\nof self-attention",
  " mesh autoencoders,\u201d in European Conference on\nComputer Vision (ECCV), 2018, pp. 704\u2013720. 9, 10\n[89] G. Bouritsas, S. Bokhnyak, S. Ploumpis, M. Bronstein, and S. Zafeiriou,\n\u201cNeural 3d morphable models: Spiral convolutional networks for 3d shape\nrepresentation learning and generation,\u201d in International Conference on\nComputer Vision (ICCV), 2019. 9\n[90] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Li\u00f2, and\nY. Bengio, \u201cGraph attention networks,\u201d International Conference on\nLearning Repr",
  " Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\nlenge.\nInternational Journal of Computer Vision (IJCV),\n115(3):211\u2013252, 2015.\n[28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted\nresiduals and linear bottlenecks. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog",
  "res). Error bars show mean and standard deviation over ten runs. Bottom row: Goodness of \ufb01t\nvisualization of RBF ensemble, the best accuracy predictor.\nTABLE 6: Comparing the relative search ef\ufb01ciency of NAT to other methods. \u201c\u2013\u201d denotes for not applicable, \u201cWS\u201d stands for weight sharing and \u201cSMBO\u201d\nstands for sequential model-based optimization [79]. \u2020 is taken from [32], \u2021 estimate based on the # of models evaluated during search (20K in [8], 1.2K in [23],\n27K in [38]). \u2217denotes re-ranking stag",
  "e hope of Pseudo Labels is that the obtained\n\u03b8PL\nS would ultimately achieve a low loss on labeled data, i.e.\nExl,yl\nh\nCE\n\u0000yl, S(xl; \u03b8PL\nS )\n\u0001i\n:= Ll\n\u0000\u03b8PL\nS\n\u0001\n.\nUnder the framework of Pseudo Labels, notice that the\noptimal student parameter \u03b8PL\nS always depends on the teacher\nparameter \u03b8T via the pseudo targets T(xu; \u03b8T ). To facili-\ntate the discussion of Meta Pseudo Labels, we can explicitly\nexpress the dependency as \u03b8PL\nS (\u03b8T ). As an immediate obser-\nvation, the ultimate student loss on label",
  " Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee, and Gary Bradski. Kornia: an open source\ndifferentiable computer vision library for pytorch. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 3674\u20133683, 2020.\n12\n\n--------------------------------------------------\n",
  "framework\nare dependent on the version of user\u2019s installed torchvision. For instance, when users\n\ufb01nd new models in the latest torchvision, they can shortly try the models simply by up-\ndating the torchvision and con\ufb01guration \ufb01les for their experiments with our framework.\n2.1\nModule Abstractions\nAn objective of module abstractions in our framework is to enable researchers to experi-\nment with various modules by simply changing a PyYAML con\ufb01guration \ufb01le described\nin Section 2.3. We focus abstracti",
  "u-\ntion, a batch size of 512, cosine learning rate decay, no weight decay, and grad clipping at global\nnorm 1.\nDataset\nTotal steps\nWarmup steps\nBase LR\nCIFAR-10\n10K\n500\n{0.001, 0.003, 0.01, 0.03}\nCIFAR-100\n10K\n500\nFlowers\n500\n100\nPets\n500\n100\n15\n\n--------------------------------------------------\nPreprint\nTable 9: Accuracy on downstream tasks of the models pre-trained on ImageNet. SAM improves\nViTs and MLP-Mixers\u2019 transferabilities to the tasks. ViTs transfer better than ResNets of similar\nsizes",
  "aohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning. In\nProceedings of the European Conference on Computer Vision\n(ECCV), 2020. 7\n[34] Alex Krizhevsky. Learning multiple layers of features from\ntiny images. Technical report, 2009. 4\n[35] Samuli Laine and Timo Aila. Temporal ensembling for semi-\nsupervised learning. In International Conference on Learning\nRepresentations, 2017. 5, 8\n[36] Dong-Hyun Lee. Pseudo-Label",
  "ng is constructed as spatial attention, which is more \ufb02exible and\ncan tackle with input images of arbitrary size without interpolation or \ufb01ne-tune;\n(3) Instead of the straightforward tokenization at the beginning of each stage, we\ndesign the patch embedding as a stack of overlapping convolution operation with\nstride on the token map. We comprehensively validate ResT on image classi\ufb01cation\nand downstream tasks. Experimental results show that the proposed ResT can\noutperform the recently state-of-",
  "e framework continues with an upsample layer (using also bilinear interpolation) to restore\nthe feature maps to the initial input image size; \ufb01nally, there is a classi\ufb01cation layer which contains\na 1\u00d71 conv, to provide the output with a dimension equal to the number of classes. As illustrated\nin Fig. 5, our proposed framework is able to capture local and global information at multiple scales\nof kernels, parsing the image and providing a strong representation. Furthermore, our framework is\nalso v",
  " \u00d7 C4. These stage\nsettings are widely utilized in both CNN [43, 20] and Trans-\nformer [56, 36] models. Therefore, our proposed models\ncan conveniently serve as a generic replacement for exist-\ning models.\n3.2. Cycle FC Block\nThe Cycle FC block is illustrated in Figure 2b. Com-\nparing with the pioneering MLP blocks [49] shown in Fig-\nure 2a, the key difference of Cycle FC block is that it uti-\nlize our proposed Cycle Fully-Connected Layer (Cycle FC)\nfor spatial projection and advance the models ",
  " approaches\noutperformed other options we explored, e.g., directly parameterising and learning the function s.\nWe reuse the router outputs as a proxy for the priority of allocation. Our experiments show this\npreserves the performant predictive behaviour of the model, even though the router outputs primarily\nencode how well tokens and experts can be paired, not the token\u2019s \u201cimportance\u201d for the \ufb01nal classi\ufb01ca-\ntion task. Figure 4 visualizes token prioritisation with Batch Prioritized Routing for i",
  "ith MobileNets [19, 34] (Fig. 1).\nTo sum up, our contributions are three-fold:\n\u2022 For the \ufb01rst time, we show by carefully designing\ntransformers architecture (T2T module and ef\ufb01cient\nbackbone), visual transformers can outperform CNNs\nat different complexities on ImageNet without pre-\ntraining on JFT-300M.\n\u2022 We develop a novel progressive tokenization for ViT\nand demonstrate its advantage over the simple tok-\nenization approach by ViT, and we propose a T2T\nmodule that can encode the important loca",
  "ViT\n(b) ImageNet 1-shot (total training runtime).\nFigure 11: ImageNet/1shot performance of sparse and dense models. The x-axis in (a) shows the total\nFLOPs required during training, while (b) represents the total training time for identical hardware.\n21\n\n--------------------------------------------------\n101\n102\n103\nTotal Training Compute (ExaFLOPs)\n45\n50\n55\n60\n65\n70\n75\nImageNet 5-shot Accuracy (%)\nH/14\nL/16\nL/16\nL/32\nB/16\nB/32\nS/32\nV-MoE (Last n)\nV-MoE (Every 2)\nViT\n(a) ImageNet 5-shot (total t",
  "decay 0.9 and\nmomentum 0.9; batch normalization momentum 0.99; weight\ndecay 1e-5. We use a batch size of 512 and an initial learning\nrate of 0.012 that gradually reduces to zero following the cosine\nannealing schedule. Our regularization settings are similar as in\n[28]: we use augmentation policy [63], drop connect ratio 0.2, and\ndropout ratio 0.2.\nTable 4 shows the performance of NAT models obtained\nthrough bi-objective optimization of maximizing accuracy and\nminimizing #MAdds. Our models, refe",
  "Ruoming Pang, Vijay Vasudevan,\nMark Sandler, Andrew Howard, and Quoc V Le.\nMnas-\nnet: Platform-aware neural architecture search for mobile.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2820\u20132828, 2019.\n9\n\n--------------------------------------------------\n[31] Mingxing Tan and Quoc Le. Ef\ufb01cientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105\u20136114, 2019.\n[32] Yehui Tang, Shan You",
  "9). We train\nevery con\ufb01guration from scratch for 160,000 steps, using\na batch size of 16 and a segment length of 70. We use the\ncosine learning rate schedule so that each trial converges to a\nreasonable perplexity. On 4 TPU v2 chips, each of our runs\ntakes about 40 minutes. The performance of a con\ufb01guration\nr is computed by Perf(r) = 80/ValidPPL(r).\n\n--------------------------------------------------\nMethods\nPennTreebank\nWikiText-2\nIWSLT-14 DeEn\nWMT-14 EnFr\nValidPPL (\u2193)\nTestPPL (\u2193)\nValidPPL (\u2193)\n",
  "computational cost and the number of parameters is similar\nto the standard convolution with a single kernel size K1\n2. To link the illustration in Fig. 3 with\nEquations 1, the denominator of FMi in Equations 1 refers to the number of groups (G) that the\ninput feature maps FMi are split in Fig. 3.\nIn practice, when building a PyConv there are several additional rules. The denominator of FMi\nat each level of the pyramid in Equations 1, should be a divisor of FMi. In other words, at each\npyramid le",
  " International conference on\nmachine learning, pages 2048\u20132057, 2015. 1, 5\n[65] Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinx-\nian Liu, Mengdie Zhou, and Qi Tian. Modeling point clouds\nwith self-attention and gumbel subset sampling. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3323\u20133332, 2019. 1\n[66] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender,\nPieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xi-\naodan Song, Ruoming Pang, and Q",
  "an explo-\nsion in model sizes and datasets. It has become a common\nunderstanding that transformers are data hungry models and\nthat they need suf\ufb01ciently large datasets to perform as well\nor better than their convolutional counterparts. ViT authors\nargued that \u201cTransformers lack some of the inductive biases\ninherent to CNNs, such as translation equivariance and lo-\ncality, and therefore do not generalize well when trained on\ninsuf\ufb01cient amounts of data.\u201d\n1\narXiv:2104.05704v3  [cs.CV]  13 Aug 2021",
  "ResMLP: Feedforward networks for image\nclassi\ufb01cation with data-ef\ufb01cient training\nHugo Touvron1,2\nPiotr Bojanowski1\nMathilde Caron1,3\nMatthieu Cord2,4\nAlaaeldin El-Nouby1,3\nEdouard Grave1\nGautier Izacard1\nArmand Joulin1\nGabriel Synnaeve1\nJakob Verbeek1\nHerv\u00e9 J\u00e9gou1\n1Facebook AI\n2Sorbonne University\n3Inria\n4valeo.ai\nAbstract\nWe present ResMLP, an architecture built entirely upon multi-layer perceptrons for\nimage classi\ufb01cation. It is a simple residual network that alternates (i) a linear layer\nin w",
  "milar values and the range signi\ufb01cantly decreases in the last 50 epochs.\nCredit: this \ufb01gure and experiment was inspired by Picard [30].\nTable 5: Performance comparison on transfer-learning tasks for different pre-training recipes.\nDataset\nTrain size\nTest size\n#classes\nPytorch [1]\nA1\nA2\nA3\nImageNet-val [36]\n1,281,167\n50,000\n1000\n76.1\n80.4\n79.8\n78.1\niNaturalist 2019 [18]\n265,240\n3,003\n1,010\n73.2\n73.9\n75.0\n73.8\nFlowers-102 [29]\n2,040\n6,149\n102\n97.9\n97.9\n97.9\n97.5\nStanford Cars [24]\n8,144\n8,041\n196\n",
  "peration and Instance\nNormalization in Eq. 4.\nMethods\nTop-1 (%)\nTop-5 (%)\norigin\n72.88\n90.62\nw/o IN\n71.98\n90.32\nw/o Conv-1&IN\n71.72\n89.93\nTable 8: Comparison of various positional encod-\ning (PE) strategies on ResT-Lite.\nEncoding\nTop-1 (%)\nTop-5 (%)\nw/o position\n71.54\n89.82\n+ LE\n71.98\n90.32\n+ GL\n72.04\n90.41\n+ PA\n72.88\n90.62\nIn addition, EMSA also adding two important elements to the standard MSA, i.e., one 1 \u00d7 1 convolu-\ntion operation to model the interaction among different heads, and the Inst",
  "ageNet and Standard ImageNet. Both of these mod-\nels used ResNet18 as the backbone. The plot shows that the model trained on Stylized ImageNet\nquickly over\ufb01ts bit by \ufb01nding shortcuts after around 6000 steps. Therefore, it gives poor perfor-\nmance on downstream tasks by relying on texture based shortcuts. Refer to the Table 1 from the\nmain paper.\n17\n\n--------------------------------------------------\nTable 12: Comparison of our approach with Jigsaw baseline methods. Using our best model, we\nimpro",
  " On top of E\ufb03cientNet and RandAugment, we set the\nnew state-of-the-art, a 85.8% top-1 accuracy, on the ImageNet dataset (without\nextra training data).\nIn spite of the consistent improvement brought by our approach, there are\nstill problems that remain mostly uncovered. For example, it remains unclear if\nuseful information only exists in top-ranked classes determined by the teacher\nmodel, and whether mimicking the class-level distribution is the optimal choice.\nWe will continue investigating thes",
  "ional graph of the network and is key in determin-\ning its accuracy and ef\ufb01ciency.\nThe basic design of networks in our AnyNet design\nspace is straightforward. Given an input image, a network\nconsists of a simple stem, followed by the network body that\nperforms the bulk of the computation, and a \ufb01nal network\nhead that predicts the output classes, see Figure 3a. We\nkeep the stem and head \ufb01xed and as simple as possible, and\ninstead focus on the structure of the network body that is\ncentral in deter",
  "gning network design spaces.\narXiv\npreprint arXiv: 2003.13678, 2020.\nRamachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev-\nskaya, A., and Shlens, J. Stand-alone self-attention in\nvision models. arXiv preprint arXiv:1906.05909, 2019.\nReal, E., Aggarwal, A., Huang, Y., and Le, Q. V. Regu-\nlarized evolution for image classi\ufb01er architecture search.\nIn Proceedings of the aaai conference on arti\ufb01cial intel-\nligence, volume 33, pp. 4780\u20134789, 2019.\nRen, S., He, K., Girshick, R., and Sun, J. Faster",
  "oss\nbecomes closer to supervised cross-entropy. To make good\nuse of contrastive learning and rebalance at the same time,\nwe observe that \u03b1=0.05 is a reasonable choice.\n3.4. PaCo under Balanced Setting\nFor balanced datasets, all classes have the same fre-\nquency, i.e., q\u2217=q(yi)=q(yj) and K\u2217=Kyi=Kyj for any\nclass yi and class yj. In this case, PaCo reduces to an im-\nproved version of multi-task with the weighted sum of su-\npervised cross-entropy loss and supervised contrastive loss.\nThe connection",
  "ty of tasks, including:\nmachine translation [20, 21, 22], visual question answering\n[23, 24], action recognition [25, 26], and the like. Dosovit-\nskiy et al. [2] introduced the \ufb01rst stand-alone transformer\nbased model (ViT) for image classi\ufb01cation. In the follow-\ning subsections we brie\ufb02y revisit ViT and several related\nworks.\n2.1. Vision Transformer\nVision Transformers (ViT) were introduced as a way to\ncompete with CNNs on image classi\ufb01cation tasks and uti-\nlize the bene\ufb01ts of attention within ",
  ".1\n80.6\nDeiT-T\u2697[72]\n5.7\n2.6\n224\n74.5\n91.9\n82.1\nLocalViT-T [39]\n5.9\n2.6\n224\n74.8\n92.6\n-\nLocalViT-T2T [39]\n4.3\n2.4\n224\n72.5\n-\n-\nConT-Ti [84]\n5.8\n1.6\n224\n74.9\n-\n-\nPiT-Ti [26]\n4.9\n1.4\n224\n73.0\n-\n-\nT2T-ViT-7 [88]\n4.3\n1.2\n224\n71.7\n90.9\n79.7\nViTAE-T\n4.8\n1.5\n224\n75.3\n92.7\n82.9\nViTAE-T-Stage\n4.8\n2.3\n224\n76.8\n93.5\n84.0\nCeiT-T [87]\n6.4\n2.4\n224\n76.4\n93.4\n83.6\nConViT-Ti [15]\n6.0\n2.0\n224\n73.1\n-\n-\nCrossViT-Ti [6]\n6.9\n3.2\n224\n73.4\n-\n-\nViTAE-6M\n6.5\n2.0\n224\n77.9\n94.1\n84.9\nPVT-T [76]\n13.2\n3.8\n224\n75.1\n-\n-\nLocalViT",
  " at the end of training.\nB\nTransfer Experiment Details\nB.1\nAdditional \ufb01ne-tuning datasets\nAlongside \ufb01netuning on ImageNet (ILSVRC2012[16]), we also train on four other datasets shown in\nTable 5. For the Visual Task Adaptation Benchmark (VTAB[70]), we \ufb01netune on 19 datasets with\n1 000 datapoints per class. We refer interested readers to the original work by Zhai et al. [70] for\nmore details, but in brief, the benchmark consists of 3 task categories:\n\u2022 Natural tasks CalTech101 [41] \u22c5CIFAR100 [37] ",
  "ly separable self-attention shares\nsome similarities with them.\nPositional Encodings. Most vision transformers use absolute/relative positional encodings, de-\npending on downstream tasks, which are based on sinusoidal functions [14] or learnable [1, 2].\nIn CPVT [9], the authors propose the conditional positional encodings, which are dynamically\nconditioned on the inputs and show better performance than the absolute and relative ones.\n3\nOur Method: Twins\nWe present two simple yet powerful spatial",
  "race Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig\nAdam. Searching for MobileNetV3. In ICCV, 2019. 2, 3, 7,\n12\n[30] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam.\nMobilenets: Ef\ufb01cient con-\nvolutional neural networks for mobile vision applications.\narXiv:1704.04861, 2017. 1, 7\n[31] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation\nnetwor",
  "en conducting the soft split, the size of each patch is\nk\u00d7k with s overlapping and p padding on the image, where\nk\u2212s is similar to the stride in convolution operation. So for\nthe reconstructed image I \u2208Rh\u00d7w\u00d7c, the length of output\ntokens To after soft split is\nlo =\n\u0016h + 2p \u2212k\nk \u2212s\n+ 1\n\u0017\n\u00d7\n\u0016w + 2p \u2212k\nk \u2212s\n+ 1\n\u0017\n.\n(3)\nEach split patch has size k \u00d7 k \u00d7 c. We \ufb02atten all patches in\nspatial dimensions to tokens To \u2208Rlo\u00d7ck2. After the soft\nsplit, the output tokens are fed for the next T2T process.\nT2T ",
  " and Events, 2018.\nZhao, H., Jia, J., and Koltun, V. Exploring self-attention for\nimage recognition. In Proceedings of IEEE Conference\non Computer Vision and Pattern Recognition (CVPR),\n2020.\nZoran, D., Chrzanowski, M., Huang, P.-S., Gowal, S., Mott,\nA., and Kohli, P. Towards robust image classi\ufb01cation us-\ning sequential attention models. In Proceedings of IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2020.\n\n--------------------------------------------------\nPerceiver: Gener",
  "ximation to a single sample of the\nheteroscedastic output layer is given in Eq. (6).\nsk(W \u22bax + V \u03f5) \u2248sk(W \u22bax) + \u2207sk(W \u22bax)\u22baV \u03f5\n+ 1\n2\u03f5\u22baV \u22ba\u22072sk(W \u22bax)V \u03f5\n(6)\nwhere, \u03f5 \u223cN(0K, IK\u00d7K).\nMarginalizing over \u03f5, the second term vanishes as E [\u03f5] =\n0. Hence the second order Taylor series approximation to\nthe likelihood is:\nE\u03f5 [sk(W \u22bax + V \u03f5)] \u2248sk(W \u22bax) + 1\n2tr(\u22072sk(W \u22bax)V V \u22ba)\n(7)\nLemma C.1 The Hessian of sk, Hsk, has the following\nstructure:\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n...\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\nsk(1 \u2212sk)(1 \u22122sk) \u00b7 \u00b7 \u00b7 \u2212sjsk(1",
  "es as the global image representa-\ntion. Moreover, since self-attention in the transformer en-\ncoder is position-agnostic and vision applications highly re-\nquire position information, ViT adds position embedding\ninto each token, including the CLS token. Afterwards, all\ntokens are passed through stacked transformer encoders and\nthe CLS token is used for \ufb01nal classi\ufb01cation.\nThe transformer is composed of a series of stacked en-\ncoders where each encoder consists of two modules, namely,\na multi-he",
  "onal neural\nnetworks. In ICML, pages 6105\u20136114. PMLR, 2019. 7, 8\n[25] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv\u00e9 J\u00e9gou. Training data-ef\ufb01cient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020. 1, 2, 3, 6, 7, 8, 9, 13, 14\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.0",
  "e, i.e. assigning one\nor several, rather than all, models conditionally to make the prediction. Some recently proposed\nconditional computation methods [11, 12, 7] achieve remarkable performance with low computation\ncost based on dynamic convolution. Nonetheless, they are not so hardware-friendly. Overall, the\nPreprint. Under review.\narXiv:2107.03815v1  [cs.CV]  8 Jul 2021\n\n--------------------------------------------------\n\ud835\udc63\ud835\udc63\n\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3f\ud835\udc3fCE\n\ud835\udc73\ud835\udc73\ud835\udc73\ud835\udc73\ud835\udc73\ud835\udc73\ud835\udc73\ud835\udc73\ud835\udc7a\ud835\udc7a\nDelegator\nWeight \nGeneration\nOnly For Training\n0",
  "harambe, and Laurens van der Maaten.\nExploring the limits of weakly supervised\npretraining. In ECCV, 2018.\nM. Nilsback and A. Zisserman. Automated \ufb02ower classi\ufb01cation over a large number of classes. In\nICVGIP, 2008.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR,\n2012.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In ICML, 2018.\nB. T. Polyak and A. B. Juditsky. Acceleratio",
  "works by optimizing quantization intervals with task loss.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 4350\u20134359.\nKingma, D. P.; and Ba, J. 2015. Adam: A method for stochas-\ntic optimization. In Proc. of the International Conference on\nLearning Representations.\nKrishnamoorthi, R. 2018. Quantizing deep convolutional\nnetworks for ef\ufb01cient inference: A whitepaper. arXiv preprint\narXiv:1806.08342.\nKrizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple\nla",
  "full \ufb01ne-tuning metrics in absolute terms. Moreover, at inference time, the\nV-MoE models can be adjusted to either (i) match the performance of the largest dense model while\nusing as little as half of the amount of compute, or actual runtime, or (ii) signi\ufb01cantly outperform it\nat the same cost.\nBatch Prioritized Routing. We propose a new priority-based routing algorithm that allows V-MoEs\nto discard the least useful patches. Thus, we devote less compute to each image. In particular, we\nshow V-Mo",
  "abels within each SSAL group\ncan be used to identify which low-level features are respon-\nsible for the prediction.\nFigure 6. CAM w.r.t. each auxiliary branch gi of the SSAL model.\ngf denotes the \ufb01nal classi\ufb01cation output of the SSAL model, and\norg is the CAM of a normal Resnet50. Labels within predicted\nSSAL groups are shown below each branch.\nFigure 6 shows the Class Activation Mapping [33] of two\nexamples: a false-, and a true-positive. For the former, pre-\ndicted SSAL groups contain labels w",
  " al., 2017; Bernstein et al., 2020;\nYou et al., 2019), which ignore the scale of the gradient by\nchoosing an adaptive learning rate inversely proportional to\nthe gradient norm. In particular, You et al. (2017) propose\nLARS, a momentum variant which sets the norm of the\nparameter update to be a \ufb01xed ratio of the parameter norm,\ncompletely ignoring the gradient magnitude. AGC can be\ninterpreted as a relaxation of normalized optimizers, which\nimposes a maximum update size based on the parameter\nnor",
  " the baseline minus some\nLeViT component (1st column: experiment id). The train-\ning is run for 100 epochs only.\nare the pooling-based vision transformer (PiT) [59] and\nCvT [60], ViT variants with a pyramid structure. PiT, the\nmost promising one, incorporates many of the optimiza-\ntion ingredients for DeiT but is still 1.2\u00d7 to 2.4\u00d7 slower\nthan LeViT.\nAlternaltive evaluations.\nIn Table 3 we evaluate LeViT\non alternative test sets, Imagenet Real [63] and Ima-\ngenet V2 matched frequency [64]. The t",
  "ur teacher supervision is an ensemble of\nmultiple networks, it is not convenient to obtain the interme-\ndiate outputs. Also, to make the whole framework neater,\nwe only adopt the similarity loss and discriminator on the\n\ufb01nal outputs of networks for distillation. We show from our\nexperimental results that supervision from the last layer of\nteacher ensemble is competent to distill a strong student.\n5.1. Model Capacity and Weight Decay\nWeight decay is a widely used regularization technique\nin neura",
  "variant\nwhich we refer to as Split-Attention Network (ResNeSt).\nWe benchmark the performance of the proposed\nResNeSt networks on ImageNet dataset [14].\nThe pro-\nposed ResNeSt achieves better speed-accuracy trade-offs\nthan state-of-the-art CNN models produced via neural ar-\nchitecture search [56] as shown in Table 2. In addition, we\nalso study the transfer learning results on object detection,\ninstance segmentation and semantic segmentation. The pro-\nposed ResNeSt has achieved superior performanc",
  "2] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European\nconference on computer vision, pages 649\u2013666. Springer, 2016. 3\n[63] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised\nlearning by cross-channel prediction. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 1058\u20131067, 2017. 3\n[64] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural net-\nworks w",
  " 5,000 examples for validation. The\ncontroller\u2019s reward is the accuracy of the dropout pattern on\nthe validation set. We train each dropout pattern on CIFAR-\n10 for 32,000 steps, and train each pattern on ImageNet for\n16,000 steps. Under these settings, each dropout pattern trains\nin approximately 40 minutes on both datasets. Our search\nexplores 16,384 patterns for each task.\nBaselines.\nFor WRN-28-10 and ResNet-50, we compare\nAutoDropout against DropBlock (Ghiasi, Lin, and Le 2018),\nsince DropBl",
  "i = [2, 5, 13, 1]\nwi = [72, 216, 576, 1512]\ng = 24, b = 1, e = 20.7%\nwa = 43, w0 = 80, wm = 2.7\n0\n3\n6\n9\n12\n15\n18\n21\n32\n64\n128\n256\n512\n1024\n2048\nRegNetY-4.0GF\ndi = [2, 6, 12, 2]\nwi = [128, 192, 512, 1088]\ng = 64, b = 1, e = 20.5%\nwa = 31, w0 = 96, wm = 2.2\n0\n3\n6\n9\n12\n15\n18\n21\n24\n32\n64\n128\n256\n512\n1024\n2048\nRegNetY-6.4GF\ndi = [2, 7, 14, 2]\nwi = [144, 288, 576, 1296]\ng = 72, b = 1, e = 19.9%\nwa = 33, w0 = 112, wm = 2.3\n0\n2\n4\n6\n8\n10\n12\n14\n16\nblock index\n64\n128\n256\n512\n1024\n2048\n4096\nwidth\nRegNetY-8.",
  "ose to progressively structurize the image to tokens to\ncapture rich local structural patterns. Nevertheless, the pre-\nvious literature all assumes the same architecture to the NLP\ntask, without the adaptation to the image recognition tasks.\nIn this paper, we propose several simple yet effective mod-\ni\ufb01cations to improve the scalability of current ViT models.\nEf\ufb01cient Transformers.\nTransformer-based models are\nresource-hungry and compute-intensive despite their state-\nof-the-art performance. We ",
  " consistently\nwith model scaling.\n4. Experiments\nIn this section, we mainly introduce our experiments\non ImageNet-1k, CIFAR, Flowers-102, MS COCO and\nADE20K benchmark. We \ufb01rst show ablation studies on dif-\nferent convolution modules in our ConvMLP framework to\nevaluate their effectiveness. Then, we compare ConvMLP\nto other state-of-the-art models on ImageNet-1k. We then\n4\n\n--------------------------------------------------\nModel\nBackbone\n# Params (M)\nGMACs\nTop-1 (%)\nAcc/GMACs\nAcc/MParams\nSmall m",
  "is involves two major dif\ufb01culties. First, for a deep network with realistic\nsize, the Jacobians are typically prohibitively large to compute and store. For instance, for a layer\nconverting an input tensor of dimension 32 \u00d7 32 \u00d7 80 (e.g., height \u00d7 width \u00d7 channels) to an output\nof the same shape, the resulting Jacobian will have dimension 81920 \u00d7 81920, which needs 25GB\nof memory to store. Second, even if we can store this Jacobian, inverting it would be an extremely\nexpensive (cubic complexity) ",
  "Goyal, Samira Ebrahimi Kahou, Vincent Michal-\nski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\nValentin Haenel, Ingo Fruend, Peter Yianilos, Moritz\nMueller-Freitag, et al. The \u201cSomething Something\" video\ndatabase for learning and evaluating visual common sense.\nIn ICCV, 2017. 2, 6, 7, 14\n[39] Chunhui Gu, Chen Sun, David A. Ross, Carl Von-\ndrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijaya-\nnarasimhan, George Toderici, Susanna Ricco, Rahul Suk-\nthankar, Cordelia Schmid, and Jitendra Mal",
  "very large batch sizes, containing suf\ufb01ciently many positive\nand negative examples, and consequently is very computa-\ntionally expensive, e.g., requiring between 800\u20131000 epochs\nof pre-training to learn state-of-the-art representations on\nImageNet. Some recent works have demonstrated that the\nbatch-size requirements can be reduced at the expense of\nmaintaining an additional memory bank [32, 29, 30, 4, 3].\nFurther performance bene\ufb01ts have been obtained by distilling\nvery large pre-trained teacher",
  "h re-\nspect to different channels into doubt.\nTo conquer the aforementioned limitations, we present\nthe operation coined as involution that has symmetrically\ninverse inherent characteristics compared to convolution,\nnamely, spatial-speci\ufb01c and channel-agnostic. Concretely\nspeaking, involution kernels are distinct in the spatial ex-\ntent but shared across channels. Being subject to its spatial-\nspeci\ufb01c peculiarity, if involution kernels are parameterized\nas \ufb01xed-sized matrices like convolution ke"
]