{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask GPT for answers for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/Users/seanhuang/Grad/adapter/LLMAdapter/SummaryNScores/formatted_scores.json\n",
    "#/Users/seanhuang/Grad/\n",
    "formatted_pdfs = \"/Users/seanhuang/Grad/adapter/LLMAdapter/SummaryNScores/formatted_scores.json\"\n",
    "receipt_pdfs = \"/Users/seanhuang/Grad/adapter/LLMAdapter/SummaryNScores/receipt_scores.json\"\n",
    "report_pdfs = \"/Users/seanhuang/Grad/adapter/LLMAdapter/SummaryNScores/report_scores.json\"\n",
    "random_pdfs = \"/Users/seanhuang/Grad/adapter/LLMAdapter/SummaryNScores/random_scores.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs with formats 4840\n",
      "PDFs of recipts that need ocr 2441\n",
      "PDFs of well written long papers, no ocr 98\n",
      "PDFs of all sorts 600\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/WithFormat/Company Documents Dataset/CompanyDocuments/Inventory Report/monthly-Category/monthly-Category/StockReport_2016-07_1_normal.txt': {'Scores': '8', 'Chunks': 'Stock Report for 2016-07 \\n Category : Beverages  \\n id category : 1  \\nProduct\\nUnits Sold\\nUnits in Stock\\nUnit Price\\nChang\\n105\\n17\\n19\\nGuaraná Fantástica\\n43\\n20\\n4.5\\nSteeleye Stout\\n20\\n20\\n18\\nChartreuse verte\\n48\\n69\\n18\\nOutback Lager\\n41\\n15\\n15\\nLakkalikööri\\n15\\n57\\n18\\n\\n--------------------------------------------------\\n', 'Summaries': 'Stock Report for 2016-07 \\n id category : 1 \\nBeverages: \\n- Guaraná Fantástica: 43 units sold, 20 in stock, $4.5/unit \\n- Steeleye Stout: 20 units sold, 20 in stock, $18/unit \\n- Chartreuse verte: 48 units sold, 69 in stock, $18/unit \\n- Outback Lager: 41 units sold, 15 in stock, $15/unit \\n- Lakkalikööri: 15 units sold, 57 in stock, $18/unit'}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Receipts/2017/de/public transport/118NP8_normal.txt': {'Scores': '8\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4', 'Chunks': 'Online-Ticket\\nIC/EC Fahrkarte\\nFahrtantritt am 10.11.2017\\nFlexpreis (Einfache Fahrt)\\nKlasse:\\n1\\nErw:\\n1, mit 1 BC50\\nHinfahrt:\\nHamburg+City \\n Bonn-Beuel+City, mit IC/EC\\nÜber:\\nVIA: HB*MS*(HA/D)*K\\nUmtausch/Erstattung kostenlos bis 1 Tag vor Reiseantritt (Hinfahrt).\\nZahlungspositionen und Preis\\nPositionen\\nPreis\\nMwSt (D) 19%\\nMwSt D: 7%\\nIC/EC Fahrkarte\\n1\\n78,00€\\n78,00€\\n12,45€\\nZahlungsmittelentgelt 1\\n0,75€\\n0,75€\\n0,12€\\nSumme\\n78,75€\\n78,75€\\n12,57€\\nKreditkartenzahlung\\nBetrag\\n78,75€\\nVU-Nr\\n4556695619\\nTransaktion\\n}<->{\\n,75€\\n0,12€\\nSumme\\n78,75€\\n78,75€\\n12,57€\\nKreditkartenzahlung\\nBetrag\\n78,75€\\nVU-Nr\\n4556695619\\nTransaktions-Nr\\n705427\\nDatum\\n10.11.2017\\nGen-Nr\\nN49HF8\\nIhre Kreditkarte wurde mit dem oben genannten Betrag belastet. Die Buchung Ihres\\nOnline-Tickets erfolgte am 10.11.2017 14:25 Uhr. DB Fernverkehr AG/DB Regio AG,\\nStephensonstr. 1, 60326 Frankfurt, Steuernummer: 29/001/60002.\\nBarcode bitte nicht knicken!               \\nHinfahrt:\\nGültig ab:\\n10.11.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\n118NP8\\nIh\\n}<->{\\n          \\nHinfahrt:\\nGültig ab:\\n10.11.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\n118NP8\\nIhre Reiseverbindung und Reservierung Hinfahrt am 10.11.2017\\nHalt\\nDatum\\nZeit\\nGleis\\nProdukte\\nReservierung\\nHamburg Hbf\\n10.11.\\nab 14:46 11\\nBonn-Beuel\\n10.11.\\nan 19:10 1\\nIC 2213\\nWichtige Nutzungshinweise:\\n- Ihre Fahrkarte gilt nur zusammen mit einem amtlichen Lichtbildausweis (z.B. Personalausweis) oder Ihrer BahnCard.\\n- Mit Ihrer Fahrkarte zum Flexpreis können Sie jeden Zug der gewählten Verbindung nutz\\n}<->{\\nrer BahnCard.\\n- Mit Ihrer Fahrkarte zum Flexpreis können Sie jeden Zug der gewählten Verbindung nutzen: mit einer IC/EC-Fahrkarte alle IC- und EC-Züge, mit\\neiner ICE-Fahrkarte auch alle anderen Züge.\\n- Das Online-Ticket gilt nur für den unter \"Fahrkarte\" angegebenen Reiseabschnitt. Die Übersicht \"Ihre Reiseverbindung\" enthält gegebenenfalls\\nReiseinformationen zu Teilstrecken (z.B. Bus oder Straßenbahn), für die eine weitere Fahrkarte erforderlich sein kann.\\n- Wenn Ihr Ticket den Zusatz \"+City\" o\\n}<->{\\nbahn), für die eine weitere Fahrkarte erforderlich sein kann.\\n- Wenn Ihr Ticket den Zusatz \"+City\" oder \"City mobil\" zeigt, gilt dieser nur am Tag der Hinfahrt bzw. am Tag der Rückfahrt.\\n- Es gelten die nationalen und internationalen Beförderungsbedingungen der DB AG. Innerhalb von Verkehrsverbünden und Tarifgemeinschaften\\ngelten deren Bedingungen. Alle Bedingungen finden Sie unter: www.bahn.de/agb und www.diebefoerderer.de.\\nIhre Reisedaten können sich kurzfristig durch Bauarbeiten oder andere e\\n}<->{\\nb und www.diebefoerderer.de.\\nIhre Reisedaten können sich kurzfristig durch Bauarbeiten oder andere erforderliche Fahrplananpassungen ändern.\\nBitte informieren Sie sich kurz vor Ihrer Reise über mögliche Änderungen Ihrer Reisedaten unter www.bahn.de/reiseplan oder mobil über die\\nApp DB Navigator. Achten Sie auch auf Informationen und Ansagen im Zug und am Bahnhof. Wir danken Ihnen für Ihre Buchung und wünschen\\nIhnen eine angenehme Reise!.\\n118NP8\\nSeite 1 / 1\\n\\n--------------------------------------\\n}<->{\\nund wünschen\\nIhnen eine angenehme Reise!.\\n118NP8\\nSeite 1 / 1\\n\\n--------------------------------------------------\\n', 'Summaries': 'Online-Ticket \\n- Fahrkarte: IC/EC \\n- Reiseantritt: 10.11.2017 \\n- Klasse: Erw. Klasse 1 \\n- Preis: 78,75€ (mit MwSt) \\n- Zahlungsmittelentgelt: 0,75€\\n}<->{\\nA German receipt summary:\\n\\nYou were charged €78,75 for a ticket purchase.\\nDate: 10.11.2017\\nTime: 14:25 Uhr\\nPayment method: Credit card (Kreditkartenzahlung)\\nVendor: DB Fernverkehr AG/DB Regio AG\\nAddress: Stephensonstr. 1, 60326 Frankfurt\\n}<->{\\nDeine Hinfahrt:\\n- Gültig: 10.11.2017\\n- Reservierung: Auftragsnummer 118NP8, Herr Jens Walter\\n- Halt:\\n  - Hamburg Hauptbahnhof: 14:46 Uhr, Gleis 11\\n  - Bonn-Beuel: 19:10 Uhr, Gleis 1\\n- Produkte:\\n  - IC 2213\\n}<->{\\nRer BahnCard: Mit Flexpreis-Ticket nutzen Sie jeden Zug der gewählten Verbindung, entweder alle IC/EC-Züge mit IC/EC-Fahrkarte oder andere Züge mit ICE-Fahrkarte. Online-Ticket gilt nur für den angegebenen Reiseabschnitt und kann bei Teilstrecken (z.B. Bus) zusätzliche Fahrkarten erfordern.\\n}<->{\\nFür eine weitere Fahrkarte kann die Bahn gebucht werden. Das Ticket zeigt nur am Tag der Hinfahrt bzw. Rückfahrt den Zusatz \"+City\" oder \"City mobil\". Die Beförderungsbedingungen gelten nach DB AG und Verkehrsverbünden/Tarifgemeinschaften.\\n}<->{\\nDie Deutsche Bahn (DB) kann Ihre Reisedaten kurzfristig ändern wegen Bauarbeiten oder Fahrplananpassungen. Informieren Sie sich vor Ihrer Reise unter www.bahn.de/reiseplan oder in der DB-App über mögliche Änderungen und achten auch auf Informationen im Zug und am Bahnhof.\\n}<->{\\nThere is not enough text to provide a summary. The given text appears to be part of an email or message in German, but it lacks relevant information for summarization. \\n\\nIf you could provide more context or the actual content, I would be happy to assist you further.'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#load and sort the data inferenced by on device model\n",
    "with open(formatted_pdfs, \"r\") as f1, open(receipt_pdfs, \"r\") as f2, open(report_pdfs, \"r\") as f3, open(random_pdfs, \"r\") as f4:\n",
    "    formatted = json.load(f1)\n",
    "    receipts = json.load(f2)\n",
    "    reports = json.load(f3)\n",
    "    randoms = json.load(f4)\n",
    "    print(\"PDFs with formats\",len(formatted))\n",
    "    print(\"PDFs of recipts that need ocr\", len(receipts))\n",
    "    print(\"PDFs of well written long papers, no ocr\",len(reports))\n",
    "    print(\"PDFs of all sorts\",len(randoms))\n",
    "formatted = sorted(formatted, key=lambda x: list(x.keys())[0])\n",
    "receipts = sorted(receipts, key=lambda x: list(x.keys())[0])\n",
    "reports = sorted(reports, key=lambda x: list(x.keys())[0])\n",
    "randoms = sorted(randoms, key=lambda x: list(x.keys())[0])\n",
    "print (formatted[0])\n",
    "print (receipts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training will happen after the first summary for both ocr detection and text extraction versions. \n",
    "##### The input now is no longer gibberish and not natural language.  \n",
    "##### Now we need to suppress the apologies/commentaries on sensitive data and gibberish text. There is no need to train on the initial summary process since it will skew the model into detecting random gibberish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the scores to average and omit the scores when summaries are not usable\n",
    "def get_average_scores(pdf, separator : str, filtered_txt = [\"\"]):\n",
    "    #print(filtered_txt)\n",
    "    file_name = list(pdf.keys())[0]\n",
    "    values = list(pdf.values())[0]\n",
    "    scoretxt =  values.get(\"Scores\")\n",
    "    scoretxt = scoretxt.strip()\n",
    "    scores = scoretxt.split(separator)\n",
    "    score = 0.0\n",
    "    count = 0\n",
    "    summaries = values.get(\"Summaries\")\n",
    "    summaries = summaries.split(separator)\n",
    "    for s, txt in zip(scores, summaries): \n",
    "        lower_txt = txt.lower()\n",
    "        #print(s , txt)\n",
    "        for filt in filtered_txt:\n",
    "            if filt.lower() in lower_txt:\n",
    "                #print(f\"In file {file_name} filtered\", txt)\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    score += float(s)\n",
    "                    count += 1\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return 0 if count == 0 else score/count\n",
    "\n",
    "def count_pdf_available_and_filtered_chunks(pdf, separator : str, filtered_txt = [\"\"]):\n",
    "    file_name = list(pdf.keys())[0]\n",
    "    available_chunks = []\n",
    "    filtered_chunks = []\n",
    "    values = list(pdf.values())[0]\n",
    "    chunks = values.get(\"Chunks\").split(separator)\n",
    "    summaries = values.get(\"Summaries\").split(separator)\n",
    "    for c ,summary  in zip(chunks,summaries): \n",
    "        lower_txt = summary.lower()\n",
    "        if any(filt.lower() in lower_txt for filt in filtered_txt):\n",
    "            #print(f\"In file {file_name} filtered\", txt)\n",
    "            filtered_chunks.append(c)\n",
    "        else:\n",
    "            available_chunks.append(c)\n",
    "    #print(\"ava vs filt:\",len(available_chunks), len(filtered_chunks))\n",
    "    return available_chunks, filtered_chunks\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higher_scored_pdf(pdfs, separator : str, filtered_txt = [\"\"]):\n",
    "    if len(pdfs) == 0:\n",
    "        return None\n",
    "    solution = None\n",
    "    score = 0 \n",
    "    for pdf in pdfs:\n",
    "        if solution == None:\n",
    "            solution = pdf\n",
    "            score = get_average_scores(pdf, separator,filtered_txt=filtered_txt)\n",
    "        else:\n",
    "            new_score = get_average_scores(pdf, separator, filtered_txt=filtered_txt)\n",
    "            if new_score > score:\n",
    "                solution = pdf\n",
    "                score = new_score\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "## get sorted adjacent pdfs that originated from the same source\n",
    "def get_adjacent_pdfs(pdfs, start, reg = r'_(normal|ocr)\\.txt$'):\n",
    "    adjacent = []\n",
    "    i = start\n",
    "    while i < len(pdfs):\n",
    "        if i == start:\n",
    "            adjacent.append(pdfs[i])\n",
    "        else:\n",
    "            prev = re.sub(reg , '', list(adjacent[-1].keys())[0])\n",
    "            current = re.sub(reg , '', list(pdfs[i].keys())[0])\n",
    "            if prev == current:\n",
    "                adjacent.append(pdfs[i])\n",
    "            else:\n",
    "                break\n",
    "        i += 1\n",
    "    return adjacent, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_best_version(pdfs, separator: str, filtered_txt):\n",
    "    higher_scored_pdfs = []\n",
    "    higher_scored_filtered = []\n",
    "    i = 0\n",
    "    useful_chunks = []\n",
    "    filtered_chunks = []\n",
    "    while i < len(pdfs):\n",
    "        start = i\n",
    "        adjacents, i = get_adjacent_pdfs(pdfs, start)\n",
    "        # for f in adjacents:\n",
    "        #     print(f\"PDF from: {start} to {i}:\",f)\n",
    "        winner = get_higher_scored_pdf(adjacents, separator=separator, filtered_txt=filtered_txt)\n",
    "        useful, filtered = count_pdf_available_and_filtered_chunks(winner, separator=separator, filtered_txt=filtered_txt) \n",
    "        if len(filtered) != 0:\n",
    "            higher_scored_filtered.append(winner)\n",
    "        higher_scored_pdfs.append(winner)\n",
    "        #print(\"Incoming:\", len(useful), len(filtered))\n",
    "        useful_chunks.extend(useful)\n",
    "        filtered_chunks.extend(filtered)\n",
    "        #print(\"Current Lenghts:\",len(useful_chunks), len(filtered_chunks))\n",
    "        \n",
    "        #print(\"Winner\", winner)\n",
    "    return higher_scored_pdfs, higher_scored_filtered, useful_chunks, filtered_chunks \n",
    "def get_censored_text(text):\n",
    "    return text.replace(\"CENSORED\", \" \").replace(\"censored\", \" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The resulted data will still contain appologies and commentaries but it should contain the best availale information from the original pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs with formats 2599, that are filtered 366, useful chunks 4340, filtered chunks 366\n",
      "PDFs of recipts that need ocr 1222, that are filtered 709, useful chunks 3807, filtered chunks 1204\n",
      "PDFs of well written long papers, no ocr 98, that are filtered 21, useful chunks 14371, filtered chunks 30\n",
      "PDFs of all sorts 508, that are filtered 188, useful chunks 24405, filtered chunks 694\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/WithFormat/Company Documents Dataset/CompanyDocuments/Inventory Report/monthly-Category/monthly-Category/StockReport_2016-07_1_normal.txt': {'Scores': '8', 'Chunks': 'Stock Report for 2016-07 \\n Category : Beverages  \\n id category : 1  \\nProduct\\nUnits Sold\\nUnits in Stock\\nUnit Price\\nChang\\n105\\n17\\n19\\nGuaraná Fantástica\\n43\\n20\\n4.5\\nSteeleye Stout\\n20\\n20\\n18\\nChartreuse verte\\n48\\n69\\n18\\nOutback Lager\\n41\\n15\\n15\\nLakkalikööri\\n15\\n57\\n18\\n\\n--------------------------------------------------\\n', 'Summaries': 'Stock Report for 2016-07 \\n id category : 1 \\nBeverages: \\n- Guaraná Fantástica: 43 units sold, 20 in stock, $4.5/unit \\n- Steeleye Stout: 20 units sold, 20 in stock, $18/unit \\n- Chartreuse verte: 48 units sold, 69 in stock, $18/unit \\n- Outback Lager: 41 units sold, 15 in stock, $15/unit \\n- Lakkalikööri: 15 units sold, 57 in stock, $18/unit'}}\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/WithFormat/Company Documents Dataset/CompanyDocuments/Inventory Report/monthly-Category/monthly-Category/StockReport_2016-07_2_normal.txt': {'Scores': '8', 'Chunks': \"Stock Report for 2016-07 \\n Category : Condiments  \\n id category : 2  \\nProduct\\nUnits Sold\\nUnits in Stock\\nUnit Price\\nChef Anton's Gumbo Mix\\n77\\n0\\n21.35\\nLouisiana Fiery Hot Pepper Sauce\\n35\\n76\\n21.05\\nOriginal Frankfurter grüne Soße\\n27\\n32\\n13\\n\\n--------------------------------------------------\\n\", 'Summaries': \"Stock Report for 2016-07:\\n\\nCondiments Category:\\n- Chef Anton's Gumbo Mix: 77 units sold, 0 in stock, $21.35/unit price\\n- Louisiana Fiery Hot Pepper Sauce: 35 units sold, 76 in stock, $21.05/unit price\\n- Original Frankfurter grüne Soße: 27 units sold, 32 in stock, $13/unit price\"}}\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/WithFormat/Company Documents Dataset/CompanyDocuments/Inventory Report/monthly-Category/monthly-Category/StockReport_2016-07_3_normal.txt': {'Scores': '6', 'Chunks': \"Stock Report for 2016-07 \\n Category : Confections  \\n id category : 3  \\nProduct\\nUnits Sold\\nUnits in Stock\\nUnit Price\\nPavlova\\n95\\n29\\n17.45\\nSir Rodney's Marmalade\\n40\\n40\\n81\\nSir Rodney's Scones\\n30\\n3\\n10\\nSchoggi Schokolade\\n25\\n49\\n43.9\\nMaxilaku\\n40\\n10\\n20\\nTarte au sucre\\n15\\n17\\n49.3\\n\\n--------------------------------------------------\\n\", 'Summaries': \"Stock Report for 2016-07 \\n\\nCategory: Confections, id category: 3 \\nProduct Units Sold Units in Stock Unit Price \\nPavlova 95 29 17.45 \\nSir Rodney's Marmalade 40 40 81 \\nSir Rodney's Scones 30 3 10 \\nSchoggi Schokolade 25 49 43.9 \\nMaxilaku 40 10 20 \\nTarte au sucre 15 17 49.3\"}}\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/WithFormat/Company Documents Dataset/CompanyDocuments/Inventory Report/monthly-Category/monthly-Category/StockReport_2016-07_4_normal.txt': {'Scores': '6', 'Chunks': 'Stock Report for 2016-07 \\n Category : Dairy Products  \\n id category : 4  \\nProduct\\nUnits Sold\\nUnits in Stock\\nUnit Price\\nQueso Cabrales\\n12\\n22\\n21\\nQueso Manchego La Pastora\\n12\\n86\\n38\\nGorgonzola Telino\\n20\\n0\\n12.5\\nMascarpone Fabioli\\n6\\n9\\n32\\nGeitost\\n85\\n112\\n2.5\\nRaclette Courdavault\\n100\\n79\\n55\\nCamembert Pierrot\\n40\\n19\\n34\\nMozzarella di Giovanni\\n29\\n14\\n34.8\\n\\n--------------------------------------------------\\n', 'Summaries': 'Stock Report for 2016-07 \\n\\nCategory: Dairy Products (id:4)\\n\\nProduct | Units Sold | Units in Stock | Unit Price \\n--------|------------|------------------|-------------\\nQueso Cabrales | 12 | 22 | $21\\nQueso Manchego La Pastora | 12 | 86 | $38\\nGorgonzola Telino | 20 | 0 | $12.5\\nMascarpone Fabioli | 6 | 9 | $32\\nGeitost | 85 | 112 | $2.5\\nRaclette Courdavault | 100 | 79 | $55\\nCamembert Pierrot | 40 | 19 | $34\\nMozzarella di Giovanni | 29 | 14 | $34.8'}}\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/WithFormat/Company Documents Dataset/CompanyDocuments/Inventory Report/monthly-Category/monthly-Category/StockReport_2016-07_5_normal.txt': {'Scores': '7', 'Chunks': \"Stock Report for 2016-07 \\n Category : Grains/Cereals  \\n id category : 5  \\nProduct\\nUnits Sold\\nUnits in Stock\\nUnit Price\\nGustaf's Knäckebröd\\n6\\n104\\n21\\nSingaporean Hokkien Fried Mee\\n10\\n26\\n14\\nGnocchi di nonna Alice\\n2\\n21\\n38\\nRavioli Angelo\\n65\\n36\\n19.5\\n\\n--------------------------------------------------\\n\", 'Summaries': \"Stock Report for 2016-07:\\n\\n- Grains/Cereals (id: 5)\\n \\n- Gustaf's Knäckebröd:\\n    Units Sold: 6\\n    Units in Stock: 104\\n    Unit Price: $21\\n \\n- Singaporean Hokkien Fried Mee:\\n    Units Sold: 10\\n    Units in Stock: 26\\n    Unit Price: $14\\n \\n- Gnocchi di nonna Alice:\\n    Units Sold: 2\\n    Units in Stock: 21\\n    Unit Price: $38\\n \\n- Ravioli Angelo:\\n    Units Sold: 65\\n    Units in Stock: 36\\n    Unit Price: $19.5\"}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Receipts/2017/de/public transport/118NP8_normal.txt': {'Scores': '8\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4', 'Chunks': 'Online-Ticket\\nIC/EC Fahrkarte\\nFahrtantritt am 10.11.2017\\nFlexpreis (Einfache Fahrt)\\nKlasse:\\n1\\nErw:\\n1, mit 1 BC50\\nHinfahrt:\\nHamburg+City \\n Bonn-Beuel+City, mit IC/EC\\nÜber:\\nVIA: HB*MS*(HA/D)*K\\nUmtausch/Erstattung kostenlos bis 1 Tag vor Reiseantritt (Hinfahrt).\\nZahlungspositionen und Preis\\nPositionen\\nPreis\\nMwSt (D) 19%\\nMwSt D: 7%\\nIC/EC Fahrkarte\\n1\\n78,00€\\n78,00€\\n12,45€\\nZahlungsmittelentgelt 1\\n0,75€\\n0,75€\\n0,12€\\nSumme\\n78,75€\\n78,75€\\n12,57€\\nKreditkartenzahlung\\nBetrag\\n78,75€\\nVU-Nr\\n4556695619\\nTransaktion\\n}<->{\\n,75€\\n0,12€\\nSumme\\n78,75€\\n78,75€\\n12,57€\\nKreditkartenzahlung\\nBetrag\\n78,75€\\nVU-Nr\\n4556695619\\nTransaktions-Nr\\n705427\\nDatum\\n10.11.2017\\nGen-Nr\\nN49HF8\\nIhre Kreditkarte wurde mit dem oben genannten Betrag belastet. Die Buchung Ihres\\nOnline-Tickets erfolgte am 10.11.2017 14:25 Uhr. DB Fernverkehr AG/DB Regio AG,\\nStephensonstr. 1, 60326 Frankfurt, Steuernummer: 29/001/60002.\\nBarcode bitte nicht knicken!               \\nHinfahrt:\\nGültig ab:\\n10.11.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\n118NP8\\nIh\\n}<->{\\n          \\nHinfahrt:\\nGültig ab:\\n10.11.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\n118NP8\\nIhre Reiseverbindung und Reservierung Hinfahrt am 10.11.2017\\nHalt\\nDatum\\nZeit\\nGleis\\nProdukte\\nReservierung\\nHamburg Hbf\\n10.11.\\nab 14:46 11\\nBonn-Beuel\\n10.11.\\nan 19:10 1\\nIC 2213\\nWichtige Nutzungshinweise:\\n- Ihre Fahrkarte gilt nur zusammen mit einem amtlichen Lichtbildausweis (z.B. Personalausweis) oder Ihrer BahnCard.\\n- Mit Ihrer Fahrkarte zum Flexpreis können Sie jeden Zug der gewählten Verbindung nutz\\n}<->{\\nrer BahnCard.\\n- Mit Ihrer Fahrkarte zum Flexpreis können Sie jeden Zug der gewählten Verbindung nutzen: mit einer IC/EC-Fahrkarte alle IC- und EC-Züge, mit\\neiner ICE-Fahrkarte auch alle anderen Züge.\\n- Das Online-Ticket gilt nur für den unter \"Fahrkarte\" angegebenen Reiseabschnitt. Die Übersicht \"Ihre Reiseverbindung\" enthält gegebenenfalls\\nReiseinformationen zu Teilstrecken (z.B. Bus oder Straßenbahn), für die eine weitere Fahrkarte erforderlich sein kann.\\n- Wenn Ihr Ticket den Zusatz \"+City\" o\\n}<->{\\nbahn), für die eine weitere Fahrkarte erforderlich sein kann.\\n- Wenn Ihr Ticket den Zusatz \"+City\" oder \"City mobil\" zeigt, gilt dieser nur am Tag der Hinfahrt bzw. am Tag der Rückfahrt.\\n- Es gelten die nationalen und internationalen Beförderungsbedingungen der DB AG. Innerhalb von Verkehrsverbünden und Tarifgemeinschaften\\ngelten deren Bedingungen. Alle Bedingungen finden Sie unter: www.bahn.de/agb und www.diebefoerderer.de.\\nIhre Reisedaten können sich kurzfristig durch Bauarbeiten oder andere e\\n}<->{\\nb und www.diebefoerderer.de.\\nIhre Reisedaten können sich kurzfristig durch Bauarbeiten oder andere erforderliche Fahrplananpassungen ändern.\\nBitte informieren Sie sich kurz vor Ihrer Reise über mögliche Änderungen Ihrer Reisedaten unter www.bahn.de/reiseplan oder mobil über die\\nApp DB Navigator. Achten Sie auch auf Informationen und Ansagen im Zug und am Bahnhof. Wir danken Ihnen für Ihre Buchung und wünschen\\nIhnen eine angenehme Reise!.\\n118NP8\\nSeite 1 / 1\\n\\n--------------------------------------\\n}<->{\\nund wünschen\\nIhnen eine angenehme Reise!.\\n118NP8\\nSeite 1 / 1\\n\\n--------------------------------------------------\\n', 'Summaries': 'Online-Ticket \\n- Fahrkarte: IC/EC \\n- Reiseantritt: 10.11.2017 \\n- Klasse: Erw. Klasse 1 \\n- Preis: 78,75€ (mit MwSt) \\n- Zahlungsmittelentgelt: 0,75€\\n}<->{\\nA German receipt summary:\\n\\nYou were charged €78,75 for a ticket purchase.\\nDate: 10.11.2017\\nTime: 14:25 Uhr\\nPayment method: Credit card (Kreditkartenzahlung)\\nVendor: DB Fernverkehr AG/DB Regio AG\\nAddress: Stephensonstr. 1, 60326 Frankfurt\\n}<->{\\nDeine Hinfahrt:\\n- Gültig: 10.11.2017\\n- Reservierung: Auftragsnummer 118NP8, Herr Jens Walter\\n- Halt:\\n  - Hamburg Hauptbahnhof: 14:46 Uhr, Gleis 11\\n  - Bonn-Beuel: 19:10 Uhr, Gleis 1\\n- Produkte:\\n  - IC 2213\\n}<->{\\nRer BahnCard: Mit Flexpreis-Ticket nutzen Sie jeden Zug der gewählten Verbindung, entweder alle IC/EC-Züge mit IC/EC-Fahrkarte oder andere Züge mit ICE-Fahrkarte. Online-Ticket gilt nur für den angegebenen Reiseabschnitt und kann bei Teilstrecken (z.B. Bus) zusätzliche Fahrkarten erfordern.\\n}<->{\\nFür eine weitere Fahrkarte kann die Bahn gebucht werden. Das Ticket zeigt nur am Tag der Hinfahrt bzw. Rückfahrt den Zusatz \"+City\" oder \"City mobil\". Die Beförderungsbedingungen gelten nach DB AG und Verkehrsverbünden/Tarifgemeinschaften.\\n}<->{\\nDie Deutsche Bahn (DB) kann Ihre Reisedaten kurzfristig ändern wegen Bauarbeiten oder Fahrplananpassungen. Informieren Sie sich vor Ihrer Reise unter www.bahn.de/reiseplan oder in der DB-App über mögliche Änderungen und achten auch auf Informationen im Zug und am Bahnhof.\\n}<->{\\nThere is not enough text to provide a summary. The given text appears to be part of an email or message in German, but it lacks relevant information for summarization. \\n\\nIf you could provide more context or the actual content, I would be happy to assist you further.'}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Receipts/2017/de/public transport/3ZCCCW_normal.txt': {'Scores': '8\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n6', 'Chunks': 'Online-Ticket\\nICE Fahrkarte\\nFahrtantritt am 17.11.2017\\nFlexpreis (Einfache Fahrt)\\nKlasse:\\n1\\nErw:\\n1, mit 1 BC50\\nHinfahrt:\\nHamburg+City \\n Leipzig+City, mit ICE\\nÜber:\\nVIA: (WBE/SDL)*(WB/KOET)\\nUmtausch/Erstattung kostenlos bis 1 Tag vor Reiseantritt (Hinfahrt).\\nZahlungspositionen und Preis\\nPositionen\\nPreis\\nMwSt (D) 19%\\nMwSt D: 7%\\nICE Fahrkarte\\n1\\n87,00€\\n87,00€\\n13,89€\\nZahlungsmittelentgelt 1\\n0,75€\\n0,75€\\n0,12€\\nSumme\\n87,75€\\n87,75€\\n14,01€\\nKreditkartenzahlung\\nBetrag\\n87,75€\\nVU-Nr\\n4556695619\\nTransaktions-Nr\\n}<->{\\n\\n0,12€\\nSumme\\n87,75€\\n87,75€\\n14,01€\\nKreditkartenzahlung\\nBetrag\\n87,75€\\nVU-Nr\\n4556695619\\nTransaktions-Nr\\n710058\\nDatum\\n17.11.2017\\nGen-Nr\\nNURM11\\nIhre Kreditkarte wurde mit dem oben genannten Betrag belastet. Die Buchung Ihres\\nOnline-Tickets erfolgte am 17.11.2017 16:08 Uhr. DB Fernverkehr AG/DB Regio AG,\\nStephensonstr. 1, 60326 Frankfurt, Steuernummer: 29/001/60002.\\nBarcode bitte nicht knicken!               \\nHinfahrt:\\nGültig ab:\\n17.11.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\n3ZCCCW\\nIhre R\\n}<->{\\n      \\nHinfahrt:\\nGültig ab:\\n17.11.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\n3ZCCCW\\nIhre Reiseverbindung und Reservierung Hinfahrt am 17.11.2017\\nHalt\\nDatum\\nZeit\\nGleis\\nProdukte\\nReservierung\\nHamburg Hbf\\n17.11.\\nab 16:36 8\\nBitterfeld\\n17.11.\\nan 19:24 3\\nICE 1517\\nBitterfeld\\n17.11.\\nab 19:32 3\\nLeipzig Hbf\\n17.11.\\nan 20:15\\nS 2\\nWichtige Nutzungshinweise:\\n- Ihre Fahrkarte gilt nur zusammen mit einem amtlichen Lichtbildausweis (z.B. Personalausweis) oder Ihrer BahnCard.\\n- Mit Ihrer Fahrkarte zum Fle\\n}<->{\\namtlichen Lichtbildausweis (z.B. Personalausweis) oder Ihrer BahnCard.\\n- Mit Ihrer Fahrkarte zum Flexpreis können Sie jeden Zug der gewählten Verbindung nutzen: mit einer IC/EC-Fahrkarte alle IC- und EC-Züge, mit\\neiner ICE-Fahrkarte auch alle anderen Züge.\\n- Das Online-Ticket gilt nur für den unter \"Fahrkarte\" angegebenen Reiseabschnitt. Die Übersicht \"Ihre Reiseverbindung\" enthält gegebenenfalls\\nReiseinformationen zu Teilstrecken (z.B. Bus oder Straßenbahn), für die eine weitere Fahrkarte erfor\\n}<->{\\nReiseinformationen zu Teilstrecken (z.B. Bus oder Straßenbahn), für die eine weitere Fahrkarte erforderlich sein kann.\\n- Wenn Ihr Ticket den Zusatz \"+City\" oder \"City mobil\" zeigt, gilt dieser nur am Tag der Hinfahrt bzw. am Tag der Rückfahrt.\\n- Es gelten die nationalen und internationalen Beförderungsbedingungen der DB AG. Innerhalb von Verkehrsverbünden und Tarifgemeinschaften\\ngelten deren Bedingungen. Alle Bedingungen finden Sie unter: www.bahn.de/agb und www.diebefoerderer.de.\\nIhre Reisedate\\n}<->{\\ngungen. Alle Bedingungen finden Sie unter: www.bahn.de/agb und www.diebefoerderer.de.\\nIhre Reisedaten können sich kurzfristig durch Bauarbeiten oder andere erforderliche Fahrplananpassungen ändern.\\nBitte informieren Sie sich kurz vor Ihrer Reise über mögliche Änderungen Ihrer Reisedaten unter www.bahn.de/reiseplan oder mobil über die\\nApp DB Navigator. Achten Sie auch auf Informationen und Ansagen im Zug und am Bahnhof. Wir danken Ihnen für Ihre Buchung und wünschen\\nIhnen eine angenehme Reise!.\\n3\\n}<->{\\nim Zug und am Bahnhof. Wir danken Ihnen für Ihre Buchung und wünschen\\nIhnen eine angenehme Reise!.\\n3ZCCCW\\nSeite 1 / 1\\n\\n--------------------------------------------------\\n', 'Summaries': 'Online-Ticket Details:\\n\\n- ICE Fahrkarte mit Flexpreis für Erwachsene (Klasse 1)\\n- Reiseantritt am 17.11.2017\\n- Hinfahrt: Hamburg+City zu Leipzig+City mit ICE\\n- Über: VIA WBE/SDL oder WB/KOET\\n- Umtausch/Erstattung kostenlos bis 1 Tag vor Reiseantritt\\n- Preise:\\n  - Gesamtpreis: 87,75 €\\n  - MwSt (D): 19%\\n  - Zahlungspositionen und MwSt: \\n    - Position 1: 87,00 €\\n    - Position 2: 13,89 €\\n    - Summe: 87,75 €\\n}<->{\\nDB Fernverkehr AG/DB Regio AG hat den Betrag von 87,75€ auf Ihrer Kreditkarte belastet. Die Transaktion fand am 17.11.2017 um 16:08 Uhr statt und ist mit der VU-Nr. 4556695619 und Transaktionsnr. 710058 gekennzeichnet.\\n}<->{\\nGültig ab 17.11.2017, Zangenabdruck von Herrn Jens Walter (3ZCCCW). Ihre Reiseverbindung: \\n\\n* Hamburg Hbf - Bitterfeld\\n + Abfahrt am 17.11.17 um 16:36 Uhr, Gleis 8\\n + Ankunft am 17.11.17 um 19:24 Uhr, Gleis 3\\n* Bitterfeld - Leipzig Hbf\\n + Abfahrt am 17.11.17 um 19:32 Uhr, Gleis 3\\n + Ankunft am 17.11.17 um 20:15 Uhr\\n}<->{\\nYou need to show a valid ID (e.g. Personalausweis) or BahnCard when buying Flexpreis tickets. The ticket is only valid for the specified route section and includes travel information on other modes of transport for that section, if applicable.\\n}<->{\\nReiseinformationen zu Teilstrecken:\\n\\nFür einige Strecken wie Bus oder Straßenbahn kann eine weitere Fahrkarte erforderlich sein.\\n\\nDie Bedingungen für das Ticket mit Zusatz \"+City\" oder \"City mobil\" gelten nur am Tag der Hinfahrt bzw. Rückfahrt.\\n\\nNationale und internationale Beförderungsbedingungen der DB AG gelten, innerhalb von Verkehrsverbünden und Tarifgemeinschaften den Bedingungen dieser Organisationen.\\n}<->{\\nYour travel conditions can be found at www.bahn.de/agb and www.diebefoerderer.de. \\nTravel dates are subject to change due to construction work or other necessary schedule adjustments. Please check for possible changes on your trip before departure under www.bahn.de/reiseplan, using the DB Navigator app, or notice announcements in the train and at the station.\\n}<->{\\n\"Ticket im Zug, am Bahnhof: Vielen Dank für die Buchung. Wir wünschen Ihnen eine angenehme Reise!\"'}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Receipts/2017/de/public transport/4GBEC9_normal.txt': {'Scores': '8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n6', 'Chunks': 'Online-Ticket\\nICE Fahrkarte\\nFahrtantritt am 03.11.2017\\nFlexpreis (Einfache Fahrt)\\nKlasse:\\n1\\nErw:\\n1, mit 1 BC50\\nHinfahrt:\\nHamburg+City \\n Leipzig+City, mit ICE\\nÜber:\\nVIA: (WBE/SDL)*(WB/KOET)\\nUmtausch/Erstattung kostenlos bis 1 Tag vor Reiseantritt (Hinfahrt).\\nZahlungspositionen und Preis\\nPositionen\\nPreis\\nMwSt (D) 19%\\nMwSt D: 7%\\nICE Fahrkarte\\n1\\n87,00€\\n87,00€\\n13,89€\\nReservierung\\n1\\n0,00€\\nZahlungsmittelentgelt 1\\n0,75€\\n0,75€\\n0,12€\\nSumme\\n87,75€\\n87,75€\\n14,01€\\nKreditkartenzahlung\\nBetrag\\n87,75€\\nVU-Nr\\n45566\\n}<->{\\nentgelt 1\\n0,75€\\n0,75€\\n0,12€\\nSumme\\n87,75€\\n87,75€\\n14,01€\\nKreditkartenzahlung\\nBetrag\\n87,75€\\nVU-Nr\\n4556695619\\nTransaktions-Nr\\n697315\\nDatum\\n03.11.2017\\nGen-Nr\\nN4TM7R\\nIhre Kreditkarte wurde mit dem oben genannten Betrag belastet. Die Buchung Ihres\\nOnline-Tickets erfolgte am 03.11.2017 17:09 Uhr. DB Fernverkehr AG/DB Regio AG,\\nStephensonstr. 1, 60326 Frankfurt, Steuernummer: 29/001/60002.\\nBarcode bitte nicht knicken!               \\nHinfahrt:\\nGültig ab:\\n03.11.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftrags\\n}<->{\\ncht knicken!               \\nHinfahrt:\\nGültig ab:\\n03.11.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\n4GBEC9\\nIhre Reiseverbindung und Reservierung Hinfahrt am 03.11.2017\\nHalt\\nDatum\\nZeit\\nGleis\\nProdukte\\nReservierung\\nHamburg Hbf\\n03.11.\\nab 17:35 6\\nLeipzig Hbf\\n03.11.\\nan 20:43\\nICE 1717\\n1 Sitzplatz, Wg. 28, Pl. 65, 1 Fenster, Abteil,\\nNichtraucher, Res.Nr. 8082 2020 8881 40\\nWichtige Nutzungshinweise:\\n- Ihre Fahrkarte gilt nur zusammen mit einem amtlichen Lichtbildausweis (z.B. Personalausweis) ode\\n}<->{\\n:\\n- Ihre Fahrkarte gilt nur zusammen mit einem amtlichen Lichtbildausweis (z.B. Personalausweis) oder Ihrer BahnCard.\\n- Mit Ihrer Fahrkarte zum Flexpreis können Sie jeden Zug der gewählten Verbindung nutzen: mit einer IC/EC-Fahrkarte alle IC- und EC-Züge, mit\\neiner ICE-Fahrkarte auch alle anderen Züge.\\n- Das Online-Ticket gilt nur für den unter \"Fahrkarte\" angegebenen Reiseabschnitt. Die Übersicht \"Ihre Reiseverbindung\" enthält gegebenenfalls\\nReiseinformationen zu Teilstrecken (z.B. Bus oder Str\\n}<->{\\n \"Ihre Reiseverbindung\" enthält gegebenenfalls\\nReiseinformationen zu Teilstrecken (z.B. Bus oder Straßenbahn), für die eine weitere Fahrkarte erforderlich sein kann.\\n- Wenn Ihr Ticket den Zusatz \"+City\" oder \"City mobil\" zeigt, gilt dieser nur am Tag der Hinfahrt bzw. am Tag der Rückfahrt.\\n- Es gelten die nationalen und internationalen Beförderungsbedingungen der DB AG. Innerhalb von Verkehrsverbünden und Tarifgemeinschaften\\ngelten deren Bedingungen. Alle Bedingungen finden Sie unter: www.bahn.d\\n}<->{\\nnden und Tarifgemeinschaften\\ngelten deren Bedingungen. Alle Bedingungen finden Sie unter: www.bahn.de/agb und www.diebefoerderer.de.\\nIhre Reisedaten können sich kurzfristig durch Bauarbeiten oder andere erforderliche Fahrplananpassungen ändern.\\nBitte informieren Sie sich kurz vor Ihrer Reise über mögliche Änderungen Ihrer Reisedaten unter www.bahn.de/reiseplan oder mobil über die\\nApp DB Navigator. Achten Sie auch auf Informationen und Ansagen im Zug und am Bahnhof. Wir danken Ihnen für Ihre Buch\\n}<->{\\n Achten Sie auch auf Informationen und Ansagen im Zug und am Bahnhof. Wir danken Ihnen für Ihre Buchung und wünschen\\nIhnen eine angenehme Reise!.\\n4GBEC9\\nSeite 1 / 1\\n\\n--------------------------------------------------\\n', 'Summaries': 'Online-Ticket: \\nHamburg+City - Leipzig+City ICE Fahrkarte \\nErwachsener (1) 1 BC50 Klasse 1, Preis 87,00€, inkl. MwSt 19%\\n}<->{\\nEntgelt für Online-Ticket der DB Fernverkehr AG/DB Regio AG:\\n\\n- Betrag: 87,75€\\n- Datum: 03.11.2017\\n- Transaktions-Nr: 697315\\n- Kreditkartenzahlung: VU-Nr 4556695619\\n}<->{\\nReiseinformationen:\\n\\n* gültig ab: 03.11.2017\\n* Auftragsnummer: 4GBEC9\\n* Reiseverbindung:\\n + Hamburg Hbf - Leipzig Hbf am 03.11.\\n   - Abfahrt: 17:35 Uhr, Gleis 6\\n   - Ankunft: 20:43 Uhr, ICE 1717\\n* Sitzplatz: Wg. 28, Pl. 65, 1 Fenster\\n* Produkte: Nichtraucher, Res.Nr. 8082\\n}<->{\\nDeine Fahrkarte kann nur mit einem amtlichen Lichtbildausweis verwendet werden. \\nFlexpreis-Tickets lassen den Zug auswählen, der für die gesamte Verbindung genutzt wird. \\nOnline-Tickets sind nur gültig für den angegebenen Reiseabschnitt.\\n}<->{\\nIhre Reiseverbindung enthält gegebenenfalls zusätzliche Informationen zu Teilstrecken, für die eine weitere Fahrkarte erforderlich sein kann. Bei Tickets mit \"+City\" oder \"City mobil\" gilt diese nur an einem Tag. Die nationalen und internationalen Beförderungsbedingungen der DB AG gelten innerhalb von Verkehrsverbünden und Tarifgemeinschaften.\\n}<->{\\nnden und Tarifgemeinschaften gelten ihren Bedingungen, sie finden sich unter: www.bahn.de/agb und www.diebefoerderer.de.\\nReisedaten können kurzfristig ändern. Informieren Sie sich vor Ihrer Reise über Änderungen unter www.bahn.de/reiseplan oder DB Navigator App.\\n}<->{\\nInformationsanzeige\\n\\nBleiben Sie informiert über Ankünfte, Verzögerungen und andere wichtige Informationen im Zug und am Bahnhof. Vielen Dank für Ihre Buchung, eine angenehme Reise wünschen wir Ihnen.'}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Receipts/2017/de/public transport/96W2GF_normal.txt': {'Scores': '4\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6', 'Chunks': 'Online-Ticket\\nICE Fahrkarte\\nFahrtantritt am 17.12.2017\\nFlexpreis (Einfache Fahrt)\\nKlasse:\\n1\\nErw:\\n1, mit 1 BC50\\nHinfahrt:\\nLeipzig+City \\n Hamburg+City, mit ICE\\nÜber:\\nVIA: (KOET*P/WB)*BSP*(SDL*UE/WBE*LWL)\\nUmtausch/Erstattung kostenlos bis 1 Tag vor Reiseantritt (Hinfahrt).\\nZahlungspositionen und Preis\\nPositionen\\nPreis\\nMwSt (D) 19%\\nMwSt D: 7%\\nICE Fahrkarte\\n1\\n89,50€\\n89,50€\\n14,29€\\nReservierung\\n1\\n0,00€\\nSumme\\n89,50€\\n89,50€\\n14,29€\\nKreditkartenzahlung\\nBetrag\\n89,50€\\nVU-Nr\\n4556695619\\nTransaktions-Nr\\n742382\\n\\n}<->{\\numme\\n89,50€\\n89,50€\\n14,29€\\nKreditkartenzahlung\\nBetrag\\n89,50€\\nVU-Nr\\n4556695619\\nTransaktions-Nr\\n742382\\nDatum\\n17.12.2017\\nGen-Nr\\nNIZAMR\\nIhre Kreditkarte wurde mit dem oben genannten Betrag belastet. Die Buchung Ihres\\nOnline-Tickets erfolgte am 17.12.2017 16:30 Uhr. DB Fernverkehr AG/DB Regio AG,\\nStephensonstr. 1, 60326 Frankfurt, Steuernummer: 29/001/60002.\\nBarcode bitte nicht knicken!               \\nHinfahrt:\\nGültig ab:\\n17.12.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\n96W2GF\\nIhre Reiseverb\\n}<->{\\ninfahrt:\\nGültig ab:\\n17.12.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\n96W2GF\\nIhre Reiseverbindung und Reservierung Hinfahrt am 17.12.2017\\nHalt\\nDatum\\nZeit\\nGleis\\nProdukte\\nReservierung\\nLeipzig Hbf\\n17.12.\\nab 20:16 11\\nHamburg Hbf\\n17.12.\\nan 23:33 7\\nICE 1504\\n1 Sitzplatz, Wg. 28, Pl. 36, 1 Fenster, Großraum,\\nNichtraucher, Res.Nr. 8086 6008 8066 52\\nWichtige Nutzungshinweise:\\n- Ihre Fahrkarte gilt nur zusammen mit einem amtlichen Lichtbildausweis (z.B. Personalausweis) oder Ihrer BahnCard.\\n- Mit \\n}<->{\\nnur zusammen mit einem amtlichen Lichtbildausweis (z.B. Personalausweis) oder Ihrer BahnCard.\\n- Mit Ihrer Fahrkarte zum Flexpreis können Sie jeden Zug der gewählten Verbindung nutzen: mit einer IC/EC-Fahrkarte alle IC- und EC-Züge, mit\\neiner ICE-Fahrkarte auch alle anderen Züge.\\n- Das Online-Ticket gilt nur für den unter \"Fahrkarte\" angegebenen Reiseabschnitt. Die Übersicht \"Ihre Reiseverbindung\" enthält gegebenenfalls\\nReiseinformationen zu Teilstrecken (z.B. Bus oder Straßenbahn), für die eine \\n}<->{\\nenthält gegebenenfalls\\nReiseinformationen zu Teilstrecken (z.B. Bus oder Straßenbahn), für die eine weitere Fahrkarte erforderlich sein kann.\\n- Wenn Ihr Ticket den Zusatz \"+City\" oder \"City mobil\" zeigt, gilt dieser nur am Tag der Hinfahrt bzw. am Tag der Rückfahrt.\\n- Es gelten die nationalen und internationalen Beförderungsbedingungen der DB AG. Innerhalb von Verkehrsverbünden und Tarifgemeinschaften\\ngelten deren Bedingungen. Alle Bedingungen finden Sie unter: www.bahn.de/agb und www.diebefoerd\\n}<->{\\nften\\ngelten deren Bedingungen. Alle Bedingungen finden Sie unter: www.bahn.de/agb und www.diebefoerderer.de.\\nIhre Reisedaten können sich kurzfristig durch Bauarbeiten oder andere erforderliche Fahrplananpassungen ändern.\\nBitte informieren Sie sich kurz vor Ihrer Reise über mögliche Änderungen Ihrer Reisedaten unter www.bahn.de/reiseplan oder mobil über die\\nApp DB Navigator. Achten Sie auch auf Informationen und Ansagen im Zug und am Bahnhof. Wir danken Ihnen für Ihre Buchung und wünschen\\nIhnen e\\n}<->{\\normationen und Ansagen im Zug und am Bahnhof. Wir danken Ihnen für Ihre Buchung und wünschen\\nIhnen eine angenehme Reise!.\\n96W2GF\\nSeite 1 / 1\\n\\n--------------------------------------------------\\n', 'Summaries': 'Online-Ticket for ICE Fahrkarte:\\n- Travel Date: 17.12.2017\\n- Class: 1, Erwachsener mit BC50\\n- Return Trip: Leipzig+City to Hamburg+City with ICE\\n- Cancellation Policy: Free change or refund until 1 day before travel.\\n}<->{\\numme \\n89,50€\\n14,29€\\nKreditkartenzahlung \\nVU-Nr: 4556695619 \\nTransaktions-Nr: 742382 \\nDatum: 17.12.2017 \\nDB Fernverkehr AG/DB Regio AG \\nSteuernummer: 29/001/60002\\n}<->{\\n17.12.2017 \\n\\nHamburger Hauptbahnhof \\n20:16 11 \\nLeipzig Hauptbahnhof \\n23:33 7 ICE 1504 1 Sitzplatz, Wg. 28, Pl. 36, Fenster\\n}<->{\\nYou need a valid ID (e.g. Personalausweis) or your BahnCard to travel. Your Flexpreis ticket allows you to use any train of the chosen connection: with IC/EC card all IC and EC trains, with ICE card all other trains. Online tickets are only valid for the specified route section.\\n}<->{\\nDB-Tickets können für Teilstrecken zusätzliche Fahrkarten erfordern. Stadt-Tickets sind nur am Tag der Hinfahrt oder Rückfahrt gültig. Die Beförderungsbedingungen der DB AG gelten, sowie innerhalb von Verkehrsverbünden und Tarifgemeinschaften. Informationen finden Sie auf: www.bahn.de/agb und www.diebeförd\\n}<->{\\nTrain tickets are subject to conditions. See terms and conditions: www.bahn.de/agb and www.diebefoerderer.de.\\n\\nTravel dates may be changed due to construction or other necessary schedule adjustments.\\n\\nCheck your travel dates before your trip on: www.bahn.de/reiseplan or mobile via DB Navigator App.\\n\\nAlso, monitor information and announcements in the train and at the station.\\n}<->{\\nInformation über Fahrpläne, Ankunftszeiten und Ankündigungen im Zug und am Bahnhof. Danke für Ihre Buchung. Eine angenehme Reise wünschen wir Ihnen.'}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Receipts/2017/de/public transport/ATYWHD_normal.txt': {'Scores': '5\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4', 'Chunks': 'Online-Ticket\\nICE Fahrkarte\\nFahrtantritt am 22.12.2017\\nFlexpreis (Einfache Fahrt)\\nKlasse:\\n1\\nErw:\\n1, mit 1 BC50\\nHinfahrt:\\nHamburg+City \\n Leipzig+City, mit ICE\\nÜber:\\nVIA: (LWL*WBE/UE*SDL)*BSP*(WB/P*KOET)\\nUmtausch/Erstattung kostenlos bis 1 Tag vor Reiseantritt (Hinfahrt).\\nZahlungspositionen und Preis\\nPositionen\\nPreis\\nMwSt (D) 19%\\nMwSt D: 7%\\nICE Fahrkarte\\n1\\n92,00€\\n92,00€\\n14,69€\\nReservierung\\n1\\n0,00€\\nSumme\\n92,00€\\n92,00€\\n14,69€\\nKreditkartenzahlung\\nBetrag\\n92,00€\\nVU-Nr\\n4556695619\\nTransaktions-Nr\\n748203\\n\\n}<->{\\numme\\n92,00€\\n92,00€\\n14,69€\\nKreditkartenzahlung\\nBetrag\\n92,00€\\nVU-Nr\\n4556695619\\nTransaktions-Nr\\n748203\\nDatum\\n21.12.2017\\nGen-Nr\\nNRTEV7\\nIhre Kreditkarte wurde mit dem oben genannten Betrag belastet. Die Buchung Ihres\\nOnline-Tickets erfolgte am 21.12.2017 16:31 Uhr. DB Fernverkehr AG/DB Regio AG,\\nStephensonstr. 1, 60326 Frankfurt, Steuernummer: 29/001/60002.\\nBarcode bitte nicht knicken!               \\nHinfahrt:\\nGültig ab:\\n22.12.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\nATYWHD\\nIhre Reiseverb\\n}<->{\\ninfahrt:\\nGültig ab:\\n22.12.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\nATYWHD\\nIhre Reiseverbindung und Reservierung Hinfahrt am 22.12.2017\\nHalt\\nDatum\\nZeit\\nGleis\\nProdukte\\nReservierung\\nHamburg Hbf\\n22.12.\\nab 14:36 8\\nLeipzig Hbf\\n22.12.\\nan 17:42 11\\nICE 1515\\n1 Sitzplatz, Wg. 28, Pl. 64, 1 Gang, Abteil,\\nNichtraucher, Res.Nr. 8087 0013 4986 97\\nWichtige Nutzungshinweise:\\n- Ihre Fahrkarte gilt nur zusammen mit einem amtlichen Lichtbildausweis (z.B. Personalausweis) oder Ihrer BahnCard.\\n- Mit Ihrer\\n}<->{\\nusammen mit einem amtlichen Lichtbildausweis (z.B. Personalausweis) oder Ihrer BahnCard.\\n- Mit Ihrer Fahrkarte zum Flexpreis können Sie jeden Zug der gewählten Verbindung nutzen: mit einer IC/EC-Fahrkarte alle IC- und EC-Züge, mit\\neiner ICE-Fahrkarte auch alle anderen Züge.\\n- Das Online-Ticket gilt nur für den unter \"Fahrkarte\" angegebenen Reiseabschnitt. Die Übersicht \"Ihre Reiseverbindung\" enthält gegebenenfalls\\nReiseinformationen zu Teilstrecken (z.B. Bus oder Straßenbahn), für die eine weite\\n}<->{\\nlt gegebenenfalls\\nReiseinformationen zu Teilstrecken (z.B. Bus oder Straßenbahn), für die eine weitere Fahrkarte erforderlich sein kann.\\n- Wenn Ihr Ticket den Zusatz \"+City\" oder \"City mobil\" zeigt, gilt dieser nur am Tag der Hinfahrt bzw. am Tag der Rückfahrt.\\n- Es gelten die nationalen und internationalen Beförderungsbedingungen der DB AG. Innerhalb von Verkehrsverbünden und Tarifgemeinschaften\\ngelten deren Bedingungen. Alle Bedingungen finden Sie unter: www.bahn.de/agb und www.diebefoerderer.\\n}<->{\\ngelten deren Bedingungen. Alle Bedingungen finden Sie unter: www.bahn.de/agb und www.diebefoerderer.de.\\nIhre Reisedaten können sich kurzfristig durch Bauarbeiten oder andere erforderliche Fahrplananpassungen ändern.\\nBitte informieren Sie sich kurz vor Ihrer Reise über mögliche Änderungen Ihrer Reisedaten unter www.bahn.de/reiseplan oder mobil über die\\nApp DB Navigator. Achten Sie auch auf Informationen und Ansagen im Zug und am Bahnhof. Wir danken Ihnen für Ihre Buchung und wünschen\\nIhnen eine a\\n}<->{\\nionen und Ansagen im Zug und am Bahnhof. Wir danken Ihnen für Ihre Buchung und wünschen\\nIhnen eine angenehme Reise!.\\nATYWHD\\nSeite 1 / 1\\n\\n--------------------------------------------------\\n', 'Summaries': 'Online-Ticket for ICE Fahrkarte on 22.12.2017:\\n\\n* Class: 1 (Economy) with seat reservation BC50\\n* Price:\\n\\t+ Train ticket: €92.00\\n\\t+ VAT (D): €14.69\\n\\t+ Total: €92.00\\n* Payment methods:\\n\\t+ Kreditkarte (Payment amount: €92.00)\\n\\t\\t- VU-Nr: 4556695619\\n\\t\\t- Transaktions-Nummer: 748203\\n}<->{\\nUmme, €92,00 \\n€14,69 \\nVU-Nr: 4556695619 \\nTransaktions-Nr: 748203 \\nDatum: 21.12.2017 \\nDB Fernverkehr AG/DB Regio AG\\n}<->{\\nGültig bis: 22.12.2017\\n\\nReiseinformationen:\\n\\n* Halt: Hamburg Hbf, Leipzig Hbf\\n* Datum und Zeit:\\n + Abfahrt: 22.12., 14:36, Gleis 8\\n + Ankunft: 22.12., 17:42, Gleis 11\\n* Zug: ICE 1515\\n* Sitzplatz: 1, Wag 28, Platz 64, Gang 1\\n* Reservierungsnr.: 8087 0013 4986 97\\n}<->{\\nWenn Sie ein Flexpreis-Ticket kaufen, benötigen Sie Ihren Personalausweis oder BahnCard. Ihre Fahrkarte gilt für den gesamten Reiseabschnitt angegeben. Das Ticket deckt alle Züge der gewählten Verbindung ab, unabhängig von IC/EC-Fahrkarten oder ICE-Fahrkarte.\\n}<->{\\nFür Teilstrecken kann eine zusätzliche Fahrkarte erforderlich sein. \\nEin City-Ticket gilt nur für einen Tag der In- oder Rückfahrt. \\nDie Beförderungsbedingungen der DB AG gelten innerhalb von Verkehrsverbünden und Tarifgemeinschaften.\\n}<->{\\nYour train tickets may be affected by conditions. See www.bahn.de/agb and www.diebefoerderer.de for details.\\nTrain schedules can change due to construction or other necessary changes. Check your travel dates before departure at www.bahn.de/reiseplan or via DB Navigator App.\\nAlso, monitor information from within the train and at stations.\\n}<->{\\nDeutsche Bahn schickt Kunden nach Abschluss einer Fahrkarte ein Nachweisbrief mit der Ankunfts- oder Abreisezeit und anderen wichtigen Informationen über die Zugfahrkarte.'}}\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/Reports/2001.06268v2_normal.txt': {'Scores': '8\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n9\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n5\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n2\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8', 'Chunks': 'arXiv:2001.06268v2  [cs.CV]  13 Mar 2020\\nCompounding the Performance Improvements of Assembled Techniques\\nin a Convolutional Neural Network\\nJungkyu Lee, Taeryun Won, Tae Kwan Lee, Hyemin Lee, Geonmo Gu, Kiho Hong∗\\nClova Vision, NAVER Corp.\\n{jungkyu.lee, lory.tail, taekwan.lee, hmin.lee, geonmo.gu, kiho.hong}@navercorp.com\\nAbstract\\nRecent studies in image classiﬁcation have demonstrated\\na variety of techniques for improving the performance of\\nConvolutional Neural Networks (CNNs).\\nHowever, at-\\ntem\\n}<->{\\nf techniques for improving the performance of\\nConvolutional Neural Networks (CNNs).\\nHowever, at-\\ntempts to combine existing techniques to create a practi-\\ncal model are still uncommon. In this study, we carry out\\nextensive experiments to validate that carefully assembling\\nthese techniques and applying them to basic CNN mod-\\nels (e.g., ResNet and MobileNet) can improve the accuracy\\nand robustness of the models while minimizing the loss of\\nthroughput. Our proposed assembled ResNet-50 shows im-\\npro\\n}<->{\\nf the models while minimizing the loss of\\nthroughput. Our proposed assembled ResNet-50 shows im-\\nprovements in top-1 accuracy from 76.3% to 82.78%, mCE\\nfrom 76.0% to 48.9% and mFR from 57.7% to 32.3% on\\nILSVRC2012 validation set. With these improvements, in-\\nference throughput only decreases from 536 to 312. To ver-\\nify the performance improvement in transfer learning, ﬁne\\ngrained classiﬁcation and image retrieval tasks were tested\\non several public datasets and showed that the improvement\\nto ba\\n}<->{\\nd image retrieval tasks were tested\\non several public datasets and showed that the improvement\\nto backbone network performance boosted transfer learn-\\ning performance signiﬁcantly. Our approach achieved 1st\\nplace in the iFood Competition Fine-Grained Visual Recog-\\nnition at CVPR 20191, and the source code and trained\\nmodels will be made publicly available2.\\n1. Introduction\\nSince the introduction of AlexNet [18], many studies\\nhave mainly focused on designing new network architec-\\ntures for image \\n}<->{\\nf AlexNet [18], many studies\\nhave mainly focused on designing new network architec-\\ntures for image classiﬁcation to increase accuracy. For ex-\\nample, new architectures such as Inception [30], ResNet [8],\\nDenseNet [14], NASNet [38], MNASNet [31] and Efﬁcient-\\nNet [32] have been proposed. Inception introduced new\\nmodules into the network with convolution layers of dif-\\nferent kernel sizes. ResNet utilized the concept of skip con-\\nnection, and DenseNet added dense feature connections to\\n∗Correspon\\n}<->{\\ntilized the concept of skip con-\\nnection, and DenseNet added dense feature connections to\\n∗Corresponding author.\\n1https://www.kaggle.com/c/ifood-2019-fgvc6/leaderboard\\n2https://github.com/clovaai/assembled-cnn\\nboost the performance of the model. In addition, in the area\\nof AutoML, network design was automatically decided to\\ncreate models such as NASNet and MNASNet. Efﬁcient-\\nNet proposes an efﬁcient network by balancing the resolu-\\ntion, height, and width of the network. The performance of\\nEfﬁci\\n}<->{\\nnt network by balancing the resolu-\\ntion, height, and width of the network. The performance of\\nEfﬁcientNet for ILSVRC2012 top-1 accuracy was greatly\\nimproved relative to AlexNet.\\nUnlike these studies which focus on designing new net-\\nwork architecture, He et al. [9] proposes different ap-\\nproaches to improve model performance. They noted that\\nperformance can be improved not only through changes in\\nthe model structure, but also through other aspects of net-\\nwork training such as data preprocessin\\n}<->{\\n\\nthe model structure, but also through other aspects of net-\\nwork training such as data preprocessing, learning rate de-\\ncay, and parameter initialization. They also demonstrate\\nthat these minor “tricks” play a major part in boosting\\nmodel performance when applied in combination. As a re-\\nsult of using these tricks, ILSVRC2012 top-1 validation ac-\\ncuracy of ResNet-50 improved from 75.3% to 79.29% and\\nMobileNet improved from 69.03% to 71.90%. This im-\\nprovement is highly signiﬁcant because it sho\\n}<->{\\nand\\nMobileNet improved from 69.03% to 71.90%. This im-\\nprovement is highly signiﬁcant because it shows as much\\nperformance improvement as a novel network design does.\\nInspired by [9], we conducted a more extensive and\\nsystematic study of assembling several CNN-related tech-\\nniques into a single network. We ﬁrst divided the CNN-\\nrelated techniques into two categories: network tweaks and\\nregularization. Network tweaks are methods that modify the\\nCNN architectures to be more efﬁcient. (e.g., SENet \\n}<->{\\nion. Network tweaks are methods that modify the\\nCNN architectures to be more efﬁcient. (e.g., SENet [13],\\nSKNet [19]).\\nRegularization includes methods that pre-\\nvent overﬁtting by increasing the training data through\\ndata augmentation processes such as AutoAugment [4] and\\nMixup [36], or by limiting the complexity of the CNN with\\nprocesses such as Dropout [29], and DropBlock [6]. We\\nthen systematically analyze the process of assembling these\\ntwo types of techniques through extensive experiments a\\n}<->{\\nally analyze the process of assembling these\\ntwo types of techniques through extensive experiments and\\ndemonstrate that our approach leads to signiﬁcant perfor-\\nmance improvements.\\nIn addition to top-1 accuracy, mCE, mFR and throughput\\nwere used as performance indicators for combining these\\nvarious techniques. Hendrycks et al. [10] proposed mCE\\n1\\n\\n--------------------------------------------------\\nModel\\nTop-1\\nmCE\\nmFR\\nThroughput\\nEfﬁcientNet B4 [32] + AutoAugment [4]\\n83.0\\n60.7\\n-\\n95\\nEfﬁcientNet B6 \\n}<->{\\n\\nModel\\nTop-1\\nmCE\\nmFR\\nThroughput\\nEfﬁcientNet B4 [32] + AutoAugment [4]\\n83.0\\n60.7\\n-\\n95\\nEfﬁcientNet B6 [32] + AutoAugment [4]\\n84.2\\n60.6\\n-\\n28\\nEfﬁcientNet B7 [32] + AutoAugment [4]\\n84.5\\n59.4\\n-\\n16\\nResNet-50 [8] (baseline)\\n76.3\\n76.0\\n57.7\\n536\\nAssemble-ResNet-50 (ours)\\n82.8\\n48.9\\n32.3\\n312\\nAssemble-ResNet-152 (ours)\\n84.2\\n43.3\\n29.3\\n143\\nTable 1. Summary of key results. Top-1 is ILSVRC2012 top-1 validation accuracy. mCE is mean corruption error and mFR is mean ﬂip\\nrate (Lower is better.) [10]. The Throughput \\n}<->{\\nracy. mCE is mean corruption error and mFR is mean ﬂip\\nrate (Lower is better.) [10]. The Throughput refers to how many images per second the model processes during inference.\\n(mean corruption error) and mFR (mean ﬂip rate). mCE is a\\nmeasure of network robustness against input image corrup-\\ntion, and mFR is a measure of network stability on pertur-\\nbations in image sequences. Moreover, we used throughput\\n(images/sec) instead of the commonly used measurement of\\nFLOPS (ﬂoating point operations per \\n}<->{\\nughput\\n(images/sec) instead of the commonly used measurement of\\nFLOPS (ﬂoating point operations per second) because we\\nobserved that FLOPS is not proportional to the inference\\nspeed of the actual GPU device.\\nOur contributions can be summarized as follows:\\n1. By organizing the existing CNN-related techniques for\\nimage classiﬁcation, we ﬁnd techniques that can be as-\\nsembled into a single CNN. We then demonstrate that\\nour resulting model surpasses the state-of-the-art mod-\\nels with similar accurac\\n}<->{\\nen demonstrate that\\nour resulting model surpasses the state-of-the-art mod-\\nels with similar accuracy in terms of mCE, mFR and\\nthroughput (Table 1).\\n2. We provide detailed experimental results for the pro-\\ncess of assembling CNN techniques and release the\\ncode for accessibility and reproducibility.\\n2. Preliminaries\\nBefore introducing our approach, we describe default ex-\\nperimental settings and evaluation metrics used in Sections\\n3 and 4.\\n2.1. Training Procedure\\nWe use the ofﬁcial TensorFlow [1]\\n}<->{\\naluation metrics used in Sections\\n3 and 4.\\n2.1. Training Procedure\\nWe use the ofﬁcial TensorFlow [1] ResNet 3 as base\\ncode. The ILSVRC2012 [27] dataset is used to train and\\nevaluate models. All models were trained on a single ma-\\nchine with 8 Nvidia Tesla P40 GPUs compatible with the\\nCUDA 10 platform and cuDNN 7.6. TensorFlow version\\n1.14.0 was used.\\nThe techniques proposed by He et al. [9] are basically\\napplied to all our models described in Section 3.\\nWe\\nbrieﬂy describe the default hyperparame\\n}<->{\\nsically\\napplied to all our models described in Section 3.\\nWe\\nbrieﬂy describe the default hyperparameters and training\\ntechniques as follows.\\nPreprocessing In the training phase, a rectangular region\\nis randomly cropped using a randomly sampled aspect ratio\\n3https://github.com/tensorﬂow/models\\nfrom 3/4 to 4/3, and the fraction of cropped area over whole\\nimage is randomly chosen from 5% to 100%. Then, the\\ncropped region is resized to 224 × 224 and ﬂipped horizon-\\ntally with a random probability of\\n}<->{\\nn, the\\ncropped region is resized to 224 × 224 and ﬂipped horizon-\\ntally with a random probability of 0.5 followed by the RGB\\nchannel normalization. During validation, shorter dimen-\\nsion of each image is resized to 256 pixels while the aspect\\nratio is maintained. Next, the image is center-cropped to\\n224 × 224, and the RGB channels are normalized.\\nHyperparameter We use 1,024 batch size for training\\nwhich is close to the maximum size that can be received\\nby a single machine with 8 P40 GPUs. Stocha\\n}<->{\\n\\nwhich is close to the maximum size that can be received\\nby a single machine with 8 P40 GPUs. Stochastic gradi-\\nent descent with momentum 0.9 is used as the optimizer.\\nThe initial learning rate is 0.4 and the weight decay is set to\\n0.0001. The default number of training epochs is 120, but\\nsome techniques require different number of epochs. This\\nis explicitly speciﬁed when necessary.\\nLearning rate warmup If the batch size is large, a high\\nlearning rate may result in numerical instability. To prev\\n}<->{\\nwarmup If the batch size is large, a high\\nlearning rate may result in numerical instability. To prevent\\nthis, Goyal et al. [7] proposes a warmup strategy that lin-\\nearly increases the learning rate from 0 to the initial value.\\nThe warm-up period is set to the ﬁrst 5 epochs.\\nZero γ We initialize γ = 0 for all batch-norm layers that\\nsit at the end of residual blocks. Therefore, all the residual\\nblocks only return their shortcut branch results in the early\\nstages of training. This has the effect of\\n}<->{\\nks only return their shortcut branch results in the early\\nstages of training. This has the effect of shrinking the entire\\nlayer at the initial stage and helps training.\\nMixed-precision ﬂoating point We use mixed-precision\\nﬂoating point in the training phase because mixed-precision\\naccelerates the overall training speed if the GPU supports\\nit [22]. However, this does not result in the improvement of\\ntop-1 accuracy.\\nCosine learning rate decay\\nThe cosine decay sched-\\nule [20] reduces the initial le\\n}<->{\\nf\\ntop-1 accuracy.\\nCosine learning rate decay\\nThe cosine decay sched-\\nule [20] reduces the initial learning rate to close to 0 at the\\nend of training by following a cosine curve.\\n2.2. Evaluation Metrics\\nThe selection of metrics used to measure the perfor-\\nmance of the model is important because it indicates the di-\\n2\\n\\n--------------------------------------------------\\nrection in which the model is developed. We use the follow-\\ning three metrics as key indicators of model performance.\\nTop-1 The to\\n}<->{\\ndeveloped. We use the follow-\\ning three metrics as key indicators of model performance.\\nTop-1 The top-1 is a measure of classiﬁcation accuracy on\\nthe ILSVRC2012 [27] validation dataset. The validation\\ndataset consists of 50,000 images of 1,000 classes.\\nThroughput Throughput is deﬁned as how many images\\nare processed per second on the GPU device. We measured\\ninference throughput for an Nvidia P40 1 GPU. For com-\\nparison with other models, we used FP32 instead of FP16\\nin our experiments, using a b\\n}<->{\\n GPU. For com-\\nparison with other models, we used FP32 instead of FP16\\nin our experiments, using a batch size of 64.\\nmCE and mFR The mean corruption error (mCE) and\\nthe mean ﬂip rate (mFR) were proposed by Hendrycks et\\nal. [10] to measure the performance of the classiﬁcation\\nmodel on corrupted images and network stability on per-\\nturbations in image sequences, respectively.\\n3. Assembling CNN\\nIn this section, we introduce various network tweaks and\\nregularization techniques to be assembled, and d\\n}<->{\\nis section, we introduce various network tweaks and\\nregularization techniques to be assembled, and describe the\\ndetails of the implementation. We also perform prelimi-\\nnary experiments to study the effect of different parameter\\nchoices.\\n3.1. Network Tweaks\\nFigure 1 shows the overall ﬂow of our ﬁnal ResNet\\nmodel.\\nVarious network tweaks are applied to vanilla\\nResNet. The network tweaks we use are as follows.\\nResNet-D ResNet-D is a minor adjustment to the vanilla\\nResNet network architecture model p\\n}<->{\\n follows.\\nResNet-D ResNet-D is a minor adjustment to the vanilla\\nResNet network architecture model proposed by He et\\nal. [9]. It is known to work well in practice and has little\\nimpact to computational cost [9]. Three changes are added\\nto the ResNet model. First, the stride sizes of the ﬁrst two\\nconvolutions in the residual path have been switched. Sec-\\nond, a 2×2 average pooling layer with a stride of 2 is added\\nbefore the convolution in the skip connection path. Last, a\\nlarge 7 × 7 convolution\\n}<->{\\nde of 2 is added\\nbefore the convolution in the skip connection path. Last, a\\nlarge 7 × 7 convolution is replaced with three smaller 3 × 3\\nconvolutions in the stem layer.\\nChannel Attention We examine two tweaks in relation to\\nchannel attention. First, Squeeze and Excitation (SE) net-\\nwork [13] focuses on enhancing the representational capac-\\nity of the network by modeling channel-wise relationships.\\nSE eliminates spatial information by global pooling to get\\nchannel information only, and then two \\n}<->{\\n.\\nSE eliminates spatial information by global pooling to get\\nchannel information only, and then two fully connected lay-\\ners in this module learn the correlation between channels.\\nSecond, Selective Kernel (SK) [19] is inspired by the fact\\nthat the receptive sizes of neurons in the human visual cor-\\ntex are different from each other.\\nSK unit has multiple\\nbranches with different kernel sizes, and all branches are\\nfused using softmax attention.\\nThe original SK generates multiple paths with 3 × 3 an\\n}<->{\\nl branches are\\nfused using softmax attention.\\nThe original SK generates multiple paths with 3 × 3 and\\n5 × 5 convolutions, but we instead use two 3 × 3 convo-\\nlutions to split the given feature map. This is because two\\nconvolutions of the same kernel size can be replaced with\\none convolution with twice as many channels, thereby low-\\nering the inference cost. Figure 2 shows an SK unit where\\nthe original two branches are replaced with one convolution\\noperation.\\nExp\\nNo.\\nModel\\nSK\\nConﬁguration\\nSK\\nr\\nTo\\n}<->{\\ninal two branches are replaced with one convolution\\noperation.\\nExp\\nNo.\\nModel\\nSK\\nConﬁguration\\nSK\\nr\\nTop-1\\nThroughput\\nC0\\nR50 (baseline)\\n-\\n-\\n76.30\\n536\\nC1\\nR50+SE\\n-\\n-\\n77.40\\n466\\nC2\\nR50+SK\\n3x3+5x5\\n2\\n78.00\\n326\\nC3\\nR50+SK\\n3x3, 2x-channel\\n2\\n77.92\\n382\\nC4\\nR50+SK\\n3x3, 2x-channel\\n16\\n77.57\\n402\\nC5\\nR50+SK+SE\\n3x3, 2x-channel\\n2\\n77.50\\n345\\nTable 2. Result of channel attention with different conﬁgurations.\\nR50 is a simple notation for ResNet-50. r is the reduction ratio\\nof SK in the Fuse operation. The piecewise learni\\n}<->{\\ne notation for ResNet-50. r is the reduction ratio\\nof SK in the Fuse operation. The piecewise learning rate decay is\\nused in these experiments.\\nTable 2 shows the results for different conﬁgurations\\nof channel attention. Compared with SK, SE has higher\\nthroughput but lower accuracy (C1 and C2 in Table 2). Be-\\ntween C3 and C2, the top-1 accuracy only differs by 0.08%\\n(78.00% and 77.92%), but the throughput is signiﬁcantly\\ndifferent (326 and 382). Considering this trade-off between\\naccuracy and thr\\n}<->{\\noughput is signiﬁcantly\\ndifferent (326 and 382). Considering this trade-off between\\naccuracy and throughput, we decide to use one 3 × 3 kernel\\nwith doubled channel size instead of 3×3 and 5×5 kernels.\\nComparing C3 and C4, we see that changing the setting of\\nreduction ratio r for SK units from 2 to 16 yields a large\\ndegradation of top-1 accuracy relative to the improvement\\nof throughput. Applying both SE and SK (C5) not only\\ndecreases accuracy by 0.42% (from 77.92% to 77.50%),\\nbut also decreases \\n}<->{\\noth SE and SK (C5) not only\\ndecreases accuracy by 0.42% (from 77.92% to 77.50%),\\nbut also decreases inference throughput by 37 (from 382 to\\n345). Overall, for a better trade-off between top-1 accuracy\\nand throughput, the conﬁguration of C3 is preferred.\\nAnti-Alias Downsampling (AA) CNN models for image\\nclassiﬁcation are known to be very vulnerable to small\\namounts of distortion [34]. Zhang et al. [37] proposes AA\\nto improve the shift-equivariance of deep networks. The\\nmax-pooling is commonly vie\\n}<->{\\n[37] proposes AA\\nto improve the shift-equivariance of deep networks. The\\nmax-pooling is commonly viewed as a competing down-\\nsampling strategy, and is inherently composed of two op-\\nerations. The ﬁrst operation is to densely evaluate the max\\noperator and second operation is naive subsampling [37].\\nAA is proposed as a low-pass ﬁlter between them to achieve\\npractical anti-aliasing in any existing strided layer such as\\nstrided-conv.\\nThe smoothing factor can be adjusted by\\nchanging the blur kernel ﬁ\\n}<->{\\nrided layer such as\\nstrided-conv.\\nThe smoothing factor can be adjusted by\\nchanging the blur kernel ﬁlter size, where a larger ﬁlter\\nsize results in increased blur.\\nIn [37], AA is applied to\\nmax-pooling, projection-conv, and strided-conv of ResNet.\\nTable 3 shows the experimental results for AA. We ob-\\nserve that reducing the ﬁlter size from 5 to 3 maintains the\\n3\\n\\n--------------------------------------------------\\nDown Sampling\\nBlock\\n(+Anti Aliasing)\\nStage 3\\nDown Sampling\\nBlock\\n(+Anti Aliasing)\\nS\\n}<->{\\n----------------\\nDown Sampling\\nBlock\\n(+Anti Aliasing)\\nStage 3\\nDown Sampling\\nBlock\\n(+Anti Aliasing)\\nStage 2\\nDown Sampling\\nBlock\\nStage 1\\nDown Sampling\\nBlock\\n(+Anti Aliasing)\\nStage 4\\nResidual\\nBlocks\\nResidual\\nBlocks\\nResidual\\nBlocks\\nResidual\\nBlocks\\nResidual\\nBlocks\\nResidual\\nBlocks\\nResidual\\nBlocks\\nResidual\\nBlocks\\nStem\\nLayer\\nFC\\nLayer\\nResidual\\nBlock\\nResidual\\nBlock\\nResidual\\nBlock\\nLittle Branch\\nfrom BigLittleNet\\nInput\\nConv \\n(1x1)\\nDrop\\nBlock\\nSK Conv \\n(3x3)\\nDrop\\nBlock\\nConv \\n(1x1)\\nDrop\\nBlock\\nConv \\n(1x1)\\nDrop\\n\\n}<->{\\neNet\\nInput\\nConv \\n(1x1)\\nDrop\\nBlock\\nSK Conv \\n(3x3)\\nDrop\\nBlock\\nConv \\n(1x1)\\nDrop\\nBlock\\nConv \\n(1x1)\\nDrop\\nBlock\\nAnti Aliasing\\nDownsampling\\nAvgPooling \\n(2x2, S=2)\\nOutput\\nSkip Conn.\\n+ResNet-D +SK +AA +DropBlock\\n+ResNet-D +SK +AA\\n+ResNet-D +SK\\n+ResNet-D +SK +AA +DropBlock\\nDown Sampling Block\\nFigure 1. Assembling techniques into ResNet. We apply network tweaks such as ResNet-D, SK, Anti-alias, DropBlock, and BigLittleNet\\nto vanilla ResNet. In more detail, ResNet-D and SK are applied to all blocks in all s\\n}<->{\\nd BigLittleNet\\nto vanilla ResNet. In more detail, ResNet-D and SK are applied to all blocks in all stages. Downsampling with anti-aliasing is only applied\\nto the downsampling block from Stage 2 to Stage 4. DropBlock is applied to all blocks in Stage 3 and Stage 4. Little-Branch from\\nBigLittleNet uses one residual block with smaller width.\\nFuse Op.\\n3x3 kernel\\nSoftmax\\ns\\nz\\nFigure 2. Modiﬁed SK Unit. We use one 3 × 3 kernel with dou-\\nbled output channel size instead of 5 × 5 and 3 × 3 kernels.\\ntop-1\\n}<->{\\nWe use one 3 × 3 kernel with dou-\\nbled output channel size instead of 5 × 5 and 3 × 3 kernels.\\ntop-1 accuracy while increasing inference throughput (A1\\nand A2 in Table 3). However, removing the AA applied\\nto the projection-conv does not affect the accuracy (A3).\\nWe also observe that applying AA to max-pooling degrades\\nthroughput signiﬁcantly (A1, A2, and A3) compared to A4.\\nBased on the result, we apply AA only to strided-conv in\\nour model (Green box in Figure 1).\\nExp\\nNo.\\nFilter\\nSize\\nMax\\nPooling\\n}<->{\\n apply AA only to strided-conv in\\nour model (Green box in Figure 1).\\nExp\\nNo.\\nFilter\\nSize\\nMax\\nPooling\\nProjection\\nStrided\\nConv\\nTop-1\\nThroughput\\nA0\\n-\\nX\\nX\\nX\\n76.30\\n536\\nA1\\n5\\nO\\nO\\nO\\n76.81\\n422\\nA2\\n3\\nO\\nO\\nO\\n76.83\\n456\\nA3\\n3\\nO\\nX\\nO\\n76.84\\n483\\nA4\\n3\\nX\\nX\\nO\\n76.67\\n519\\nTable 3. Results for downsampling with anti-aliasing. The perfor-\\nmance of the model was tested with different conﬁgurations for\\ndownsampling with anti-aliasing. The piecewise learning rate de-\\ncay is used in these experiments.\\nBig Little Network (BL) B\\n}<->{\\nliasing. The piecewise learning rate de-\\ncay is used in these experiments.\\nBig Little Network (BL) BigLittleNet [3] applies multiple\\nbranches (Big-Branch and Little-Branch) with different res-\\nolutions while aiming at reducing computational cost and\\nincreasing accuracy. The Big-Branch has the same struc-\\nture as the baseline model and operates at a low image reso-\\nlution, whereas the Little-Branch reduces the convolutional\\nlayers and operates at same image resolution as the base-\\nline model. Big\\n}<->{\\n reduces the convolutional\\nlayers and operates at same image resolution as the base-\\nline model. BigLittleNet has two hyperparameters, α and\\nβ, which adjust the width and depth of the Little-Branch,\\nrespectively. We use α = 2 and β = 4 for ResNet-50 and\\nuse α = 1 and β = 2 for ResNet-152. The upper small\\nbranch in Figure 1 represents the Little-Branch. The Little-\\nBranch has one residual block and is smaller in width than\\nthe main Big-Branch. Since BigLittleNet saves budget in\\ncomputation, the m\\n}<->{\\n is smaller in width than\\nthe main Big-Branch. Since BigLittleNet saves budget in\\ncomputation, the models can be evaluated with a larger in-\\nput image scale for better performance while maintaining\\nsimilar throughput [3].\\n3.2. Regularization\\nAutoAugment (Autoaug) AutoAugment [4] is a data aug-\\nmentation procedure which learns augmentation strategies\\nfrom data. It uses reinforcement learning to select a se-\\nquence of image augmentation operations with the best ac-\\ncuracy by searching a discrete s\\n}<->{\\nect a se-\\nquence of image augmentation operations with the best ac-\\ncuracy by searching a discrete search space of their prob-\\nability of application and magnitude. We borrow the aug-\\nmentation policy of Autoaug on ILSVRC2012 4.\\nLabel Smoothing (LS) In the classiﬁcation problem, class\\nlabels are expressed as one hot encoding. If CNN is trained\\nto minimize cross entropy with this one hot encoding tar-\\n4https://github.com/tensorﬂow/models/tree/master/research/autoaugment\\n4\\n\\n-----------------------\\n}<->{\\nar-\\n4https://github.com/tensorﬂow/models/tree/master/research/autoaugment\\n4\\n\\n--------------------------------------------------\\nget, the logits of the last fully connected layer of CNN grow\\nto inﬁnity, which leads to over-ﬁtting [9]. Label smooth-\\ning [25] suppresses inﬁnite output and prevents over-ﬁtting.\\nWe set the label smoothing factor ǫ to 0.1.\\nMixup Mixup [36] creates one example by interpolating\\ntwo examples of the training set for data augmentation.\\nNeural networks are known to memorize\\n}<->{\\nlating\\ntwo examples of the training set for data augmentation.\\nNeural networks are known to memorize training data rather\\nthan generalize from the data [35]. As a result, the neural\\nnetwork produces unexpected outputs when it encounters\\ndata which are different from the distribution of the train-\\ning set. Mixup mitigates the problem by showing the neural\\nnetwork interpolated examples, and this helps to ﬁll up the\\nempty feature space of the training dataset.\\nModel\\nConﬁguration\\nTop-1\\nR50D\\nLS\\n77.37\\n}<->{\\nps to ﬁll up the\\nempty feature space of the training dataset.\\nModel\\nConﬁguration\\nTop-1\\nR50D\\nLS\\n77.37\\nR50D\\nLS + Mixup (type2)\\n78.85\\nR50D\\nLS + Mixup (type1)\\n79.10\\nTable 4. Result of different Mixup implementation types.\\nMixup has two types of implementation. The ﬁrst type\\nuses two mini batches to create a mixed mini batch. this\\ntype of implementation is suggested in the original pa-\\nper [36]. The second type uses a single mini batch to create\\nthe mixed mini batch by mixing the single mini batch wi\\n}<->{\\ncond type uses a single mini batch to create\\nthe mixed mini batch by mixing the single mini batch with\\na shufﬂed clone of itself. The second type of implementa-\\ntion uses less CPU resources because only one mini batch\\nneeds to be preprocessed to create one mixed mini batch.\\nHowever, experiments show that the second type of imple-\\nmentation reduces top-1 accuracy (Table 4). Therefore, in\\nlater experiments, we use the ﬁrst type of implementation.\\nWe set the Mixup hyperparameter α to 0.2.\\nDropBlock\\n}<->{\\nriments, we use the ﬁrst type of implementation.\\nWe set the Mixup hyperparameter α to 0.2.\\nDropBlock Dropout [29] is a popular technique for regu-\\nlarizing deep neural networks. It prevents the network from\\nbeing over-ﬁtted to the training set by dropping neurons\\nat random. However, Dropout does not work well for ex-\\ntremely deep networks such as ResNet [6]. DropBlock [6]\\ncan remove speciﬁc semantic information by dropping a\\ncontinuous region of activation. Thus, it is efﬁcient for the\\nregulariz\\n}<->{\\nic information by dropping a\\ncontinuous region of activation. Thus, it is efﬁcient for the\\nregularization of very deep networks. We borrow the same\\nDropBlock setting used in the original paper [6]. We apply\\nDropBlock to Stage 3 and 4 of ResNet-50 and linearly de-\\ncay the keep_prob hyperparameter from 1.0 to 0.9 during\\ntraining.\\nKnowledge Distillation (KD) Knowledge Distillation [11]\\nis a technique for transferring knowledge from one neural\\nnetwork (teacher) to another (student). Teacher models a\\n}<->{\\n for transferring knowledge from one neural\\nnetwork (teacher) to another (student). Teacher models are\\noften complex with high accuracy, and a weak but light stu-\\ndent model can improve its own accuracy by mimicking a\\nteacher model. The T hyperparameter of KD was said to be\\noptimal when set to 2 or 3 in the original paper [11], but we\\nuse T =1 for our model. Because our model uses Mixup and\\nKD techniques together, the teacher network should also be\\napplied to Mixup. This leads to better performa\\n}<->{\\nhniques together, the teacher network should also be\\napplied to Mixup. This leads to better performance at lower\\ntemperatures because the teacher’s signal itself is already\\nsmoothed by the Mixup (Table 5). We used EfﬁcientNet\\nB7 as a teacher with 84.5% of ILSVRC2012 validation top-\\n1 accuracy. In addition, the logits of the teacher were not\\ncomputed during the training time, but computed ofﬂine be-\\nfore training. The saved teacher logits were then used dur-\\ning training. Although this ofﬂine imp\\n}<->{\\n-\\nfore training. The saved teacher logits were then used dur-\\ning training. Although this ofﬂine implementation of KD\\ncannot calculate the teacher logits of augmented data (e.g.\\nAutoAugment) during training time, it worked well in our\\nexperiments.\\nModel\\nConﬁguration\\nTop-1\\nR50D+SK\\nLS+Mixup+DropBlock\\n81.40\\nR50D+SK\\nLS+Mixup+DropBlock+KD (T =2)\\n81.47\\nR50D+SK\\nLS+Mixup+DropBlock+KD (T =1.5)\\n81.50\\nR50D+SK\\nLS+Mixup+DropBlock+KD (T =1)\\n81.69\\nTable 5. Result of the change of KD temperature. We apply KD by\\n}<->{\\nK\\nLS+Mixup+DropBlock+KD (T =1)\\n81.69\\nTable 5. Result of the change of KD temperature. We apply KD by\\nvarying the temperature T to ﬁnd the optimal T value. We choose\\nT = 1 for next experiments.\\n4. Experiment Results\\n4.1. Ablation Study on ResNet\\nIn this section we will describe ablation study for assem-\\nbling the individual network tweaks covered in Section 3.1\\nto ﬁnd a better model. The results are shown in Table 6.\\nExp.\\nNo.\\nModel\\nInput\\nSize\\nTop-1\\nThroughput\\nT0\\nR50 (baseline)\\n224\\n76.87\\n536\\nT1\\nR5\\n}<->{\\ne shown in Table 6.\\nExp.\\nNo.\\nModel\\nInput\\nSize\\nTop-1\\nThroughput\\nT0\\nR50 (baseline)\\n224\\n76.87\\n536\\nT1\\nR50D\\n224\\n77.37\\n493\\nT2\\nR50D+SK\\n224\\n78.83\\n359\\nT3\\nR50D+SK+BL\\n224\\n78.26\\n445\\nT4\\nR50D+SK+BL\\n256\\n79.27\\n359\\nT5\\nR50D+SK+BL+AA\\n256\\n79.39\\n312\\nTable 6. Performance comparison of stacking network tweaks.\\nBy stacking the ResNet-D, Selective Kernel (SK), BigLittleNet\\n(BL) and downsampling with anti-aliasing (AA), we have steadily\\nimproved the ResNet-50 model with some inference throughput\\nlosses. The focus of each\\n}<->{\\n have steadily\\nimproved the ResNet-50 model with some inference throughput\\nlosses. The focus of each experiment is highlighted in boldface.\\nAdding ResNet-D to the baseline model improves top-1\\naccuracy by 0.5% from 76.87% to 77.37% (T1 in Table 6),\\nand adding SK tweaks improves accuracy by 1.46% from\\n77.37% to 78.83% (T2). In Table 2, We show that the accu-\\nracy is increased by 1.62% when SK is independently ap-\\nplied to ResNet from 76.30% to 77.92%. Stacking ResNet-\\n5\\n\\n-------------------------\\n}<->{\\ndependently ap-\\nplied to ResNet from 76.30% to 77.92%. Stacking ResNet-\\n5\\n\\n--------------------------------------------------\\nExp.\\nNo.\\nModel\\nRegularization Conﬁguration\\nTrain\\nEpoch\\nInput\\nSize\\nTop-1\\nmCE\\nmFR\\nThroughput\\nEfﬁcientNet B0 [32]\\nAutoaug\\n-\\n224\\n77.3\\n70.7\\n-\\n510\\nEfﬁcientNet B1 [32]\\nAutoaug\\n-\\n240\\n79.2\\n65.1\\n-\\n352\\nEfﬁcientNet B2 [32]\\nAutoaug\\n-\\n260\\n80.3\\n64.1\\n-\\n279\\nEfﬁcientNet B3 [32]\\nAutoaug\\n-\\n300\\n81.7\\n62.9\\n-\\n182\\nEfﬁcientNet B4 [32]\\nAutoaug\\n-\\n380\\n83.0\\n60.7\\n-\\n95\\nEfﬁcientNet B5 [32]\\nAutoaug\\n-\\n456\\n\\n}<->{\\n\\n81.7\\n62.9\\n-\\n182\\nEfﬁcientNet B4 [32]\\nAutoaug\\n-\\n380\\n83.0\\n60.7\\n-\\n95\\nEfﬁcientNet B5 [32]\\nAutoaug\\n-\\n456\\n83.7\\n62.3\\n-\\n49\\nEfﬁcientNet B6 [32]\\nAutoaug\\n-\\n528\\n84.2\\n60.6\\n-\\n28\\nEfﬁcientNet B7 [32]\\nAutoaug\\n-\\n600\\n84.5\\n59.4\\n-\\n16\\nE0\\nR50 (baseline)\\n120\\n224\\n76.87\\n75.55\\n56.55\\n536\\nE1\\nR50D\\n120\\n224\\n77.37\\n75.73\\n58.17\\n493\\nE2\\nR50D\\nLS\\n120\\n224\\n78.35\\n74.27\\n54.75\\n493\\nE3\\nR50D\\nLS+Mixup\\n200\\n224\\n79.10\\n68.19\\n51.24\\n493\\nE4\\nR50D+SE\\nLS+Mixup\\n200\\n224\\n79.71\\n64.48\\n47.47\\n420\\nE5\\nR50D+SE\\nLS+Mixup+DropBlock\\n270\\n224\\n80.40\\n62.64\\n42.34\\n420\\nE6\\n\\n}<->{\\n+Mixup\\n200\\n224\\n79.71\\n64.48\\n47.47\\n420\\nE5\\nR50D+SE\\nLS+Mixup+DropBlock\\n270\\n224\\n80.40\\n62.64\\n42.34\\n420\\nE6\\nR50D+SK\\nLS+Mixup+DropBlock\\n270\\n224\\n81.40\\n58.34\\n39.61\\n359\\nE7\\nR50D+SK\\nLS+Mixup+DropBlock+KD\\n270\\n224\\n81.69\\n57.08\\n38.15\\n359\\nE8\\nR50D+SK\\nLS+Mixup+DropBlock+KD\\n600\\n224\\n82.10\\n56.48\\n37.43\\n359\\nE9\\nR50D+BL+SK\\nLS+Mixup+DropBlock+KD\\n600\\n256\\n82.44\\n55.20\\n37.24\\n359\\nE10\\nR50D+BL+SK+AA\\nLS+Mixup+DropBlock+KD\\n600\\n256\\n82.69\\n54.12\\n36.81\\n312\\nE11\\nR50D+BL+SK+AA\\nLS+Mixup+DropBlock+KD+Autoaug\\n600\\n256\\n82.78\\n48.89\\n32.31\\n312\\nE12\\n}<->{\\n69\\n54.12\\n36.81\\n312\\nE11\\nR50D+BL+SK+AA\\nLS+Mixup+DropBlock+KD+Autoaug\\n600\\n256\\n82.78\\n48.89\\n32.31\\n312\\nE12\\nR152D+BL+SK+AA\\nLS+Mixup+DropBlock+KD+Autoaug\\n600\\n256\\n84.19\\n43.27\\n29.34\\n143\\nTable 7. Ablation study for assembling the network tweaks and regularizations with ResNet-50 on ILSVRC2012 dataset. The top-1\\naccuracy and mCE scores for EfﬁcientNet are borrowed from the ofﬁcial code in [16] and [34] respectively. As with other experiments, the\\ninference throughput measurements of EfﬁcientNet were perform\\n}<->{\\ntively. As with other experiments, the\\ninference throughput measurements of EfﬁcientNet were performed on a single Nvidia P40 using ofﬁcial EfﬁcientNet code [16].\\nD and SK increases the top-1 accuracy gain almost in equal\\nmeasure to the sum of the performance gains of applying\\nResNet-D and SK separately. The results show that the two\\ntweaks can improve performance independently with little\\neffect on each other. Applying BL to R50D+SK decreases\\ntop-1 accuracy from 78.83% to 78.26% , but throughpu\\n}<->{\\non each other. Applying BL to R50D+SK decreases\\ntop-1 accuracy from 78.83% to 78.26% , but throughput is\\nincreased from 359 to 445 (T3). To achieve higher accu-\\nracy by 0.44% while maintaining throughput similar to that\\nof the R50D+SK, we use 256 × 256 image resolution for\\ninference, whereas we use 224 × 224 image resolution for\\ntraining (T4). Applying AA to the R50D+SK+BL improves\\ntop-1 accuracy by 0.12% from 79.27% to 79.39% and de-\\ncreases throughput by 47 from 359 to 312 (T5).\\nThe ablation s\\n}<->{\\nby 0.12% from 79.27% to 79.39% and de-\\ncreases throughput by 47 from 359 to 312 (T5).\\nThe ablation study in Table 7 shows the impact of assem-\\nbling the regularization techniques described in Section 3.2.\\nThe regularization techniques increase accuracy, mCE and\\nmFR altogether, but the performance improvement of mCE\\nand mFR is greater than the improvement of accuracy (E2,\\n3, 5, 7, and 11). It can be seen that regularization helps to\\nmake CNNs more robust to image distortions. Adding SE\\nimproves t\\n}<->{\\ne seen that regularization helps to\\nmake CNNs more robust to image distortions. Adding SE\\nimproves top-1 accuracy by 0.61% and improves mCE by\\n3.71% (E4). We conﬁrm that channel attention is also help-\\nful for robustness to image distortions. Replacing SE with\\nSK improves top-1 and mCE by 1.0% and 4.3% (E6). In Ta-\\nble 2, when SE is changed to SK without regularization, the\\naccuracy increases by 0.5%. With regularization, replacing\\nSE with SK nearly doubles the accuracy improvement (E5\\nand E6). \\n}<->{\\n.5%. With regularization, replacing\\nSE with SK nearly doubles the accuracy improvement (E5\\nand E6). This means that SK is more complementary for\\nregularization techniques than SE.\\nChanging the epochs from 270 to 600 improves perfor-\\nmance (E8). Because data augmentation and regularization\\nare stacked, they have a stronger effect of regularization, so\\nlonger epochs seems to yield better generalization perfor-\\nmance. BL shows a performance improvement not only on\\ntop-1, but also on mCE and mFR wit\\n}<->{\\nion perfor-\\nmance. BL shows a performance improvement not only on\\ntop-1, but also on mCE and mFR without inference through-\\nput loss (E9). AA also shows higher performance gain in\\nmCE and mFR relative to top-1 (E10), which agrees with\\nAA being used as a network tweak to make the CNN robust\\nfor image translations as claimed in [37].\\nThe assembled model of all the techniques described so\\nfar has top-1 accuracy of 82.78%, mCE of 48.89% and mFR\\nof 32.31%. This ﬁnal model is listed in Table 7 as E11,\\n}<->{\\n-1 accuracy of 82.78%, mCE of 48.89% and mFR\\nof 32.31%. This ﬁnal model is listed in Table 7 as E11, and\\nwe call this model Assemble-ResNet-50. We also experi-\\nment with ResNet-152 for comparison as E12, we call this\\nmodel Assemble-ResNet-152.\\nTo further show that the boosted performance of the\\nproposed ResNet-50 is not mainly due to the increase in\\nnetwork parameters, we compared ResNet-50 with net-\\nwork tweaks and regularizations (E9 in Table 7) to ResNet-\\n101 with regularizations with a simil\\n}<->{\\net-\\nwork tweaks and regularizations (E9 in Table 7) to ResNet-\\n101 with regularizations with a similar number of param-\\n6\\n\\n--------------------------------------------------\\nModel\\nRegularization Conﬁguration\\nInput Size\\nTop-1\\nmCE\\nFLOPS\\nParams\\nThroughput\\nR50 (baseline)\\n224\\n76.87\\n75.55\\n4.1B\\n25.5M\\n536\\nR101\\n224\\n78.35\\n71.32\\n7.9B\\n44.6M\\n330\\nR152\\n224\\n78.51\\n68.95\\n11.6B\\n60.2M\\n233\\nR50D+SK+BL\\n256\\n79.27\\n67.59\\n5.4B\\n41.8M\\n359\\nR101\\nLS+Mixup+DropBlock+KD\\n224\\n81.26\\n57.26\\n7.9B\\n44.6M\\n330\\nR152\\nLS+Mixup+DropBlock+KD\\n2\\n}<->{\\n.4B\\n41.8M\\n359\\nR101\\nLS+Mixup+DropBlock+KD\\n224\\n81.26\\n57.26\\n7.9B\\n44.6M\\n330\\nR152\\nLS+Mixup+DropBlock+KD\\n224\\n81.96\\n54.99\\n11.6B\\n60.2M\\n233\\nR50D+SK+BL\\nLS+Mixup+DropBlock+KD\\n256\\n82.44\\n55.20\\n5.4B\\n41.8M\\n359\\nTable 8. Performance comparison among “ResNet-50D+SK+BL+Regularization”,\\n“ResNet-101+Regularization” and “ResNet-\\n152+Regularization” on ILSVRC2012 dataset.\\neters.\\nAs shown in Table 8, ResNet-50 with network\\ntweaks and regularizations shows approximately 1.2% bet-\\nter performance in top-1, and 2% in mCE \\n}<->{\\nrk\\ntweaks and regularizations shows approximately 1.2% bet-\\nter performance in top-1, and 2% in mCE compared to\\nResNet101 with regularizations, while having less parame-\\nters and FLOPS. Moreover, ResNet-50 with network tweaks\\nand regularizations outperforms ResNet-152 with regular-\\nizations which have far larger parameters and FLOPS.\\nThese observations prove that the combination of network\\ntweaks and regularizations in ResNet-50 creates a synergis-\\ntic effect.\\n4.2. Ablation Study on MobileNet\\nIn\\n}<->{\\nand regularizations in ResNet-50 creates a synergis-\\ntic effect.\\n4.2. Ablation Study on MobileNet\\nIn this section, the results of applying CNN-related tech-\\nniques to MobileNet-V1 [12] are presented. MobileNet-\\nV1, as its name suggests, is a baseline CNN model for\\nuse in mobile edge-devices.\\nTo follow the design prin-\\nciple of MobileNet, which prioritizes inference speed, we\\napplied the aforementioned techniques such that the reduc-\\ntion in throughput is minimized. Therefore, among network\\ntweak\\n}<->{\\noned techniques such that the reduc-\\ntion in throughput is minimized. Therefore, among network\\ntweaks, only SE was applied to MobileNet-V1 and boosted\\nthe accuracy by 1.69 % (M0, M1). The top-1 accuracy gain\\nof using SE-MobileNet-V1 together with LS+Mixup+KD\\nwas 2.05% more than that of vanilla MobileNet-V1 with\\nthe same regularizations applied (M5, M6). In other words,\\nthe synergistic effect of using network tweaks and regu-\\nlarizations is also demonstrated in mobile-oriented mod-\\nels. Based on \\n}<->{\\ning network tweaks and regu-\\nlarizations is also demonstrated in mobile-oriented mod-\\nels. Based on this, we reduced the reduction ratio r of the\\nSE block from 16 to 2 to maximize synergy between net-\\nwork tweaks and regularization. By doing so, we could im-\\nprove MobileNet-V1’s top-1 accuracy by 1% with minimal\\nthroughput loss. However, unlike ResNet, the top-1 accu-\\nracy of SE-MobileNet-V1 decreased when DropBlock was\\napplied. As the network capacity of MobileNet-V1 back-\\nbone is smaller than \\n}<->{\\nased when DropBlock was\\napplied. As the network capacity of MobileNet-V1 back-\\nbone is smaller than that of ResNet, more training epoch\\nand the adjustment of keep_prob hyperparameter (from\\n1.0-0.9 to 1.0-0.95) are needed for DropBlock regulariza-\\ntion to have a sufﬁcient effect in MobileNet-V1 model as\\nthat in ResNet (M8).\\n4.3. Transfer Learning: FGVC\\nIn this section, we investigate whether the improvements\\ndiscussed so far can help with transfer learning. We ﬁrst\\nanalyzed the contribution of tr\\n}<->{\\nprovements\\ndiscussed so far can help with transfer learning. We ﬁrst\\nanalyzed the contribution of transfer learning for each tech-\\nnique.\\nAn ablation study was performed on the Food-\\n101 [2] dataset, which is the largest public ﬁne-grained vi-\\nsual classiﬁcation (FGVC) dataset. The basic experiment\\nsetup and hyperparameters that differ from the backbone\\ntraining are described in supplementary material.\\nAs shown in Table 10, stacking network tweaks and reg-\\nularization techniques steadily improve\\n}<->{\\nrial.\\nAs shown in Table 10, stacking network tweaks and reg-\\nularization techniques steadily improved both top-1 accu-\\nracy and mCE for the transfer learning task on the Food-\\n101 dataset. In particular, comparing the experiments F4-\\nF8 with experiments F9-F13 (in Table 10) shows the effect\\nof regularization on the backbone. We use the same net-\\nwork structure in F4-F13, but for F9-F13, they have regu-\\nlarization such as Mixup, DropBlock, KD and Autoaug on\\nthe backbone. This regularization of th\\n}<->{\\nregu-\\nlarization such as Mixup, DropBlock, KD and Autoaug on\\nthe backbone. This regularization of the backbone gives\\nperformance improvements for top-1 accuracy as expected.\\nOn the other hand, the aspect of mCE performance differed\\nfrom the top-1 accuracy. Without regularization during ﬁne-\\ntuning such as in F4 and F9, the backbone with regulariza-\\ntion leads to better mCE performance than backbone with-\\nout regularization. However, adding regularization during\\nﬁne-tuning narrows the mCE perform\\n}<->{\\ne with-\\nout regularization. However, adding regularization during\\nﬁne-tuning narrows the mCE performance gap (F5-8 and\\nF10-13). For convenience, we call the ﬁnal F13 model in\\nTable 10 as Assemble-ResNet-FGVC-50.\\nWe also evaluated Assemble-ResNet-FGVC-50 in Ta-\\nble 10 on the following datasets:\\nCARS196 (Stanford\\nCars) [17], Oxford 102 Flowers [23], FGVC-Aircraft [21],\\nOxford-IIIT Pets [24] and Food-101 [2].\\nThe statistics\\nfor each dataset are as shown in supplementary material.\\nTable 11 shows the\\n}<->{\\n-101 [2].\\nThe statistics\\nfor each dataset are as shown in supplementary material.\\nTable 11 shows the transfer learning performance. Com-\\npared to EfﬁcientNet [32] and AmoebaNet-B [15] which\\nare state-of-the-art models for image classiﬁcation task,\\nour Assemble-ResNet-FGVC-50 model achieves compara-\\nble accuracy with 20x faster inference throughput.\\n7\\n\\n--------------------------------------------------\\nExp.\\nNo.\\nModel\\nRegularization Conﬁguration\\nTrain\\nEpoch\\nInput\\nSize\\nTop-1\\nmCE\\nmFR\\nThroughput\\n(FP3\\n}<->{\\n----\\nExp.\\nNo.\\nModel\\nRegularization Conﬁguration\\nTrain\\nEpoch\\nInput\\nSize\\nTop-1\\nmCE\\nmFR\\nThroughput\\n(FP32/Quantized)\\nM0\\nMobileNet-V1 (baseline)\\n120\\n224\\n72.59\\n83.85\\n74.29\\n12.98 / 22.49\\nM1\\nSE-MobileNet-V1 (r=16) (baseline)\\n360\\n224\\n74.28\\n78.86\\n69.98\\n10.33 / 19.71\\nM2\\nMobileNet-V1\\nLS\\n120\\n224\\n72.66\\n83.05\\n71.89\\n12.98 / 22.49\\nM3\\nMobileNet-V1\\nLS+Mixup\\n200\\n224\\n73.54\\n78.56\\n65.78\\n12.98 / 22.49\\nM4\\nMobileNet-V1\\nLS+Mixup+KD\\n200\\n224\\n74.18\\n77.51\\n64.41\\n12.98 / 22.49\\nM5\\nMobileNet-V1\\nLS+Mixup+KD\\n360\\n224\\n74.37\\n76.39\\n65.\\n}<->{\\nMixup+KD\\n200\\n224\\n74.18\\n77.51\\n64.41\\n12.98 / 22.49\\nM5\\nMobileNet-V1\\nLS+Mixup+KD\\n360\\n224\\n74.37\\n76.39\\n65.02\\n12.98 / 22.49\\nM6\\nSE-MobileNet-V1 (r=16)\\nLS+Mixup+KD\\n360\\n224\\n76.42\\n71.67\\n56.73\\n10.33 / 19.71\\nM7\\nSE-MobileNet-V1 (r=2)\\nLS+Mixup+KD\\n360\\n224\\n76.82\\n70.67\\n55.76\\n9.9 / 19.32\\nM8\\nSE-MobileNet-V1 (r=2)\\nLS+Mixup+KD+DropBlock\\n900\\n224\\n77.30\\n68.12\\n49.99\\n9.9 / 19.32\\nTable 9. Ablation study for assembling the network tweaks and regularization with MobileNet on ILSVRC2012 dataset. In order to measure\\nthroughput\\n}<->{\\ntwork tweaks and regularization with MobileNet on ILSVRC2012 dataset. In order to measure\\nthroughput, we use the standard TFLite Benchmark Tool. We measure the ﬂoating point and quantized performance using single-threaded\\nlarge core of Google Pixel 3 with batch size 1.\\nExp.\\nNo.\\nBackbone Model\\nBackbone\\nTop-1\\nRegularization\\nFood-101\\nTop-1\\nFood-101\\nmCE\\nF0\\nR50 (baseline)\\n76.87\\n-\\n86.99\\n61.50\\nF1\\nR50D\\n77.37\\n-\\n87.63\\n62.12\\nF2\\nR50D+SK\\n78.83\\n-\\n89.77\\n57.20\\nF3\\nR50D+SK+BL\\n79.27\\n-\\n90.15\\n57.16\\nF4\\nR50D+SK+BL+AA\\n\\n}<->{\\n.37\\n-\\n87.63\\n62.12\\nF2\\nR50D+SK\\n78.83\\n-\\n89.77\\n57.20\\nF3\\nR50D+SK+BL\\n79.27\\n-\\n90.15\\n57.16\\nF4\\nR50D+SK+BL+AA\\n79.39\\n-\\n90.37\\n56.66\\nF5\\nR50D+SK+BL+AA\\n79.39\\nDropBlock\\n91.25\\n53.13\\nF6\\nR50D+SK+BL+AA\\n79.39\\nDropBlock+Mixup\\n91.64\\n48.53\\nF7\\nR50D+SK+BL+AA\\n79.39\\nDropBlock+Mixup+Autoaug\\n91.85\\n41.73\\nF8\\nR50D+SK+BL+AA\\n79.39\\nDropBlock+Mixup+Autoaug+LS\\n91.76\\n41.40\\nF9\\nR50D+SK+BL+AA+REG\\n82.78\\n-\\n90.63\\n53.98\\nF10\\nR50D+SK+BL+AA+REG\\n82.78\\nDropBlock\\n91.62\\n51.01\\nF11\\nR50D+SK+BL+AA+REG\\n82.78\\nDropBlock+Mixup\\n92.11\\n45.73\\nF12\\nR50D+SK+BL+A\\n}<->{\\n82.78\\nDropBlock\\n91.62\\n51.01\\nF11\\nR50D+SK+BL+AA+REG\\n82.78\\nDropBlock+Mixup\\n92.11\\n45.73\\nF12\\nR50D+SK+BL+AA+REG\\n82.78\\nDropBlock+Mixup+Autoaug\\n92.21\\n41.69\\nF13\\nR50D+SK+BL+AA+REG\\n82.78\\nDropBlock+Mixup+Autoaug+LS\\n92.47\\n41.99\\nTable 10. Ablation study\\nof transfer\\nlearning with the\\nFood-101 dataset.\\nREG means\\nthat\\nregularization techniques\\n“LS+Mixup+DropBlock+KD+Autoaug” are applied during training backbone. The Food-101 mCE is not normalized by AlexNet’s er-\\nrors. We use the augmentation policy which is fou\\n}<->{\\nhe Food-101 mCE is not normalized by AlexNet’s er-\\nrors. We use the augmentation policy which is found by Autoaug on CIFAR-10 in these experiments [4].\\n4.4. Transfer Learning: Image Retrieval\\nWe also conducted an ablation study on three public ﬁne-\\ngrained image retrieval (IR) datasets: Stanford Online Prod-\\nucts (SOP) [28], CUB200 [33] and CARS196 [17]. We bor-\\nrowed the zero-shot data split protocol from [28].\\nOn top of that, cosine-softmax based losses were used\\nfor image retrieval. In this w\\n}<->{\\nocol from [28].\\nOn top of that, cosine-softmax based losses were used\\nfor image retrieval. In this work, we use ArcFace [5] loss\\nwith a margin of 0.3 and use generalized mean-pooling\\n(GeM) [26] for a pooling method without performing down-\\nsampling at Stage 4 of backbone networks because it has\\nbetter performance for the image retrieval task. The basic\\nexperiment setup and hyperparameters are described in sup-\\nplementary material.\\nIn the case of SOP, the degree of the effect was exam-\\nined by an\\n}<->{\\nribed in sup-\\nplementary material.\\nIn the case of SOP, the degree of the effect was exam-\\nined by an ablation study with the results listed in Table 12.\\nThe particular combinations of network tweaks and regular-\\nizations that worked well on the SOP dataset were different\\nfrom that for FGVC datasets. Comparing S2-4, we see that\\nBL and AA did not work well on the SOP dataset. Among\\nthe regularizers, DropBlock works well, but Autoaug does\\nnot improve the recall at 1 performance (S2 and S5,6). Nev-\\n\\n}<->{\\n DropBlock works well, but Autoaug does\\nnot improve the recall at 1 performance (S2 and S5,6). Nev-\\nertheless, in the best conﬁguration, there was a signiﬁcant\\nperformance improvement of 3.0% compared to the base-\\nline ResNet-50. The recall at 1 results for image retrieval\\ndatasets are reported in Table 13. There is also a signiﬁ-\\ncant performance improvement on CUB200 and CARS196\\ndatasets.\\n8\\n\\n--------------------------------------------------\\nDataset\\nThe state-of-the-art Models\\nResNet-50\\nAssemb\\n}<->{\\n-----------------------------------------------\\nDataset\\nThe state-of-the-art Models\\nResNet-50\\nAssemble-ResNet-FGVC-50\\nFood-101\\nEfﬁcientNet B7 [32]\\n93.0\\n87.0\\n92.5\\nCARS196\\nEfﬁcientNet B7 [32]\\n94.7\\n89.1\\n94.4\\nOxford-Flowers\\nEfﬁcientNet B7 [32]\\n98.8\\n96.1\\n98.9\\nFGVC Aircraft\\nEfﬁcientNet B7 [32]\\n92.9\\n78.8\\n92.4\\nOxford-IIIT Pets\\nAmoebaNet-B [15]\\n95.9\\n92.5\\n94.3\\nTable 11. Transfer learning results for FGVC. Numbers in the table indicate top-1 accuracy.\\nExp.\\nNo.\\nBackbone\\nRegularization\\nRecall@1\\nS0\\nR50 (basel\\n}<->{\\numbers in the table indicate top-1 accuracy.\\nExp.\\nNo.\\nBackbone\\nRegularization\\nRecall@1\\nS0\\nR50 (baseline)\\n82.9\\nS1\\nR50D\\n84.2\\nS2\\nR50D+SK\\n85.4\\nS3\\nR50D+SK+BL\\n85.2\\nS4\\nR50D+SK+BL+AA\\n85.1\\nS5\\nR50D+SK\\nDropBlock\\n85.9\\nS6\\nR50D+SK\\nDropBlock+Autoaug\\n83.7\\nS7\\nR50D+SK + REG\\n85.2\\nS8\\nR50D+SK + REG\\nDropBlock\\n85.9\\nS9\\nR50D+SK + REG\\nDropBlock+Autoaug\\n84.0\\nTable 12. Ablation study of transfer learning with SOP dataset.\\nREG means “LS+Mixup+DropBlock+KD”.\\nDataset\\nResNet-50\\nAssemble-ResNet-IR-50\\nSOP\\n82.9\\n85.9\\nCUB200\\n75.9\\n8\\n}<->{\\nG means “LS+Mixup+DropBlock+KD”.\\nDataset\\nResNet-50\\nAssemble-ResNet-IR-50\\nSOP\\n82.9\\n85.9\\nCUB200\\n75.9\\n80.3\\nCARS196\\n92.9\\n96.1\\nTable 13. Transfer learning for IR task with our method.\\nAssemble-ResNet-IR-50 represents the best conﬁguration model\\nfor each dataset. The best conﬁgurations for each dataset are spec-\\niﬁed in supplementary material. Numbers in the table indicate re-\\ncall@1.\\n5. Conclusion\\nIn this paper, we show that assembling various tech-\\nniques for CNNs to single convolutional networks le\\n}<->{\\nhis paper, we show that assembling various tech-\\nniques for CNNs to single convolutional networks leads\\nto improvements of top-1 accuracy, mCE and mFR on the\\nILSVRC2012 validation dataset. Synergistic effects have\\nbeen achieved by using a variety of network tweaks and\\nregularization techniques together in a single network. Our\\napproach has also improved performance consistently on\\ntransfer learning such as FGVC and image retrieval tasks.\\nMore excitingly, our network is not frozen, but is still e\\n}<->{\\ng such as FGVC and image retrieval tasks.\\nMore excitingly, our network is not frozen, but is still evolv-\\ning, and can be further developed with future research. We\\nexpect that there will be further improvements if we change\\nthe vanilla backbone to a more powerful backbone.\\nReferences\\n[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene\\nBrevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy\\nDavis, Jeffrey Dean, Matthieu Devin, et al.\\nTensorﬂow:\\nLarge-scale machine learning on heterogeneous dis\\n}<->{\\ns, Jeffrey Dean, Matthieu Devin, et al.\\nTensorﬂow:\\nLarge-scale machine learning on heterogeneous distributed\\nsystems. arXiv preprint arXiv:1603.04467, 2016.\\n[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\\nFood-101–mining discriminative components with random\\nforests. In European Conference on Computer Vision, pages\\n446–461. Springer, 2014.\\n[3] Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and\\nRogerio Feris. Big-little net: An efﬁcient multi-scale fea-\\nture representation for visu\\n}<->{\\n Sercu, and\\nRogerio Feris. Big-little net: An efﬁcient multi-scale fea-\\nture representation for visual and speech recognition. arXiv\\npreprint arXiv:1807.03848, 2018.\\n[4] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\\nvan, and Quoc V Le. Autoaugment: Learning augmentation\\npolicies from data. arXiv preprint arXiv:1805.09501, 2018.\\n[5] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. Arcface: Additive angular margin loss for deep\\nface recognition.\\nIn Proceedings of the IEEE Co\\n}<->{\\nriou. Arcface: Additive angular margin loss for deep\\nface recognition.\\nIn Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, pages 4690–\\n4699, 2019.\\n[6] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock:\\nA regularization method for convolutional networks.\\nIn\\nAdvances in Neural Information Processing Systems, pages\\n10727–10737, 2018.\\n[7] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noord-\\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\\nYangqing Jia, and K\\n}<->{\\nss Girshick, Pieter Noord-\\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\\nYangqing Jia, and Kaiming He.\\nAccurate, large mini-\\nbatch sgd: Training imagenet in 1 hour.\\narXiv preprint\\narXiv:1706.02677, 2017.\\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition, pages 770–778, 2016.\\n[9] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Jun-\\nyuan Xie, and Mu Li. \\n}<->{\\n pages 770–778, 2016.\\n[9] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Jun-\\nyuan Xie, and Mu Li. Bag of tricks for image classiﬁcation\\nwith convolutional neural networks. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion, pages 558–567, 2019.\\n[10] Dan Hendrycks and Thomas Dietterich. Benchmarking neu-\\nral network robustness to common corruptions and perturba-\\ntions. arXiv preprint arXiv:1903.12261, 2019.\\n[11] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\\nDist\\n}<->{\\nons. arXiv preprint arXiv:1903.12261, 2019.\\n[11] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\\nDistill-\\ning the knowledge in a neural network.\\narXiv preprint\\narXiv:1503.02531, 2015.\\n9\\n\\n--------------------------------------------------\\n[12] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\\ndreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-\\ntional neural networks for mobile vision applications. arXiv\\npreprint arXiv:1704.04861, 2017.\\n[13] Jie\\n}<->{\\nonal neural networks for mobile vision applications. arXiv\\npreprint arXiv:1704.04861, 2017.\\n[13] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\\nworks. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pages 7132–7141, 2018.\\n[14] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger.\\nDensely connected convolutional net-\\nworks. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pages 4700–4708, 2017.\\n[15]\\n}<->{\\ndings of the IEEE conference on computer\\nvision and pattern recognition, pages 4700–4708, 2017.\\n[15] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,\\nDehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam,\\nQuoc V Le, Yonghui Wu, et al.\\nGpipe: Efﬁcient train-\\ning of giant neural networks using pipeline parallelism. In\\nAdvances in Neural Information Processing Systems, pages\\n103–112, 2019.\\n[16] Google Inc. Efﬁcientnet ofﬁcial code. https://github.com/\\ntensorﬂow/tpu/tree/master/models/ofﬁcial/ef\\n}<->{\\noogle Inc. Efﬁcientnet ofﬁcial code. https://github.com/\\ntensorﬂow/tpu/tree/master/models/ofﬁcial/efﬁcientnet,\\n12\\n2019.\\n[17] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\\n3d object representations for ﬁne-grained categorization. In\\nProceedings of the IEEE International Conference on Com-\\nputer Vision Workshops, pages 554–561, 2013.\\n[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\\nImagenet classiﬁcation with deep convolutional neural net-\\nworks. In Advances in neural infor\\n}<->{\\ninton.\\nImagenet classiﬁcation with deep convolutional neural net-\\nworks. In Advances in neural information processing sys-\\ntems, pages 1097–1105, 2012.\\n[19] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selec-\\ntive kernel networks. 2019.\\n[20] Ilya Loshchilov and Frank Hutter.\\nSgdr:\\nStochas-\\ntic gradient descent with warm restarts.\\narXiv preprint\\narXiv:1608.03983, 2016.\\n[21] Subhransu Maji,\\nEsa Rahtu,\\nJuho Kannala,\\nMatthew\\nBlaschko, and Andrea Vedaldi. Fine-grained visual classi-\\nﬁcation of a\\n}<->{\\n Rahtu,\\nJuho Kannala,\\nMatthew\\nBlaschko, and Andrea Vedaldi. Fine-grained visual classi-\\nﬁcation of aircraft. arXiv preprint arXiv:1306.5151, 2013.\\n[22] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory\\nDiamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael\\nHouston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed\\nprecision training. arXiv preprint arXiv:1710.03740, 2017.\\n[23] Maria-Elena Nilsback and Andrew Zisserman. Automated\\nﬂower classiﬁcation over a large number of classes. In 2\\n}<->{\\nna Nilsback and Andrew Zisserman. Automated\\nﬂower classiﬁcation over a large number of classes. In 2008\\nSixth Indian Conference on Computer Vision, Graphics &\\nImage Processing, pages 722–729. IEEE, 2008.\\n[24] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\\nCV Jawahar. Cats and dogs. In 2012 IEEE conference on\\ncomputer vision and pattern recognition, pages 3498–3505.\\nIEEE, 2012.\\n[25] Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz\\nKaiser, and Geoffrey Hinton. Regularizing neural netwo\\n}<->{\\nPereyra, George Tucker, Jan Chorowski, Łukasz\\nKaiser, and Geoffrey Hinton. Regularizing neural networks\\nby penalizing conﬁdent output distributions. arXiv preprint\\narXiv:1701.06548, 2017.\\n[26] Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum. Fine-\\ntuning cnn image retrieval with no human annotation. IEEE\\ntransactions on pattern analysis and machine intelligence,\\n41(7):1655–1668, 2018.\\n[27] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej \\n}<->{\\n Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, et al.\\nImagenet large\\nscale visual recognition challenge. International journal of\\ncomputer vision, 115(3):211–252, 2015.\\n[28] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\\nSavarese. Deep metric learning via lifted structured feature\\nembedding. In IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), 2016.\\n[29] Nitish Srivastava, Geof\\n}<->{\\nEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), 2016.\\n[29] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\\nSutskever, and Ruslan Salakhutdinov. Dropout: a simple way\\nto prevent neural networks from overﬁtting. The journal of\\nmachine learning research, 15(1):1929–1958, 2014.\\n[30] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich.\\nGoing deeper with\\nconvolutions.\\nIn Proceeding\\n}<->{\\nmitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich.\\nGoing deeper with\\nconvolutions.\\nIn Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 1–9, 2015.\\n[31] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,\\nMark Sandler, Andrew Howard, and Quoc V Le.\\nMnas-\\nnet: Platform-aware neural architecture search for mobile.\\nIn Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 2820–2828, 2019.\\n[32] Mingxing Tan and Quoc V Le.\\nEfﬁcient\\n}<->{\\nter Vision\\nand Pattern Recognition, pages 2820–2828, 2019.\\n[32] Mingxing Tan and Quoc V Le.\\nEfﬁcientnet: Rethinking\\nmodel scaling for convolutional neural networks.\\narXiv\\npreprint arXiv:1905.11946, 2019.\\n[33] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-\\nona, and Serge Belongie. The caltech-ucsd birds-200-2011\\ndataset. 2011.\\n[34] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan\\nYuille, and Quoc V Le. Adversarial examples improve image\\nrecognition. arXiv preprint arXiv:1911.096\\n}<->{\\nYuille, and Quoc V Le. Adversarial examples improve image\\nrecognition. arXiv preprint arXiv:1911.09665, 2019.\\n[35] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin\\nRecht, and Oriol Vinyals.\\nUnderstanding deep learn-\\ning requires rethinking generalization.\\narXiv preprint\\narXiv:1611.03530, 2016.\\n[36] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\\ntion. arXiv preprint arXiv:1710.09412, 2017.\\n[37] Richard Zhang.\\nMaking convolutional netwo\\n}<->{\\ninimiza-\\ntion. arXiv preprint arXiv:1710.09412, 2017.\\n[37] Richard Zhang.\\nMaking convolutional networks shift-\\ninvariant again. In ICML, 2019.\\n[38] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\\nLe. Learning transferable architectures for scalable image\\nrecognition.\\nIn Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 8697–8710,\\n2018.\\n10\\n\\n--------------------------------------------------\\nAppendices\\nA. FLOPS and throughput\\nWe observe in several exper\\n}<->{\\n-------------------------------------\\nAppendices\\nA. FLOPS and throughput\\nWe observe in several experiments that FLOPS is not\\nproportional to the inference speed of the actual GPU.\\nFLOPS and throughput for variations of EfﬁcientNet [32]\\nand ResNet [8] are described in Table 14. For example,\\nFLOPS of EfﬁcientNet B0 is very small compared to that of\\nResNet-50, but throughput is rather lower.\\nModel\\nResolution\\nFLOPS\\nThroughput\\nEfﬁcientNet B0\\n224\\n0.39B\\n510\\nEfﬁcientNet B1\\n240\\n0.70B\\n352\\nEfﬁcientNet B2\\n2\\n}<->{\\nsolution\\nFLOPS\\nThroughput\\nEfﬁcientNet B0\\n224\\n0.39B\\n510\\nEfﬁcientNet B1\\n240\\n0.70B\\n352\\nEfﬁcientNet B2\\n260\\n1.0B\\n279\\nEfﬁcientNet B3\\n300\\n1.8B\\n182\\nEfﬁcientNet B4\\n380\\n4.2B\\n95\\nEfﬁcientNet B5\\n456\\n9.9B\\n49\\nEfﬁcientNet B6\\n528\\n19B\\n28\\nEfﬁcientNet B7\\n600\\n37B\\n16\\nR50\\n224\\n4.1B\\n536\\nR50D\\n224\\n4.4B\\n493\\nR50D+SK\\n224\\n6.2B\\n359\\nR50D+SK+BL\\n256\\n5.4B\\n359\\nR50D+SK+BL+AA\\n256\\n7.5B\\n312\\nR152D+SK+BL+AA\\n256\\n20.6B\\n143\\nTable 14. FLOPS and throughput for variation of EfﬁcientNet and\\nResNet. We use the TensorFlow ofﬁcial proﬁler code to \\n}<->{\\nd throughput for variation of EfﬁcientNet and\\nResNet. We use the TensorFlow ofﬁcial proﬁler code to measure\\nFLOPS. EfﬁcientNet’s FLOPS is borrowed from [32]. We mea-\\nsured inference throughput for an Nvidia P40 single GPU using a\\nbatch size of 64.\\nB. FGVC Task Conﬁguration\\nIn this section, we will describe experimental conﬁgu-\\nrations for public ﬁne-grained visual classiﬁcation (FGVC)\\ndatasets: Food-101 [2], CARS196 [17], Oxford 102 Flow-\\ners [23], Oxford-IIIT Pets [24] and FGVC-Aircraft [21].\\nT\\n}<->{\\nod-101 [2], CARS196 [17], Oxford 102 Flow-\\ners [23], Oxford-IIIT Pets [24] and FGVC-Aircraft [21].\\nThe basic experimental setup and hyperparameters that\\ndiffer from the backbone training are described as follows.\\n• Initial learning rate is 0.01.\\n• Weight decay is set to 0.001.\\n• Momentum for BN is set to (max(1 −10/s, 0.9)).\\n• Keep probability of DropBlock starts at 0.9 and de-\\ncreases linearly to 0.7 at the end of training\\n• The training epoch varies for each dataset.\\nWe use the same hyperparam\\n}<->{\\n 0.7 at the end of training\\n• The training epoch varies for each dataset.\\nWe use the same hyperparameters for all datasets for\\ntransfer learning except for training epochs. The training\\nepochs for each dataset are described in Table 15.\\nFGVC Dataset\\nTraining Epochs\\nFood-101\\n100\\nCARS196\\n1,000\\nOxford-Flowers\\n1,000\\nFGVC Aircraft\\n800\\nOxford-IIIT Pets\\n1,300\\nTable 15. Training conﬁguration of FGVC datasets.\\nDataset\\nTrain Size\\nTest Size\\n# Classes\\nFood-101\\n75,750\\n25,250\\n101\\nCARS196\\n8,144\\n8,041\\n196\\nOxfor\\n}<->{\\nets.\\nDataset\\nTrain Size\\nTest Size\\n# Classes\\nFood-101\\n75,750\\n25,250\\n101\\nCARS196\\n8,144\\n8,041\\n196\\nOxford-Flowers\\n2,040\\n6,149\\n102\\nFGVC Aircraft\\n6,667\\n3,333\\n100\\nOxford-IIIT Pets\\n3,680\\n3,669\\n37\\nTable 16. Statistics of FGVC datasets.\\nC. IR Task Conﬁguration\\nIn this section, we will describe experimental conﬁg-\\nurations for three public ﬁne-grained image retrieval (IR)\\ndatasets: Stanford Online Products (SOP) [28], CUB200\\n[33] and CARS196 [17]. The basic experimental setup and\\nhyperparameters are descri\\n}<->{\\nSOP) [28], CUB200\\n[33] and CARS196 [17]. The basic experimental setup and\\nhyperparameters are described as follows.\\n• Image preprocessing resizes to 224 × 224 without\\nmaintaining aspect ratio with probability 0.5 and re-\\nsizes to 256 × 256 and random crop to 224 × 224 with\\nprobability 0.5.\\n• Data augmentation includes random horizontal ﬂip\\nwith 0.5 probability.\\n• Momentum for BN is set to max(1 −10/s, 0.9).\\n• Weight decay is set to 0.0005.\\n• Feature size is set to 1536.\\n• The training epoch, bat\\n}<->{\\n0/s, 0.9).\\n• Weight decay is set to 0.0005.\\n• Feature size is set to 1536.\\n• The training epoch, batch size, learning rate decay and\\nassembling conﬁguration vary for each dataset.\\nThe different parameter settings for each dataset are de-\\nscribed in Table 17. The best conﬁgurations for each dataset\\nare speciﬁed.\\nDataset\\nLoss\\nFuction\\nLearning\\nrate\\nBatch\\nsize\\nTraining\\nEpochs\\nSOP\\nArcface\\n0.008\\n128\\n60\\nCUB200\\nSoftmax\\n0.001\\n32\\n100\\nCARS196\\nSoftmax\\n0.01\\n32\\n100\\nTable 17. Different hyperparameter settings \\n}<->{\\nCUB200\\nSoftmax\\n0.001\\n32\\n100\\nCARS196\\nSoftmax\\n0.01\\n32\\n100\\nTable 17. Different hyperparameter settings for IR tasks.\\n11\\n\\n--------------------------------------------------\\nDataset\\nBackbone\\nRegularization\\nSOP\\nR50D+SK+REG\\nDropBlock\\nCUB200\\nR50D+SK+REG\\nDropBlock\\nCARS196\\nR50D+SK+REG\\nDropBlock+LS+Autoaug\\nTable 18. Model conﬁguration for IR tasks.\\nREG means\\n“LS+Mixup+DropBlock+KD”\\n12\\n\\n--------------------------------------------------\\n\\n}<->{\\n----------------------------\\n', 'Summaries': 'arXiv:2001.06268v2  [cs.CV]  13 Mar 2020 \\nJungkyu Lee et al., Clova Vision, NAVER Corp.\\nCompounding the Performance Improvements of Assembled Techniques\\nin a Convolutional Neural Network\\nRecent studies demonstrate techniques for improving CNN performance in image classification.\\n}<->{\\nExtensive experiments validate combining existing techniques to improve CNN performance, particularly in assembling ResNet and MobileNet models. This approach improves accuracy and robustness while minimizing throughput loss. Proposed Assembled ResNet-50 achieves these benefits, demonstrating the effectiveness of integrating multiple techniques into a practical model.\\n}<->{\\nOur proposed assembled ResNet-50 model improves top-1 accuracy from 76.3% to 82.78%, mCE from 76.0% to 48.9%, and mFR from 57.7% to 32.3%. Inference throughput decreases from 536 to 312 on the ILSVRC2012 validation set, despite improvements in model performance.\\n}<->{\\nd image retrieval tasks were tested on public datasets with significant improvement to backbone networks, boosting transfer learning performance. Our approach achieved 1st place in iFood Competition Fine-Grained Visual Recognition at CVPR 2019 and will be publicly available along with source code and trained models.\\n}<->{\\nMany studies on AlexNet focused on designing image classification networks for increased accuracy. New architectures such as Inception, ResNet, DenseNet, NASNet, MNASNet, and EfficientNet have been proposed. These architectures introduce new concepts like convolution layers with varying kernel sizes (Inception), skip connections (ResNet), dense feature connections (DenseNet).\\n}<->{\\nDenseNet utilizes skip connections and added dense feature connections to boost model performance. AutoML models like NASNet and MNASNet automatically decide network design for better performance. Efﬁcient-Net balances resolution, height, and width for efficient networks.\\n}<->{\\nHe et al. improves ILSVRC2012 top-1 accuracy for EfficientNet by balancing resolution, height, and width. The performance surpasses AlexNet. Improvements can be made not only through architectural changes, but also through data preprocessing and other network training aspects.\\n}<->{\\nILSVRC2012 top-1 validation accuracy of ResNet-50 improved from 75.3% to 79.29%, and MobileNet improved from 69.03% to 71.90%. These improvements demonstrate the impact of applying minor \"tricks\" in network training, such as data preprocessing, learning rate decay, and parameter initialization.\\n}<->{\\nMobileNet\\'s performance improved from 69.03% to 71.90%, a significant gain. Inspired by [9], we analyzed assembling multiple CNN techniques into one network, dividing them into two categories: tweaks and regularization, which modify architectures for efficiency.\\n}<->{\\nNetwork tweaks modify CNN architectures for efficiency, while regularization prevents overfitting. Methods include data augmentation (AutoAugment, Mixup) and complexity limitation (Dropout, DropBlock). Regularization increases training data and reduces model complexity to prevent overfitting, improving network performance.\\n}<->{\\nEfficientNet B6 achieved 95% throughput with 83.0 top-1 accuracy, outperforming EfficientNet B4 and other models in mCE, mFR, and overall performance.\\n}<->{\\nTop-1 validation accuracy: \\nAssemble-ResNet-50: 82.8%\\nAssemble-ResNet-152: 84.2%\\n\\nMean Corruption Error (mCE):\\nAssemble-ResNet-50: 48.9%\\nAssemble-ResNet-152: 43.3%\\n\\nMean Flip Rate (mFR): \\nAssemble-ResNet-50: 32.3%\\nAssemble-ResNet-152: 29.3%\\n}<->{\\nThe paper evaluates two metrics for neural networks: mean corruption error (mCE) and mean flip rate (mFR). \\nmCE measures network robustness against image corruption, while mFR assesses stability on image sequence perturbations. The paper uses images/sec throughput instead of FLOPS (floating point operations per second).\\n}<->{\\nGPU performance measured in images/sec surpasses FLOPS due to non-linear inference speed of GPU devices. Researchers assembled existing CNN techniques into a single model, demonstrating it outperforms state-of-the-art models with similar accuracy.\\n}<->{\\nOur model outperforms state-of-the-art models with similar accuracy in terms of mean classification error (mCE), mean frequency recall (mFR), and throughput. We provide detailed experimental results and release our code for reproducibility. \\n\\nDefault experimental settings: \\n- Official TensorFlow [1]\\n\\nEvaluation metrics: \\n- Mean Classification Error (mCE)\\n- Mean Frequency Recall (mFR)\\n}<->{\\nILSVRC2012 dataset is used for training and evaluation of models using official TensorFlow ResNet 3 as base code, trained on 8 Nvidia Tesla P40 GPUs with CUDA 10 platform and cuDNN 7.6.\\n}<->{\\nDefault hyperparameters include: aspect ratio (3/4 to 4/3), cropped area fraction (5-100%), image resizing (224x224) and horizontal flipping (random probability).\\n}<->{\\nThe cropped region of an image is resized to 224 × 224 and flipped horizontally with a 50% probability. During validation, the shorter dimension is resized to 256 pixels while maintaining the aspect ratio.\\n}<->{\\nStochastic gradient descent with momentum 0.9 and initial learning rate 0.4 is used for training on an 8-P40 GPU machine. Weight decay is set to 0.0001. The default number of epochs is 120, but this may vary depending on specific techniques.\\n}<->{\\nA high learning rate may cause numerical instability with large batch sizes. Goyal et al. propose a warm-up strategy to increase the learning rate linearly from 0 to its initial value for the first 5 epochs. For all batch-normalized layers at the end of residual blocks, γ is initialized as 0, making them only return their shortcut branch results in early training stages.\\n}<->{\\nMixed-precision floating point is used to accelerate training, but doesn\\'t improve top-1 accuracy. Cosine learning rate decay schedules reduce the initial learning rate.\\n}<->{\\nHere\\'s a summary below 100 words:\\n\\nTop-1 accuracy: measures model performance, particularly in classification tasks.\\nCosine learning rate decay schedule reduces initial learning rate near 0 over time, following a cosine curve. \\nThree main evaluation metrics used:\\n- Top-1 accuracy\\n- Top-k accuracy\\n- AUC (Area Under Curve) of the precision-recall curve\\n}<->{\\nThe model performance is measured by three metrics: Top-1 accuracy (ILSVRC2012 validation dataset), image processing speed (throughput) on an Nvidia P40 GPU.\\n}<->{\\nGPU Comparison: Experiments used FP32 instead of FP16 with a batch size of 64. Two metrics were proposed: mean corruption error (mCE) and mean flip rate (mFR).\\n}<->{\\nVanilla ResNet is modified by various tweaks, including:\\n- ResNet-D (minor adjustment to the original ResNet)\\n- Other unspecified tweaks\\n}<->{\\nResNet-D: A variant of ResNet with minor adjustments to vanilla architecture. Three changes are made:\\n\\n- Switched stride sizes for first two convolutions in residual path.\\n- Added 2x2 average pooling layer before skip connection convolution.\\n- Replaced last layer with a large 7x7 convolution.\\n}<->{\\nTwo tweaks in Channel Attention are examined. First, Squeeze and Excitation (SE) network enhances representational capacity by modeling channel-wise relationships, eliminating spatial info through global pooling for channel info. Then, a large 7x7 convolution is replaced with three smaller 3x3 convolutions in the stem layer.\\n}<->{\\nSE eliminates spatial information by global pooling to get channel information only. The Selective Kernel (SK) unit has multiple branches with different kernel sizes that are fused using softmax attention, allowing the model to capture diverse receptive fields.\\n}<->{\\nBranches are fused using softmax attention.\\nSK uses multiple paths with 3 × 3 and 5 × 5 convolutions.\\nTwo 3 × 3 convolutions replace original branches, reducing inference cost.\\n}<->{\\nTwo convolution operations replaced one convolution and one Exp, No., and Top-1 are used to evaluate models. \\n\\nSK: Simple Kinematic model \\nSK+SE: Simple Kinematic model + Channel Attention\\nR50: Baseline ResNet-50 model with a reduction ratio of 2x.\\n}<->{\\nTable 2 results for different channel attention configurations:\\n\\nSK: \\n- Lower throughput\\n- Higher accuracy (C1, C2)\\n- Top-1 accuracy differs by 0.08% between C3 and C2 (78.00%, 77.92%)\\n\\nSE:\\n- Higher throughput\\n- Lower accuracy (C1, C2)\\n- Top-1 accuracy only differs by 0.08% between C3 and C2 (78.00%, 77.92%)\\n- Significantly different throughput (326, 382)\\n}<->{\\nThe 3 × 3 kernel with doubled channel size resulted in a significant improvement of 382 compared to 326. Using SK units with r = 16 caused large degradation in accuracy, while using both SE and SK resulted in an 0.42% decrease in top-1 accuracy.\\n}<->{\\nSE and SK (C5) decreases accuracy by 0.42% and inference throughput by 37 compared to C3, leading to a better trade-off between top-1 accuracy and throughput for configuration C3.\\n}<->{\\nA method (AA) is proposed to improve deep networks. It addresses max-pooling, a common down-sampling strategy, by adding a low-pass filter between the dense evaluation and naive subsampling operations to achieve anti-aliasing, reducing the smoothing factor through blur kernel adjustments.\\n}<->{\\nSmoothing factor adjusted by changing blur kernel filter size, larger filter size results in increased blur. AA applied to max-pooling, projection-conv, and strided-conv of ResNet. Filter size reduction from 5 to 3 maintains performance.\\n}<->{\\nDown Sampling Stages:\\n- Stage 1: Down sampling, block with anti aliasing.\\n- Stage 2: Down sampling, block with anti aliasing.\\n- Stage 3: Down sampling, block with anti aliasing.\\n- Stage 4: Residual blocks.\\n\\nNetwork Structure:\\n- Stem layer with fully connected (FC) layer.\\n- First residual block followed by little branch from Big Little Net.\\n- Input layer with a 1x1 convolution followed by multiple dropouts.\\n}<->{\\neNet architecture \\nInput -> Conv (1x1) -> Block \\nSK Conv (3x3) -> Drop \\nConv (1x1) -> Drop -> Block \\nAnti Aliasing -> Downsampling -> AvgPooling (2x2, S=2)\\nOutput: +ResNet-D +SK +AA +DropBlock or other variants.\\n}<->{\\nResNet-D and SK are applied to all blocks in stages, except downsampling block from Stage 2 to Stage 4. \\nDropBlock is applied to Stage 3 & 4 blocks. \\nLittle-Branch uses one residual block with smaller width. \\nModified SK Unit uses a 3x3 kernel with doubled output channel size.\\n}<->{\\nUsing a 3 × 3 kernel with doubled output channel size improves top-1 accuracy and inference throughput. Applying Aperture Awareness (AA) to projection-conv does not affect accuracy, but degrades max-pooling performance significantly. AA is applied only to strided-conv in the model.\\n}<->{\\nTable 3 results:\\n\\nA0: 76.30, 536\\nA1: 76.81, 422\\nA2: 76.83, 456\\nA3: 76.84, 483\\nA4: 76.67, 519\\n}<->{\\nBigLittleNet uses multiple branches with different resolutions to reduce computational cost and increase accuracy. The Big-Branch has the same structure as the baseline model but operates at low image resolution, while the Little-Branch reduces convolutional layers and maintains the same image resolution as the baseline model.\\n}<->{\\nBigLittleNet reduces convolutional layers, operating at same image resolution as baseline models. It has two hyperparameters: α (width of Little-Branch) and β (depth). For ResNet-50, α = 2 and β = 4; for ResNet-152, α = 1 and β = 2.\\n}<->{\\nBigLittleNet has smaller width than Big-Branch, allowing evaluation with larger input images while maintaining similar throughput. AutoAugment is a data augmentation procedure that learns optimal augmentation strategies from data using reinforcement learning to achieve better accuracy.\\n}<->{\\nSequence of image augmentation operations can be optimized by searching a discrete search space of probability and magnitude to achieve the best accuracy. Label Smoothing (LS) uses one-hot encoding in classification, training CNN with cross-entropy loss with this encoding results in improved performance compared to traditional softmax loss.\\n}<->{\\nLabel smoothing suppresses infinite output and over-fitting in CNNs, preventing growth of logits in last fully connected layer. A label smoothing factor of ǫ = 0.1 is used, similar to Mixup which creates interpolated examples for data augmentation by interpolating two training set examples.\\n}<->{\\nNeural networks memorize training data instead of generalizing from it. Mixup solves this by displaying interpolated examples, filling the empty feature space of the training set, thus mitigating the problem of unexpected outputs when encountering new data.\\n}<->{\\nHere is a summary below 100 words:\\n\\nTable 4 compares different Mixup implementations:\\n- LS (baseline): 77.37\\n- R50D: 78.85\\n- R50D + LS: 79.10\\nMixup has two types, type1 and type2.\\nType1 uses two mini batches, while type2 uses a single batch to create the mixed mini batch.\\nThe results show that adding Mixup to R50D increases accuracy, with type1 providing slightly higher performance than type2.\\n}<->{\\nCond-type implementation uses a single mini-batch to create mixed batch by mixing with shuffled clone of itself. \\nIt uses less CPU resources but reduces top-1 accuracy.\\n}<->{\\nDropBlock is used instead of Dropout to prevent over-fitting in extremely deep networks like ResNet. It removes specific semantic information by randomly dropping a continuous region of activation, making it efficient for regularization.\\n}<->{\\nDropBlock is applied to Stage 3 and 4 of ResNet-50, using a keep_prob decay from 1.0 to 0.9 during training. Knowledge Distillation involves transferring knowledge from a teacher model to a student model.\\n}<->{\\nThe knowledge of complex teacher neural networks is transferred to student models through a technique called Knowledge Distillation (KD). The student can improve its accuracy by mimicking the teacher\\'s performance with a smaller architecture and T hyperparameter set to 1, using a combination of Mixup and KD techniques.\\n}<->{\\nThe Mixup technique is combined with a teacher network approach, improving performance at lower temperatures due to the teacher\\'s signal being smoothed by Mixup. An EfficientNet B7 was used as a teacher, achieving 84.5% ILSVRC2012 validation top-1 accuracy.\\n}<->{\\nThe results show that adding knowledge distillation (KD) to a model improved performance, with top-1 accuracy increasing from 81.40 to 81.69 when using KD at T=1. The optimal KD temperature is T=1.5, yielding an accuracy of 81.47.\\n}<->{\\nKLS+Mixup+DropBlock+KD (T=1) achieved 81.69 results. \\n\\nTable 6 shows ablation study for individual network tweaks with varying input sizes, top-1 accuracy and throughput values for models T0 and T1.\\n}<->{\\nTable 6 shows performance comparison of stacked network tweaks on ResNet-50:\\n \\n- T0: R50 (baseline) - 76.87, 536 samples/second\\n- T1: R50D - 77.37, 493 samples/second\\n- T2: R50D+SK - 78.83, 359 samples/second\\n- T3: R50D+SK+BL - 78.26, 445 samples/second\\n- T4/T5: R50D+SK+BL (256x) - 79.27/79.39, 359/312 samples/second\\n}<->{\\nThe ResNet-50 model has been improved through experiments with **inference throughput losses** and **SK tweaks**. Adding ResNet-D improves top-1 accuracy by 0.5%, and adding SK tweaks improve it by 1.46%. Applying SK independently increases accuracy by 1.62%.\\n}<->{\\nRegularization configurations and train results for EfficientNet models:\\n\\n- EfficientNet B0: 224x, Autoaug, Train Epoch 77.3, Top-1 70.7%, Throughput 510.\\n- EfficientNet B1: 240x, Autoaug, Train Epoch 79.2, Top-1 65.1%.\\n- EfficientNet B2: 260x, Autoaug, Train Epoch 80.3, Top-1 64.1%.\\n- EfficientNet B3: 300x, Autoaug, Train Epoch 81.7, Top-1 62.9%.\\n- EfficientNet B4: 380x, Autoaug, Train Epoch 83.0, Top-1 60.7%.\\n}<->{\\nEfficientNet B4, B5, B6, and B7 models with Autoaugmentation outperform the baseline E0, R50, and its variants:\\n- EfficientNet B4: 380 (83.0), 456 (83.7)\\n- EfficientNet B5: 528 (84.2)\\n- EfficientNet B6: 600 (84.5)\\n- EfficientNet B7: 620 not provided\\nE1, E2, and E3 models with LS+Mixup and SE variants achieve high accuracy:\\n- E1: 493 (77.37%), 493 (76.87%)\\n- E2: 493 (78.35%), 493 (75.73%)\\n- E3: 493 (79.10%), 493 (68.19%)\\nE4, E5, and E6 models with LS+Mixup+DropBlock achieve the highest accuracy:\\n- E4: 420 (79.71%), 420 (64.48%)\\n- E5: 420 (80.40%), 420 (62.64%)\\n}<->{\\nTest Results:\\n\\n- Mixup: 200, 224 ( success rate)\\n- DropBlock: 79.71, 64.48\\n- R50D+SE: 47.47, 420 ( failure )\\n- LS+Mixup+DropBlock: 270, 224\\n- E6: 80.40, 62.64\\n- R50D+SK: 42.34, 359\\n- LS+Mixup+DropBlock+KD: 81.69, 57.08\\n- E8: 38.15, 359\\n- R50D+BL+SK: 600, 256\\n- E10: 82.44, 55.20\\n- R50D+BL+SK+AA: 600, 312\\n}<->{\\nAblation study on ResNet-50 with EfficientNet tweaks and regularizations:\\n\\n* Top-1 accuracy: 54.12% (R50D), 36.81% (Regularization only)\\n* mCE scores: 82.78% (R50D), 48.89% (Regularization only)\\n* Inference throughput: 600 FPS\\n* Model variants:\\n\\t+ R50D+BL+SK+AA: 312 FPS, 84.19% accuracy\\n\\t+ E12 with tweaks and regularizations: 143 FPS, 43.27% accuracy\\n}<->{\\nMeasurements of EfficientNet were taken on a single Nvidia P40 using official code [16]. \\nTop-1 accuracy increased by D and SK separately, with minimal impact from combining them. \\nApplying BL (Batch Normalization) to R50-D+SK decreased top-1 accuracy from 78.83% to 78.26%.\\n}<->{\\nR50D+SK with BL: top-1 accuracy decreases from 78.83% to 78.26%, but throughput increases from 359 to 445. \\nUsing 256 × 256 image resolution for inference achieves similar accuracy and increased throughput.\\nAA on R50D+SK+BL improves top-1 accuracy by 0.12%, decreasing throughput by 47.\\n}<->{\\nThe ablation study increased accuracy, mean cross-entropy (mCE), and mean Frobenius radius (mFR) by 0.12%. Regularization techniques improved performance of mCE and mFR greater than accuracy.\\n}<->{\\nRegularization and channel attention improve CNN robustness to image distortions. Adding SE improves top-1 accuracy by 0.61% and mCE by 3.71%. Replacing SE with SK increases accuracy, with regularization nearly doubling improvements.\\n}<->{\\nReplacing SE with SK doubles accuracy improvement (E5 and E6) and improves performance when changing epochs from 270 to 600 (E8). Longer epochs yield better generalization performance due to stacked data augmentation and regularization.\\n}<->{\\nBL improves performance on top-1, mCE, and mFR without inference loss (E9). AA enhances gains in mCE and mFR relative to top-1 (E10). The final assembled model achieves 82.78% top-1 accuracy, 48.89% mCE, and 32.31% mFR (E11).\\n}<->{\\nThe final model Assemble-ResNet-50 achieved accuracy of 82.78%, mean classification error (mCE) of 48.89% and mean forward rate (mFR) of 32.31%. It is compared to Assemble-ResNet-152 for comparison, as well as ResNet-101 with regularizations.\\n}<->{\\nRegularization configurations for ResNet models with input size 224:\\n\\n* R50: mCE, 4.1B FLOPS, 25.5M params\\n* R101: mCE, 7.9B FLOPS, 44.6M params\\n* R152: mCE, 11.6B FLOPS, 60.2M params\\n* R50D+SK+BL: mCE, 5.4B FLOPS, 41.8M params\\n* R101 (LS+Mixup+DropBlock+KD): mCE, 7.9B FLOPS, 44.6M params\\n* R152 (LS+Mixup+DropBlock+KD): mCE, 11.6B FLOPS, 60.2M params\\n}<->{\\nResNet-50+ tweaks + reg shows 1.2% better top-1 accuracy, 2% mCE, compared to other models.\\n}<->{\\nResNet101 with tweaks and regularizations shows 1.2% better top-1 performance and 2% mCE improvement compared to regularized ResNet101. ResNet-50 outperforms ResNet-152, which has more parameters and FLOPS. Network tweaks and regularizations create a synergistic effect, resulting in improved performance with fewer parameters and FLOPS.\\n}<->{\\nAblation study on MobileNet: Applying CNN techniques to MobileNet-V1 minimizes reduction in throughput while following its design principle of prioritizing inference speed.\\n}<->{\\nSE was applied to MobileNet-V1 boosting accuracy by 1.69%. Using SE-MobileNet-V1 with LS+Mixup+KD resulted in a top-1 accuracy gain of 2.05%, surpassing vanilla MobileNet-V1 with the same regu-larizations.\\n}<->{\\nReducing SE block reduction ratio r from 16 to 2 improved MobileNet-V1\\'s top-1 accuracy by 1%. However, applying DropBlock degraded its accuracy. This differs from ResNet, where DropBlock improved accuracy.\\n}<->{\\nDropBlock regularization was first applied when the MobileNet-V1 backbone had a smaller network capacity compared to ResNet. It required more training epochs and adjustments to the keep_prob hyperparameter for effective results in MobileNet-V1, unlike in ResNet (M8).\\n}<->{\\nTransfer learning improvements discussed so far include:\\n\\nAblation study on Food-101 dataset using transfer learning.\\nStacking network tweaks and regularization techniques showed steady improvement.\\n}<->{\\nStacking network tweaks and regularization techniques improved top-1 accuracy and mCE for transfer learning on the Food101 dataset. Regularization had a significant effect on the backbone, with experiments F9-F13 showing better results due to added techniques such as Mixup, DropBlock, KD, and Autoaug.\\n}<->{\\nRegularization techniques such as Mixup, DropBlock, KD, and Autoaugment improve top-1 accuracy when applied to the backbone. However, their effect on mCE performance varies depending on fine-tuning, with regularized backbones outperforming unregularized ones initially but losing ground upon fine-tuning.\\n}<->{\\nModel performance without regularization is limited. Adding regularization during fine-tuning improves mCE performance, narrowing the gap between F5-8 and F10-13 models. The Assemble-ResNet-FGVC-50 model achieved results on CARS196, Oxford 102 Flowers, FGVC-Aircraft, Oxford-IIIT Pets, and Food-101 datasets.\\n}<->{\\nTable 11: Transfer Learning Performance \\n\\nAssemble-ResNet-FGVC-50 model achieves comparable accuracy \\nwith state-of-the-art models EfficientNet and AmoebaNet-B. \\nInference throughput is 20x faster than the competitors.\\n}<->{\\nHere is a summary of the table in original language, below 100 words:\\n\\nThe table compares different models with varying regularization configurations for image classification tasks. The models are MobileNet-V1 and its variants (M0-M5), with additional techniques such as L1 regularization, mixup, and knowledge distillation. Performance metrics include top-1 accuracy, forward pass throughput, and inference time. The results show that the SE-MobileNet-V1 variant outperforms the baseline model, while the addition of mixup and knowledge distillation improves performance on certain models.\\n}<->{\\nMixup+KD (M5, M6, M7): 74.18-76.82%, 64.41-77.51% accuracy, 12.98/22.49 ratio.\\nLS+Mixup+KD (M5, M6, M7, M8): 74.37-76.82%, 65.02-77.30% accuracy, 10.33/19.71 and 9.9/19.32 ratios respectively.\\n}<->{\\nMobileNet backbones on ILSVRC2012 dataset:\\n\\n- F0: R50 baseline - 86.99% Top-1 accuracy with mCE and F1 regularization.\\n- F1: R50D - 87.63% Top-1 accuracy with mCE, F1, and SK regularization.\\n- F2: R50D+SK - 89.77% Top-1 accuracy with mCE, F1, SK, and BL regularization.\\n- F3: R50D+SK+BL+AA - 90.15% Top-1 accuracy with mCE, F1, SK, BL, and AA regularization.\\n}<->{\\nScores: \\n- R50D+SK: 79.39\\n- R50D+SK+BL: 90.15, 90.37, 91.85\\n- R50D+SK+BL+AA: 79.39, 90.37, 91.76\\n- R50D+SK+BL+AA+REG: 82.78, 92.11\\n}<->{\\n82.78: DropBlock \\n91.62: F11\\nR50D+SK+BL+AA+REG\\n82.78: DropBlock+Mixup\\n92.11: F12\\nR50D+SK+BL+AA+REG\\n82.78: DropBlock+Mixup+Autoaug\\n92.21: F13\\nR50D+SK+BL+AA+REG\\n82.78: DropBlock+Mixup+Autoaug+LS\\n}<->{\\nThe Food-101 mCE is not normalized by AlexNet\\'s errors. An augmentation policy found by Autoaug on CIFAR-10 was used. \\n\\nAblation study conducted on three public fine-grained image retrieval (IR) datasets: SOP, CUB200, and CARS196, using cosine-softmax based losses for image retrieval.\\n}<->{\\nArcFace loss with a margin of 0.3 is used for image retrieval alongside GeM pooling method.\\n}<->{\\nThe effect of various combinations of network tweaks and regularizations on the SOP dataset was examined through an ablation study. BL and AA did not perform well, while DropBlock worked well. Autoaug showed no improvement in recall at 1 performance.\\n}<->{\\nDropBlock improves recall by 3.0% over ResNet-50 in best configuration, but AutoAugment does not improve recall at S2/S5,6. Significant performance improvement also seen on CUB200 and CARS196 datasets.\\n}<->{\\nResNet-50 \\n93.0\\nAssemble-ResNet-FGVC-50 \\n92.5\\nFood-101 \\n87.0\\nEfﬁcientNet B7 [32] 94.4\\nCARS196 \\n94.7\\nOxford-Flowers \\n98.8\\nFGVC Aircraft \\n92.9\\nOxford-IIIT Pets \\n95.9\\n}<->{\\nTop-1 accuracy results:\\n\\n| Dataset | Transfer Learning Method | Top-1 Accuracy |\\n| --- | --- | --- |\\n| SOP (Selfie Only Pose) | No Transfer Learning | 82.9% |\\n| SOP | Assemble-ResNet-IR-50 | 85.9% |\\n| CUB200 | No Transfer Learning | 75.9% |\\n| CUB200 | Assemble-ResNet-IR-50 | N/A |\\n}<->{\\nG = LS+Mixup+DropBlock+KD \\n- ResNet-50: 82.9%\\n- Assemble-ResNet-IR-50: 85.9%\\n- SOP: 75.9%\\n- CUB200: 80.3%\\n- CARS196: 92.9%\\n}<->{\\nAssembling various techiques for CNNs into single convolutional networks improves top-1 accuracy, mCE and mFR on ILSVRC2012 validation dataset. Synergistic effects are achieved with tweaks and regularization techniques together in a single network, enhancing performance in transfer learning and image retrieval tasks, and outperforming frozen networks.\\n}<->{\\nOur network is not frozen but evolving and can be further developed with future research. It can improve if a more powerful backbone is used instead of the vanilla one.\\n}<->{\\nJeffrey Dean et al., Tensorflow:\\nLarge-scale machine learning on heterogeneous distributed systems (2016).\\n\\nLukas Bossard et al., Food-101–mining discriminative components with random forests (2014).\\n \\nChun-Fu Chen et al., Big-little net: An efficient multi-scale feature representation for visual object detection.\\n}<->{\\nSercu, and Rogerio Feris. Big-little net: An efﬁcient multi-scale fea-\\nture representation for visual and speech recognition (2018).\\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\\nvan, and Quoc V Le. Autoaugment: Learning augmentation\\npolicies from data (2018).\\nJiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. Arcface: Additive angular margin loss for deep\\n}<->{\\nArcface: Additive angular margin loss for deep face recognition was presented in IEEE Conference on Computer Vision and Pattern Recognition (2019). Dropblock regularization method for convolutional networks was also introduced by Golnaz Ghiasi et al. (Advances in Neural Information Processing Systems, 2018).\\n}<->{\\nKaiming He, along with colleagues, achieved accurate large mini-batch SGD training of ImageNet in 1 hour on arXiv preprint (arXiv:1706.02677, 2017). \\nHe also co-authored \\'Deep Residual Learning for Image Recognition\\' with Zhang et al. published at IEEE conference on CVPR 2016.\\n}<->{\\npages 770–778, 2016\\nTong He et al., \"Bag of tricks for image classification with convolutional neural networks\", IEEE CVPR 2019.\\n\\n pages 558-567, 2019 \\nDan Hendrycks & Thomas Dietterich,  \"Benchmarking neural network robustness to common corruptions and perturbations\" \\n\\npages  (no date mentioned)  \\nGeoffrey Hinton et al.,  \"Dist\"\\n}<->{\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean.\\nDistill-ing the knowledge in a neural network.\\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreettto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-\\ntional neural networks for mobile vision applications.\\n}<->{\\nSqueeze-and-excitation networks, densely connected convolutional networks, and other architectures have been used in mobile vision applications due to their efficiency and effectiveness in achieving high accuracy.\\n}<->{\\nYanping Huang et al. proposed Gpipe, an efficient training method for giant neural networks using pipeline parallelism in CVPR 2017. The paper was later expanded upon by Google Inc., which released EfficientNet as an official model with TPUs available on GitHub.\\n}<->{\\nGoogle Inc. EfficientNet official code. https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet,\\n2019.\\n\\n* Introduced EfficientNet architecture for fine-grained object categorization (2013) \\n* Developed 3D object representations (2013)\\n* Proposed ImageNet classification with deep convolutional neural networks (2017)\\n}<->{\\ninton \\nImagenet \\nDeep convolutional neural networks \\nAdvances in neural information processing systems \\n2012 \\nXiang Li et al., selective kernel networks (2019) \\nIlya Loshchilov and Frank Hutter, Sgdr (arXiv 1608.03983, 2016)\\n}<->{\\nFine-grained visual classification of aircraft using mixed precision training by Paulius Micikevicius et al. (2017).\\n}<->{\\nNilsback and Zisserman: \\nAutomated flower classification in CVGPIC 2008 (IEEE). \\n\\nOmkar Parkhi et al.: \\nAutomated cat/dog classification in ICCV 2012 (IEEE).\\n\\nGabriel Pereyra et al.:\\nRegularizing neural networks in ICLR.\\n}<->{\\nPereyra et al. (2017) proposed regularizing neural networks by penalizing confident output distributions, improving accuracy and stability. Filip Radenovi´c et al. (2018) developed a fine-tuning approach for CNN image retrieval with no human annotation, achieving state-of-the-art results.\\n}<->{\\nKey researchers behind deep learning in computer vision:\\n\\nRussakovsky et al., ImageNet large scale visual recognition challenge (2015)\\nHyun Oh Song et al., Deep metric learning via lifted structured feature embedding (2016)\\n}<->{\\nEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. \\nDropout by Nitish Srivastava et al., a technique to prevent neural networks from overfitting, was presented in The Journal of Machine Learning Research in 2014.\\nAdditionally, Christian Szegedy et al. introduced \"Going deeper with convolutions\" at CVPR 2016.\\n}<->{\\nErhan, Vanhoucke, Rabinovich \\n- Proceeding CVPR 2015 - Convolutions \\n\\nMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. \\n- Mnasnet (Platform-aware neural architecture search for mobile) \\n- Proceedings CVPR 2019 [32]\\n}<->{\\nMingxing Tan and Quoc V Le introduced EfficientNet in 2019, rethinking model scaling for convolutional neural networks. The Caltech-UCSD Birds dataset was developed by Catherine Wah et al. Adversarial examples improve image recognition, as shown by Cihang Xie et al.\\n}<->{\\nAdversarial examples improve image recognition. Chiyuan Zhang et al. (2016) rethought generalization in deep learning. Hongyi Zhang et al. (2017) introduced mixup: beyond empirical risk minimization, improving robustness.\\n}<->{\\ninimiza-tion \\nA 2017 preprint arXiv:1710.09412 by Richard Zhang proposes a method to make convolutional networks shift-invariant again. This is further explored in the 2019 ICML paper \"Making convolutional networks shift-invariant again\" and in Barret Zoph et al.\\'s work on learning transferable architectures for scalable image recognition published at CVPR 2018, achieving high FLOPS and throughput.\\n}<->{\\nA comparison of FLOPS and throughput for variations of EfficientNet [32] and ResNet [8] models:\\n\\n- EfﬁcientNet B0: 224, 0.39B FLOPS, 510 throughput\\n- EfﬁcientNet B1: 240, 0.70B FLOPS, 352 throughput\\n- EfﬁcientNet B2: 512\\n}<->{\\nFLOPS, Throughput:\\n- EfﬁcientNet B0: 510 FLOPS, 0.39B Throughput\\n- R50: 536 FLOPS, 4.1B Throughput\\n}<->{\\nd throughput for EfficientNet and ResNet is measured using the TensorFlow official profiler code. FLOPS are used to measure performance, with inference throughput recorded on an Nvidia P40 GPU with a batch size of 64.\\n}<->{\\nInitial learning rate: 0.01 \\nWeight decay: 0.001 \\nMomentum: (1 - 10/s, 0.9) \\nDropBlock keep probability: starts at 0.9 and decreases to 0.7 \\nTraining epoch varies by dataset\\n}<->{\\nFGVC Dataset Training Epochs:\\n- Food-101: 100 epochs \\n- CARS196: 1,000 epochs \\n- Oxford-Flowers: 1,000 epochs \\n- FGVC Aircraft: 800 epochs \\n- Oxford-IIIT Pets: 1,300 epochs\\n}<->{\\nets \\nDataset\\nTrain Size\\nTest Size\\n# Classes\\nFood-101 75,750 25,250 101\\nCARS196 8,144 8,041 196\\nOxford-Flowers 2,040 6,149 102\\nFGVC Aircraft 6,667 3,333 100 \\nOxford-IIIT Pets 3,680 3,669 37\\n}<->{\\nImage preprocessing involves resizing to 224 × 224 and 256 × 256 with random cropping, as well as applying a horizontal flip with a probability of 0.5. Hyperparameters include momentum for batch normalization at 0.9, weight decay at 0.0005, feature size at 1536, and training epoch length is not specified.\\n}<->{\\n• Weight decay set to 0.0005.\\n• Feature size set to 1536.\\n• Training epoch and batch size vary by dataset.\\n• Best parameter configurations for each dataset are specified in Table 17.\\n}<->{\\nDataset models using R50D with SK and REG. \\n- CUB200: DropBlock \\n- CARS196: DropBlock + LS + Autoaug\\n}<->{\\nThere is nothing to summarize yet. Please provide the text you\\'d like me to work on. I\\'ll be happy to help!'}}\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/Reports/2002.09024v1_normal.txt': {'Scores': '6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6} for MaxUp with ResNet-100. \\n\\n Table 5 results: 7 \\n Ablation Study results: 8 \\n MaxUp+Cutout results: 9 \\n Overall text: 8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n5\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n5\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4', 'Chunks': 'arXiv:2002.09024v1  [cs.LG]  20 Feb 2020\\nMaxUp: A Simple Way to Improve Generalization of Neural Network Training\\nChengyue Gong∗1 Tongzheng Ren∗1 Mao Ye 1 Qiang Liu 1\\nAbstract\\nWe propose MaxUp, an embarrassingly sim-\\nple, highly effective technique for improving the\\ngeneralization performance of machine learning\\nmodels, especially deep neural networks. The\\nidea is to generate a set of augmented data\\nwith some random perturbations or transforms,\\nand minimize the maximum, or worst case loss\\nover t\\n}<->{\\nta\\nwith some random perturbations or transforms,\\nand minimize the maximum, or worst case loss\\nover the augmented data.\\nBy doing so, we\\nimplicitly introduce a smoothness or robustness\\nregularization against the random perturbations,\\nand hence improve the generation performance.\\nFor example, in the case of Gaussian perturba-\\ntion, MaxUp is asymptotically equivalent to us-\\ning the gradient norm of the loss as a penalty\\nto encourage smoothness.\\nWe test MaxUp on\\na range of tasks, including image clas\\n}<->{\\ne loss as a penalty\\nto encourage smoothness.\\nWe test MaxUp on\\na range of tasks, including image classiﬁcation,\\nlanguage modeling, and adversarial certiﬁcation,\\non which MaxUp consistently outperforms the\\nexisting best baseline methods, without introduc-\\ning substantial computational overhead. In par-\\nticular, we improve ImageNet classiﬁcation from\\nthe state-of-the-art top-1 accuracy 85.5% without\\nextra data to 85.8%. Code will be released soon.\\n1. Introduction\\nA central theme of machine learning\\n}<->{\\nextra data to 85.8%. Code will be released soon.\\n1. Introduction\\nA central theme of machine learning is to alleviate the issue\\nof overﬁtting, improving the generalization performance on\\ntesting data. This is often achieved by leveraging important\\nprior knowledge of the models and data of interest. For ex-\\nample, the regularization-based methods introduce penalty\\non the complexity of the model, which often amount to en-\\nforcing certain smoothness properties. Data augmentation\\ntechniques, on the o\\n}<->{\\nch often amount to en-\\nforcing certain smoothness properties. Data augmentation\\ntechniques, on the other hand, leverage important invari-\\nance properties of the data (such as the shift and rotation\\ninvariance of images) to improve performance. Novel ap-\\nproaches that exploit important knowledge of the models\\nand data hold the potential of substantially improving the\\nperformance of machine learning systems.\\n*Equal contribution\\n1UT Austin.\\nCorrespondence to:\\nChengyue Gong <cygong@cs.utexas.edu>.\\nP\\n}<->{\\n systems.\\n*Equal contribution\\n1UT Austin.\\nCorrespondence to:\\nChengyue Gong <cygong@cs.utexas.edu>.\\nPreprint\\nWe propose MaxUp, a simple yet powerful training method\\nto improve the generalization performance and alleviate the\\nover-ﬁtting issue.\\nDifferent from standard methods that\\nminimize the average risk on the observed data, MaxUp\\ngenerates a set of random perturbations or transforms of\\neach observed data point, and minimizes the average risk\\nof the worst augmented data of each data point. This\\n}<->{\\nrved data point, and minimizes the average risk\\nof the worst augmented data of each data point. This al-\\nlows us to enforce robustness against the random perturba-\\ntions and transforms, and hence improve the generalization\\nperformance. MaxUp can easily leverage arbitrary state-\\nof-the-art data augmentation schemes (e.g. Zhang et al.,\\n2018; DeVries & Taylor, 2017; Cubuk et al., 2019a), and\\nsubstantially improves over them by minimizing the worst\\n(instead of average) risks on the augmented data, w\\n}<->{\\nially improves over them by minimizing the worst\\n(instead of average) risks on the augmented data, without\\nadding signiﬁcant computational ahead.\\nTheoretically, in the case of Gaussian perturbation, we\\nshow that MaxUp effectively introduces a gradient-norm\\nregularization term that serves to encourage smoothness of\\nthe loss function, which does not appear in standard data\\naugmentation methods that minimize the average risk.\\nMaxUp can be viewed as a “lightweight” variant of adver-\\nsarial training \\n}<->{\\nminimize the average risk.\\nMaxUp can be viewed as a “lightweight” variant of adver-\\nsarial training against adversarial input pertubrations (e.g.\\nTram`er et al., 2018; Madry et al., 2017), but is mainly de-\\nsigned to improve the generalization on the clean data, in-\\nstead of robustness on perturbed data (although MaxUp\\ndoes also increase the adversarial robustness in Gaussian\\nadversarial certiﬁcation as we shown in our experiments\\n(Section 4.4)). In addition, compared with standard adver-\\nsarial\\n}<->{\\nion as we shown in our experiments\\n(Section 4.4)). In addition, compared with standard adver-\\nsarial training methods such as projected gradient descent\\n(PGD) (Madry et al., 2017), MaxUp is much simpler and\\ncomputationally much faster, and can be easily adapted to\\nincrease various robustness deﬁned by the corresponding\\ndata augmentation schemes.\\nWe test MaxUp on three challenging tasks: image classi-\\nﬁcation, language modeling, and certiﬁed defense against\\nadversarial examples (Cohen et al., 201\\n}<->{\\nsi-\\nﬁcation, language modeling, and certiﬁed defense against\\nadversarial examples (Cohen et al., 2019).\\nWe ﬁnd that\\nMaxUp can leverage the different state-of-the-art data aug-\\nmentation methods and boost their performance to achieve\\nnew state-of-the-art on a range of tasks, datasets, and neural\\narchitectures. In particular, we set up a new state-of-the-art\\nresult on ImageNet classiﬁcation without extra data, which\\nimproves the best 85.5% top1 accuracy by Xie et al. (2019)\\nto 85.8%. For the adver\\n}<->{\\nextra data, which\\nimproves the best 85.5% top1 accuracy by Xie et al. (2019)\\nto 85.8%. For the adversarial certiﬁcation task, we ﬁnd\\n\\n--------------------------------------------------\\nMaxUp: A Simple Way to Improve Generalization of Neural Network Training\\nMaxup allows us to train more veriﬁably robust classiﬁers\\nthan prior arts such as the PGD-based adversarial training\\nproposed by Salman et al. (2019).\\n2. Main Method\\nWe start with introducing the main idea of MaxUp, and then\\ndiscuss its effec\\n}<->{\\n (2019).\\n2. Main Method\\nWe start with introducing the main idea of MaxUp, and then\\ndiscuss its effect of introducing smoothness regularization\\nin Section 2.1.\\nERM\\nGiving a dataset Dn = {xi}n\\ni=1, learning often re-\\nduces to a form of empirical risk minimization (ERM):\\nmin\\nθ Ex∼Dn [L(x, θ)] ,\\n(1)\\nwhere θ is a parameter of interest (e.g., the weights of a\\nneural network), and L(x, θ) denotes the loss associated\\nwith data point x. A key issue of ERM is the risk of over-\\nﬁtting, especially when the \\n}<->{\\ns associated\\nwith data point x. A key issue of ERM is the risk of over-\\nﬁtting, especially when the data information is insufﬁcient.\\nMaxUp\\nWe propose MaxUp to alleviate overﬁtting. The\\nidea is to generate a set of random augmented data and min-\\nimize the maximum loss over the augmented data.\\nFormally, for each data point x in Dn, we generate a set\\nof perturbed data points {x′\\ni}m\\ni=1 that are similar to x, and\\nestimate θ by minimizing the maximum loss over {x′\\ni}:\\nMaxUp:\\nmin\\nθ Ex∼Dn\\n\\x14\\nmax\\ni∈[m] \\n}<->{\\nlar to x, and\\nestimate θ by minimizing the maximum loss over {x′\\ni}:\\nMaxUp:\\nmin\\nθ Ex∼Dn\\n\\x14\\nmax\\ni∈[m] L(x′\\ni, θ)\\n\\x15\\n.\\n(2)\\nThis loss can be easily minimized with stochastic gradient\\ndescent (SGD). Note that the gradient of the maximum loss\\nis simply the gradient of the worst copy, that is,\\n∇θ\\n\\x12\\nmax\\ni∈[m] L(x′\\ni, θ)\\n\\x13\\n= ∇θL(x′\\ni∗, θ),\\n(3)\\nwhere i∗= arg maxi∈[m] L(x′\\ni, θ). This yields a simple\\nand practical algorithm shown in Algorithm 1.\\nIn our work, we assume the augmented data {x′\\ni}m\\ni=1 is\\ni.i.d\\n}<->{\\ntical algorithm shown in Algorithm 1.\\nIn our work, we assume the augmented data {x′\\ni}m\\ni=1 is\\ni.i.d. generated from a distribution P(·|x). The P(·|x) can\\nbe based on small perturbations around x, e.g., P(·|x) =\\nN(x, σ2I), the Gaussian distribution with mean x and\\nisotropic variance σ2. The P(·|x) can also be constructed\\nbased on invariant data transformations that are widely\\nused in the data augmentation literature, such as random\\ncrops, equalizing, rotations, and clips for images (see e.g\\nCubu\\n}<->{\\nentation literature, such as random\\ncrops, equalizing, rotations, and clips for images (see e.g\\nCubuk et al., 2019a; DeVries & Taylor, 2017; Cubuk et al.,\\n2019b).\\n2.1. MaxUp as a Smoothness Regularization\\nWe provide a theoretical interpretation of Maxup as in-\\ntroducing a gradient-norm regularization to the original\\nERM objective to encourage smoothness. Here we con-\\nsider the simple case of isotropic Gaussian perturbation,\\nwhen P(·|x) = N(x, σ2I). For simplifying notation, we\\ndeﬁne\\n˜LP,m(x, θ) \\n}<->{\\nopic Gaussian perturbation,\\nwhen P(·|x) = N(x, σ2I). For simplifying notation, we\\ndeﬁne\\n˜LP,m(x, θ) := E{x′\\ni}m\\ni=1∼P(·|x)m\\n\\x14\\nmax\\ni∈[m] L(x′\\ni, θ)\\n\\x15\\n,\\n(4)\\nwhich represents the expected MaxUp risk of data point x\\nwith m augmented copies.\\nTheorem 1 (MaxUp as Gradient-Norm Regularization).\\nConsider\\n˜LP,m(x, θ) deﬁned in (4) with P(·|x)\\n=\\nN(x, σ2I). Assume L(x, θ) is second-order differentiable\\nw.r.t. x. Then\\n˜LP,m(x, θ) = L(x, θ) + cm,σ ∥∇xL(x, θ)∥2 + O(σ2),\\nwhere cm,σ is a constant and cm,σ = Θ(σ√\\n}<->{\\n x. Then\\n˜LP,m(x, θ) = L(x, θ) + cm,σ ∥∇xL(x, θ)∥2 + O(σ2),\\nwhere cm,σ is a constant and cm,σ = Θ(σ√log m), where\\nΘ(·) denotes the big-Theta notation.\\nTheorem 1 shows that, the expected MaxUp risk can be\\nviewed as introducing a Lipschitz-like regularization with\\nthe gradient norm ∥∇xL(x, θ)∥2, which encourages the\\nsmoothness of L(x, θ) w.r.t. the input x. The strength of\\nthe regularization is controlled by cm,σ, which depends on\\nthe number of samples m and perturbation magnitude σ.\\nProof. Using \\n}<->{\\nrolled by cm,σ, which depends on\\nthe number of samples m and perturbation magnitude σ.\\nProof. Using Taylor expansion, we have\\n˜LP,m(x, θ)\\n= E\\n\\x14\\nmax\\ni∈[m] L(x′\\ni, θ)\\n\\x15\\n= L(x, θ) + E\\n\\x14\\nmax\\ni∈[m] (L(x′\\ni, θ) −L(x, θ))\\n\\x15\\n= L(x, θ) + E\\n\\x14\\nmax\\ni∈[q]⟨∇xL(x, θ), zi⟩\\n\\x15\\n+ O(σ2),\\nwhere we assume zi = x′\\ni −x, which follows N(0, σ2I).\\nThe rest of the proof is due to the Lemma 1 below.\\nLemma 1. Let g be a ﬁxed vector in Rd, and {zi}m\\ni=1 are\\nm i.i.d. random variables from N(0, σ2I). We have\\nE\\n\\x14\\nmax\\ni∈[m]⟨g, z\\n}<->{\\nvector in Rd, and {zi}m\\ni=1 are\\nm i.i.d. random variables from N(0, σ2I). We have\\nE\\n\\x14\\nmax\\ni∈[m]⟨g, zi⟩\\n\\x15\\n= cm,σ∥g∥2,\\nwhere cm,σ = Θ\\n\\x00σ√log m\\n\\x01\\n.\\nProof. Deﬁne yi\\n=\\n⟨g, zi⟩/ ∥g∥2.\\nThen {yi}m\\ni=1 is\\ni.i.d. from N(0, σ2). Therefore, cm,σ = E[maxi∈[m] yi],\\nwhich is well known to be Θ(σ√log m).\\nSee e.g.,\\nOrabona & P´al (2015); Kamath (2015) for bounds re-\\nlated to E[maxi∈[m] yi].\\nMore speciﬁcally, we have\\n0.23σ√log m ≤cm,σ ≤\\n√\\n2σ√log m following Kamath\\n(2015).\\n\\n----------------------------------------\\n}<->{\\nve\\n0.23σ√log m ≤cm,σ ≤\\n√\\n2σ√log m following Kamath\\n(2015).\\n\\n--------------------------------------------------\\nMaxUp: A Simple Way to Improve Generalization of Neural Network Training\\nAlgorithm 1 MaxUp with Stochastic Gradient Descent\\nInput: Dataset Dn = {xi}n\\ni=1; transformation distribution P(·|x); number of augmented data m; initialization θ0; SGD\\nparameters (batch size, step size η, etc).\\nrepeat\\nDraw a mini-batch M from Dn, and update θ via\\nθ ←θ −ηEx∼M\\n\\x14\\n∇θ\\n\\x12\\nmax\\ni∈[m] L(x′\\ni, θ)\\n\\x13\\x15\\n,\\nwhere \\n}<->{\\nat\\nDraw a mini-batch M from Dn, and update θ via\\nθ ←θ −ηEx∼M\\n\\x14\\n∇θ\\n\\x12\\nmax\\ni∈[m] L(x′\\ni, θ)\\n\\x13\\x15\\n,\\nwhere {x′\\ni}m\\ni=1 are drawn i.i.d. from P(·|x) for each x in the mini batch M. See Equation 3.\\nuntil convergence\\n3. Related Methods and Discussion\\nMaxUp is closely related to both data augmentation and ad-\\nversarial training. It can be viewed as an adversarial vari-\\nant of data augmentation, in that it minimizes the worse\\ncase loss on the perturbed data, instead of an average loss\\nlike typical data augm\\n}<->{\\nnimizes the worse\\ncase loss on the perturbed data, instead of an average loss\\nlike typical data augmentation methods. MaxUp can also\\nbe viewed as a “lightweight” variant of adversarial train-\\ning, in that the maximum loss is calculated by simple ran-\\ndom sampling, instead of more accurate gradient-based op-\\ntimizers for ﬁnding the adversarial loss, such as projected\\ngradient descent (PGD); MaxUp is much simpler and faster\\nthan the PGD-based adversarial training, and is more suit-\\nable for our pu\\n}<->{\\ns much simpler and faster\\nthan the PGD-based adversarial training, and is more suit-\\nable for our purpose of alleviating over-ﬁtting on clean data\\n(instead of adversarial defense). We now elaborate on these\\nconnections in depth.\\n3.1. Data Augmentation\\nData augmentation has been widely used in machine learn-\\ning, especially on image data which admits a rich set\\nof invariance transforms (e.g.\\ntranslation, rotation, ran-\\ndom cropping).\\nRecent augmentation techniques, such\\nas MixUp (Zhang et al., 20\\n}<->{\\nation, rotation, ran-\\ndom cropping).\\nRecent augmentation techniques, such\\nas MixUp (Zhang et al., 2018), CutMix (Yun et al., 2019)\\nand manifold MixUp (Verma et al., 2019) have been found\\nhighly useful in training deep neural networks, especially\\nin achieving state-of-the-art results on important image\\nclassiﬁcation benchmarks such as SVHN, CIFAR and Im-\\nageNet. More recently, more advanced methods have been\\ndeveloped to ﬁnd the optimal data augmentation policies\\nusing reinforcement learning or a\\n}<->{\\n have been\\ndeveloped to ﬁnd the optimal data augmentation policies\\nusing reinforcement learning or adversarial generative net-\\nwork (e.g. Cubuk et al., 2019a;b; Zhang et al., 2020).\\nMaxUp can easily leverage these advanced data augmen-\\ntation techniques to achieve good performance. The key\\ndifference, however, is that MaxUp in (2) minimizes the\\nmaximum loss on the augmented data, while typical data\\naugmentation methods minimize the average loss, that is,\\nmin\\nθ Ex∼Dn\\n\"\\n1\\nm\\nm\\nX\\ni=1\\nL(x′\\ni, θ)\\n#\\n,\\n\\n}<->{\\na\\naugmentation methods minimize the average loss, that is,\\nmin\\nθ Ex∼Dn\\n\"\\n1\\nm\\nm\\nX\\ni=1\\nL(x′\\ni, θ)\\n#\\n,\\n(5)\\nwhich we refer to as standard data augmentation through-\\nout the paper. It turns out (2) and (5) behave very differ-\\nent as regularization mechanisms, in that (5) does not in-\\ntroduce the gradient-norm regularization as (2), and hence\\ndoes not have the beneﬁt of having gradient-norm regular-\\nization. This is because the ﬁrst-order term in the Taylor\\nexpansion is canceled out due to the averagi\\n}<->{\\ntion. This is because the ﬁrst-order term in the Taylor\\nexpansion is canceled out due to the averaging in (5).\\nSpeciﬁcally, let P(·|x) be any distribution whose expecta-\\ntion is x and L(x, θ) is second-order differentiable w.r.t x.\\nDeﬁne the expected loss related to (5) on data point x:\\nˆLP,m(x, θ) := E{x′\\ni}m\\ni=1∼P(·|x)m\\n\"\\n1\\nm\\nm\\nX\\ni=1\\nL(x′\\ni, θ)\\n#\\n. (6)\\nThen with a simple Taylor expansion, we have\\nˆLP,m(x, θ) = L(x, θ) + O(σ2),\\nwhich misses the gradient-norm regularization term when\\ncompared wi\\n}<->{\\ne\\nˆLP,m(x, θ) = L(x, θ) + O(σ2),\\nwhich misses the gradient-norm regularization term when\\ncompared with MaxUp decomposition in Theorem 1.\\nNote that the MaxUp update is computationally faster than\\nthe solving (5) with the same m, because we only need\\nto backpropagate on the worst augmented copy for each\\ndata point (see Equation 3), while solving (5) requires to\\nbackpropagate on all the m copies at each iteration.\\n3.2. Adversarial Training\\nAdversarial training has been developed to defense various\\n\\n}<->{\\nach iteration.\\n3.2. Adversarial Training\\nAdversarial training has been developed to defense various\\nadversarial attacks on the data inputs (Madry et al., 2017).\\nIt estimates θ by solving the following problem:\\nmin\\nθ Ex∼Dn\\n\\x14\\nmax\\nx′∈B(x,r) L(x′, θ)\\n\\x15\\n,\\n(7)\\nwhere B(x, r) represents a ball centered at x with radius r\\nunder some metrics (e.g. ℓ0, ℓ1, ℓ2, or ℓ∞distances). The\\ninner maximization is often solved by running projected\\ngradient descent (PGD) for a number of iterations.\\nMaxUp in (2) can be \\n}<->{\\n solved by running projected\\ngradient descent (PGD) for a number of iterations.\\nMaxUp in (2) can be roughly viewed as solving the inner\\nadversarial maximization problem in (7) using a “mild”,\\nor “lightweight” optimizer by randomly drawing m points\\n\\n--------------------------------------------------\\nMaxUp: A Simple Way to Improve Generalization of Neural Network Training\\nfrom P(·|x) and ﬁnding the best. Such mild adversarial\\noptimization increases the robustness against the random\\nperturbation it\\n}<->{\\nbest. Such mild adversarial\\noptimization increases the robustness against the random\\nperturbation it introduces, and hence enhance the general-\\nization performance. Adversarial ideas have also been used\\nto improvement generalization in a series of recent works\\n(e.g., Xie et al., 2019; Zhu et al., 2020).\\nDifferent from our method, typical adversarial training\\nmethods, especially these based PGD (Madry et al., 2017),\\ntend to solve the adversarial optimization much more ag-\\ngressively to achieve hi\\n}<->{\\ndry et al., 2017),\\ntend to solve the adversarial optimization much more ag-\\ngressively to achieve higher robustness, but at the cost of\\nscarifying the accuracy on clean data.\\nThere has been\\nshown a clear trade-off between the accuracy of a classiﬁer\\non clean data and its robustness against adversarial attacks\\n(see e.g., Tsipras et al., 2019; Zhang et al., 2019; Yin et al.,\\n2019; Schmidt et al., 2018). By using a mild adversarial\\noptimizer, MaxUp strikes a better balance between the ac-\\ncuracy on\\n}<->{\\n18). By using a mild adversarial\\noptimizer, MaxUp strikes a better balance between the ac-\\ncuracy on clean data and adversarial robustness.\\nBesides, MaxUp is much more computationally efﬁcient\\nthan PGD-based adversarial training, because it does not\\nintroduce additional back-propagation steps as PGD. In\\npractice, MaxUp can be equipped with various complex\\ndata augmentation methods (in which case P(·|x) can be\\ndiscrete distributions), while PGD-based adversarial train-\\ning mostly focuses on pertu\\n}<->{\\n(·|x) can be\\ndiscrete distributions), while PGD-based adversarial train-\\ning mostly focuses on perturbations in ℓp balls.\\n3.3. Online Hard Example Mining\\nOnline hard example mining (OHEM) (Shrivastava et al.,\\n2016) is a training method originally developed for region-\\nbased objective detection, which improves the performance\\nof neural networks by picking the hardest examples within\\nmini batches of stochastic gradient descent (SGD). It can\\nbe viewed as running SGD for minimizing the following\\nexp\\n}<->{\\n stochastic gradient descent (SGD). It can\\nbe viewed as running SGD for minimizing the following\\nexpected loss\\nmin\\nθ EM\\n\\x14\\nmax\\nx∈M L(x, θ)\\n\\x15\\n,\\nwhich amounts to randomly picking a mini-batch M\\nat each iteration and minimizing the loss of the hard-\\nest example within M.\\nBy doing so, OHEM can fo-\\ncus more on the hard examples and hence improves\\nthe performance on borderline cases.\\nThis makes\\nOHEM particularly useful for class-imbalance tasks, e.g.\\nobject detection (Shrivastava et al., 2016),\\nperson \\n}<->{\\nicularly useful for class-imbalance tasks, e.g.\\nobject detection (Shrivastava et al., 2016),\\nperson re-\\nidentiﬁcation (Luo et al., 2019).\\nDifferent with MaxUp, the hardest examples in OHEM are\\nselected in mini-batches consisting of independently se-\\nlected examples, with no special correlation or similarity.\\nMathematically, it can be viewed as reweighing the data\\ndistribution to emphasize harder instances. This is sub-\\nstantially different from MaxUp, which is designed to en-\\nforce the robustnes\\n}<->{\\nnstances. This is sub-\\nstantially different from MaxUp, which is designed to en-\\nforce the robustness against existing random data augmen-\\ntation/perturbation schemes.\\nMethod\\nTop-1 error\\nTop-5 error\\nVanilla (He et al., 2016b)\\n76.3\\n-\\nDropout (Srivastava et al., 2014)\\n76.8\\n93.4\\nDropPath (Larsson et al., 2017)\\n77.1\\n93.5\\nManifold Mixup (Verma et al., 2019)\\n77.5\\n93.8\\nAutoAugment (Cubuk et al., 2019a)\\n77.6\\n93.8\\nMixup (Zhang et al., 2018)\\n77.9\\n93.9\\nDropBlock (Ghiasi et al., 2018)\\n78.3\\n94.1\\nCutMix (Yun \\n}<->{\\n7.6\\n93.8\\nMixup (Zhang et al., 2018)\\n77.9\\n93.9\\nDropBlock (Ghiasi et al., 2018)\\n78.3\\n94.1\\nCutMix (Yun et al., 2019)\\n78.6\\n94.0\\nMaxUp+CutMix\\n78.9\\n94.2\\nTable 1. Summary of top1 and top5 accuracies on the validation\\nset of ImageNet for ResNet-50.\\n4. Experiments\\nWe test our method using both image classiﬁcation and lan-\\nguage modeling for which a variety of strong regularization\\ntechniques and data augmentation methods have been pro-\\nposed. We show that MaxUp can outperform all of these\\nmethods on the \\n}<->{\\nntation methods have been pro-\\nposed. We show that MaxUp can outperform all of these\\nmethods on the most challenging datasets (e.g. ImageNet,\\nPenn Treebank, and Wikitext-2) and state-of-the-art mod-\\nels (e.g. ResNet, EfﬁcientNet, AWD-LSTM). In addition,\\nwe apply our method to adversarial certiﬁcation via Gaus-\\nsian smoothing (Cohen et al., 2019), for which we ﬁnd that\\nMaxUp can outperform both the augmented data baseline\\nand PGD-based adversarial training baseline.\\nFor all the tasks, if training\\n}<->{\\n augmented data baseline\\nand PGD-based adversarial training baseline.\\nFor all the tasks, if training from scratch, we ﬁrst train the\\nmodel with standard data augmentation with 5 epochs and\\nthen switch to MaxUp.\\nTime and Memory Cost\\nMaxUp only slightly increase\\nthe time and memory cost compared with standard training.\\nDuring MaxUp, we only need to ﬁnd the worst instance out\\nof the m augmented copies through forward-propagation,\\nand then only back-propagateon the worst instance. There-\\nfore, the a\\n}<->{\\ns through forward-propagation,\\nand then only back-propagateon the worst instance. There-\\nfore, the additional cost of MaxUp over standard training\\nis m forward-propagation, which introduces no signiﬁcant\\noverhead on both memory and time cost.\\n4.1. ImageNet\\nWe evaluate MaxUp on ILSVRC2012, a subset of Im-\\nageNet classiﬁcation dataset (Deng et al., 2009).\\nThis\\ndataset contains around 1.3 million training images and\\n50,000 validation images.\\nWe follow the standard data\\nprocessing pipeline including\\n}<->{\\nining images and\\n50,000 validation images.\\nWe follow the standard data\\nprocessing pipeline including scale and aspect ratio distor-\\ntions, random crops, and horizontal ﬂips in training. Dur-\\ning the evaluation, we only use the single-crop setting.\\nImplementation\\nDetails\\nWe\\ntest\\nMaxUp\\nwith\\nP(·|x) deﬁned by the CutMix data augmentation tech-\\nnique (Yun et al., 2019) (referred to as MaxUp+CutMix).\\n\\n--------------------------------------------------\\nMaxUp: A Simple Way to Improve Generalization of N\\n}<->{\\n-------------------------------------------------\\nMaxUp: A Simple Way to Improve Generalization of Neural Network Training\\nModel\\nModel Size\\nFLOPs\\n+CutMix (%)\\n+MaxUp+CutMix (%)\\nResNet-101\\n44.55M\\n7.85G\\n79.83\\n80.26\\nProxylessNet-CPU\\n7.12M\\n481M\\n75.32\\n75.65\\nProxylessNet-GPU\\n4.36M\\n470M\\n75.08\\n75.42\\nProxylessNet-Mobile ×1.4\\n6.86M\\n603M\\n76.71\\n77.17\\nEfﬁcientNet-B7\\n66.35M\\n38.20G\\n85.22∗\\n85.45∗\\nFix-EfﬁcientNet-B8\\n87.42M\\n101.79G\\n85.57∗\\n85.80∗\\nTable 2. Top1 accuracies of different models on the validation set of\\n}<->{\\n8\\n87.42M\\n101.79G\\n85.57∗\\n85.80∗\\nTable 2. Top1 accuracies of different models on the validation set of ImageNet 2012. The “∗” indicates that MaxUp is applied to the\\npre-trained model and trained for 5 epochs.\\nCutMix randomly cuts and pasts patches among training\\nimages, while the ground truth labels are also mixed\\nproportionally to the area of the patches. MaxUp+CutMix\\napplies CutMix on one image for m times (cutting different\\nrandomly sampled patches), and select the worst case to\\ndo backpropagat\\n}<->{\\nr m times (cutting different\\nrandomly sampled patches), and select the worst case to\\ndo backpropagation.\\nWe test our method on ResNet-50, ResNet-101 (He et al.,\\n2016b), as well as recent energy-efﬁcient architectures,\\nincluding ProxylessNet (Cai et al., 2019) and Efﬁcient-\\nNet (Tan & Le, 2019). We resize the images to 600 × 600\\nand 845 × 845 for EfﬁcientNet-B7 and EfﬁcientNet-B8,\\nrespectively (Tan & Le, 2019), for which we process the\\nimages with the data processing pipelines proposed by\\nTouvron\\n}<->{\\n & Le, 2019), for which we process the\\nimages with the data processing pipelines proposed by\\nTouvron et al. (2019). For the other models, the input im-\\nage size is 224 × 224. To save computation resources, we\\nonly ﬁne-tune the pre-trained models with MaxUp for a few\\nepochs. We set m = 4 for MaxUp in the ImageNet-2012\\nexperiments unless indicated otherwise. This means that\\nwe optimize the worst case in 4 augmented samples for\\neach image.\\nFor ResNet-50, ResNet-101 and ProxylessNets, we train\\nthe m\\n}<->{\\ne in 4 augmented samples for\\neach image.\\nFor ResNet-50, ResNet-101 and ProxylessNets, we train\\nthe models for 20 epochs with learning rate 10−5 and batch\\nsize 256 on 4 GPUs for 20 epochs. For EfﬁcientNet, we ﬁx\\nthe parameters in the batch normalization layers and train\\nthe other parameters with learning rate 10−4 and batch size\\n1000 for 5 epochs.\\nAs shown in Table 2, for ResNet-50 and ResNet-101, we\\nachieve the best results among all the data augmentation\\nmethod. For EfﬁcientNet-B8, we further i\\n}<->{\\nwe\\nachieve the best results among all the data augmentation\\nmethod. For EfﬁcientNet-B8, we further improve the state-\\nof-the-art result on ImageNet with no extra data.\\nResNet-50 on ImageNet\\nTable 1 compares a num-\\nber of state-of-the-art regularization techniques with\\nMaxUp+CutMix on ImageNet with ResNet-50.1 We can\\nsee that MaxUp+CutMix achieves better performance com-\\npared to all the strong data augmentation and regularization\\nbaselines. From Table 1, we see that CutMix gives the best\\ntop1 er\\n}<->{\\na augmentation and regularization\\nbaselines. From Table 1, we see that CutMix gives the best\\ntop1 error (78.6%) among all the augmentation tasks, but\\nour method further improves it to 78.9%. DropBlock out-\\n1All the FLOPS and model size reported in this paper is cal-\\nculated by https://pypi.org/project/ptflops.\\nperforms all the other methods in terms of the top5 error,\\nbut by augmenting CutMix with MaxUp, we improve the\\n94.1% top5 error rate obtained by DropbBlock to 94.2%.\\nMore Results on Differ\\n}<->{\\n MaxUp, we improve the\\n94.1% top5 error rate obtained by DropbBlock to 94.2%.\\nMore Results on Different Architectures\\nTable 2 shows\\nthe result of ImageNet on ResNet-101, ProxylessNet-\\nCPU/GPU/Mobile (Cai et al., 2019) and EfﬁcientNet. We\\ncan see that MaxUp consistently improves the results\\nin all these cases.\\nOn ResNet-101, it improves the\\n79.83% baseline to 80.26%. On ProxylessNet-CPU and\\nProxylessNet-GPU, MaxUp enhances the 75.32% and\\n75.08% top1 accuracy to 75.65% and 75.42%, respectively.\\nOn\\n}<->{\\nssNet-GPU, MaxUp enhances the 75.32% and\\n75.08% top1 accuracy to 75.65% and 75.42%, respectively.\\nOn ProxylessNet-Mobile, we improve the 76.71% top1 ac-\\ncuracy to 77.17%.\\nFor EfﬁcientNet-B7, CutMix enhances the original top1 ac-\\ncuracy 85.0% (by Tan & Le, 2019) to 85.22%.\\nMaxUp\\nfurther improves the top1 accuracy to 88.45%.\\nOn\\nFix-EfﬁcientNet-B8, MaxUp obtains the state-of-the-art\\n85.80% top1 accuracy. The previous state-of-the-art top1\\naccuracy, 85.50%, is achieved by EfﬁcientNet-L2.\\n4.2. CIFAR-\\n}<->{\\nacy. The previous state-of-the-art top1\\naccuracy, 85.50%, is achieved by EfﬁcientNet-L2.\\n4.2. CIFAR-10 and CIFAR-100\\nWe test MaxUp equipped with Cutout (DeVries & Taylor,\\n2017) on CIFAR-10 and CIFAR-100, and denote it by\\nMaxUp+Cutout. We conduct our method on several neu-\\nral architectures, including ResNet-110 (He et al., 2016b),\\nPreAct-ResNet-110 (He et al., 2016a) and WideResNet-\\n28-10 (Zagoruyko & Komodakis, 2016). We set m = 10\\nfor WideResNet and m = 4 for the other models. We use\\nthe publi\\n}<->{\\nko & Komodakis, 2016). We set m = 10\\nfor WideResNet and m = 4 for the other models. We use\\nthe public code2 and keep their hyper-parameters.\\nImplementation Details\\nFor CIFAR-10 and CIFAR-\\n100, we use the standard data processing pipeline (mirror+\\ncrop) and train the model with 200 epochs. All the results\\nreported in this section are averaged over ﬁve runs.\\nWe train the models for 200 epochs on the training\\nset with 256 examples per mini-batch, and evaluate\\nthe trained models on the test set.\\nThe\\n}<->{\\n training\\nset with 256 examples per mini-batch, and evaluate\\nthe trained models on the test set.\\nThe learning rate\\n2The\\ncode\\nis\\ndownloaded\\nfrom\\nhttps://github.com/junyuseu/pytorch-cifar-models\\n\\n--------------------------------------------------\\nMaxUp: A Simple Way to Improve Generalization of Neural Network Training\\nModel\\n+ Cutout\\n+ MaxUp+Cutout\\nResNet-110\\n94.84 ± 0.11\\n95.41 ± 0.08\\nPreAct-ResNet-110\\n95.02 ± 0.15\\n95.52 ± 0.06\\nWideResNet-28-10\\n96.92 ± 0.16\\n97.18 ± 0.06\\nTable 3. Test accuracy on CI\\n}<->{\\n10\\n95.02 ± 0.15\\n95.52 ± 0.06\\nWideResNet-28-10\\n96.92 ± 0.16\\n97.18 ± 0.06\\nTable 3. Test accuracy on CIFAR10 for different architectures.\\nModel\\n+ Cutout\\n+ MaxUp+Cutout\\nResNet-110\\n73.64 ± 0.15\\n75.26 ± 0.21\\nPreAct-ResNet-110\\n74.37 ± 0.13\\n75.63 ± 0.26\\nWideResNet-28-10\\n81.59 ± 0.27\\n82.48 ± 0.23\\nTable 4. Test accuracy on CIFAR100 for different architectures.\\nstarts at 0.1 and is divided by 10 after 100 and 150\\nepochs for ResNet-110 and PreAct-ResNet-110.\\nFor\\nWideResNet-28-10, we follow the settings in t\\n}<->{\\nd 150\\nepochs for ResNet-110 and PreAct-ResNet-110.\\nFor\\nWideResNet-28-10, we follow the settings in the original\\npaper (Zagoruyko & Komodakis, 2016), where the learn-\\ning rate is divided by 10 after 60, 120 and 180 epochs.\\nWeight decay is set to 2.5−4 for all the models, and we\\ndo not use dropout.\\nResults\\nThe results on CIFAR-10 and CIFAR-100 are\\nsummarized in Table 3 and Table 4. We can see that the\\nmodels trained using MaxUp+Cutout signiﬁcantly outper-\\nform the standard Cutout for all the cases\\n}<->{\\nhe\\nmodels trained using MaxUp+Cutout signiﬁcantly outper-\\nform the standard Cutout for all the cases.\\nOn CIAFR-10, MaxUp improves the standard Cutout base-\\nline from 94.84% ± 0.11% to 95.41% ± 0.08% on ResNet-\\n110. It also improves the accuracy from 95.02% ± 0.15%\\nto 95.52% ± 0.06% on PreAct-ResNet-110.\\nOn CIFAR-100, MaxUp obtains improvements by a large\\nmargin. On ResNet-110 and PreAct-ResNet-110, MaxUp\\nimproves the performance of Cutout from 73.64%±0.15%\\nand 74.37% ± 0.13% to 75.26% ± 0.21% an\\n}<->{\\n, MaxUp\\nimproves the performance of Cutout from 73.64%±0.15%\\nand 74.37% ± 0.13% to 75.26% ± 0.21% and 75.63% ±\\n0.26%, respectively.\\nMaxUp+Cutout also improves the\\nstandard Cutout from 81.59%±0.27% to 82.48%±0.23%\\non WideResNet-28-10 on CIFAR-100.\\nAblation Study\\nWe test MaxUp with different sample\\nsize m and investigate its impact on the performance on\\nResNet-100 (a relatively small model) and WideResNet-\\n28-10 (a larger model).\\nTable 5 shows the result when we vary the sample size in\\nm ∈{1, 4, 1\\n}<->{\\nResNet-\\n28-10 (a larger model).\\nTable 5 shows the result when we vary the sample size in\\nm ∈{1, 4, 10, 20}. Note that MaxUp reduces to the na¨ıve\\ndata augmentation method when m = 1. As shown in Ta-\\nble 5, MaxUp with all m > 1 can improve the result of\\nstandard augmentation (m = 1). Setting m = 4 or m = 10\\nachieves best performance on ResNet-110 , and m = 10\\nobtains best performance on WideResNet-28-10. We can\\nsee that the results are not sensitive once m is in a proper\\nrange (e.g., m ∈[4 : 10])\\n}<->{\\n28-10. We can\\nsee that the results are not sensitive once m is in a proper\\nrange (e.g., m ∈[4 : 10]), and it is easy to outperform the\\nstandard data augmentation (m = 1) without much tuning\\nm\\nResNet-110\\nWideResNet-28-10\\n1\\n73.64 ± 0.15\\n81.59 ± 0.27\\n4\\n75.26 ± 0.21\\n81.82 ± 0.22\\n10\\n75.19 ± 0.13\\n82.48 ± 0.23\\n20\\n74.37 ± 0.18\\n82.43 ± 0.24\\nTable 5. Test accuracy on CIFAR100 with ResNet-110 and\\nWideResNet-28-10, when the sample size m varies.\\nof m. Furthermore, we suggest to use a large m for large\\nmodel\\n}<->{\\n28-10, when the sample size m varies.\\nof m. Furthermore, we suggest to use a large m for large\\nmodels, and a small m for relatively small models.\\n4.3. Language Modeling\\nFor language modeling, we test MaxUp on two benchmark\\ndatasets: Penn Treebank (PTB) and Wikitext-2 (WT2). We\\nuse the code provided by Wang et al. (2019) as our base-\\nline3, which stacks a three-layer LSTM and implements a\\nbag of regularization and optimization tricks for neural lan-\\nguage modeling proposed by Merity et al. (2018)\\n}<->{\\ngularization and optimization tricks for neural lan-\\nguage modeling proposed by Merity et al. (2018), such as\\nweight tying, word embedding drop and Averaged SGD.\\nFor this task, we apply MaxUp using word embedding\\ndropout (Merity et al., 2018) as the random data augmen-\\ntation method.\\nWord embedding dropout implements\\ndropout on the embedding matrix at the word level, where\\nthe dropout is broadcasted across all the embeddings of all\\nthe word vectors. For the selected words, their embedding\\nvector\\n}<->{\\ned across all the embeddings of all\\nthe word vectors. For the selected words, their embedding\\nvectors are set to be zero vectors. The other word embed-\\ndings in the vocabulary are scaled by\\n1\\n1−p, where p is the\\nprobability of embedding dropout.\\nAs the word embedding layer serves as the ﬁrst layer in a\\nneural language model, we apply MaxUp in this layer. We\\ndo feed-forward for m times and select the worst case to do\\nbackpropagation for each given sentence. In this section,\\nwe set a small m = 2 s\\n}<->{\\nhe worst case to do\\nbackpropagation for each given sentence. In this section,\\nwe set a small m = 2 since the models are already well-\\nregularized by other regularization techniques.\\nImplement Details\\nThe PTB corpus (Marcus et al.,\\n1993) is a standard dataset for benchmarking language\\nmodels. It consists of 923k training, 73k validation and\\n82k test words. We use the processed version provided by\\nMikolov et al. (2010) that is widely used for PTB.\\nThe WT2 dataset is introduced in Merity et al. (20\\n}<->{\\nikolov et al. (2010) that is widely used for PTB.\\nThe WT2 dataset is introduced in Merity et al. (2018) as\\nan alternative to PTB. It contains pre-processed Wikipedia\\narticles, and the training set contains 2 million words.\\nThe training procedure can be decoupled into two stages:\\n1) optimizing the model with SGD and averaged SGD\\n(ASGD); 2) restarting ASGD for ﬁne-tuning twice. We ap-\\nply MaxUp in both stages, and report the perplexity scores\\nat the end of the second stage. We also report the perp\\n}<->{\\noth stages, and report the perplexity scores\\nat the end of the second stage. We also report the perplexity\\nscores with a recently-proposed post-process method, dy-\\n3https://github.com/ChengyueGongR/advsoft\\n\\n--------------------------------------------------\\nMaxUp: A Simple Way to Improve Generalization of Neural Network Training\\nMethod\\nParams\\nValid\\nTest\\nNAS-RNN (Zoph & Le, 2017)\\n54M\\n-\\n62.40\\nAWD-LSTM (Merity et al., 2018)\\n24M\\n58.50\\n56.50\\nAWD-LSTM + FRAGE (Gong et al., 2018)\\n24M\\n58.10\\n56.10\\nAWD-LS\\n}<->{\\nTM (Merity et al., 2018)\\n24M\\n58.50\\n56.50\\nAWD-LSTM + FRAGE (Gong et al., 2018)\\n24M\\n58.10\\n56.10\\nAWD-LSTM + MoS (Yang et al., 2018)\\n22M\\n56.54\\n54.44\\nw/o dynamic evaluation\\nADV-AWD-LSTM (Wang et al., 2019)\\n24M\\n57.15\\n55.01\\nADV-AWD-LSTM + MaxUp\\n24M\\n56.25\\n54.27\\n+ dynamic evaluation (Krause et al., 2018)\\nADV-AWD-LSTM (Wang et al., 2019)\\n24M\\n51.60\\n51.10\\nADV-AWD-LSTM + MaxUp\\n24M\\n50.83\\n50.29\\nTable 6. Perplexities on the validation and test sets on the Penn Treebank dataset. Smaller perplexities refer to bet\\n}<->{\\nties on the validation and test sets on the Penn Treebank dataset. Smaller perplexities refer to better language modeling\\nperformance. Params denotes the number of model parameters.\\nMethod\\nParams\\nValid\\nTest\\nAWD-LSTM (Merity et al., 2018)\\n33M\\n68.60\\n65.80\\nAWD-LSTM + FRAGE (Gong et al., 2018)\\n33M\\n66.50\\n63.40\\nAWD-LSTM + MoS (Yang et al., 2018)\\n35M\\n63.88\\n61.45\\nw/o dynamic evaluation\\nADV-AWD-LSTM (Wang et al., 2019)\\n33M\\n63.68\\n61.34\\nADV-AWD-LSTM + MaxUp\\n33M\\n62.48\\n60.19\\n+ dynamic evaluation (Krause et a\\n}<->{\\net al., 2019)\\n33M\\n63.68\\n61.34\\nADV-AWD-LSTM + MaxUp\\n33M\\n62.48\\n60.19\\n+ dynamic evaluation (Krause et al., 2018)\\nADV-AWD-LSTM (Wang et al., 2019)\\n33M\\n42.36\\n40.53\\nADV-AWD-LSTM + MaxUp\\n33M\\n41.29\\n39.61\\nTable 7. Perplexities on the validation and test sets on the WikiText-2 dataset. Smaller perplexities refer to better language modeling\\nperformance. Params denotes the number of model parameters.\\nnamical evaluation (Krause et al., 2018) after the training\\nprocess.\\nResults on PTB and WT2\\nThe results on P\\n}<->{\\nevaluation (Krause et al., 2018) after the training\\nprocess.\\nResults on PTB and WT2\\nThe results on PTB and\\nWT2 corpus are illustrated in Table 6 and Table 7, re-\\nspectively. We calculate the perplexity on the validation\\nand test set for each method to evaluate its performance.\\nWe can see that MaxUp outperforms the state-of-the-art\\nresults achieved by Frage (Gong et al., 2018) and Mix-\\nture of SoftMax (Yang et al., 2018). We further compare\\nMaxUp to the result of Wang et al. (2019) based on AWD-\\n\\n}<->{\\nMax (Yang et al., 2018). We further compare\\nMaxUp to the result of Wang et al. (2019) based on AWD-\\nLSTM (Merity et al., 2018) at two checkpoints, with or\\nwithout dynamic evaluation (Krause et al., 2018). On PTB,\\nwe enhance the baseline from 55.01/51.10 to 54.27/50.29\\nat these two checkpoints on the test set.\\nOn WT2, we\\nenhance the baseline from 61.34/40.53 to 60.19/39.61 at\\nthese two checkpoints on the test set. Results on valida-\\ntion set are reported in both Table 6 and 7 to show that the\\nimp\\n}<->{\\non the test set. Results on valida-\\ntion set are reported in both Table 6 and 7 to show that the\\nimprovement can not achieved by simple hyper-parameter\\ntuning on the test set.\\n4.4. Adversarial Certiﬁcation\\nModern\\nimage\\nclassiﬁers\\nare\\nknown\\nto\\nbe\\nsensi-\\ntive to small, adversarially-chosen perturbations on in-\\nputs (Goodfellow et al., 2014).\\nTherefore, for making\\nhigh-stakes decisions, it is of critical importance to de-\\nvelop methods with certiﬁed robustness, which provide\\n(high probability) prov\\n}<->{\\nical importance to de-\\nvelop methods with certiﬁed robustness, which provide\\n(high probability) provable guarantees on the correctness\\nof the prediction subject to arbitrary attacks within certain\\nperturbation ball.\\nRecently, Cohen et al. (2019) proposed to construct certi-\\nﬁably robust classiﬁers against ℓ2 attacks by introducing\\nGaussian smoothing on the inputs, which is shown to out-\\nperform all the previous ℓ2-robust classiﬁers in CIFAR-\\n10. There has been two major methods for training such\\n}<->{\\nl the previous ℓ2-robust classiﬁers in CIFAR-\\n10. There has been two major methods for training such\\nsmoothed classiﬁers: Cohen et al. (2019) trains the classi-\\nﬁer with a Gaussian data augmentation technique, while\\nSalman et al. (2019) improves the original Gaussian data\\naugmentation by using PGD (projected gradient descent)\\nadversarial training, in which PGD is used to ﬁnd a local\\nmaximal within a given ℓ2 perturbation ball.\\nIn our experiment, we use MaxUp with Gaussian per-\\nturbation (referre\\n}<->{\\n a given ℓ2 perturbation ball.\\nIn our experiment, we use MaxUp with Gaussian per-\\nturbation (referred to as MaxUp+Gauss) to train better\\n\\n--------------------------------------------------\\nMaxUp: A Simple Way to Improve Generalization of Neural Network Training\\nℓ2 RADIUS (CIFAR-10)\\n0.25\\n0.5\\n0.75\\n1.0\\n1.25\\n1.5\\n1.75\\n2.0\\n2.25\\n2.5\\n2.75\\nCohen et al. (2019) (%)\\n60\\n43\\n34\\n23\\n17\\n14\\n12\\n10\\n8\\n6\\n4\\nSalman et al. (2019) (%)\\n74\\n57\\n48\\n38\\n33\\n29\\n25\\n19\\n17\\n14\\n12\\nOurs (%)\\n74\\n57\\n49\\n40\\n35\\n31\\n27\\n22\\n19\\n17\\n15\\nTable 8. Cert\\n}<->{\\n (2019) (%)\\n74\\n57\\n48\\n38\\n33\\n29\\n25\\n19\\n17\\n14\\n12\\nOurs (%)\\n74\\n57\\n49\\n40\\n35\\n31\\n27\\n22\\n19\\n17\\n15\\nTable 8. Certiﬁed accuracy on CIFAR-10 of the best classiﬁers by different methods, evaluated against ℓ2 attacks of different radiuses.\\nsmoothed classiﬁers than the methods by Cohen et al.\\n(2019) and Salman et al. (2019).\\nLike how MaxUp im-\\nproves upon standard data augmentation, it is natural to ex-\\npect that our MaxUp+Gauss can learn more robust classi-\\nﬁers than the standard Gaussian data augmentation metho\\n}<->{\\nur MaxUp+Gauss can learn more robust classi-\\nﬁers than the standard Gaussian data augmentation method\\nin Cohen et al. (2019).\\nTraining Details\\nWe applied MaxUp to Gaussian aug-\\nmented data on CIFAR-10 with ResNet-110 (He et al.,\\n2016b).\\nWe follow the training pipelines described in\\nSalman et al. (2019). We set a batch size of 256, an ini-\\ntial learning rate of 0.1 which drops by a factor of 10 every\\n50 epochs, and train the models for 150 epochs.\\nEvaluation\\nAfter training the smoothed classiﬁers\\n}<->{\\nry\\n50 epochs, and train the models for 150 epochs.\\nEvaluation\\nAfter training the smoothed classiﬁers, we\\nevaluation the certiﬁed accuracy of different models under\\ndifferent ℓ2 perturbation sets. Given an input image x and\\na perturbation region B, the smoothed classiﬁer is called\\ncertiﬁably correct if its prediction is correct and has a guar-\\nanteed lower bound larger than 0.5 in B. The certiﬁed accu-\\nracy is the percentage of images that are certiﬁably correct.\\nFollowing Salman et al. (2019), w\\n}<->{\\nccu-\\nracy is the percentage of images that are certiﬁably correct.\\nFollowing Salman et al. (2019), we calculate the certiﬁed\\naccuracy of all the classiﬁers for various radius and report\\nthe best results overall of the classiﬁers. We use the codes\\nprovided by Cohen et al. (2019) to calculate certiﬁed ac-\\ncuracy.4\\nFollowing Salman et al. (2019), we select the best hyperpa-\\nrameters with grid search. The only two hyperparameters\\nof our MaxUp+Gauss are the sample size m and the vari-\\nance σ2 of the \\n}<->{\\n The only two hyperparameters\\nof our MaxUp+Gauss are the sample size m and the vari-\\nance σ2 of the Gaussian perturbation, which we search in\\nm ∈{5, 25, 50, 100, 150} and σ ∈{0.12, 0.25, 0.5, 1.0}.\\nIn comparison, Salman et al. (2019) requiers to search a\\nlarger number of hyper-parameters, including the number\\nof steps of the PGD, the number of noise samples, the\\nmaximum ℓ2 perturbation, and the variance of Gaussian\\ndata augmentation during training and testing.\\nOverall,\\nSalman et al. (2019) requ\\n}<->{\\nriance of Gaussian\\ndata augmentation during training and testing.\\nOverall,\\nSalman et al. (2019) requires to train and evaluate over 150\\nmodels for hyperparmeter tuning, while MaxUp+Gauss re-\\nquires only 20 models.\\nResults\\nWe show the certiﬁed accuraries on CIFAR-10\\nin Table 8 under ℓ2 attacks for each ℓ2 radius. We ﬁnd that\\nMaxUp outperforms Cohen et al. (2019) for all the ℓ2 ra-\\ndiuses by a large margin. For example, MaxUp can im-\\n4https://github.com/locuslab/smoothing\\nprove the certiﬁed accura\\n}<->{\\n margin. For example, MaxUp can im-\\n4https://github.com/locuslab/smoothing\\nprove the certiﬁed accuracy at radius 0.25 from 60% to\\n74% and improve the 4% accuracy on radius 2.75 to 15%.\\nMaxUp also outperforms the PGD-based adversarial train-\\ning of Salman et al. (2019) for all the radiuses, boosting the\\naccuracy from 14% to 17% at radius 2.5, and from 12% to\\n15% at radius 2.75.\\nIn summary, MaxUp clearly outperforms both Cohen et al.\\n(2019) and Salman et al. (2019).\\nMaxUp is also much\\nfaster and r\\n}<->{\\nearly outperforms both Cohen et al.\\n(2019) and Salman et al. (2019).\\nMaxUp is also much\\nfaster and requires less hyperparameter tuning than\\nSalman et al. (2019). Although the PGD-based method of\\nSalman et al. (2019) was designed to outperform the orig-\\ninal method by Cohen et al. (2019), MaxUp+Gauss further\\nimproves upon Salman et al. (2019), likely because MaxUp\\nwith Gaussian perturbation is more compatible with the\\nGaussian smoothing based certiﬁcation of Cohen et al.\\n(2019) than PGD adversari\\n}<->{\\n compatible with the\\nGaussian smoothing based certiﬁcation of Cohen et al.\\n(2019) than PGD adversarial optimization.\\n5. Conclusion\\nIn this paper, we propose MaxUp, a simple and efﬁcient\\ntraining algorithms for improving generalization, espe-\\ncially for deep neural networks. MaxUp can be viewed as a\\nintroducing a gradient-norm smoothness regularization for\\nGaussian perturbation, but does not require to evaluate the\\ngradient norm explicitly, and can be easily combined with\\nany existing data augmen\\n}<->{\\ne to evaluate the\\ngradient norm explicitly, and can be easily combined with\\nany existing data augmentation methods. We empirically\\nshow that MaxUp can improve the performance of data\\naugmentation methods in image classiﬁcation, language\\nmodeling, and certiﬁed defense. Especially, we achieve\\nSOTA performance on ImageNet.\\nFor future works, we will apply MaxUp to more applica-\\ntions and models, such as BERT (Devlin et al., 2019). Fur-\\nthermore, we will generalize MaxUp to apply mild adver-\\nsarial o\\n}<->{\\nas BERT (Devlin et al., 2019). Fur-\\nthermore, we will generalize MaxUp to apply mild adver-\\nsarial optimization on feature and label spaces for other\\nchallenging tasks in machine learning, including transfer\\nlearning, semi-supervised learning.\\nReferences\\nCai, H., Zhu, L., and Han, S. Proxylessnas: Direct neural\\narchitecture search on target task and hardware. ICLR,\\n2019.\\nCohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed ad-\\nversarial robustness via randomized smoothing. ICML,\\n2019.\\n\\n------\\n}<->{\\n., and Kolter, J. Z. Certiﬁed ad-\\nversarial robustness via randomized smoothing. ICML,\\n2019.\\n\\n--------------------------------------------------\\nMaxUp: A Simple Way to Improve Generalization of Neural Network Training\\nCubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le,\\nQ. V.\\nAutoaugment: Learning augmentation policies\\nfrom data. CVPR, 2019a.\\nCubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Ran-\\ndaugment: Practical data augmentation with no separate\\nsearch. arXiv preprint arXiv:1909.13719, \\n}<->{\\nan-\\ndaugment: Practical data augmentation with no separate\\nsearch. arXiv preprint arXiv:1909.13719, 2019b.\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and\\nFei-Fei, L. Imagenet: A large-scale hierarchical image\\ndatabase. In CVPR, pp. 248–255. IEEE, 2009.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. 2019.\\nDeVries, T. and Taylor, G. W.\\nImproved regularization\\nof convolutional neural networks with cuto\\n}<->{\\n19.\\nDeVries, T. and Taylor, G. W.\\nImproved regularization\\nof convolutional neural networks with cutout.\\narXiv\\npreprint arXiv:1708.04552, 2017.\\nGhiasi, G., Lin, T.-Y., and Le, Q. V. Dropblock: A regular-\\nization method for convolutional networks. In NeurIPS,\\npp. 10727–10737, 2018.\\nGong, C., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.-\\nY. Frage: Frequency-agnostic word representation. In\\nNeurIPS, pp. 1334–1345, 2018.\\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining\\nand harnessing adve\\n}<->{\\n, pp. 1334–1345, 2018.\\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining\\nand harnessing adversarial examples. ICLR, 2014.\\nHe, K., Zhang, X., Ren, S., and Sun, J.\\nIdentity map-\\npings in deep residual networks. In ECCV, pp. 630–645.\\nSpringer, 2016a.\\nHe, K., Zhang, X., Ren, S., and Sun, J.\\nDeep residual\\nlearning for image recognition. In CVPR, pp. 770–778,\\n2016b.\\nKamath, G. Bounds on the expectation of the maximum of\\nsamples from a gaussian. URL http://www. gautamka-\\nmath. com/writings/gaus\\n}<->{\\nctation of the maximum of\\nsamples from a gaussian. URL http://www. gautamka-\\nmath. com/writings/gaussian max. pdf, 2015.\\nKrause, B., Kahembwe, E., Murray, I., and Renals, S. Dy-\\nnamic evaluation of neural sequence models.\\nICML,\\n2018.\\nLarsson, G., Maire, M., and Shakhnarovich, G. Fractal-\\nnet: Ultra-deep neural networks without residuals. ICLR,\\n2017.\\nLuo, H., Gu, Y., Liao, X., Lai, S., and Jiang, W.\\nBag\\nof tricks and a strong baseline for deep person re-\\nidentiﬁcation. In CVPRW, pp. 0–0, 2019.\\nMa\\n}<->{\\n.\\nBag\\nof tricks and a strong baseline for deep person re-\\nidentiﬁcation. In CVPRW, pp. 0–0, 2019.\\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and\\nVladu, A.\\nTowards deep learning models resistant to\\nadversarial attacks. ICLR, 2017.\\nMarcus, M., Santorini, B., and Marcinkiewicz, M. A.\\nBuilding a large annotated corpus of english: The penn\\ntreebank. 1993.\\nMerity, S., Keskar, N. S., and Socher, R. Regularizing and\\noptimizing lstm language models. ICLR, 2018.\\nMikolov, T., Karaﬁ´at, M., Burget, L\\n}<->{\\n. Regularizing and\\noptimizing lstm language models. ICLR, 2018.\\nMikolov, T., Karaﬁ´at, M., Burget, L., ˇCernock`y, J., and\\nKhudanpur, S. Recurrent neural network based language\\nmodel. In ISCA, 2010.\\nOrabona, F. and P´al, D.\\nOptimal non-asymptotic lower\\nbound on the minimax regret of learning with expert ad-\\nvice. arXiv preprint arXiv:1511.02176, 2015.\\nSalman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen-\\nshteyn, I., and Bubeck, S. Provably robust deep learning\\nvia adversarially trained smoo\\n}<->{\\n, H., Razen-\\nshteyn, I., and Bubeck, S. Provably robust deep learning\\nvia adversarially trained smoothed classiﬁers. NeurIPS,\\n2019.\\nSchmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and\\nMadry, A. Adversarially robust generalization requires\\nmore data. In NeurIPS, pp. 5014–5026, 2018.\\nShrivastava, A., Gupta, A., and Girshick, R.\\nTraining\\nregion-based object detectors with online hard example\\nmining. In CVPR, pp. 761–769, 2016.\\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\\nand Sala\\n}<->{\\ning. In CVPR, pp. 761–769, 2016.\\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\\nand Salakhutdinov, R. Dropout: a simple way to prevent\\nneural networks from overﬁtting. JMLR, pp. 1929–1958,\\n2014.\\nTan, M. and Le, Q. V. Efﬁcientnet: Rethinking model scal-\\ning for convolutional neural networks. ICML, 2019.\\nTouvron, H., Vedaldi, A., Douze, M., and J´egou, H. Fix-\\ning the train-test resolution discrepancy. arXiv preprint\\narXiv:1906.06423, 2019.\\nTram`er, F., Kurakin, A., Papernot, N., Goodf\\n}<->{\\non discrepancy. arXiv preprint\\narXiv:1906.06423, 2019.\\nTram`er, F., Kurakin, A., Papernot, N., Goodfellow, I.,\\nBoneh, D., and McDaniel, P. Ensemble adversarial train-\\ning: Attacks and defenses. ICLR, 2018.\\nTsipras, D., Santurkar, S., Engstrom, L., Turner, A., and\\nMadry, A. Robustness may be at odds with accuracy. In\\nICLR, 2019.\\nVerma, V., Lamb, A., Beckham, C., Courville, A.,\\nMitliagkis, I., and Bengio, Y. Manifold mixup: Encour-\\naging meaningful on-manifold interpolation as a regular-\\nizer. ICM\\n}<->{\\nengio, Y. Manifold mixup: Encour-\\naging meaningful on-manifold interpolation as a regular-\\nizer. ICML, 2019.\\nWang, D., Gong, C., and Liu, Q. Improving neural lan-\\nguage modeling via adversarial training. ICML, 2019.\\nXie, C., Tan, M., Gong, B., Wang, J., Yuille, A., and Le,\\nQ. V. Adversarial examples improve image recognition.\\narXiv preprint arXiv:1911.09665, 2019.\\n\\n--------------------------------------------------\\nMaxUp: A Simple Way to Improve Generalization of Neural Network Training\\nYang, Z.\\n}<->{\\n------------------\\nMaxUp: A Simple Way to Improve Generalization of Neural Network Training\\nYang, Z., Dai, Z., Salakhutdinov, R., and Cohen, W. W.\\nBreaking the softmax bottleneck: A high-rank RNN lan-\\nguage model. In ICLR, 2018.\\nYin, D., Kannan, R., and Bartlett, P. Rademacher complex-\\nity for adversarially robust generalization. In ICML, pp.\\n7085–7094, 2019.\\nYun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo,\\nY. Cutmix: Regularization strategy to train strong clas-\\nsiﬁers with localizable\\n}<->{\\n Choe, J., and Yoo,\\nY. Cutmix: Regularization strategy to train strong clas-\\nsiﬁers with localizable features. ICCV, 2019.\\nZagoruyko, S. and Komodakis, N.\\nWide residual net-\\nworks. In BMVC, pp. 87.1–87.12.BMVA Press, Septem-\\nber 2016.\\nZhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D.\\nmixup: Beyond empirical risk minimization. In ICLR,\\n2018.\\nZhang, H., Yu, Y., Jiao, J., Xing, E., Ghaoui, L. E., and\\nJordan, M. Theoretically principled trade-off between\\nrobustness and accuracy. In Chaudhuri, \\n}<->{\\n., and\\nJordan, M. Theoretically principled trade-off between\\nrobustness and accuracy. In Chaudhuri, K. and Salakhut-\\ndinov, R. (eds.), ICML, pp. 7472–7482, 2019.\\nZhang, X., Wang, Q., Zhang, J., and Zhong, Z. Adversarial\\nautoaugment. ICLR, 2020.\\nZhu, C., Cheng, Y., Gan, Z., Sun, S., Goldstein, T., and Liu,\\nJ. Freelb: Enhanced adversarial training for language\\nunderstanding. ICLR, 2020.\\nZoph, B. and Le, Q. V. Neural architecture search with\\nreinforcement learning. ICLR, 2017.\\n\\n--------------------\\n}<->{\\n Le, Q. V. Neural architecture search with\\nreinforcement learning. ICLR, 2017.\\n\\n--------------------------------------------------\\n', 'Summaries': 'arXiv:2002.09024v1 [cs.LG] 20 Feb 2020 \\nMaxUp: A Simple Technique for Improving Generalization Performance of Neural Networks.\\nThe technique generates augmented data with random perturbations and minimizes the maximum loss over training, improving generalization performance of deep neural networks.\\n}<->{\\nMinimizing maximum loss over augmented data introduces regularity against random perturbations. This improves generation performance by implicitly encouraging smoothness or robustness.\\n}<->{\\nMaxUp improves smoothness by reducing loss as a penalty in various tasks such as image classification, language modeling, and adversarial certification with top-1 accuracy increased from 85.5% to 85.8% on ImageNet without extra data.\\n}<->{\\nMachine learning aims to reduce overfitting by utilizing prior knowledge.\\nRegularization introduces penalties for model complexity.\\nData augmentation techniques also aid in reducing overfitting.\\n}<->{\\nData augmentation techniques focus on enforcing smoothness properties, whereas machine learning improves with invariance properties. Novel approaches combining knowledge of models and data hold potential for significant performance improvements.\\n}<->{\\nMaxUp: A Training Method to Improve Generalization Performance.\\n\\nProposed by researchers at UT Austin, MaxUp is a method that alleviates over-ﬁtting issue in machine learning models.\\nIt generates perturbations or transforms for each observed data point and minimizes the risk of the worst augmented data.\\n}<->{\\nMaxUp minimizes the risk of the worst augmented data point to improve generalization performance. It reduces random perturbations and transforms, leading to better robustness against data augmentation.\\n}<->{\\nMaxUp improves over existing methods by minimizing worst-case risks on augmented data without significant computational overhead. It introduces a gradient-norm regularization term to encourage smoothness in loss functions, unlike standard data augmentation methods that only minimize average risk.\\n}<->{\\nMaxUp improves generalization on clean data rather than solely increasing robustness against perturbations. It is a lightweight variant of adversarial training and also enhances Gaussian adversarial certification.\\n}<->{\\nMaxUp is a simple and computationally efficient adversarial training method. It surpasses standard methods like projected gradient descent in efficiency. MaxUp can be easily adapted to various data augmentation schemes for increased robustness.\\n}<->{\\nMaxUp can improve performance of various state-of-the-art data augmentation methods and achieve new state-of-the-art results on ImageNet classification with 85.8% top1 accuracy, surpassing previous best result by Xie et al. (2019) of 85.5%.\\n}<->{\\nMaxUp improves generalization accuracy to 85.8% by allowing more robust classifier training than prior arts like PGD-based adversarial training, improving top1 accuracy from 85.5% to 85.8%.\\n}<->{\\n2019 \\n2. Main Method \\nEMPIRICAL RISK MINIMIZATION (ERM): min θ Ex∼Dn [L(x, θ)] where θ is a parameter and L(x, θ) is loss associated with data point x. A key issue of ERM is the risk of overfitting.\\n}<->{\\nThe Maximum Up (MaxUp) method alleviates over-fitting in ERM. To alleviate this, a set of m perturbed data points are generated for each original data point x, similar to x. The goal is to minimize the maximum loss over these perturbed data points while estimating θ, as shown by Min(θ Ex∼Dn - max i∈[m]).\\n}<->{\\nMinimizing maximum loss over {x′i} by estimating θ using stochastic gradient descent (SGD). The gradient of the maximum loss is the gradient of the worst copy. Using this, a practical algorithm can be derived where ∇θ equals the gradient of the worst copy L(x′i*, θ) at i*.\\n}<->{\\nWe assume {x′i}m i=1 to be independent and identically distributed (i.i.d.) generated from P(·|x), a distribution based on small perturbations around x. This can be the Gaussian distribution N(x, σ2I) with mean x and isotropic variance σ2.\\n}<->{\\nMaxUp introduces a gradient-norm regularization to ERM objective for smoothness. It considers isotropic Gaussian perturbation with P(·|x) = N(x, σ²I), where LP,m(x, θ) is defined as the likelihood of data x given model parameters θ and magnitude m.\\n}<->{\\nGaussian Perturbation:\\nE{x′i}m i=1∼N(x, σ2I)\\nmax i∈[m] L(x′i, θ)\\n\\nTheorem 1: \\n˜LP,m(x, θ) = L(x, θ) + cm,σ ∥∇xL(x, θ)∥2 + O(σ2),\\nwhere cm,σ is a constant and cm,σ = Θ(σ√\\n}<->{\\nx. Then \\n˜LP,m(x, θ) = L(x, θ) + cm,σ ∥∇xL(x, θ)∥2 + O(σ2),\\nwhere cm,σ = Θ(σ√log m), \\nthe expected MaxUp risk can be viewed as Lipschitz-like regularization with \\ngradient norm ∥∇xL(x, θ)∥2.\\n}<->{\\nThe perturbed Log-Likelihood (LP) can be expressed as L(x, θ) + E max i∈[m] ⟨∇xL(x, θ), zi⟩ + O(σ2), where zi follows a multivariate normal distribution N(0, σ2I).\\n}<->{\\nIn Rd and {zi}m, \\nmax i.i.d. random variables from N(0, σ2I) are:\\nE[maxi∈[m] ⟨g, zi⟩] = cm,σ = Θ(σ√log m), where 0.23σ√log m ≤ cm,σ ≤ √2σ√log m.\\n}<->{\\nThe given information is about the MaxUp algorithm. Here\\'s a summary:\\n\\nMaxUp Algorithm Improves Generalization in Neural Network Training\\n\\n* Uses stochastic gradient descent with augmentation.\\n* Involves:\\n\\t+ Drawing a mini-batch from dataset Dn.\\n\\t+ Updating parameters θ via backpropagation and gradient descent.\\n\\t+ Maximizing the expected loss L(x′, θ) over augmented data.\\n}<->{\\nDraw a mini-batch M from Dn, update θ via:\\nθ ←θ −ηEx∼M\\nmax\\ni∈[m] L(x′\\ni, θ)\\nwhere {x′\\ni}m\\ni=1 are drawn i.i.d. from P(·|x) for each x in the mini batch M.\\n\\nRelated methods: data augmentation, adversarial training.\\n}<->{\\nMaxUp reduces worse-case loss on perturbed data, unlike typical augmentation methods that use average loss. It\\'s a \"lightweight\" variant of adversarial training using random sampling instead of gradient-based optimizers like projected gradient descent (PGD).\\n}<->{\\nData augmentation offers simpler and faster adversarial training than PGD-based methods. It is more suitable for alleviating overfitting on clean data rather than adversarial defense. Data augmentation techniques like translation, rotation, and random cropping have been widely used in machine learning, particularly with image data, to introduce variance into the dataset.\\n}<->{\\nDeep neural networks have achieved state-of-the-art results using MixUp, CutMix, and manifold MixUp data augmentation techniques. These methods are highly effective for image classification benchmarks like SVHN, CIFAR, and ImageNet.\\n}<->{\\nMaxUp minimizes maximum loss on augmented data using reinforcement learning or adversarial generative networks. This differs from typical methods that minimize average loss, with MaxUp\\'s approach ensuring better performance.\\n}<->{\\nStandard data augmentation involves calculating L(x\\', θ) for each i, then averaging these values over all X (equation 5). Unlike a regularization mechanism, this method does not introduce gradient-norm regularization. This is because the first-order term in its Taylor expansion cancels out due to averaging, resulting in less benefit from gradient-norm regularization.\\n}<->{\\nFirst-order Taylor series is canceled due to averaging in the formula. The expected loss ˆLP,m(x, θ) is defined as the average of losses over a distribution P with expectation x and second-order differentiable L with respect to x. This results in an approximation of L(x, θ) equal to L(x, θ) + O(σ^2), missing the gradient-norm regularization term.\\n}<->{\\nLP,m(x, θ) = L(x, θ) + O(σ2),\\n misses gradient-norm regularization term.\\n MaxUp update is faster than solving (5)\\n due to only backpropagating on the worst augmented copy.\\n}<->{\\nAdversarial training estimates θ by solving:\\nmin θ Ex∼Dn - max x′∈B(x,r) L(x′, θ)\\nwhere B(x, r) is a ball with center x and radius r under some metrics. \\nInner maximization is often solved using projected gradient descent (PGD) for multiple iterations.\\n}<->{\\nSolving MaxUp involves running projected gradient descent (PGD) for multiple iterations, utilizing a \"mild\" optimizer to minimize perturbations in the input data. This approach enhances the generalization of neural network training by increasing robustness to random perturbations and improving performance on adversarial attacks.\\n}<->{\\nMild adversarial optimization improves robustness against random perturbations, enhancing generalization performance. Recent works utilize adversarial ideas to improve generalization (Xie et al., 2019; Zhu et al., 2020). Traditional PGD-based methods aggressively solve adversarial optimization for high accuracy gains.\\n}<->{\\nClassifiers tend to aggressively improve robustness against adversarial attacks at the cost of accuracy on clean data. A trade-off exists between classifier accuracy and robustness, with studies showing that increasing one often comes at the expense of the other (e.g., Tsipras et al., 2019; Zhang et al., 2019).\\n}<->{\\nMaxUp balances accuracy and robustness using a mild adversarial optimizer. It is computationally more efficient than PGD-based adversarial training, eliminating additional back-propagation steps. MaxUp can incorporate various complex data augmentation methods.\\n}<->{\\nOnline Hard Example Mining (OHEM) is a training method that selects the hardest examples in mini-batches to improve performance.\\n}<->{\\nStochastic Gradient Descent (SGD) minimizes expected loss:\\nmin θ EM max x∈M L(x, θ)\\nRandomly picking mini-batch M at each iteration to focus on hard examples, improving performance on borderline cases, particularly useful for class-imbalance tasks like object detection.\\n}<->{\\nObject detection and person re-identification tasks benefit from OHEM (Online Hard Example Mining), a method that selects hardest examples in mini-batches with no correlation or similarity. It reweights the data distribution to emphasize harder instances, contrasting with MaxUp which focuses on enforcing robustness.\\n}<->{\\nTop-1 and Top-5 errors for ImageNet classification are compared across various data augmentation methods:\\n \\n- Vanilla: 76.3, 93.4\\n- Dropout: 76.8, 93.4\\n- DropPath: 77.1, 93.5\\n- Manifold Mixup: 77.5, 93.8\\n- AutoAugment: 77.6, 93.8\\n- Mixup: 77.9, 93.9\\n- DropBlock: 78.3, 94.1\\n- CutMix: results not shown\\n}<->{\\nTop1 accuracy on ImageNet validation set: \\n- ResNet-50 with Mixup: 77.9, 93.8\\n- ResNet-50 with DropBlock: 78.3, 94.1\\n- ResNet-50 with CutMix: 78.6, 94.0\\n- ResNet-50 with MaxUp+CutMix: 78.9, 94.2\\n}<->{\\nMaxUp outperforms existing methods on challenging datasets and state-of-the-art models such as ImageNet, Penn Treebank, Wikitext-2, ResNet, EfficientNet, AWD-LSTM, and achieves better results than augmented data and PGD-based adversarial training baseline for adversarial certification.\\n}<->{\\nAugmented data baseline: \\n- Train model with standard data augmentation for 5 epochs.\\n- Switch to MaxUp.\\n\\nPGD-based adversarial training baseline:\\n- Train model from scratch using PGD-based adversarial training.\\n\\nTime and Memory Cost:\\n- MaxUp slightly increases time and memory cost compared to standard training.\\n}<->{\\nMaxUp uses forward-propagation followed by back-propagation on the worst instance, introducing minimal additional overhead in memory and time costs. Evaluated on ILSVRC2012 (ImageNet), a subset of ImageNet with 1.3 million training images and 50,000 validation images.\\n}<->{\\nWe use a standard data processing pipeline with images and 50,000 validation images. The implementation involves using the CutMix data augmentation technique for testing MaxUp.\\n}<->{\\nMaxUp improves generalization by adding a simple technique to neural networks training. \\n\\nModel sizes and FLOPs vary between ResNet-101, ProxylessNet variants and EfficientNet-B7. MaxUp+CutMix shows high accuracy for some models like ResNet-101 (80.26%), ProxylessNet-CPU (75.32%) and Fix-EfﬁcientNet-B8 (85.57*).\\n}<->{\\nTop1 accuracies of different models on ImageNet 2012 validation set:\\n\\n- 87.42M: base model accuracy\\n- 101.79G: MaxUp+CutMix with 5 epochs applied\\n- 85.57∗: MaxUp without CutMix\\n- 85.80∗: MaxUp+CutMix\\n}<->{\\nWe test our method on ResNet-50 and ResNet-101. We also evaluate it on ProxylessNet and EfficientNet architectures. Image sizes vary: 600x600, 845x845, to accommodate different networks\\' requirements.\\n}<->{\\nWe process images with data processing pipelines proposed by Touvron et al. (2019). For MaxUp, we fine-tune pre-trained models with 224 × 224 input size for a few epochs, optimizing worst-case in 4 augmented samples per image.\\n}<->{\\nFor each image model, the following training parameters were used:\\n\\n* ResNet-50: 20 epochs, learning rate 10−5, batch size 256 on 4 GPUs.\\n* ResNet-101: 20 epochs, learning rate 10−5, batch size 256 on 4 GPUs.\\n* ProxylessNets: 20 epochs, learning rate 10−5, batch size 256 on 4 GPUs.\\n* EfficientNet-B8: 5 epochs, learning rate 10−4, batch size 1000.\\n}<->{\\nMaxUp+CutMix achieves the best results among various state-of-the-art techniques on ImageNet with ResNet-50, outperforming other methods with no extra data.\\n}<->{\\nCutMix performs best among augmentation tasks with top1 error at 78.6%, improved by our method to 78.9%. DropBlock\\'s top5 error is 94.1%, improved to 94.2% when combined with MaxUp.\\n}<->{\\nMaxUp improves top5 error rate from 94.1% to 94.2%. On ImageNet tests, it boosts results: \\n- ResNet-101 (79.83% to 80.26%)\\n- ProxylessNet-CPU (75.32% to 75.65%)\\n- ProxylessNet-GPU (75.08% to 75.42%)\\n}<->{\\nMaxUp and CutMix enhance model accuracies:\\n- ssNet-GPU: 75.32% to 75.65%\\n- ProxylessNet-Mobile: 76.71% to 77.17%\\n- EfﬁcientNet-B7: 85.0% to 85.22%, then 88.45% by MaxUp\\n- Fix-EfﬁcientNet-B8: achieved state-of-the-art 85.80%\\n}<->{\\nEfficientNet-L2 achieves top1 accuracy of 85.50%. The study tests MaxUp with Cutout on CIFAR-10 and CIFAR-100 datasets, using ResNet-110, PreAct-ResNet-110, and WideResNet models. The optimal values for m are set to 4 for the first two models and 10 for WideResNet.\\n}<->{\\nFor WideResNet with m = 10, we use standard data processing pipeline (mirror+crop) and train with 200 epochs. We keep public code2\\'s hyper-parameters and report results averaged over five runs. Training is done on CIFAR-10 and CIFAR-100 sets.\\n}<->{\\nTraining with 256 examples per mini-batch, and evaluating trained models on the test set shows the following results:\\n\\n* MaxUp+Cutout: ResNet-110 - 94.84%\\n* PreAct-ResNet-110: 95.02%\\n* WideResNet-28-10: 96.92%\\n}<->{\\nTest accuracy on CIFAR10 for different architectures:\\n\\n* WideResNet-28-10: 96.92 ± 0.16\\n* MaxUp+Cutout + WideResNet-28-10: 97.18 ± 0.06\\n\\nTest accuracy on CIFAR100 for different architectures:\\n\\n* ResNet-110: \\n  * PreAct-ResNet-110: 74.37 ± 0.13 \\n  * ResNet-110: 73.64 ± 0.15\\n* WideResNet-28-10: 81.59 ± 0.27\\n}<->{\\nFor ResNet-110, PreAct-ResNet-110: 150 epochs.\\nFor WideResNet-28-10: 60,120,180 epoch milestones with learning rate decrease by 10 times.\\n\\nWeight decay: 2.5-4.\\nNo dropout used.\\n}<->{\\nMaxUp+Cutout model outperforms standard Cutout on both CIAFR-10 and CIFAR-100, with significant improvements. On ResNet-110, MaxUp improves baseline accuracy from 94.84% to 95.41%, while increasing overall performance by 0.57%.\\n}<->{\\nMaxUp improves Cutout\\'s performance from \\n73.64% to 75.26% on ResNet-28-10 and 74.37% to 75.63% on CIFAR-100.\\nMaxUp+Cutout increases standard Cutout\\'s performance from \\n81.59% to 82.48% on WideResNet-28-10.\\nAblation study shows varying sample size affects MaxUp\\'s performance, particularly on smaller models like ResNet-100.\\n}<->{\\nResNet-28-10 with MaxUp improves performance for varying sample sizes: \\nm = 1, standard augmentation; \\nm = 4 or 10, best performance on ResNet-110 and WideResNet-28-10.\\n}<->{\\nThe results show that test accuracy is relatively stable once the sample size (m) falls within a proper range ([4 : 10]). Outperforming standard data augmentation (m = 1) is easy with minimal tuning of m. ResNet-110 and WideResNet-28-10 achieved better test accuracy at larger values of m, specifically between 10 to 20.\\n}<->{\\nWhen sample size m varies with model size, large m is used for large models, small m for relatively small models. Language Model MaxUp tested on PTB and WT2 datasets using a 3-layer LSTM baseline from Wang et al. (2019).\\n}<->{\\nMerity et al. (2018) proposed weight tying, word embedding drop and Averaged SGD for neural language modeling. MaxUp using word embedding dropout implements dropout on the embedding matrix at the word level, where dropout is broadcasted across all embeddings of all word vectors.\\n}<->{\\nSelected words\\' embedding vectors are set to zero. Other word embeddings are scaled by (1 - p), where p is the probability of embedding dropout, with p=1-m=2.\\n}<->{\\nPTB corpus uses 923k training, 73k validation and 82k test words with a small m = 2 for backpropagation.\\n}<->{\\nikolov et al. (2010) is widely used for PTB. The WT2 dataset was introduced by Merity et al. (2018), containing 2 million pre-processed Wikipedia articles. A two-stage training procedure using SGD and averaged SGD (ASGD) with MaxUp is applied, followed by fine-tuning.\\n}<->{\\nPerplexity scores for AWD-RNN models:\\n\\nNAS-RNN: 62.40, 56.50\\nAWD-LSTM: 58.50, 56.50\\nAWD-LSTM + FRAGE: 58.10, 56.10\\nAWD-LS (no perplexity score given)\\n}<->{\\nTM (Merity et al., 2018): \\n24M, 58.50, 56.50 \\n\\nAWD-LSTM + FRAGE: \\n24M, 58.10, 56.10 \\n\\nAWD-LSTM + MoS: \\n22M, 56.54, 54.44 \\n\\nADV-AWD-LSTM (2019): \\n24M, 51.60, 51.10 \\n\\nADV-AWD-LSTM + MaxUp: \\n24M, 50.83, 50.29\\n}<->{\\nTies on validation and test sets on the Penn Treebank dataset resulted in better language modeling performance. Smaller perplexities indicate improved model performance. AWD-LSTMs with additional methods showed increased accuracy.\\n}<->{\\nAdvantage of using MaxUp in ADV-AWD-LSTM models:\\n\\n- WikiText-2 dataset validation: \\n  - 33M parameters, ADV-AWD-LSTM + MaxUp: 62.48 perplexity\\n  - 33M parameters, ADV-AWD-LSTM: 42.36 perplexity\\n\\n- WikiText-2 dataset test:\\n  - 33M parameters, ADV-AWD-LSTM + MaxUp: 60.19 perplexity\\n  - 33M parameters, ADV-AWD-LSTM: 40.53 perplexity\\n}<->{\\nEvaluation results show MaxUp outperforming state-of-the-art results from Frage and a mixture of SoftMax in perplexity calculations for PTB and WT2 datasets, surpassing even the 2019 work by Wang et al.\\n}<->{\\nMax (Yang et al., 2018) compared MaxUp with Wang et al.\\'s result using AWD-LSTM at two checkpoints, with and without dynamic evaluation. On PTB test set, baseline improved from 55.01/51.10 to 54.27/50.29. On WT2, it improved from 61.34/40.53 to 60.19/39.61.\\n}<->{\\nImage classifiers are sensitive to small adversarially-chosen perturbations on inputs. Certified robustness provides high-probability proofs for making high-stakes decisions, improving against adversarial attacks.\\n}<->{\\nMethods with certified robustness provide guarantees on prediction correctness under arbitrary attacks within a perturbation ball. Cohen et al. (2019) introduced Gaussian smoothing to construct certifiably robust classifiers, outperforming previous ℓ2-robust classifiers in CIFAR-10.\\n}<->{\\nCohen et al. (2019) trains classifiers with Gaussian data augmentation, while Salman et al. (2019) improves it by using PGD adversarial training within an ℓ2 perturbation ball to find local maxima.\\n}<->{\\nℓ2 perturbation balls of varying radii (0.25-2.75) were used to evaluate the performance of MaxUp, a method for improving neural network training generalization. Cohen et al.\\'s results showed improvements up to 60% accuracy, while Salman et al.\\'s results reached 74%. The authors\\' own results exceeded those of Salman et al., achieving 74%, 57%, and 49% accuracies on the same radii as the others respectively\\n}<->{\\nHere is a summary of the text in 50 words or less:\\n\\nThe best classifiers on CIFAR-10 achieved higher accuracy with smoothed classifiers compared to methods by Cohen et al. and Salman et al. MaxUp+Gauss improves upon standard Gaussian data augmentation, expected to learn more robust classifiers.\\n}<->{\\nMaxUp+Gauss outperforms standard Gaussian data augmentation in classifying CIFAR-10 images using ResNet-110. Training involved a batch size of 256, initial learning rate 0.1 with decaying factor 10 every 50 epochs, for 150 epochs.\\n}<->{\\n50 epochs, train for 150 epochs. Evaluate models with ℓ2 perturbation sets. A model is considered certified correct if its prediction is accurate and has a guaranteed lower bound larger than 0.5 in the perturbation region B. The certified accuracy is the percentage of images that meet this condition.\\n}<->{\\nccu- accuracy is the percentage of correctly classified images.\\nIt follows Salman et al. (2019) for calculation using Cohen et al. (2019) codes.\\nBest results are reported by classifiers for various radii. \\nGrid search selects best hyperparameters for MaxUp+Gauss, including m and σ2.\\n}<->{\\nHyperparameters for MaxUp+Gauss: \\nm ∈{5, 25, 50, 100, 150} \\nσ ∈{0.12, 0.25, 0.5, 1.0}.\\n}<->{\\nSalman et al. (2019) compared MaxUp+Gauss to Cohen et al. (2019). MaxUp outperformed Cohen et al. by a large margin on CIFAR-10 with ℓ2 attacks, requiring only 20 models for hyperparameter tuning instead of over 150 models used in Salman et al.\\n}<->{\\nThe MaxUp model improves certified accuracy in facial recognition with smoothing techniques, beating existing methods by Cohen et al. (2019) and Salman et al. (2019), with significant increases in accuracy at various radii values.\\n}<->{\\nMaxUp outperforms Cohen et al. (2019) and Salman et al. (2019). It is faster and requires less hyperparameter tuning than Salman et al. (2019). MaxUp+Gauss improves upon Salman et al., likely due to compatibility with Gaussian smoothing in Cohen et al.\\'s certification method.\\n}<->{\\nMaxUp is an efficient training algorithm for improving generalization of deep neural networks. It introduces a gradient-norm smoothness regularization through Gaussian perturbation, avoiding explicit computation of gradient norms, and is compatible with Gaussian smoothing based certification of Cohen et al. (2019).\\n}<->{\\nMaxUp improves data augmentation performance in image classification, language modeling, and certified defense, achieving SOTA on ImageNet. It can be easily combined with existing methods and applied to other models like BERT.\\n}<->{\\nBERT (Devlin et al., 2019) generalizes MaxUp for mild adversarial optimization in feature and label spaces, applicable to tasks like transfer learning and semi-supervised learning.\\n}<->{\\nCubiK, E.D., et al (2019)\\n - Improves neural network generalization through simple regularization\\n \\nZoph,B.,et al (2019a)\\n - Autoaugment: learns augmentation policies from data\\n \\n\\nE.D.Cubuk,Z.B.Zoph.J.Shlens.Q.V.Le(2019)\\n- Randuaugment : Practical data augmentation without separate search\\n}<->{\\npractical data augmentation with no separate search \\narXiv:1909.13719 (2019)\\nImagenet: A large-scale hierarchical image database by Deng et al., CVPR 2009\\nPre-training of deep bidirectional transformers for language understanding by Devlin et al., 2019\\nImproved regularization of convolutional neural networks with cutout by DeVries and Taylor, W.\\n}<->{\\nT. DeVries and G. W. Taylor introduced improved regularization of convolutional neural networks with cutout (arXiv:1708.04552, 2017). Ghiasi et al. proposed Dropblock for regularization (NeurIPS 2018). Gong et al. developed frequency-agnostic word representation Frage (NeurIPS 2018). Goodfellow et al. explained and harnessed adversarial examples in Explaining and harnessing adversarial examples (arXiv, date not specified).\\n}<->{\\nGoodfellow et al. (2014) explored adversarial examples to understand their explanation and harnessing potential.\\nHe et al. (2016a) introduced identity mappings in deep residual networks, while He et al. (2016b) proposed deep residual learning for image recognition.\\nKamath (URL not specified) derived bounds on the expectation of the maximum of samples from a Gaussian.\\n}<->{\\nHere is the summary:\\n\\n* Gaussian Mixture of Experts (GME) models can be used to sample from a Gaussian distribution with high efficiency.\\n* Krause et al.\\'s work (ICML 2018) presents dynamic evaluation methods for neural sequence models.\\n* Larsson et al.\\'s FractalNet (ICLR 2017) uses ultra-deep neural networks without residuals for image recognition tasks.\\n* Luo et al.\\'s work (CVPRW 2019) proposes a strong baseline for deep person re-identification using bag-of-tricks.\\n}<->{\\nMadry et al. (2017) presented a method to make deep learning models resistant to adversarial attacks.\\n\\nMarcus et al. (1993) created the Penn Treebank, a large annotated corpus of English.\\n\\nMerity et al. (2018) developed techniques for regularizing and optimizing LSTM language models.\\n\\nMikolov et al. (no publication found)\\n}<->{\\nRegularizing and optimizing LSTM language models was researched by:\\n\\n- Mikolov et al. (2010) who introduced a recurrent neural network based language model.\\n- Orabona and Pal (2015) who provided an optimal non-asymptotic lower bound on the minimax regret of learning with expert advice.\\n- Salman et al. (no date, not provided in original text) who worked on provably robust deep learning via adversarially trained smooth models.\\n}<->{\\nH.Hazan, I.Razen-shteyn, S.Bubeck, Adversarially trained smoothed classiﬁers for provably robust deep learning. NeurIPS 2019.\\nL.Schmidt et al., Need for more data in adversarially robust generalization. NeurIPS 2018.\\nA.Shrivastava et al., Training region-based object detectors with online hard example mining. CVPR 2016.\\n}<->{\\nHere is a summary in original language:\\n\\n* Srivastava et al. (2014) introduced Dropout to prevent overfitting in neural networks.\\n* Tan and Le (2019) proposed EfficientNet for efficient model scaling of convolutional neural networks.\\n* Touvron et al. (2019) addressed the train-test resolution discrepancy issue using a simple fix.\\n}<->{\\nDiscrepancy between robustness and accuracy in machine learning models is highlighted by Tsipras et al. (2019). Research papers, including Tram`er et al. (2018) and Verma et al. (2019), discuss ensemble adversarial training and manifold mixup as methods to address this issue.\\n}<->{\\nHere is a summary in the original language with useful information below 100 words:\\n\\nManifold Mixup and Adversarial Training improve neural network performance.\\n- Manifold Mixup encourages meaningful on-manifold interpolation as a regularizer (ICML, 2019).\\n- Improving Neural Language Modeling via Adversarial Training enhances model capabilities (ICML, 2019).\\n- Adversarial Examples improve Image Recognition accuracy (arXiv preprint arXiv:1911.09665, 2019).\\n\\nMaxUp proposes a simple approach to enhance generalization in neural networks training.\\n}<->{\\nMaxUp improves generalization of neural networks by breaking the softmax bottleneck using a high-rank RNN language model. It uses Rademacher complexity for adversarially robust generalization, achieving state-of-the-art results in classifier training and providing localizable features through Cutmix regularization strategy.\\n}<->{\\nChoe, J., and Yoo, Y. Cutmix: Regularization strategy to train strong classifiers with localizable features. ICCV, 2019.\\nZagoruyko, S. and Komodakis, N. Wide residual networks. BMVC, September 2016.\\nZhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization. ICLR, 2018.\\nZhang, H., Yu, Y., Jiao, J., Xing, E., Ghaoui, L. E., and Jordan, M. Theoretically principled trade-off between robustness and accuracy in deep learning.\\n}<->{\\nJordan M, Adversarial Autoaugment, ICML 2020 \\nZhang X, Wang Q et al, Adversarial Autoaugment, ICLR 2020 \\nC Zhu Y Gan et al, Freelb: Enhanced adversarial training for language understanding, ICLR 2020 \\nZoph B Le, Neural Architecture Search with Reinforcement Learning, ICLR 2017\\n}<->{\\n\"Le, Q. V.\" \\n2017 ICLR conference.\\nReinforcement learning-based neural architecture search'}}\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/Reports/2003.08237v5_normal.txt': {'Scores': '8\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n9\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n2\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n5', 'Chunks': 'FIXING THE TRAIN-TEST RESOLUTION DISCREPANCY: FIXEFFICIENTNET\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, Herv´e J´egou\\nFacebook AI Research\\nABSTRACT\\nThis paper provides an extensive analysis of the perfor-\\nmance of the EfﬁcientNet image classiﬁers with several re-\\ncent training procedures, in particular one that corrects the\\ndiscrepancy between train and test images [1]. The resulting\\nnetwork, called FixEfﬁcientNet, signiﬁcantly outperforms the\\ninitial architecture with the same number of par\\n}<->{\\ncalled FixEfﬁcientNet, signiﬁcantly outperforms the\\ninitial architecture with the same number of parameters.\\nFor instance, our FixEfﬁcientNet-B0 trained without ad-\\nditional training data achieves 79.3% top-1 accuracy on Im-\\nageNet with 5.3M parameters.\\nThis is a +0.5% absolute\\nimprovement over the Noisy student EfﬁcientNet-B0 trained\\nwith 300M unlabeled images. An EfﬁcientNet-L2 pre-trained\\nwith weak supervision on 300M unlabeled images and fur-\\nther optimized with FixRes achieves 88.5% top-1 a\\n}<->{\\nweak supervision on 300M unlabeled images and fur-\\nther optimized with FixRes achieves 88.5% top-1 accuracy\\n(top-5: 98.7%), which establishes the new state of the art for\\nImageNet with a single crop.\\nThese improvements are thoroughly evaluated with cleaner\\nprotocols than the one usually employed for Imagenet, and\\nparticular we show that our improvement remains in the\\nexperimental setting of ImageNet-v2, that is less prone to\\noverﬁtting, and with ImageNet Real Labels. In both cases we\\nalso establ\\n}<->{\\net-v2, that is less prone to\\noverﬁtting, and with ImageNet Real Labels. In both cases we\\nalso establish the new state of the art.\\n1. INTRODUCTION\\nIn order to obtain the best possible performance from Con-\\nvolutional neural nets (CNNs), the training and testing data\\ndistributions should match. However, in image recognition,\\ndata pre-processing procedures are often different for train-\\ning and testing: the most popular practice is to extract a rect-\\nangle with random coordinates from the image to \\n}<->{\\nng: the most popular practice is to extract a rect-\\nangle with random coordinates from the image to artiﬁcially\\nincrease the amount of training data. This Region of Classiﬁ-\\ncation (RoC) is then resized to obtain an image, or crop, of a\\nﬁxed size (in pixels) that is fed to the CNN. At test time, the\\nRoC is instead set to a square covering the central part of the\\nimage, which results in the extraction of a center crop. Thus,\\nwhile the crops extracted at training and test time have the\\nsame size, \\n}<->{\\nion of a center crop. Thus,\\nwhile the crops extracted at training and test time have the\\nsame size, they arise from different RoCs, which skews the\\ndata distribution seen by the CNN.\\nOver the years, training and testing pre-processing proce-\\ndures have evolved, but so far they have been optimized sepa-\\nrately [3]. Touvron et al. show [1] that this separate optimiza-\\ntion has a detrimental effect on the test-time performance of\\nmodels. They address this problem with the FixRes method,\\n0\\n20\\n40\\n60\\n\\n}<->{\\nn the test-time performance of\\nmodels. They address this problem with the FixRes method,\\n0\\n20\\n40\\n60\\n80\\n100\\nNumber of Parameters (Millions)\\n74\\n76\\n78\\n80\\n82\\n84\\n86\\nImageNet Top-1 Accuracy (%)\\nFixResNet-50-sws\\nFixResNet-50\\nFixPNASNet\\nResNet-50\\nResNet-152\\nPNASNet\\nResNet-50-sws\\nB4\\nB5\\nB6\\nOur FixEfficientNet-B7\\nAdvProp (EfficientNet-B7)\\nNoisy Student (EfficientNet-B7)\\nInception-v2\\nDenseNet-201\\nXception\\nInception-resnet-v2\\nResNeXt-101\\nAmoebaNet-A\\nFig. 1. Improvement brought by FixRes (in bold) to several\\n\\n}<->{\\neption-resnet-v2\\nResNeXt-101\\nAmoebaNet-A\\nFig. 1. Improvement brought by FixRes (in bold) to several\\npopular architectures from the literature. Our FixEfﬁcientNet\\n(orange curve) surpasses all EfﬁcientNet models, including\\nthe models trained with Noisy student (red curve) and adver-\\nsarial examples (blue curve). The sws models are from [2].\\nTables 1 and 2 report results on larger models.\\nwhich jointly optimizes the choice of resolutions and scales at\\ntraining and test time, while keeping the same \\n}<->{\\nly optimizes the choice of resolutions and scales at\\ntraining and test time, while keeping the same RoC sampling.\\nWe apply this method to the recent EfﬁcientNet [4] archi-\\ntecture, which offers an excellent compromise between num-\\nber of parameters and accuracy. This evaluation paper shows\\nthat properly combining FixRes and EfﬁcientNet further im-\\nproves the state of the art [4]. Noticeably,\\n• We report the best performance without external data\\non ImageNet (top1: 85.7%);\\n• We report the best ac\\n}<->{\\nreport the best performance without external data\\non ImageNet (top1: 85.7%);\\n• We report the best accuracy (top1: 88.5%) with ex-\\nternal data on ImageNet, and with ImageNet with\\nReallabels [5] ;\\n• We achieve state-of-the-art compromises between ac-\\ncuracy and number of parameters, see Figure 1;\\narXiv:2003.08237v5  [cs.CV]  18 Nov 2020\\n\\n--------------------------------------------------\\n• We validate the signiﬁcance of our results on the\\nImageNet-v2 test set, an improved evaluation setup\\nthat cle\\n}<->{\\nte the signiﬁcance of our results on the\\nImageNet-v2 test set, an improved evaluation setup\\nthat clearly separates the validation and test sets. Fix-\\nEfﬁcientNet achieves the best performance.\\nThis paper is organized as follows. In Section 2 we in-\\ntroduce the corrected training procedure for EfﬁcientNet, that\\nproduces FixEfﬁcientNet. Section 3 analyzes our extensive\\nevaluation and compare FixEfﬁcientNet with the state of the\\nart. Section 4 concludes the paper.\\n2. TRAINING WITH FIXRES: UPDATES\\nR\\n}<->{\\nientNet with the state of the\\nart. Section 4 concludes the paper.\\n2. TRAINING WITH FIXRES: UPDATES\\nRecent research in image classiﬁcation tends towards larger\\nnetworks and higher resolution images [6, 7, 8]. For instance,\\nthe state-of-the-art in the ImageNet ILSVRC 2012 bench-\\nmark is currently held by the EfﬁcientNet-L2 [8] architecture\\nwith 480M parameters using 800×800 images for training.\\nSimilarly, the state-of-the-art model learned from scratch is\\ncurrently EfﬁcientNet-B8 [9] with 88M para\\n}<->{\\nlarly, the state-of-the-art model learned from scratch is\\ncurrently EfﬁcientNet-B8 [9] with 88M parameters using\\n672×672 images for training. In this note, we focus on the\\nEfﬁcientNet architecture [4] due to its good accuracy/cost\\ntrade-off and its popularity.\\nData augmentation is routinely employed at training time\\nto improve model generalization and reduce overﬁtting. In\\nthis note, we use the same augmentation setup as in the orig-\\ninal FixRes paper [1]. In addition, we have integrated label\\ns\\n}<->{\\nme augmentation setup as in the orig-\\ninal FixRes paper [1]. In addition, we have integrated label\\nsmoothing, which is orthogonal to the approach. FixRes is\\na very simple ﬁne-tuning that re-trains the classiﬁer or a few\\ntop layers at the target resolution. Therefore, it has several\\nadvantages:\\n1. it is computationally cheap, the back-propagation is not\\nperformed on the whole network;\\n2. it works with any CNN classiﬁcation architecture and is\\ncomplementary with the other tricks mentioned above;\\n3\\n}<->{\\nith any CNN classiﬁcation architecture and is\\ncomplementary with the other tricks mentioned above;\\n3. it can be applied on a CNN that comes from a possibly\\nnon reproducible source.\\n3. EXPERIMENTS\\nWe experiment on the ImageNet-2012 benchmark [10], and\\nreport standard performance metrics (top-1 and top-5 accura-\\ncies) on a single image crop.\\n3.1. Experimental Setting\\nWe focus on the EfﬁcientNet [4] architectures. In the liter-\\nature, wo versions provide the best performance: Efﬁcient-\\nNet trained \\n}<->{\\narchitectures. In the liter-\\nature, wo versions provide the best performance: Efﬁcient-\\nNet trained with adversarial examples [9], and Efﬁcient-\\nNet trained with Noisy student [8] pre-trained in a weakly-\\nsupervised fashion on 300 million unlabeled images.\\nTable 1. Results on ImageNet with extra training data.\\nWe start from pre-trained models [8] learned using 300M\\nadditional unlabeled images (single crop evaluation). See\\nSection 3.3 about the signiﬁcance of these results.\\nModel\\n#params\\ntrain re\\n}<->{\\ngle crop evaluation). See\\nSection 3.3 about the signiﬁcance of these results.\\nModel\\n#params\\ntrain res\\nEfﬁcientNet [8]\\nFixEfﬁcientNet\\ntest\\nTop-1\\nTop-5\\ntest\\nTop-1\\nTop-5\\nres\\n(%)\\n(%)\\nres\\n(%)\\n(%)\\nB0\\n5.3M\\n224\\n224\\n78.8\\n94.5\\n320\\n80.2\\n95.4\\nB1\\n7.8M\\n240\\n240\\n81.5\\n95.8\\n384\\n82.6\\n96.5\\nB2\\n9.2M\\n260\\n260\\n82.4\\n96.3\\n420\\n83.6\\n96.9\\nB3\\n12M\\n300\\n300\\n84.1\\n96.9\\n472\\n85.0\\n97.4\\nB4\\n19M\\n380\\n380\\n85.3\\n97.5\\n472\\n85.9\\n97.7\\nB5\\n30M\\n456\\n456\\n86.1\\n97.8\\n576\\n86.4\\n97.9\\nB6\\n43M\\n528\\n528\\n86.4\\n97.9\\n680\\n86.7\\n98.0\\nB7\\n66M\\n600\\n600\\n86.9\\n98.1\\n632\\n87.1\\n}<->{\\n456\\n86.1\\n97.8\\n576\\n86.4\\n97.9\\nB6\\n43M\\n528\\n528\\n86.4\\n97.9\\n680\\n86.7\\n98.0\\nB7\\n66M\\n600\\n600\\n86.9\\n98.1\\n632\\n87.1\\n98.2\\nL2\\n480M\\n475\\n800\\n88.4\\n98.7\\n600\\n88.5\\n98.7\\nTable 2.\\nResults on ImageNet without external data\\n(single Crop evaluation). FixEfﬁcientNet outperforms the\\nprevious EfﬁcientNet AdvProp [9] state of the art in this\\nsetup, see Section 3.3 for the signiﬁcance of these results.\\nModel\\n#params\\ntrain res\\nEfﬁcientNet [9]\\nFixEfﬁcientNet\\ntest\\nTop-1\\nTop-5\\ntest\\nTop-1\\nTop-5\\nres\\n(%)\\n(%)\\nres\\n(%)\\n(%)\\nB0\\n5.3M\\n224\\n22\\n}<->{\\ncientNet [9]\\nFixEfﬁcientNet\\ntest\\nTop-1\\nTop-5\\ntest\\nTop-1\\nTop-5\\nres\\n(%)\\n(%)\\nres\\n(%)\\n(%)\\nB0\\n5.3M\\n224\\n224\\n77.6\\n93.3\\n320\\n79.3\\n94.6\\nB1\\n7.8M\\n240\\n240\\n79.6\\n94.3\\n384\\n81.3\\n95.7\\nB2\\n9.2M\\n260\\n260\\n80.5\\n95.0\\n420\\n82.0\\n96.0\\nB3\\n12M\\n300\\n300\\n81.9\\n95.6\\n472\\n83.0\\n96.4\\nB4\\n19M\\n380\\n380\\n83.3\\n96.4\\n512\\n84.0\\n97.0\\nB5\\n30M\\n456\\n456\\n84.3\\n97.0\\n576\\n84.7\\n97.2\\nB6\\n43M\\n528\\n528\\n84.8\\n97.1\\n576\\n84.9\\n97.3\\nB7\\n66M\\n600\\n600\\n85.2\\n97.2\\n632\\n85.3\\n97.4\\nB8\\n87.4M\\n672\\n672\\n85.5\\n97.3\\n800\\n85.7\\n97.6\\nWe start from the EfﬁcientNet models in rwightman’s\\nGitHub\\n}<->{\\n\\nB8\\n87.4M\\n672\\n672\\n85.5\\n97.3\\n800\\n85.7\\n97.6\\nWe start from the EfﬁcientNet models in rwightman’s\\nGitHub repository [11]. These models have been converted\\nfrom the original Tensorﬂow to PyTorch.\\nTraining.\\nWe mostly follow the FixRes [1] training proto-\\ncol. The only difference is that we combine the FixRes data-\\naugmentation with label smoothing during the ﬁne-tuning.\\n3.2. Comparison with the state of the art\\nTable 1 and Table 2 compare our results with those of the Ef-\\nﬁcientNet reported in the lit\\n}<->{\\n the art\\nTable 1 and Table 2 compare our results with those of the Ef-\\nﬁcientNet reported in the literature. All our FixEfﬁcientNets\\noutperform the corresponding EfﬁcientNet (see Figure 1). As\\na result and to the best of our knowledge, our FixEfﬁcientNet-\\nL2 surpasses all other results reported in the literature.\\nIt\\nachieves 88.5% Top-1 accuracy and 98.7% Top-5 accuracy\\non the ImageNet-2012 validation benchmark [10].\\n\\n--------------------------------------------------\\nTable 3. Results on ImageNe\\n}<->{\\ntion benchmark [10].\\n\\n--------------------------------------------------\\nTable 3. Results on ImageNet Real labels [5].\\nModel\\nNo Extra-Training Data\\nExtra-Training Data\\nEfﬁcientNet [8]\\nFixEfﬁcientNet\\nEfﬁcientNet [8]\\nFixEfﬁcientNet\\nTop-1\\nTop-5\\nTop-1\\nTop-5\\nTop-1\\nTop-5\\nTop-1\\nTop-5\\n(%)\\n(%)\\n(%)\\n(%)\\n(%)\\n(%)\\n(%)\\n(%)\\nB0\\n83.7\\n95.8\\n85.8\\n96.8\\n84.5\\n96.4\\n86.5\\n97.3\\nB1\\n85.1\\n96.4\\n87.0\\n97.4\\n86.7\\n97.2\\n88.1\\n98.0\\nB2\\n86.0\\n96.8\\n87.7\\n97.6\\n87.3\\n97.6\\n88.8\\n98.2\\nB3\\n87.2\\n97.4\\n88.3\\n98.0\\n88.4\\n98.0\\n89.2\\n98.4\\nB4\\n88.3\\n97.9\\n89.2\\n\\n}<->{\\n6.0\\n96.8\\n87.7\\n97.6\\n87.3\\n97.6\\n88.8\\n98.2\\nB3\\n87.2\\n97.4\\n88.3\\n98.0\\n88.4\\n98.0\\n89.2\\n98.4\\nB4\\n88.3\\n97.9\\n89.2\\n98.3\\n89.4\\n98.4\\n89.8\\n98.5\\nB5\\n88.9\\n98.2\\n89.4\\n98.4\\n89.7\\n98.5\\n90.0\\n98.6\\nB6\\n89.3\\n98.3\\n89.6\\n98.4\\n89.8\\n98.5\\n90.1\\n98.6\\nB7\\n89.4\\n98.3\\n89.7\\n98.5\\n90.1\\n98.6\\n90.3\\n98.7\\nB8\\n89.6\\n98.3\\n90.0\\n98.6\\nL2\\n90.6\\n98.8\\n90.9\\n98.8\\nClean labels.\\nIn order to complement this evaluation, Ta-\\nble 3 present the results with the ImageNet clean labels pro-\\nposed by Beyer et all. [5]. With 90.9% Top-1 accuracy and\\n98.8% Top-5 accuracy F\\n}<->{\\nt clean labels pro-\\nposed by Beyer et all. [5]. With 90.9% Top-1 accuracy and\\n98.8% Top-5 accuracy FixEfﬁcientNet-L2 surpasses all other\\nresults reported in the literature with this labels.\\n3.3. Signiﬁcance of the results\\nSeveral runs of the same training incur variations of about 0.1\\naccuracy points on Imagenet due to random initialization and\\nmini-batch sampling. In general, since the Imagenet 2012 test\\nset is not available, most works tune the hyper-parameters on\\nthe validation set, ie. there\\n}<->{\\n012 test\\nset is not available, most works tune the hyper-parameters on\\nthe validation set, ie. there is no distinction between valida-\\ntion and test set. This setting, while widely adopted, is not\\nlegitimate and can cause overﬁtting to go unnoticed.\\nEfﬁcientNets employ Neural Architecture Search, which\\nsigniﬁcantly enlarges the hyper-parameter space. Addition-\\nally, the ImageNet validation images were used to ﬁlter the\\nimages from the unlabelled set [8]. Therefore the pre-trained\\nmodels may bene\\n}<->{\\nwere used to ﬁlter the\\nimages from the unlabelled set [8]. Therefore the pre-trained\\nmodels may beneﬁt from more overﬁtting on the validation\\nset. We quantify this in the experiments presented below.\\nSince we use pre-trained EfﬁcientNet for our initializa-\\ntion, our results are comparable to those from the Noisy Stu-\\ndent [8], which uses the same degree of overﬁtting, but not\\ndirectly with other semi-supervised approaches like that of\\nYalniz et al. [2].\\n3.4. Evaluation on ImageNet-V2\\nThe ImageNe\\n}<->{\\nemi-supervised approaches like that of\\nYalniz et al. [2].\\n3.4. Evaluation on ImageNet-V2\\nThe ImageNet-V2 [17] dataset was introduced to overcome\\nthe lack of a test split in the Imagenet dataset. ImageNet-V2\\nconsists of 3 novel test sets that replace the ImageNet test set,\\nwhich is no longer available. They were carefully designed\\nto match the characteristics of the original test set. One of\\nthese test sets, Matched Frequency is the closest to the Im-\\nageNet validation set. To ensure that observe\\n}<->{\\ntest sets, Matched Frequency is the closest to the Im-\\nageNet validation set. To ensure that observed improvements\\nare not due to overﬁtting, we evaluate all our models on the\\nMatched Frequency version of the ImageNet-v2 [17] dataset.\\nWe evaluate the other methods in the same way. We present\\nthe results in Tables 4 and 5.\\n75\\n80\\n85\\nImageNet Top-1 Accuracy (%)\\n60\\n65\\n70\\n75\\n80\\nImageNet-V2 Top-1 Accuracy (%)\\nNoisy Student\\nFixRes-Noisy Student\\nFixRes-Billion Scale\\nBillion Scale\\nEfficientNet\\nResNet\\nNAS\\n}<->{\\ncy (%)\\nNoisy Student\\nFixRes-Noisy Student\\nFixRes-Billion Scale\\nBillion Scale\\nEfficientNet\\nResNet\\nNASNet\\nRegNetY\\nFig. 2.\\nEvidence of overﬁtting on Imagenet-val: We com-\\npare the results obtained on ImageNet (x-axis) and the re-\\nsults obtained on ImageNet-v2 (y-axis), without FixRes for\\ndifferent models [12, 13, 8, 14, 15, 2, 16, 1]. For a given per-\\nformance on Imagenet-val, overﬁtted models tend to have a\\nlower performance on ImageNet-v2 and therefore are below\\nthe approaches that generalize bet\\n}<->{\\no have a\\nlower performance on ImageNet-v2 and therefore are below\\nthe approaches that generalize better.\\nThe original study of [17] shows that there is signiﬁcant\\noverﬁtting of various models to the Imagenet 2012 valuation\\nset, but that it does not impact the relative order of the models.\\nQuantifying the overﬁtting on Imagenet.\\nAs mentioned\\nearlier, several choices in the Noisy Student [8] method are\\nprone to overﬁtting. We verify this hypothesis and quantify\\nits extent by comparing the relative\\n}<->{\\nare\\nprone to overﬁtting. We verify this hypothesis and quantify\\nits extent by comparing the relative accuracy of this approach\\nwith another semi-supervised approach [2] both on ImageNet\\nand ImageNet-V2 [17].\\nWithout overﬁtting, models performing similarly on Im-\\nagenet should also have similar performances on ImageNet-\\nV2 [17]. However, for a comparable performance on Ima-\\ngeNet, when evaluating on ImageNet-V2, the Billion scale\\nmodels of Yalniz et al. [2] outperform the EfﬁcientNets from\\nNoisy \\n}<->{\\nn ImageNet-V2, the Billion scale\\nmodels of Yalniz et al. [2] outperform the EfﬁcientNets from\\nNoisy Student. For example, FixResNeXt-101 32x4d [1] has\\nthe same performance as EfﬁcientNet-B3 [8] on ImageNet but\\non ImageNet-V2 FixResNeXt-101 32x4d [1] is better (+0.7%\\nTop-1 accuracy).\\nThis shows that the EfﬁcientNet Noisy student [8] tends\\nto overﬁt and does not generalize as well as the (prior) semi-\\nsupervised work [2] or other works of the literature. Figure 2\\nillustrates this effect. The FixRe\\n}<->{\\ni-\\nsupervised work [2] or other works of the literature. Figure 2\\nillustrates this effect. The FixRes ﬁne-tuning procedure is\\nneutral with respect to overﬁtting: overﬁtted models remain\\noverﬁtted and conversely.\\n\\n--------------------------------------------------\\nTable 4.\\nResults on ImageNet-V2 [17] Matched Fre-\\nquency with extra-training data. We start from pre-trained\\nmodels [8] that have been learned using 300M additional\\nunlabeled images (single crop evaluation).\\nModel\\n#params\\ntrain res\\nEfﬁc\\n}<->{\\nearned using 300M additional\\nunlabeled images (single crop evaluation).\\nModel\\n#params\\ntrain res\\nEfﬁcientNet [8]\\nFixEfﬁcientNet\\ntest\\nTop-1\\nTop-5\\ntest\\nTop-1\\nTop-5\\nres\\n(%)\\n(%)\\nres\\n(%)\\n(%)\\nB0\\n5.3M\\n224\\n224\\n67.7\\n88.1\\n320\\n69.4\\n89.6\\nB1\\n7.8M\\n240\\n240\\n70.9\\n90.1\\n384\\n72.7\\n91.4\\nB2\\n9.2M\\n260\\n260\\n72.3\\n91.1\\n420\\n73.6\\n92.0\\nB3\\n12M\\n300\\n300\\n73.9\\n91.9\\n472\\n75.0\\n93.0\\nB4\\n19M\\n380\\n380\\n75.7\\n93.1\\n472\\n76.2\\n93.6\\nB5\\n30M\\n456\\n456\\n76.8\\n93.6\\n576\\n77.0\\n94.0\\nB6\\n43M\\n528\\n528\\n77.3\\n93.9\\n680\\n77.5\\n94.3\\nB7\\n66M\\n600\\n600\\n78.5\\n94.4\\n632\\n78.6\\n94.7\\n\\n}<->{\\n.8\\n93.6\\n576\\n77.0\\n94.0\\nB6\\n43M\\n528\\n528\\n77.3\\n93.9\\n680\\n77.5\\n94.3\\nB7\\n66M\\n600\\n600\\n78.5\\n94.4\\n632\\n78.6\\n94.7\\nL2\\n480M\\n475\\n800\\n80.3\\n95.8\\n600\\n80.8\\n96.1\\nTable 5.\\nResults on ImageNet-V2 [17] Matched Fre-\\nquency without external data (single Crop evaluation).\\nModel\\n#params\\ntrain res\\nEfﬁcientNet [9]\\nFixEfﬁcientNet\\ntest\\nTop-1\\nTop-5\\ntest\\nTop-1\\nTop-5\\nres\\n(%)\\n(%)\\nres\\n(%)\\n(%)\\nB0\\n5.3M\\n224\\n224\\n65.5\\n85.6\\n320\\n67.8\\n87.9\\nB1\\n7.8M\\n240\\n240\\n67.5\\n87.8\\n384\\n70.1\\n89.6\\nB2\\n9.2M\\n260\\n260\\n68.9\\n88.4\\n420\\n70.8\\n90.2\\nB3\\n12M\\n300\\n300\\n70.9\\n89\\n}<->{\\n\\n7.8M\\n240\\n240\\n67.5\\n87.8\\n384\\n70.1\\n89.6\\nB2\\n9.2M\\n260\\n260\\n68.9\\n88.4\\n420\\n70.8\\n90.2\\nB3\\n12M\\n300\\n300\\n70.9\\n89.4\\n472\\n72.7\\n90.9\\nB4\\n19M\\n380\\n380\\n72.9\\n91.0\\n512\\n73.9\\n91.8\\nB5\\n30M\\n456\\n456\\n74.6\\n92.0\\n576\\n75.1\\n92.4\\nB6\\n43M\\n528\\n528\\n75.4\\n92.4\\n576\\n75.4\\n92.6\\nB7\\n66M\\n600\\n600\\n76.1\\n93.0\\n632\\n75.8\\n93.2\\nB8\\n87.4M\\n672\\n672\\n76.1\\n92.7\\n800\\n75.9\\n93.0\\nComparison with the state of the art.\\nDespite overﬁt-\\nting, EfﬁcientNet remains very competitive on ImageNet-V2,\\nas reported in Table 6. Interestingly, the FixEfﬁcientNet-L2\\nthat we ﬁne-\\n}<->{\\nompetitive on ImageNet-V2,\\nas reported in Table 6. Interestingly, the FixEfﬁcientNet-L2\\nthat we ﬁne-tuned from EfﬁcientNet establishes the new state\\nof the art with additional data on this benchmark.\\n4. CONCLUSION\\nThe ”Fixing Resolution” is a method that improves the per-\\nformance of any model. It is a method that is applied as a\\nﬁne-tuning step after the conventional training, during a few\\nepochs only, which makes it very ﬂexible. It is easily inte-\\ngrated into any existing training pipeline. I\\n}<->{\\n only, which makes it very ﬂexible. It is easily inte-\\ngrated into any existing training pipeline. In our paper we\\nproposed a thorough evaluation of the combination of the cur-\\nrent state-of-the-art models, namely EfﬁcientNet, with this\\nimproved training method.\\nWe provide an open-source implementation of our method 1.\\n1http://github.com/facebookresearch/FixRes\\nTable 6.\\nPerformance comparison and state of the art on\\nImageNet-v2, single crop with external data, sorted by top-1\\naccuracy. NS: Noisy\\n}<->{\\nstate of the art on\\nImageNet-v2, single crop with external data, sorted by top-1\\naccuracy. NS: Noisy Student [8]. BS: Billion-scale [2].\\nModel\\nsize\\nTop-1 (%)\\nTop-5 (%)\\nEfﬁcientNet-B0 NS [8]\\n5.3M\\n67.7\\n88.1\\nFixEfﬁcientNet-B0\\n5.3M\\n69.4\\n89.6\\nEfﬁcientNet-B1 NS [8]\\n7.8M\\n70.9\\n90.1\\nResNet50 BS [2]\\n25.6M\\n71.7\\n90.5\\nEfﬁcientNet-B2 NS [8]\\n9.1M\\n72.3\\n91.1\\nResNeXt-50 32x4d BS [2]\\n25.1M\\n72.6\\n90.9\\nFixEfﬁcientNet-B1\\n7.8M\\n72.7\\n91.4\\nFixEfﬁcientNet-B2\\n9.1M\\n73.6\\n92.0\\nEfﬁcientNet-B3 NS [8]\\n12.2M\\n73.9\\n91.9\\nResNeXt-101 \\n}<->{\\n1\\n7.8M\\n72.7\\n91.4\\nFixEfﬁcientNet-B2\\n9.1M\\n73.6\\n92.0\\nEfﬁcientNet-B3 NS [8]\\n12.2M\\n73.9\\n91.9\\nResNeXt-101 32x4d BS [2]\\n42.0M\\n74.2\\n92.0\\nFixResNeXt-101 32x4d [1]\\n42.0M\\n74.6\\n92.7\\nFixEfﬁcientNet-B3\\n12.2M\\n75.0\\n93.0\\nResNeXt-101 32x8d BS [2]\\n88.0M\\n75.5\\n92.8\\nResNeXt-101 32x16d BS [2]193.0M\\n75.6\\n93.3\\nEfﬁcientNet-B4 NS [8]\\n19.3M\\n75.7\\n93.1\\nFixEfﬁcientNet-B4\\n19.3M\\n76.2\\n93.6\\nFixResNeXt-101 32x8d [1]\\n88.0M\\n76.3\\n93.4\\nFixResNeXt-101 32x16d [1]193.0M\\n76.7\\n93.4\\nEfﬁcientNet-B5 NS [8]\\n30.4M\\n76.8\\n93.6\\nFixEfﬁcientNet-B5\\n30\\n}<->{\\nFixResNeXt-101 32x16d [1]193.0M\\n76.7\\n93.4\\nEfﬁcientNet-B5 NS [8]\\n30.4M\\n76.8\\n93.6\\nFixEfﬁcientNet-B5\\n30.4M\\n77.0\\n94.0\\nEfﬁcientNet-B6 NS [8]\\n43.0M\\n77.3\\n93.9\\nFixEfﬁcientNet-B6\\n43.0M\\n77.5\\n94.3\\nFixResNeXt-101 32x48d [1]829.0M\\n77.8\\n93.9\\nEfﬁcientNet-B7 NS [8]\\n66.4M\\n78.5\\n94.4\\nFixEfﬁcientNet-B7\\n66.4M\\n78.6\\n94.7\\nEfﬁcientNet-L2 NS [8]\\n480.3M\\n80.5\\n95.7\\nFixEfﬁcientNet-L2\\n480.3M\\n80.8\\n96.1\\n\\n--------------------------------------------------\\n5. REFERENCES\\n[1] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and\\nH´erve\\n}<->{\\n-------------------------\\n5. REFERENCES\\n[1] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and\\nH´erve J´egou, “Fixing the train-test resolution discrep-\\nancy,” Advances in Neural Information Processing Sys-\\ntems, 2019.\\n[2] Ismet Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar\\nPaluri, and Dhruv Kumar Mahajan, “Billion-scale semi-\\nsupervised learning for image classiﬁcation,”\\narXiv\\npreprint arXiv:1905.00546, 2019.\\n[3] Ekin Dogus Cubuk, Barret Zoph, Dandelion Man´e, Vi-\\njay Vasudevan, and Quoc V. Le, \\n}<->{\\n00546, 2019.\\n[3] Ekin Dogus Cubuk, Barret Zoph, Dandelion Man´e, Vi-\\njay Vasudevan, and Quoc V. Le, “Autoaugment: Learn-\\ning augmentation policies from data,”\\narXiv preprint\\narXiv:1805.09501, 2018.\\n[4] Mingxing Tan and Quoc V. Le, “Efﬁcientnet: Rethink-\\ning model scaling for convolutional neural networks,”\\narXiv preprint arXiv:1905.11946, 2019.\\n[5] Lucas Beyer, Olivier J. H´enaff, A. Kolesnikov, Xiaohua\\nZhai, and Aaron van den Oord, “Are we done with Im-\\nageNet?,” arXiv preprint arXiv:2006.07159\\n}<->{\\niaohua\\nZhai, and Aaron van den Oord, “Are we done with Im-\\nageNet?,” arXiv preprint arXiv:2006.07159, 2020.\\n[6] Yanping Huang, Yonglong Cheng, Dehao Chen, Hy-\\noukJoong Lee, Jiquan Ngiam, Quoc V. Le, and Zhifeng\\nChen,\\n“Gpipe:\\nEfﬁcient training of giant neural\\nnetworks using pipeline parallelism,”\\narXiv preprint\\narXiv:1811.06965, 2018.\\n[7] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,\\nKaiming He, Manohar Paluri, Yixuan Li, Ashwin\\nBharambe, and Laurens van der Maaten, “Exploring the\\nlimits of w\\n}<->{\\n, Manohar Paluri, Yixuan Li, Ashwin\\nBharambe, and Laurens van der Maaten, “Exploring the\\nlimits of weakly supervised pretraining,” in European\\nConference on Computer Vision, 2018.\\n[8] Qizhe Xie, Eduard H. Hovy, Minh-Thang Luong,\\nand Quoc V. Le,\\n“Self-training with noisy stu-\\ndent improves imagenet classiﬁcation,” arXiv preprint\\narXiv:1911.04252, 2019.\\n[9] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang,\\nAlan L. Yuille, and Quoc V. Le,\\n“Adversarial ex-\\namples improve image recognition,”\\narXiv p\\n}<->{\\nng Wang,\\nAlan L. Yuille, and Quoc V. Le,\\n“Adversarial ex-\\namples improve image recognition,”\\narXiv preprint\\narXiv:1911.09665, 2019.\\n[10] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\\nKarpathy, Aditya Khosla, Michael Bernstein, Alexan-\\nder C. Berg, and Li Fei-Fei, “Imagenet large scale visual\\nrecognition challenge,” International journal of Com-\\nputer Vision, 2015.\\n[11] “Pre-trained\\nefﬁcientnet\\nmodels,”\\nhttps://github.com/rwightman/\\npytorch-\\n}<->{\\nm-\\nputer Vision, 2015.\\n[11] “Pre-trained\\nefﬁcientnet\\nmodels,”\\nhttps://github.com/rwightman/\\npytorch-image-models/,\\nAccessed:\\n2020-\\n03-01.\\n[12] Barret Zoph, V. Vasudevan, Jonathon Shlens, and\\nQuoc V. Le,\\n“Learning transferable architectures for\\nscalable image recognition,” Conference on Computer\\nVision and Pattern Recognition, 2018.\\n[13] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon\\nShlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille,\\nJonathan Huang, and Kevin Murphy, “Progressive neu-\\nral arc\\n}<->{\\nHua, Li-Jia Li, Li Fei-Fei, Alan Yuille,\\nJonathan Huang, and Kevin Murphy, “Progressive neu-\\nral architecture search,” in International Conference on\\nComputer Vision, September 2018.\\n[14] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Gir-\\nshick, Kaiming He, and Piotr Doll´ar, “Designing net-\\nwork design spaces,” arXiv preprint arXiv:2003.13678,\\n2020.\\n[15] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and\\nQuoc V. Le, “Randaugment: Practical automated data\\naugmentation with a reduced search spac\\n}<->{\\nlens, and\\nQuoc V. Le, “Randaugment: Practical automated data\\naugmentation with a reduced search space,”\\narXiv\\npreprint arXiv:1909.13719, 2019.\\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\\nSun, “Deep residual learning for image recognition,” in\\nConference on Computer Vision and Pattern Recogni-\\ntion, June 2016.\\n[17] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt,\\nand Vaishaal Shankar,\\n“Do imagenet classiﬁers gen-\\neralize to imagenet?,” in International Conference on\\nMachine Learning, 2\\n}<->{\\no imagenet classiﬁers gen-\\neralize to imagenet?,” in International Conference on\\nMachine Learning, 2019.\\n\\n--------------------------------------------------\\n', 'Summaries': 'Fixing Train-Test Resolution Discrepancy: FixEfficientNet\\n\\nThis paper analyzes performance of EfficientNet image classifiers using recent training procedures. A discrepancy correction method is applied to Fix the train and test images difference [1]. The resulting network, FixEfficientNet, outperforms the initial architecture with same parameters.\\n}<->{\\nFixEfﬁicientNet outperforms initial architecture, achieving 79.3% top-1 accuracy on ImageNet with 5.3M parameters. It surpasses Noisy student EfﬁcientNet-B0 trained with 300M unlabeled images by +0.5%.\\n}<->{\\nWeak supervision on 300M unlabeled images achieves 88.5% top-1 accuracy with FixRes optimization. Top-5 accuracy reaches 98.7%. This surpasses the previous state of the art for ImageNet with a single crop and holds in cleaner protocols, including ImageNet-v2 and Real Labels.\\n}<->{\\net-v2 achieves better performance than previous models due to its reduced likelihood of overfitting and use of ImageNet Real Labels.\\n}<->{\\nA Region of Classiﬁcation (RoC) is artificially enlarged with random coordinates from an image to increase training data. The RoC is then resized or cropped into a fixed-size image for CNN input. At test time, a square crop covering the central part of the image is extracted.\\n}<->{\\nSeparate training and testing pre-processing procedures have been used, but they are not optimized together. This can skew data distribution seen by CNNs, affecting test time performance. The FixRes method addresses this issue by optimizing both training and testing processes simultaneously.\\n}<->{\\nFixRes method improves test-time performance of models. It is evaluated on various models, including ResNet and EfficientNet variants, with improved accuracy as follows:\\n\\n* ImageNet Top-1 Accuracy (%): \\n  * Our FixEfficientNet-B7: **+4.45%**\\n  * AdvProp (EfficientNet-B7): **+3.24%**\\n  * Noisy Student (EfficientNet-B7): **+2.91%**\\n}<->{\\nResNeXt-101 \\nAmoebaNet-A\\n}<->{\\nFixRes optimizes resolutions and scales for EfficientNet, improving state-of-the-art performance on ImageNet (top1: 85.7%) without using external data.\\n}<->{\\nWe report the best accuracy (top1: 88.5%) with external data on ImageNet, and also with ImageNet using Reallabels [5]. Our result is a state-of-the-art compromise between accuracy and number of parameters.\\n}<->{\\nFix-EfficientNet achieves best performance on ImageNet-v2 test set, an improved evaluation setup separating validation and test sets.\\n}<->{\\nRecent image classification research uses larger networks and higher resolution images, surpassing previous benchmarks. The current state-of-the-art models include:\\n- EfficientNet-L2 (480M parameters) using 800×800 images.\\n- EfficientNet-B8 (88M parameters), learned from scratch.\\n}<->{\\nThe state-of-the-art model EfﬁcientNet-B8 with 88M parameters uses 672×672 images for training. This paper focuses on the EfﬁcientNet architecture due to its accuracy/cost trade-off and popularity. Data augmentation is used to improve generalization and reduce overfitting, following the original FixRes paper\\'s setup.\\n}<->{\\nWe implemented a me augmentation setup as in the original FixRes paper. We also integrated label smoothing, an approach orthogonal to FixRes. Key advantages include: \\n- Computationally cheap by not propagating back through the entire network.\\n- Compatible with any CNN classification architecture.\\n- Complementary to other tricks mentioned above.\\n}<->{\\nWe experimented with ImageNet-2012 using EfficientNet architecture and reported top-1 and top-5 accuracy metrics.\\n}<->{\\nEfficientNet trained with adversarial examples and noisy student pre-trained on 300M unlabeled images performs best in literature, achieving state-of-the-art performance on ImageNet.\\n}<->{\\nSummary:\\n\\nModel comparison results:\\n\\n- B0: Top-1 78.8%, Top-5 80.2%\\n- B1: Top-1 81.5%, Top-5 82.6%\\n- B2: Top-1 82.4%, Top-5 83.6%\\n- B3: Top-1 84.1%, Top-5 85.0%\\n- B4: Top-1 85.3%, Top-5 85.9%\\n- B5: Top-1 86.1%, Top-5 86.4%\\n- B6: Top-1 86.4%, Top-5 86.7%\\n- B7: Top-1 86.9%, Top-5 87.1%\\n}<->{\\nTable 2 Results on ImageNet:\\n\\n| Model | #params | Train Res (%) | Test Top-1 (%) | Test Top-5 (%) |\\n| --- | --- | --- | --- | --- |\\n| EfﬁcientNet [9] | 5.3M | 22 | - | - |\\n| FixEfﬁcientNet | - | - | 224 | 222 |\\n| B0 | - | 86.4 | 97.8 | 98.0 |\\n}<->{\\nEfficientNet models:\\n\\n- B0: 5.3M params, 224x224, 77.6% Top-1, 93.3% Top-5\\n- B1: 7.8M params, 240x240, 79.6% Top-1, 94.3% Top-5\\n- B2: 9.2M params, 260x260, 80.5% Top-1, 95.0% Top-5\\n- B3: 12M params, 300x300, 81.9% Top-1, 95.6% Top-5\\n- B4: 19M params, 380x380, 83.3% Top-1, 96.4% Top-5\\n- B5: 30M params, 456x456, 84.3% Top-1, 97.0% Top-5\\n- B6: 43M params, 528x528, 84.8% Top-1, 97.1% Top-5\\n- B7: 66M params, 600x600, 85.2% Top-1, 97.2% Top-5\\n- B8: 87.4M params, 672x672, 85.5% Top-1, 97.3% Top-5\\n}<->{\\nB8, 87.4M, 672, 85.5, 97.3, 800, 85.7, 97.6\\nEfﬁcientNet models converted from Tensorﬂow to PyTorch.\\nTraining follows FixRes protocol with label smoothing.\\nComparison with state-of-the-art results in Tables 1 and 2.\\n}<->{\\nOur FixEfﬁcientNets outperform EfﬁcientNets in literature, achieving 88.5% Top-1 and 98.7% Top-5 accuracy on the ImageNet-2012 validation benchmark.\\n}<->{\\nTable 3 Results on ImageNet Real Labels:\\n\\n| Model | Top-1 (%) | Top-5 (%) |\\n| --- | --- | --- |\\n| B0 (No Extra) | 83.7 | 95.8 |\\n| B0 (Extra) | 85.8 | 96.8 |\\n| B1 (No Extra) | 85.1 | 96.4 |\\n| B1 (Extra) | 87.0 | 97.4 |\\n| B2 (No Extra) | 86.0 | 96.8 |\\n| B2 (Extra) | 87.7 | 97.6 |\\n| B3 (No Extra) | 87.2 | 97.4 |\\n| B3 (Extra) | 88.3 | 98.0 |\\n| B4 (Extra) | 88.3 | 97.9 |\\n}<->{\\nEvaluation Results:\\n\\n* Model Performances: 6.0, 96.8, 87.7 to 90.3 (varied across tests)\\n* Accuracy Rates:\\n\\t+ Top-1: 91.9% (Beyer et al.): 90.9%\\n\\t+ Top-5: 98.4%\\n* Test Labels: B3, B4, B5, B6, B7, B8, L2\\n}<->{\\nBeyer et al\\'s clean labels result in 90.9% Top-1 accuracy and 98.8% Top-5 accuracy with FixEfﬁcientNet-L2, surpassing all other literature results. Training variations of about 0.1 accuracy points occur due to random initialization and mini-batch sampling on Imagenet.\\n}<->{\\nThe 012 test set is not available and most models tune hyperparameters on the validation set instead. This approach can cause overfitting, as it\\'s often indistinguishable from the actual test set. Neural Architecture Search is used in EfficientNets to enlarge the hyperparameter space, and pre-trained models are filtered using ImageNet validation images.\\n}<->{\\nPre-trained models are filtered using the unlabelled set to prevent overfitting on the validation set. This approach is used in experiments with pre-trained EfficientNet, allowing for comparison with Noisy Student results that also use this level of overfitting.\\n}<->{\\nYalniz et al.\\'s emi-supervised approach was evaluated on ImageNet-V2, a dataset replacing the original ImageNet with 3 novel test sets: \\n- Matched Frequency\\n- Easy\\n- Hard\\n}<->{\\nTest sets: Matched Frequency is closest to ImageNet validation set.\\nAll models evaluated on Matched Frequency version of ImageNet-v2 dataset for accuracy.\\n}<->{\\nCybersecurity threats can be categorized into various types. \\n\\n* Noisy Student: A type of adversarial attack.\\n* FixRes-Noisy Student: A mitigation technique for noisy student attacks.\\n* FixRes-Billion Scale: A large-scale version of the fixres model.\\n* Billion Scale: A large-scale variant of a specific neural network architecture.\\n* EfficientNet: A family of neural networks designed for efficient image recognition.\\n* ResNet: A type of neural network known for its residual connections.\\n* NASNet: A neural architecture search algorithm.\\n* RegNetY: A type of neural network that uses regularization techniques.\\n}<->{\\nSeveral approaches perform poorly on ImageNet-v2 due to overfitting, unlike those that generalize better. A previous study showed significant overfitting on Imagenet 2012 but not its impact on model ranking.\\n}<->{\\nBillion scale models of Yalniz et al. outperform EfficientNets from Noisy on ImageNet-V2 despite having similar accuracy on ImageNet, suggesting that these models are prone to overfitting.\\n}<->{\\nImageNet-V2: Billion-scale models outperform EfficientNets from Noisy Student, with FixResNeXt-101 32x4d achieving +0.7% Top-1 accuracy improvement over its counterpart on ImageNet-V2.\\n}<->{\\nSupervised work or other literature works show that FixRes fine-tuning procedure is neutral with respect to overfitting. Using pre-trained models with extra-training data resulted in improved performance on ImageNet-V2, achieving matched frequency with additional data.\\n}<->{\\nHere is the summary in 92 words or less:\\n\\nEfficientNet models achieved top-1 accuracy:\\n- B0: 67.7%\\n- B1: 70.9%\\n- B2: 72.3%\\n- B3: 73.9%\\n- B4: 75.7%\\n- B5: 76.8%\\n- B6: 77.3%\\n- B7: 78.5%\\n\\nTop-5 accuracy:\\n- B0: 88.1%\\n- B1: 90.1%\\n- B2: 91.1%\\n- B3: 91.9%\\n- B4: 93.1%\\n- B5: 93.6%\\n- B6: 93.9%\\n- B7: 94.4%\\n}<->{\\nResults on ImageNet-V2 without external data: \\n- EfficientNet models: B0 (65.5% Top-1, 85.6% Top-5), B1 (67.5%, 87.8%), B2 (68.9%, 88.4%), B3 (70.9%, 89%)\\n}<->{\\n7.8M, 240, 67.5\\n9.2M, 260, 70.1\\n12M, 300, 72.7\\n19M, 380, 73.9\\n30M, 456, 75.4\\n43M, 528, 75.8\\n66M, 600, 76.1\\n87.4M, 672, 76.1\\n}<->{\\nFixing Resolution improves performance with fine-tuning on ImageNet-V2 benchmark, establishing new state-of-the-art results.\\n}<->{\\nEfﬁcientNet models combined with an improved training method provide high flexibility. An open-source implementation is available at http://github.com/facebookresearch/FixRes.\\n}<->{\\nEfﬁcientNet models outperform ResNet in ImageNet-v2 single crop with external data:\\n- EfﬁcientNet-B0: Top-1(67.7%), Top-5(88.1%)\\n- FixEfﬁcientNet-B0: Top-1(69.4%), Top-5(89.6%)\\n- ResNet50: Top-1(71.7%), Top-5(90.5%)\\nModel sizes:\\n- EfﬁcientNet-B0: 5.3M\\n- ResNet50: 25.6M\\n}<->{\\nFixed efficient networks: \\n- EfficientNet-B2: 91.4% accuracy, 9.1M parameters\\n- EfficientNet-B3 NS: 92.0% accuracy, 12.2M parameters\\n- ResNeXt-101 (32x4d BS): 91.9% accuracy, 42.0M parameters\\n}<->{\\nHere is a summary of the text in original language with useful information below 100 words:\\n\\nComparison of Efficient Neural Network architectures: \\n\\n* FixResNeXt-101 32x16d: 193.0M parameters, 93.4% accuracy\\n* EfﬁcientNet-B5 NS: 30.4M parameters, 94.3% accuracy (fixed version)\\n* EfﬁcientNet-B6 NS: 43.0M parameters, 96.1% accuracy (fixed version)\\n* FixResNeXt-101 32x48d: 829.0M parameters, 93.9% accuracy\\n* EfﬁcientNet-B7 NS: 66.4M parameters, 94.7% accuracy (fixed version)\\n\\nReferences:\\n[1] Hugo Touvron et al., \"Fixing the limitations of self-normalizing neural networks\",\\n}<->{\\nHugo Touvron et al., \"Fixing the train-test resolution discrepancy,\" (2019). \\nIsmet Yalniz et al., \"Billion-scale semi-supervised learning for image classification,\" (2019).\\nEkin Dogus Cubuk et al., (no reference given).\\n}<->{\\n00546, 2019.\\n[3] Autoaugment learns augmentation policies from data.\\n[4] Efficientnet rethinks model scaling for convolutional neural networks.\\n[5] ImageNet study questions the progress of deep learning in image classification.\\n}<->{\\nZhai and Aaron van den Oord (2020) proposed that ImageNet is no longer sufficient for deep learning research. \\nYanping Huang et al., (2018), introduced Gpipe, an efficient training method using pipeline parallelism.\\nDhruv Mahajan et al., explored the limits of weight sharing in convolutional neural networks.\\n}<->{\\nManohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten (2018) and Cihang Xie et al (unspecified date) worked on improving pretraining methods for computer vision tasks. Qizhe Xie et al (2019) discovered that self-training with noisy students can improve ImageNet classification accuracy.\\n}<->{\\n\"Adversarial Examples Improve Image Recognition\"\\n\\nOlga Russakovsky et al. and Quoc V. Le et al., \\n\"Imagenet large scale visual recognition challenge\", International journal of Computer Vision, 2015.\\n\\nThis paper introduced the ImageNet Large Scale Visual Recognition Challenge, which aims to improve image recognition accuracy.\\n}<->{\\nPre-trained efficientnet models learned for scalable image recognition \\nusing transferable architectures.\\n}<->{\\nHua et al., \"Progressive neural architecture search,\" ICVR 2018.\\n\\nIlija Radosavovic et al., \"Designing network design spaces,\" 2020.\\n\\nEkin D. Cubuk et al., \"Randaugment: Practical automated data augmentation with reduced search space\"\\n}<->{\\nQuoc V. Le and Kaiming He introduced techniques for improving image recognition. Randaugment (2019) reduced search space for data augmentation, while Deep Residual Learning (2016) improved feature reuse. Benjamin Recht et al.\\'s study (2020) found ImageNet classifiers often fail to generalize on the test set, highlighting limitations in model performance on unseen data.\\n}<->{\\n\"Generalization of ImageNet classifiers to ImageNet\" paper (2019) at International Conference on Machine Learning:\\n\\n* Most ImageNet classifiers achieve high accuracy but struggle with out-of-domain data.\\n* The challenge lies in the large variability of objects and scenes across ImageNet subsets.\\n* Researchers proposed methods to improve generalization, such as multi-task learning and ensemble methods.'}}\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/Reports/2003.10580v4_normal.txt': {'Scores': '7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n5\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n5\\n}<->{\\n4\\n}<->{\\n5\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n2\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n3\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n2\\n}<->{\\n4\\n}<->{\\n9\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n5\\n}<->{\\n7\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n3\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n5\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n5\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6', 'Chunks': 'Meta Pseudo Labels\\nHieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, Quoc V. Le\\nGoogle AI, Brain Team, Mountain View, CA 94043\\n{hyhieu,zihangd,qizhex,thangluong,qvl}@google.com\\nAbstract\\nWe present Meta Pseudo Labels, a semi-supervised learn-\\ning method that achieves a new state-of-the-art top-1 ac-\\ncuracy of 90.2% on ImageNet, which is 1.6% better than\\nthe existing state-of-the-art [16]. Like Pseudo Labels, Meta\\nPseudo Labels has a teacher network to generate pseudo la-\\nbels on unlabeled data \\n}<->{\\neudo Labels, Meta\\nPseudo Labels has a teacher network to generate pseudo la-\\nbels on unlabeled data to teach a student network. However,\\nunlike Pseudo Labels where the teacher is ﬁxed, the teacher\\nin Meta Pseudo Labels is constantly adapted by the feedback\\nof the student’s performance on the labeled dataset. As a\\nresult, the teacher generates better pseudo labels to teach\\nthe student.1\\n1. Introduction\\nThe methods of Pseudo Labels or self-training [57, 81,\\n55, 36] have been applied successfully t\\n}<->{\\ntion\\nThe methods of Pseudo Labels or self-training [57, 81,\\n55, 36] have been applied successfully to improve state-of-\\nthe-art models in many computer vision tasks such as image\\nclassiﬁcation (e.g., [79, 77]), object detection, and semantic\\nsegmentation (e.g., [89, 51]). Pseudo Labels methods work\\nby having a pair of networks, one as a teacher and one as a\\nstudent. The teacher generates pseudo labels on unlabeled\\nimages. These pseudo labeled images are then combined\\nwith labeled images to train\\n}<->{\\nbels on unlabeled\\nimages. These pseudo labeled images are then combined\\nwith labeled images to train the student. Thanks to the abun-\\ndance of pseudo labeled data and the use of regularization\\nmethods such as data augmentation, the student learns to\\nbecome better than the teacher [77].\\nDespite the strong performance of Pseudo Labels meth-\\nods, they have one main drawback: if the pseudo labels are\\ninaccurate, the student will learn from inaccurate data. As\\na result, the student may not get signiﬁ\\n}<->{\\ninaccurate, the student will learn from inaccurate data. As\\na result, the student may not get signiﬁcantly better than\\nthe teacher. This drawback is also known as the problem of\\nconﬁrmation bias in pseudo-labeling [2].\\nIn this paper, we design a systematic mechanism for the\\nteacher to correct the bias by observing how its pseudo labels\\nwould affect the student. Speciﬁcally, we propose Meta\\nPseudo Labels, which utilizes the feedback from the student\\n1Code\\nis\\navailable\\nat\\nhttps : / / github . com \\n}<->{\\nLabels, which utilizes the feedback from the student\\n1Code\\nis\\navailable\\nat\\nhttps : / / github . com / google -\\nresearch/google-research/tree/master/meta_pseudo_\\nlabels.\\nto inform the teacher to generate better pseudo labels. In our\\nimplementation, the feedback signal is the performance of\\nthe student on the labeled dataset. This feedback signal is\\nused as a reward to train the teacher throughout the course of\\nthe student’s learning. In summary, the teacher and student\\nof Meta Pseudo Labels are t\\n}<->{\\nhe course of\\nthe student’s learning. In summary, the teacher and student\\nof Meta Pseudo Labels are trained in parallel: (1) the student\\nlearns from a minibatch of pseudo labeled data annotated by\\nthe teacher, and (2) the teacher learns from the reward signal\\nof how well the student performs on a minibatch drawn from\\nthe labeled dataset.\\nWe experiment with Meta Pseudo Labels, using the\\nImageNet [56] dataset as labeled data and the JFT-300M\\ndataset [26, 60] as unlabeled data.\\nWe train a pair of\\nEf\\n}<->{\\n] dataset as labeled data and the JFT-300M\\ndataset [26, 60] as unlabeled data.\\nWe train a pair of\\nEfﬁcientNet-L2 networks, one as a teacher and one as a\\nstudent, using Meta Pseudo Labels. The resulting student\\nnetwork achieves the top-1 accuracy of 90.2% on the Im-\\nageNet ILSVRC 2012 validation set [56], which is 1.6%\\nbetter than the previous record of 88.6% [16]. This student\\nmodel also generalizes to the ImageNet-ReaL test set [6], as\\nsummarized in Table 1. Small scale semi-supervised learn-\\ni\\n}<->{\\nes to the ImageNet-ReaL test set [6], as\\nsummarized in Table 1. Small scale semi-supervised learn-\\ning experiments with standard ResNet models on CIFAR-\\n10-4K, SVHN-1K, and ImageNet-10% also show that Meta\\nPseudo Labels outperforms a range of other recently pro-\\nposed methods such as FixMatch [58] and Unsupervised\\nData Augmentation [76].\\nDatasets\\nImageNet\\nImageNet-ReaL\\nTop-1 Accuracy\\nPrecision@1\\nPrevious SOTA [16, 14]\\n88.6\\n90.72\\nOurs\\n90.2\\n91.02\\nTable 1: Summary of our key results on ImageNet ILS\\n}<->{\\nrevious SOTA [16, 14]\\n88.6\\n90.72\\nOurs\\n90.2\\n91.02\\nTable 1: Summary of our key results on ImageNet ILSVRC 2012\\nvalidation set [56] and the ImageNet-ReaL test set [6].\\n2. Meta Pseudo Labels\\nAn overview of the contrast between Pseudo Labels and\\nMeta Pseudo Labels is presented in Figure 1. The main\\ndifference is that in Meta Pseudo Labels, the teacher receives\\nfeedback of the student’s performance on a labeled dataset.\\n1\\narXiv:2003.10580v4  [cs.LG]  1 Mar 2021\\n\\n---------------------------------------\\n}<->{\\n labeled dataset.\\n1\\narXiv:2003.10580v4  [cs.LG]  1 Mar 2021\\n\\n--------------------------------------------------\\nStudent\\nPre-trained\\nTeacher\\nStudent’s performance\\non labeled data\\nPseudo-labeled data\\nStudent\\nTeacher\\nPseudo-labeled data\\nFigure 1: The difference between Pseudo Labels and Meta Pseudo Labels. Left: Pseudo Labels, where a ﬁxed pre-trained teacher generates\\npseudo labels for the student to learn from. Right: Meta Pseudo Labels, where the teacher is trained along with the student. The st\\n}<->{\\no learn from. Right: Meta Pseudo Labels, where the teacher is trained along with the student. The student is\\ntrained based on the pseudo labels generated by the teacher (top arrow). The teacher is trained based on the performance of the student on\\nlabeled data (bottom arrow).\\nNotations.\\nLet T and S respectively be the teacher net-\\nwork and the student network in Meta Pseudo Labels. Let\\ntheir corresponding parameters be θT and θS. We use (xl, yl)\\nto refer to a batch of images and their correspond\\n}<->{\\nsponding parameters be θT and θS. We use (xl, yl)\\nto refer to a batch of images and their corresponding labels,\\ne.g., ImageNet training images and their labels, and use xu\\nto refer to a batch of unlabeled images, e.g., images from\\nthe internet. We denote by T(xu; θT ) the soft predictions\\nof the teacher network on the batch xu of unlabeled images\\nand likewise for the student, e.g. S(xl; θS) and S(xu; θS).\\nWe use CE(q, p) to denote the cross-entropy loss between\\ntwo distributions q and p; if q is\\n}<->{\\nxu; θS).\\nWe use CE(q, p) to denote the cross-entropy loss between\\ntwo distributions q and p; if q is a label then it is understood\\nas a one-hot distribution; if q and p have multiple instances\\nin them then CE(q, p) is understood as the average of all\\ninstances in the batch. For example, CE\\n\\x00yl, S(xl; θS)\\n\\x01\\nis\\nthe canonical cross-entropy loss in supervised learning.\\nPseudo Labels as an optimization problem.\\nTo intro-\\nduce Meta Pseudo Labels, let’s ﬁrst review Pseudo Labels.\\nSpeciﬁcally, Pseudo La\\n}<->{\\n problem.\\nTo intro-\\nduce Meta Pseudo Labels, let’s ﬁrst review Pseudo Labels.\\nSpeciﬁcally, Pseudo Labels (PL) trains the student model to\\nminimize the cross-entropy loss on unlabeled data:\\nθPL\\nS = argmin\\nθS\\nExu\\nh\\nCE\\n\\x00T(xu; θT ), S(xu; θS)\\n\\x01i\\n|\\n{z\\n}\\n:=Lu\\n\\x00θT ,θS\\n\\x01\\n(1)\\nwhere the pseudo target T(xu; θT ) is produced by a well\\npre-trained teacher model with ﬁxed parameter θT . Given a\\ngood teacher, the hope of Pseudo Labels is that the obtained\\nθPL\\nS would ultimately achieve a low loss on labeled da\\n}<->{\\ne hope of Pseudo Labels is that the obtained\\nθPL\\nS would ultimately achieve a low loss on labeled data, i.e.\\nExl,yl\\nh\\nCE\\n\\x00yl, S(xl; θPL\\nS )\\n\\x01i\\n:= Ll\\n\\x00θPL\\nS\\n\\x01\\n.\\nUnder the framework of Pseudo Labels, notice that the\\noptimal student parameter θPL\\nS always depends on the teacher\\nparameter θT via the pseudo targets T(xu; θT ). To facili-\\ntate the discussion of Meta Pseudo Labels, we can explicitly\\nexpress the dependency as θPL\\nS (θT ). As an immediate obser-\\nvation, the ultimate student loss on label\\n}<->{\\ness the dependency as θPL\\nS (θT ). As an immediate obser-\\nvation, the ultimate student loss on labeled data Ll\\n\\x00θPL\\nS (θT )\\n\\x01\\nis also a “function” of θT . Therefore, we could further opti-\\nmize Ll with respect to θT :\\nmin\\nθT\\nLl\\n\\x00θPL\\nS (θT )\\n\\x01\\n,\\nwhere\\nθPL\\nS (θT ) = argmin\\nθS\\nLu\\n\\x00θT , θS\\n\\x01\\n.\\n(2)\\nIntuitively, by optimizing the teacher’s parameter accord-\\ning to the performance of the student on labeled data, the\\npseudo labels can be adjusted accordingly to further improve\\nstudent’s performance. As \\n}<->{\\ned data, the\\npseudo labels can be adjusted accordingly to further improve\\nstudent’s performance. As we are effectively trying to op-\\ntimize the teacher on a meta level, we name our method\\nMeta Pseudo Labels. However, the dependency of θPL\\nS (θT )\\non θT is extremely complicated, as computing the gradient\\n∇θT θPL\\nS (θT ) requires unrolling the entire student training\\nprocess (i.e. argminθS).\\nPractical approximation.\\nTo make Meta Pseudo Labels\\nfeasible, we borrow ideas from previous work in meta le\\n}<->{\\nal approximation.\\nTo make Meta Pseudo Labels\\nfeasible, we borrow ideas from previous work in meta learn-\\ning [40, 15] and approximate the multi-step argminθS with\\nthe one-step gradient update of θS:\\nθPL\\nS (θT ) ≈θS −ηS · ∇θSLu\\n\\x00θT , θS\\n\\x01\\n,\\nwhere ηS is the learning rate. Plugging this approximation\\ninto the optimization problem in Equation 2 leads to the\\npractical teacher objective in Meta Pseudo Labels:\\nmin\\nθT\\nLl\\n\\x10\\nθS −ηS · ∇θSLu\\n\\x00θT , θS\\n\\x01\\x11\\n.\\n(3)\\nNote that, if soft pseudo labels are used, i.e. \\n}<->{\\nabels:\\nmin\\nθT\\nLl\\n\\x10\\nθS −ηS · ∇θSLu\\n\\x00θT , θS\\n\\x01\\x11\\n.\\n(3)\\nNote that, if soft pseudo labels are used, i.e. T(xu; θT ) is\\nthe full distribution predicted by teacher, the objective above\\nis fully differentiable with respect to θT and we can perform\\nstandard back-propagation to get the gradient.2 However, in\\nthis work, we sample the hard pseudo labels from the teacher\\ndistribution to train the student. We use hard pseudo labels\\nbecause they result in smaller computational graphs which\\n2When optimizing Equ\\n}<->{\\nse hard pseudo labels\\nbecause they result in smaller computational graphs which\\n2When optimizing Equation (3), we always treat θS as ﬁxed parameters\\nand ignore its higher-order dependency on θT .\\n2\\n\\n--------------------------------------------------\\nare necessary for our large-scale experiments in Section 4.\\nFor smaller experiments where we can use either soft pseudo\\nlabels or hard pseudo labels, we do not ﬁnd signiﬁcant per-\\nformance difference between them. A caveat of using hard\\npseudo labels\\n}<->{\\nwe do not ﬁnd signiﬁcant per-\\nformance difference between them. A caveat of using hard\\npseudo labels is that we need to rely on a slightly modiﬁed\\nversion of REINFORCE to obtain the approximated gradient\\nof Ll in Equation 3 with respect to θT . We defer the detailed\\nderivation to Appendix A.\\nOn the other hand, the student’s training still relies on the\\nobjective in Equation 1, except that the teacher parameter\\nis not ﬁxed anymore. Instead, θT is constantly changing\\ndue to the teacher’s optimizat\\n}<->{\\nher parameter\\nis not ﬁxed anymore. Instead, θT is constantly changing\\ndue to the teacher’s optimization. More interestingly, the\\nstudent’s parameter update can be reused in the one-step\\napproximation of the teacher’s objective, which naturally\\ngives rise to an alternating optimization procedure between\\nthe student update and the teacher update:\\n• Student: draw a batch of unlabeled data xu, then sample\\nT(xu; θT ) from teacher’s prediction, and optimize objec-\\ntive 1 with SGD: θ′\\nS = θS −ηS∇θSLu(θ\\n}<->{\\nmple\\nT(xu; θT ) from teacher’s prediction, and optimize objec-\\ntive 1 with SGD: θ′\\nS = θS −ηS∇θSLu(θT , θS),\\n• Teacher: draw a batch of labeled data (xl, yl), and “reuse”\\nthe student’s update to optimize objective 3 with SGD:\\nθ′\\nT = θT −ηT ∇θT Ll\\n\\x00θS −∇θSLu\\n\\x00θT , θS\\n\\x01\\n|\\n{z\\n}\\n= θ′\\nS reused from student’s update\\n\\x01\\n.\\nTeacher’s auxiliary losses.\\nWe empirically observe that\\nMeta Pseudo Labels works well on its own. Moreover, it\\nworks even better if the teacher is jointly trained with other\\nauxiliary \\n}<->{\\nl on its own. Moreover, it\\nworks even better if the teacher is jointly trained with other\\nauxiliary objectives. Therefore, in our implementation, we\\naugment the teacher’s training with a supervised learning\\nobjective and a semi-supervised learning objective. For the\\nsupervised objective, we train the teacher on labeled data.\\nFor the semi-supervised objective, we additionally train the\\nteacher on unlabeled data using the UDA objective [76].\\nFor the full pseudo code of Meta Pseudo Labels when it\\ni\\n}<->{\\nnlabeled data using the UDA objective [76].\\nFor the full pseudo code of Meta Pseudo Labels when it\\nis combined with supervised and UDA objectives for the\\nteacher, please see Appendix B, Algorithm 1.\\nFinally, as the student in Meta Pseudo Labels only learns\\nfrom unlabeled data with pseudo labels generated by the\\nteacher, we can take a student model that has converged\\nafter training with Meta Pseudo Labels and ﬁnetune it on\\nlabeled data to improve its accuracy. Details of the student’s\\nﬁnetuning a\\n}<->{\\n Labels and ﬁnetune it on\\nlabeled data to improve its accuracy. Details of the student’s\\nﬁnetuning are reported in our experiments.\\nNext, we will present the experimental results of Meta\\nPseudo Labels, and organize them as follows:\\n• Section 3 presents small scale experiments where we com-\\npare Meta Pseudo Labels against other state-of-the-art\\nsemi-supervised learning methods on widely used bench-\\nmarks.\\n• Section 4 presents large scale experiments of Meta Pseudo\\nLabels where we push the limits \\n}<->{\\n\\nmarks.\\n• Section 4 presents large scale experiments of Meta Pseudo\\nLabels where we push the limits of ImageNet accuracy.\\n3. Small Scale Experiments\\nIn this section, we present our empirical studies of Meta\\nPseudo Labels at small scales. We ﬁrst study the role of\\nfeedback in Meta Pseudo Labels on the simple TwoMoon\\ndataset [7]. This study visually illustrates Meta Pseudo\\nLabels’ behaviors and beneﬁts. We then compare Meta\\nPseudo Labels against state-of-the-art semi-supervised learn-\\ning methods \\n}<->{\\nﬁts. We then compare Meta\\nPseudo Labels against state-of-the-art semi-supervised learn-\\ning methods on standard benchmarks such as CIFAR-10-4K,\\nSVHN-1K, and ImageNet-10%. We conclude the section\\nwith experiments on the standard ResNet-50 architecture\\nwith the full ImageNet dataset.\\n3.1. TwoMoon Experiment\\nTo understand the role of feedback in Meta Pseudo Labels,\\nwe conduct an experiment on the simple and classic TwoM-\\noon dataset [7]. The 2D nature of the TwoMoon dataset\\nallows us to visualize h\\n}<->{\\nple and classic TwoM-\\noon dataset [7]. The 2D nature of the TwoMoon dataset\\nallows us to visualize how Meta Pseudo Labels behaves\\ncompared to Supervised Learning and Pseudo Labels.\\nDataset.\\nFor this experiment, we generate our own version\\nof the TwoMoon dataset. In our version, there are 2,000 ex-\\namples forming two clusters each with 1,000 examples. Only\\n6 examples are labeled, 3 examples for each cluster, while the\\nremaining examples are unlabeled. Semi-supervised learn-\\ning algorithms are ask\\n}<->{\\nh cluster, while the\\nremaining examples are unlabeled. Semi-supervised learn-\\ning algorithms are asked to use these 6 labeled examples and\\nthe clustering assumption to separate the two clusters into\\ncorrect classes.\\nTraining details.\\nOur model architecture is a feed-forward\\nfully-connected neural network with two hidden layers, each\\nhas 8 units. The sigmoid non-linearity is used at each layer.\\nIn Meta Pseudo Labels, both the teacher and the student\\nshare this architecture but have independent we\\n}<->{\\nMeta Pseudo Labels, both the teacher and the student\\nshare this architecture but have independent weights. All\\nnetworks are trained with SGD using a constant learning\\nrate of 0.1. The networks’ weights are initialized with the\\nuniform distribution between -0.1 and 0.1. We do not apply\\nany regularization.\\nResults.\\nWe randomly generate the TwoMoon dataset for\\na few times and repeat the three methods: Supervised Learn-\\ning, Pseudo Labels, and Meta Pseudo Labels. We observe\\nthat Meta Pseudo Labels h\\n}<->{\\n: Supervised Learn-\\ning, Pseudo Labels, and Meta Pseudo Labels. We observe\\nthat Meta Pseudo Labels has a much higher success rate of\\nﬁnding the correct classiﬁer than Supervised Learning and\\nPseudo Labels. Figure 2 presents a typical outcome of our\\nexperiment, where the red and green regions correspond to\\nthe classiﬁers’ decisions. As can be seen from the ﬁgure, Su-\\npervised Learning ﬁnds a bad classiﬁer which classiﬁes the\\nlabeled instances correctly but fails to take advantage of the\\nclusterin\\n}<->{\\nassiﬁer which classiﬁes the\\nlabeled instances correctly but fails to take advantage of the\\nclustering assumption to separate the two “moons”. Pseudo\\nLabels uses the bad classiﬁer from Supervised Learning and\\nhence receives incorrect pseudo labels on the unlabeled data.\\nAs a result, Pseudo Labels ﬁnds a classiﬁer that misclassiﬁes\\n3\\n\\n--------------------------------------------------\\nFigure 2: An illustration of the importance of feedback in Meta Pseudo Labels (right). In this example, Meta Pseud\\n}<->{\\nllustration of the importance of feedback in Meta Pseudo Labels (right). In this example, Meta Pseudo Labels works better\\nthan Supervised Learning (left) and Pseudo Labels (middle) on the simple TwoMoon dataset. More details are in Section 3.1.\\nhalf of the data, including a few labeled instances. Meta\\nPseudo Labels, on the other hand, uses the feedback from\\nthe student model’s loss on the labeled instances to adjust the\\nteacher to generate better pseudo labels. As a result, Meta\\nPseudo Labels ﬁn\\n}<->{\\ninstances to adjust the\\nteacher to generate better pseudo labels. As a result, Meta\\nPseudo Labels ﬁnds a good classiﬁer for this dataset. In\\nother words, Meta Pseudo Labels can address the problem\\nof conﬁrmation bias [2] of Pseudo Labels in this experiment.\\n3.2. CIFAR-10-4K, SVHN-1K, and ImageNet-10%\\nExperiments\\nDatasets.\\nWe\\nconsider\\nthree\\nstandard\\nbenchmarks:\\nCIFAR-10-4K, SVHN-1K, and ImageNet-10%, which have\\nbeen widely used in the literature to fairly benchmark semi-\\nsupervised learning algor\\n}<->{\\n%, which have\\nbeen widely used in the literature to fairly benchmark semi-\\nsupervised learning algorithms. These benchmarks were\\ncreated by keeping a small fraction of the training set as\\nlabeled data while using the rest as unlabeled data. For\\nCIFAR-10 [34], 4,000 labeled examples are kept as labeled\\ndata while 41,000 examples are used as unlabeled data. The\\ntest set for CIFAR-10 is standard and consists of 10,000\\nexamples. For SVHN [46], 1,000 examples are used as\\nlabeled data whereas about 60\\n}<->{\\nconsists of 10,000\\nexamples. For SVHN [46], 1,000 examples are used as\\nlabeled data whereas about 603,000 examples are used as\\nunlabeled data. The test set for SVHN is also standard, and\\nhas 26,032 examples. Finally, for ImageNet [56], 128,000\\nexamples are used as labeled data which is approximately\\n10% of the whole ImageNet training set while the rest of\\n1.28 million examples are used as unlabeled data. The test\\nset for ImageNet is the standard ILSVRC 2012 version that\\nhas 50,000 examples. We u\\n}<->{\\nd data. The test\\nset for ImageNet is the standard ILSVRC 2012 version that\\nhas 50,000 examples. We use the image resolution of 32x32\\nfor CIFAR-10 and SVHN, and 224x224 for ImageNet.\\nTraining details.\\nIn our experiments, our teacher and\\nour student share the same architecture but have indepen-\\ndent weights. For CIFAR-10-4K and SVHN-1K, we use a\\nWideResNet-28-2 [84] which has 1.45 million parameters.\\nFor ImageNet, we use a ResNet-50 [24] which has 25.5\\nmillion parameters. These architectures are a\\n}<->{\\n.\\nFor ImageNet, we use a ResNet-50 [24] which has 25.5\\nmillion parameters. These architectures are also commonly\\nused by previous works in this area. During the Meta Pseudo\\nLabels training phase where we train both the teacher and\\nthe student, we use the default hyper-parameters from previ-\\nous work for all our models, except for a few modiﬁcations\\nin RandAugment [13] which we detail in Appendix C.2.\\nAll hyper-parameters are reported in Appendix C.4. After\\ntraining both the teacher and student w\\n}<->{\\n.2.\\nAll hyper-parameters are reported in Appendix C.4. After\\ntraining both the teacher and student with Meta Pseudo La-\\nbels, we ﬁnetune the student on the labeled dataset. For this\\nﬁnetuning phase, we use SGD with a ﬁxed learning rate of\\n10−5 and a batch size of 512, running for 2,000 steps for\\nImageNet-10% and 1,000 steps for CIFAR-10 and SVHN.\\nSince the amount of labeled examples is limited for all three\\ndatasets, we do not use any heldout validation set. Instead,\\nwe return the model at the ﬁ\\n}<->{\\n all three\\ndatasets, we do not use any heldout validation set. Instead,\\nwe return the model at the ﬁnal checkpoint.\\nBaselines.\\nTo ensure a fair comparison, we only compare\\nMeta Pseudo Labels against methods that use the same archi-\\ntectures and do not compare against methods that use larger\\narchitectures such as Larger-WideResNet-28-2 and Pyramid-\\nNet+ShakeDrop for CIFAR-10 and SVHN [5, 4, 72, 76],\\nor ResNet-50×{2,3,4}, ResNet-101, ResNet-152, etc. for\\nImageNet-10% [25, 23, 10, 8, 9]. We also do\\n}<->{\\n,\\nor ResNet-50×{2,3,4}, ResNet-101, ResNet-152, etc. for\\nImageNet-10% [25, 23, 10, 8, 9]. We also do not compare\\nMeta Pseudo Labels with training procedures that include\\nself-distillation or distillation from a larger teacher [8, 9].\\nWe enforce these restrictions on our baselines since it is\\nknown that larger architectures and distillation can improve\\nany method, possibly including Meta Pseudo Labels.\\nWe directly compare Meta Pseudo Labels against two\\nbaselines: Supervised Learning with full dat\\n}<->{\\nels.\\nWe directly compare Meta Pseudo Labels against two\\nbaselines: Supervised Learning with full dataset and Un-\\nsupervised Data Augmentation (UDA [76]). Supervised\\nLearning with full dataset represents the headroom because\\nit unfairly makes use of all labeled data (e.g., for CIFAR-\\n10, it uses all 50,000 labeled examples). We also compare\\nagainst UDA because our implementation of Meta Pseudo\\nLabels uses UDA in training the teacher. Both of these base-\\nlines use the same experimental protocols a\\n}<->{\\nls uses UDA in training the teacher. Both of these base-\\nlines use the same experimental protocols and hence ensure\\na fair comparison. We follow [48]’s train/eval/test splitting,\\nand we use the same amount of resources to tune hyper-\\nparameters for our baselines as well as for Meta Pseudo\\nLabels. More details are in Appendix C.\\nAdditional baselines.\\nIn addition to these two baselines,\\nwe also include a range of other semi-supervised baselines\\nin two categories: Label Propagation and Self-Supervi\\n}<->{\\nude a range of other semi-supervised baselines\\nin two categories: Label Propagation and Self-Supervised.\\nSince these methods do not share the same controlled envi-\\nronment, the comparison to them is not direct, and should\\nbe contextualized as suggested by [48]. More controlled ex-\\nperiments comparing Meta Pseudo Labels to other baselines\\n4\\n\\n--------------------------------------------------\\nMethod\\nCIFAR-10-4K\\nSVHN-1K\\nImageNet-10%\\n(mean ± std)\\n(mean ± std)\\nTop-1\\nTop-5\\nLabel Propagation Methods\\nTe\\n}<->{\\n\\nCIFAR-10-4K\\nSVHN-1K\\nImageNet-10%\\n(mean ± std)\\n(mean ± std)\\nTop-1\\nTop-5\\nLabel Propagation Methods\\nTemporal Ensemble [35]\\n83.63 ± 0.63\\n92.81 ± 0.27\\n−\\nMean Teacher [64]\\n84.13 ± 0.28\\n94.35 ± 0.47\\n−\\nVAT + EntMin [44]\\n86.87 ± 0.39\\n94.65 ± 0.19\\n−\\n83.39\\nLGA + VAT [30]\\n87.94 ± 0.19\\n93.42 ± 0.36\\n−\\nICT [71]\\n92.71 ± 0.02\\n96.11 ± 0.04\\n−\\nMixMatch [5]\\n93.76 ± 0.06\\n96.73 ± 0.31\\n−\\nReMixMatch [4]\\n94.86 ± 0.04\\n97.17 ± 0.30\\n−\\nEnAET [72]\\n94.65\\n97.08\\n−\\nFixMatch [58]\\n95.74 ± 0.05\\n97.72 ± 0.38\\n71.5\\n89.1\\nUDA∗[76]\\n94.53\\n}<->{\\n7 ± 0.30\\n−\\nEnAET [72]\\n94.65\\n97.08\\n−\\nFixMatch [58]\\n95.74 ± 0.05\\n97.72 ± 0.38\\n71.5\\n89.1\\nUDA∗[76]\\n94.53 ± 0.18\\n97.11 ± 0.17\\n68.07\\n88.19\\nSelf-Supervised Methods\\nSimCLR [8, 9]\\n−\\n−\\n71.7\\n90.4\\nMOCOv2 [10]\\n−\\n−\\n71.1\\n−\\nPCL [38]\\n−\\n−\\n−\\n85.6\\nPIRL [43]\\n−\\n−\\n−\\n84.9\\nBYOL [21]\\n−\\n−\\n68.8\\n89.0\\nMeta Pseudo Labels\\n96.11 ± 0.07\\n98.01 ± 0.07\\n73.89\\n91.38\\nSupervised Learning with full dataset∗\\n94.92 ± 0.17\\n97.41 ± 0.16\\n76.89\\n93.27\\nTable 2: Image classiﬁcation accuracy on CIFAR-10-4K, SVHN-1K, and ImageNet-10%. Higher is be\\n}<->{\\n\\n93.27\\nTable 2: Image classiﬁcation accuracy on CIFAR-10-4K, SVHN-1K, and ImageNet-10%. Higher is better. For CIFAR-10-4K and SVHN-\\n1K, we report mean ± std over 10 runs, while for ImageNet-10%, we report Top-1/Top-5 accuracy of a single run. For fair comparison,\\nwe only include results that share the same model architecture: WideResNet-28-2 for CIFAR-10-4K and SVHN-1K, and ResNet-50 for\\nImageNet-10%. ∗indicates our implementation which uses the same experimental protocols. Except for UDA, resul\\n}<->{\\n10%. ∗indicates our implementation which uses the same experimental protocols. Except for UDA, results in the ﬁrst two blocks\\nare from representative important papers, and hence do not share the same controlled environment with ours.\\nare presented in Appendix D.\\nResults.\\nTable 2 presents our results with Meta Pseudo\\nLabels in comparison with other methods. The results show\\nthat under strictly fair comparisons (as argued by [48]),\\nMeta Pseudo Labels signiﬁcantly improves over UDA. In-\\nterestingly\\n}<->{\\n comparisons (as argued by [48]),\\nMeta Pseudo Labels signiﬁcantly improves over UDA. In-\\nterestingly, on CIFAR-10-4K, Meta Pseudo Labels even\\nexceeds the headroom supervised learning on full dataset.\\nOn ImageNet-10%, Meta Pseudo Labels outperforms the\\nUDA teacher by more than 5% in top-1 accuracy, going\\nfrom 68.07% to 73.89%. For ImageNet, such relative im-\\nprovement is very signiﬁcant.\\nComparing to existing state-of-the-art methods.\\nCom-\\npared to results reported from past papers, Meta Pseudo\\nL\\n}<->{\\nto existing state-of-the-art methods.\\nCom-\\npared to results reported from past papers, Meta Pseudo\\nLabels has achieved the best accuracies among the same\\nmodel architectures on all the three datasets: CIFAR-10-\\n4K, SVHN-1K, and ImageNet-10%. On CIFAR-10-4K and\\nSVHN-1K, Meta Pseudo Labels leads to almost 10% rela-\\ntive error reduction compared to the highest reported base-\\nlines [58]. On ImageNet-10%, Meta Pseudo Labels outper-\\nforms SimCLR [8, 9] by 2.19% top-1 accuracy.\\nWhile better results on \\n}<->{\\n0%, Meta Pseudo Labels outper-\\nforms SimCLR [8, 9] by 2.19% top-1 accuracy.\\nWhile better results on these datasets exist, to our knowl-\\nedge, such results are all obtained with larger models,\\nstronger regularization techniques, or extra distillation pro-\\ncedures. For example, the best reported accuracy on CIFAR-\\n10-4K is 97.3% [76] but this accuracy is achieved with\\na PyramidNet which has 17x more parameters than our\\nWideResNet-28-2 and uses the complex ShakeDrop reg-\\nularization [80]. On the ot\\n}<->{\\n parameters than our\\nWideResNet-28-2 and uses the complex ShakeDrop reg-\\nularization [80]. On the other hand, the best reported top-1\\naccuracy for ImageNet-10% is 80.9%, achieved by Sim-\\nCLRv2 [9] using a self-distillation training phase and a\\nResNet-152×3 which has 32x more parameters than our\\nResNet-50. Such enhancements on architectures, regular-\\nization, and distillation can also be applied to Meta Pseudo\\nLabels to further improve our results.\\n3.3. ResNet-50 Experiment\\nThe previous experimen\\n}<->{\\n Meta Pseudo\\nLabels to further improve our results.\\n3.3. ResNet-50 Experiment\\nThe previous experiments show that Meta Pseudo La-\\nbels outperforms other semi-supervised learning methods\\non CIFAR-10-4K, SVHN-1K, and ImageNet-10%. In this\\nexperiment, we benchmark Meta Pseudo Labels on the en-\\ntire ImageNet dataset plus unlabeled images from the JFT\\ndataset. The purpose of this experiment is to verify if Meta\\nPseudo Labels works well on the widely used ResNet-50\\narchitecture [24] before we conduct m\\n}<->{\\n if Meta\\nPseudo Labels works well on the widely used ResNet-50\\narchitecture [24] before we conduct more large scale experi-\\nments on EfﬁcientNet (Section 4).\\nDatasets.\\nAs mentioned, we experiment with all labeled\\nexamples from the ImageNet dataset. We reserve 25,000\\nexamples from the ImageNet dataset for hyper-parameter\\ntuning and model selection. Our test set is the ILSVRC 2012\\nvalidation set. Additionally, we take 12.8 million unlabeled\\nimages from the JFT dataset. To obtain these 12.8 million\\n}<->{\\nditionally, we take 12.8 million unlabeled\\nimages from the JFT dataset. To obtain these 12.8 million\\n5\\n\\n--------------------------------------------------\\nunlabeled images, we ﬁrst train a ResNet-50 on the entire\\nImageNet training set and then use the resulting ResNet-50\\nto assign class probabilities to images in the JFT dataset. We\\nthen select 12,800 images of highest probability for each\\nof the 1,000 classes of ImageNet. This selection results in\\n12.8 million images. We also make sure that non\\n}<->{\\n1,000 classes of ImageNet. This selection results in\\n12.8 million images. We also make sure that none of the\\n12.8 million images that we use overlaps with the ILSVRC\\n2012 validation set of ImageNet. This procedure of ﬁltering\\nextra unlabeled data has been used by UDA [76] and Noisy\\nStudent [77].\\nImplementation details.\\nWe implement Meta Pseudo La-\\nbels the same as in Section 3.2 but we use a larger batch size\\nand more training steps, as the datasets are much larger for\\nthis experiment. Speciﬁcal\\n}<->{\\nr batch size\\nand more training steps, as the datasets are much larger for\\nthis experiment. Speciﬁcally, for both the student and the\\nteacher, we use the batch size of 4,096 for labeled images\\nand the batch size of 32,768 for unlabeled images. We train\\nfor 500,000 steps which equals to about 160 epochs on the\\nunlabeled dataset. After training the Meta Pseudo Labels\\nphase on ImageNet+JFT, we ﬁnetune the resulting student on\\nImageNet for 10,000 SGD steps, using a ﬁxed learning rate\\nof 10−4. Using 5\\n}<->{\\n the resulting student on\\nImageNet for 10,000 SGD steps, using a ﬁxed learning rate\\nof 10−4. Using 512 TPUv2 cores, our training procedure\\ntakes about 2 days.\\nBaselines.\\nWe compare Meta Pseudo Labels against two\\ngroups of baselines. The ﬁrst group contains supervised\\nlearning methods with data augmentation or regularization\\nmethods such as AutoAugment [12], DropBlock[18], and\\nCutMix [83]. These baselines represent state-of-the-art su-\\npervised learning methods on ResNet-50. The second group\\nof b\\n}<->{\\nselines represent state-of-the-art su-\\npervised learning methods on ResNet-50. The second group\\nof baselines consists of three recent semi-supervised learn-\\ning methods that leverage the labeled training images from\\nImageNet and unlabeled images elsewhere. Speciﬁcally,\\nbillion-scale semi-supervised learning [79] uses unlabeled\\ndata from the YFCC100M dataset [65], while UDA [76]\\nand Noisy Student [77] both use JFT as unlabeled data like\\nMeta Pseudo Labels. Similar to Section 3.2, we only com-\\npar\\n}<->{\\n77] both use JFT as unlabeled data like\\nMeta Pseudo Labels. Similar to Section 3.2, we only com-\\npare Meta Pseudo Labels to results that are obtained with\\nResNet-50 and without distillation.\\nResults.\\nTable 3 presents the results. As can be seen from\\nthe table, Meta Pseudo Labels boosts the top-1 accuracy of\\nResNet-50 from 76.9% to 83.2%, which is a large margin\\nof improvement for ImageNet, outperforming both UDA\\nand Noisy Student. Meta Pseudo Labels also outperforms\\nBillion-scale SSL [68, 79] in\\n}<->{\\norming both UDA\\nand Noisy Student. Meta Pseudo Labels also outperforms\\nBillion-scale SSL [68, 79] in top-1 accuracy. This is par-\\nticularly impressive since Billion-scale SSL pre-trains their\\nResNet-50 on weakly-supervised images from Instagram.\\nMethod\\nUnlabeled\\nAccuracy\\nImages\\n(top-1/top-5)\\nSupervised [24]\\nNone\\n76.9/93.3\\nAutoAugment [12]\\nNone\\n77.6/93.8\\nDropBlock [18]\\nNone\\n78.4/94.2\\nFixRes [68]\\nNone\\n79.1/94.6\\nFixRes+CutMix [83]\\nNone\\n79.8/94.9\\nNoisyStudent [77]\\nJFT\\n78.9/94.3\\nUDA [76]\\nJFT\\n79.0/94.\\n}<->{\\nne\\n79.1/94.6\\nFixRes+CutMix [83]\\nNone\\n79.8/94.9\\nNoisyStudent [77]\\nJFT\\n78.9/94.3\\nUDA [76]\\nJFT\\n79.0/94.5\\nBillion-scale SSL [68, 79]\\nYFCC\\n82.5/96.6\\nMeta Pseudo Labels\\nJFT\\n83.2/96.5\\nTable 3: Top-1 and Top-5 accuracy of Meta Pseudo Labels and\\nother representative supervised and semi-supervised methods on\\nImageNet with ResNet-50.\\n4. Large Scale Experiment: Pushing the Limits\\nof ImageNet Accuracy\\nIn this section, we scale up Meta Pseudo Labels to train\\non a large model and a large dataset to push the li\\n}<->{\\nsection, we scale up Meta Pseudo Labels to train\\non a large model and a large dataset to push the limits of\\nImageNet accuracy. Speciﬁcally, we use the EfﬁcientNet-L2\\narchitecture because it has a higher capacity than ResNets.\\nEfﬁcientNet-L2 was also used by Noisy Student [77] to\\nachieve the top-1 accuracy of 88.4% on ImageNet.\\nDatasets.\\nFor this experiment, we use the entire ImageNet\\ntraining set as labeled data, and use the JFT dataset as un-\\nlabeled data. The JFT dataset has 300 million images\\n}<->{\\nas labeled data, and use the JFT dataset as un-\\nlabeled data. The JFT dataset has 300 million images, and\\nthen is ﬁltered down to 130 million images by Noisy Student\\nusing conﬁdence thresholds and up-sampling [77]. We use\\nthe same 130 million images as Noisy Student.\\nModel architecture.\\nWe experiment with EfﬁcientNet-\\nL2 since it has the state-of-the-art performance on Ima-\\ngeNet [77] without extra labeled data. We use the same\\nhyper-parameters with Noisy Student, except that we use the\\ntraining\\n}<->{\\na labeled data. We use the same\\nhyper-parameters with Noisy Student, except that we use the\\ntraining image resolution of 512x512 instead of 475x475.\\nWe increase the input image resolution to be compatible with\\nour model parallelism implementation which we discuss in\\nthe next paragraph. In addition to EfﬁcientNet-L2, we also\\nexperiment with a smaller model, which has the same depth\\nwith EfﬁcientNet-B6 [63] but has the width factor increased\\nfrom 2.1 to 5.0. This model, termed EfﬁcientNet-B6-Wide,\\n}<->{\\n-B6 [63] but has the width factor increased\\nfrom 2.1 to 5.0. This model, termed EfﬁcientNet-B6-Wide,\\nhas 390 million parameters. We adopt all hyper-parameters\\nof EfﬁcientNet-L2 for EfﬁcientNet-B6-Wide. We ﬁnd that\\nEfﬁcientNet-B6-Wide has almost the same performance with\\nEfﬁcientNet-L2, but is faster to compile and train.\\nModel parallelism.\\nDue to the memory footprint of our\\nnetworks, keeping two such networks in memory for the\\nteacher and the student would vastly exceed the available\\n6\\n\\n--------\\n}<->{\\nuch networks in memory for the\\nteacher and the student would vastly exceed the available\\n6\\n\\n--------------------------------------------------\\nMethod\\n# Params\\nExtra Data\\nImageNet\\nImageNet-ReaL [6]\\nTop-1\\nTop-5\\nPrecision@1\\nResNet-50 [24]\\n26M\\n−\\n76.0\\n93.0\\n82.94\\nResNet-152 [24]\\n60M\\n−\\n77.8\\n93.8\\n84.79\\nDenseNet-264 [28]\\n34M\\n−\\n77.9\\n93.9\\n−\\nInception-v3 [62]\\n24M\\n−\\n78.8\\n94.4\\n83.58\\nXception [11]\\n23M\\n−\\n79.0\\n94.5\\n−\\nInception-v4 [61]\\n48M\\n−\\n80.0\\n95.0\\n−\\nInception-resnet-v2 [61]\\n56M\\n−\\n80.1\\n95.1\\n−\\nResNeXt-101 [78]\\n\\n}<->{\\n5\\n−\\nInception-v4 [61]\\n48M\\n−\\n80.0\\n95.0\\n−\\nInception-resnet-v2 [61]\\n56M\\n−\\n80.1\\n95.1\\n−\\nResNeXt-101 [78]\\n84M\\n−\\n80.9\\n95.6\\n85.18\\nPolyNet [87]\\n92M\\n−\\n81.3\\n95.8\\n−\\nSENet [27]\\n146M\\n−\\n82.7\\n96.2\\n−\\nNASNet-A [90]\\n89M\\n−\\n82.7\\n96.2\\n82.56\\nAmoebaNet-A [52]\\n87M\\n−\\n82.8\\n96.1\\n−\\nPNASNet [39]\\n86M\\n−\\n82.9\\n96.2\\n−\\nAmoebaNet-C + AutoAugment [12]\\n155M\\n−\\n83.5\\n96.5\\n−\\nGPipe [29]\\n557M\\n−\\n84.3\\n97.0\\n−\\nEfﬁcientNet-B7 [63]\\n66M\\n−\\n85.0\\n97.2\\n−\\nEfﬁcientNet-B7 + FixRes [70]\\n66M\\n−\\n85.3\\n97.4\\n−\\nEfﬁcientNet-L2 [63]\\n480M\\n−\\n85.5\\n97.5\\n−\\nResNet-50 B\\n}<->{\\n\\n−\\nEfﬁcientNet-B7 + FixRes [70]\\n66M\\n−\\n85.3\\n97.4\\n−\\nEfﬁcientNet-L2 [63]\\n480M\\n−\\n85.5\\n97.5\\n−\\nResNet-50 Billion-scale SSL [79]\\n26M\\n3.5B labeled Instagram\\n81.2\\n96.0\\n−\\nResNeXt-101 Billion-scale SSL [79]\\n193M\\n3.5B labeled Instagram\\n84.8\\n−\\n−\\nResNeXt-101 WSL [42]\\n829M\\n3.5B labeled Instagram\\n85.4\\n97.6\\n88.19\\nFixRes ResNeXt-101 WSL [69]\\n829M\\n3.5B labeled Instagram\\n86.4\\n98.0\\n89.73\\nBig Transfer (BiT-L) [33]\\n928M\\n300M labeled JFT\\n87.5\\n98.5\\n90.54\\nNoisy Student (EfﬁcientNet-L2) [77]\\n480M\\n300M unlabeled JFT\\n88.4\\n9\\n}<->{\\n\\n300M labeled JFT\\n87.5\\n98.5\\n90.54\\nNoisy Student (EfﬁcientNet-L2) [77]\\n480M\\n300M unlabeled JFT\\n88.4\\n98.7\\n90.55\\nNoisy Student + FixRes [70]\\n480M\\n300M unlabeled JFT\\n88.5\\n98.7\\n−\\nVision Transformer (ViT-H) [14]\\n632M\\n300M labeled JFT\\n88.55\\n−\\n90.72\\nEfﬁcientNet-L2-NoisyStudent + SAM [16]\\n480M\\n300M unlabeled JFT\\n88.6\\n98.6\\n−\\nMeta Pseudo Labels (EfﬁcientNet-B6-Wide)\\n390M\\n300M unlabeled JFT\\n90.0\\n98.7\\n91.12\\nMeta Pseudo Labels (EfﬁcientNet-L2)\\n480M\\n300M unlabeled JFT\\n90.2\\n98.8\\n91.02\\nTable 4: Top-1 and Top-5 a\\n}<->{\\nta Pseudo Labels (EfﬁcientNet-L2)\\n480M\\n300M unlabeled JFT\\n90.2\\n98.8\\n91.02\\nTable 4: Top-1 and Top-5 accuracy of Meta Pseudo Labels and previous state-of-the-art methods on ImageNet. With EfﬁcientNet-L2 and\\nEfﬁcientNet-B6-Wide, Meta Pseudo Labels achieves an improvement of 1.6% on top of the state-of-the-art [16], despite the fact that the\\nlatter uses 300 million labeled training examples from JFT.\\nmemory of our accelerators. We thus design a hybrid model-\\ndata parallelism framework to run Meta Ps\\n}<->{\\nmemory of our accelerators. We thus design a hybrid model-\\ndata parallelism framework to run Meta Pseudo Labels.\\nSpeciﬁcally, our training process runs on a cluster of 2,048\\nTPUv3 cores. We divide these cores into 128 identical repli-\\ncas to run with standard data parallelism with synchronized\\ngradients. Within each replica, which runs on 2,048/128=16\\ncores, we implement two types of model parallelism. First,\\neach input image of resolution 512x512 is split along the\\nwidth dimension into 16 patch\\n}<->{\\nlism. First,\\neach input image of resolution 512x512 is split along the\\nwidth dimension into 16 patches of equal size 512x32 and\\nis distributed to 16 cores to process. Note that we choose\\nthe input resolution of 512x512 because 512 is close to the\\nresolution 475x475 used by Noisy Student and 512 keeps the\\ndimensions of the network’s intermediate outputs divisible\\nby 16. Second, each weight tensor is also split equally into\\n16 parts that are assigned to the 16 cores. We implement our\\nhybrid data-m\\n}<->{\\ns also split equally into\\n16 parts that are assigned to the 16 cores. We implement our\\nhybrid data-model parallelism in the XLA-Sharding frame-\\nwork [37]. With this parallelism, we can ﬁt a batch size of\\n2,048 labeled images and 16,384 unlabeled images into each\\ntraining step. We train the model for 1 million steps in total,\\nwhich takes about 11 days for EfﬁcientNet-L2 and 10 days\\nfor EfﬁcientNet-B6-Wide. After ﬁnishing the Meta Pseudo\\nLabels training phase, we ﬁnetune the models on our labeled\\n\\n}<->{\\nB6-Wide. After ﬁnishing the Meta Pseudo\\nLabels training phase, we ﬁnetune the models on our labeled\\ndataset for 20,000 steps. Details of the ﬁnetuning procedures\\nare in Appendix C.4.\\nResults.\\nOur results are presented in Table 4. From the\\ntable, it can be seen that Meta Pseudo Labels achieves 90.2%\\ntop-1 accuracy on ImageNet, which is a new state-of-the-art\\non this dataset. This result is 1.8% better than the same\\nEfﬁcientNet-L2 architecture trained with Noisy Student [77]\\nand FixRes [69, 70]. M\\n}<->{\\nter than the same\\nEfﬁcientNet-L2 architecture trained with Noisy Student [77]\\nand FixRes [69, 70]. Meta Pseudo Labels also outperforms\\nthe recent results by BiT-L [33] and the previous state-of-the-\\nart by Vision Transformer [14]. The important contrast here\\nis that both Bit-L and Vision Transformer pre-train on 300\\nmillion labeled images from JFT, while our method only uses\\nunlabeled images from this dataset. At this level of accuracy,\\n7\\n\\n--------------------------------------------------\\nour g\\n}<->{\\nthis dataset. At this level of accuracy,\\n7\\n\\n--------------------------------------------------\\nour gain of 1.6% over [16] is a very signiﬁcant margin of\\nimprovement compared to recent gains. For instance, the\\ngain of Vision Transformer [14] over Noisy Student + FixRes\\nwas only 0.05%, and the gain of FixRes over Noisy Student\\nwas only 0.1%.\\nFinally, to verify that our model does not simply overﬁt\\nto the ImageNet ILSVRC 2012 validation set, we test it on\\nthe ImageNet-ReaL test set [6]. On this tes\\n}<->{\\no the ImageNet ILSVRC 2012 validation set, we test it on\\nthe ImageNet-ReaL test set [6]. On this test set, our model\\nalso works well and achieves 91.02% Precision@1 which is\\n0.4% better than Vision Transformer [14]. This gap is also\\nbigger than the gap between Vision Transformer and Noisy\\nStudent which is only 0.17%.\\nA lite version of Meta Pseudo Labels.\\nGiven the expen-\\nsive training cost of Meta Pseudo Labels, we design a lite ver-\\nsion of Meta Pseudo Labels, termed Reduced Meta Pseudo\\nLabels.\\n}<->{\\n Pseudo Labels, we design a lite ver-\\nsion of Meta Pseudo Labels, termed Reduced Meta Pseudo\\nLabels. We describe this lite version in Appendix E, where\\nwe achieve 86.9% top-1 accuracy on the ImageNet ILSRVC\\n2012 validation set with EfﬁcentNet-B7. To avoid using pro-\\nprietary data like JFT, we use the ImageNet training set as\\nlabeled data and the YFCC100M dataset [65] as unlabeled\\ndata. Reduced Meta Pseudo Labels allows us to implement\\nthe feedback mechanism of Meta Pseudo Labels while avoid-\\ning\\n}<->{\\na Pseudo Labels allows us to implement\\nthe feedback mechanism of Meta Pseudo Labels while avoid-\\ning the need to keep two networks in memory.\\n5. Related Works\\nPseudo Labels.\\nThe method of Pseudo Labels, also known\\nas self-training, is a simple Semi-Supervised Learning (SSL)\\napproach that has been successfully applied to improve the\\nstate-of-the-art of many tasks, such as: image classiﬁca-\\ntion [79, 77], object detection, semantic segmentation [89],\\nmachine translation [22], and speech recognitio\\n}<->{\\n, 77], object detection, semantic segmentation [89],\\nmachine translation [22], and speech recognition [31, 49].\\nVanilla Pseudo Labels methods keep a pre-trained teacher\\nﬁxed during the student’s learning, leading to a conﬁrma-\\ntion bias [2] when the pseudo labels are inaccurate. Unlike\\nvanilla Pseudo Labels, Meta Pseudo Labels continues to\\nadapt the teacher to improve the student’s performance on a\\nlabeled dataset. This extra adaptation allows the teacher to\\ngenerate better pseudo labels to teac\\n}<->{\\na\\nlabeled dataset. This extra adaptation allows the teacher to\\ngenerate better pseudo labels to teach the student as shown\\nin our experiments.\\nOther SSL approaches.\\nOther typical SSL methods of-\\nten train a single model by optimizing an objective func-\\ntion that combines a supervised loss on labeled data and\\nan unsupervised loss on unlabeled data. The supervised\\nloss is often the cross-entropy computed on the labeled data.\\nMeanwhile, the unsupervised loss is typically either a self-\\nsupervised l\\n}<->{\\nputed on the labeled data.\\nMeanwhile, the unsupervised loss is typically either a self-\\nsupervised loss or a label propagation loss. Self-supervised\\nlosses typically encourage the model to develop a com-\\nmon sense about images, such as in-painting [50], solv-\\ning jigsaw puzzles [47], predicting the rotation angle [19],\\ncontrastive prediction [25, 10, 8, 9, 38], or bootstraping\\nthe latent space [21]. On the other hand, label propaga-\\ntion losses typically enforce that the model is invariant\\nagain\\n}<->{\\n]. On the other hand, label propaga-\\ntion losses typically enforce that the model is invariant\\nagainst certain transformations of the data such as data aug-\\nmentations, adversarial attacks, or proximity in the latent\\nspace [35, 64, 44, 5, 76, 30, 71, 58, 32, 51, 20]. Meta Pseudo\\nLabels is distinct from the aforementioned SSL methods in\\ntwo notable ways. First, the student in Meta Pseudo Labels\\nnever learns directly from labeled data, which helps to avoid\\noverﬁtting, especially when labeled data \\n}<->{\\ner learns directly from labeled data, which helps to avoid\\noverﬁtting, especially when labeled data is limited. Second,\\nthe signal that the teacher in Meta Pseudo Labels receives\\nfrom the student’s performance on labeled data is a novel\\nway of utilizing labeled data.\\nKnowledge Distillation and Label Smoothing.\\nThe\\nteacher in Meta Pseudo Labels uses its softmax predictions\\non unlabeled data to teach the student. These softmax pre-\\ndictions are generally called the soft labels, which have\\nbeen wid\\n}<->{\\nh the student. These softmax pre-\\ndictions are generally called the soft labels, which have\\nbeen widely utilized in the literature on knowledge distilla-\\ntion [26, 17, 86]. Outside the line of work on distillation,\\nmanually designed soft labels, such as label smoothing [45]\\nand temperature sharpening or dampening [76, 77], have\\nalso been shown to improve models’ generalization. Both\\nof these methods can be seen as adjusting the labels of the\\ntraining examples to improve optimization and generali\\n}<->{\\nds can be seen as adjusting the labels of the\\ntraining examples to improve optimization and generaliza-\\ntion. Similar to other SSL methods, these adjustments do not\\nreceive any feedback from the student’s performance as pro-\\nposed in this paper. An experiment comparing Meta Pseudo\\nLabels to Label Smoothing is presented in Appendix D.2.\\nBi-level optimization algorithms.\\nWe use Meta in our\\nmethod name because our technique of deriving the teacher’s\\nupdate rule from the student’s feedback is based \\n}<->{\\nme because our technique of deriving the teacher’s\\nupdate rule from the student’s feedback is based on a bi-level\\noptimization problem which appears frequently in the litera-\\nture of meta-learning. Similar bi-level optimization problems\\nhave been proposed to optimize a model’s learning process,\\nsuch as learning the learning rate schedule [3], designing\\narchitectures [40], correcting wrong training labels [88], gen-\\nerating training examples [59], and re-weighting training\\ndata [73, 74, 54, 53]. \\n}<->{\\n labels [88], gen-\\nerating training examples [59], and re-weighting training\\ndata [73, 74, 54, 53]. Meta Pseudo Labels uses the same\\nbi-level optimization technique in this line of work to derive\\nthe teacher’s gradient from the student’s feedback. The dif-\\nference between Meta Pseudo Labels and these methods is\\nthat Meta Pseudo Labels applies the bi-level optimization\\ntechnique to improve the pseudo labels generated by the\\nteacher model.\\n6. Conclusion\\nIn this paper, we proposed the Meta Pseudo L\\n}<->{\\ndo labels generated by the\\nteacher model.\\n6. Conclusion\\nIn this paper, we proposed the Meta Pseudo Labels\\nmethod for semi-supervised learning. Key to Meta Pseudo\\nLabels is the idea that the teacher learns from the student’s\\nfeedback to generate pseudo labels in a way that best helps\\nstudent’s learning. The learning process in Meta Pseudo\\n8\\n\\n--------------------------------------------------\\nLabels consists of two main updates: updating the student\\nbased on the pseudo labeled data produced by the\\n}<->{\\n consists of two main updates: updating the student\\nbased on the pseudo labeled data produced by the teacher\\nand updating the teacher based on the student’s performance.\\nExperiments on standard low-resource benchmarks such as\\nCIFAR-10-4K, SVHN-1K, and ImageNet-10% show that\\nMeta Pseudo Labels is better than many existing semi-\\nsupervised learning methods.\\nMeta Pseudo Labels also\\nscales well to large problems, attaining 90.2% top-1 accu-\\nracy on ImageNet, which is 1.6% better than the previous\\nst\\n}<->{\\nge problems, attaining 90.2% top-1 accu-\\nracy on ImageNet, which is 1.6% better than the previous\\nstate-of-the-art [16]. The consistent gains conﬁrm the beneﬁt\\nof the student’s feedback to the teacher.\\nAcknowledgements\\nThe authors wish to thank Rohan Anil, Frank Chen, Wang\\nTao for their help with many technical issues in running\\nour experiments. We also thank David Berthelot, Nicholas\\nCarlini, Sylvain Gelly, Geoff Hinton, Mohammad Norouzi,\\nand Colin Raffel for their comments on earlier drafts of\\n}<->{\\nvain Gelly, Geoff Hinton, Mohammad Norouzi,\\nand Colin Raffel for their comments on earlier drafts of the\\npaper, and others in the Google Brain Team for their support\\nthroughout this very long project.\\nJaime Carbonell has also advised us on removing the data\\nloading bottleneck for the ResNets model ImageNet. His\\nadvice helped a lot when we did not have enough spare TPUs\\nfor our ResNet jobs. He will be deeply remembered.\\nReferences\\n[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,\\nAndy Da\\n}<->{\\n be deeply remembered.\\nReferences\\n[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,\\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-\\nmawat, Geoffrey Irving, Michael Isard, Manjunath Kud-\\nlur, Josh Levenberg, Rajat Monga, Sherry Moore, Benoit\\nG, Derek . Murrayand Steiner, Paul Tucker, Vijay Vasude-\\nvan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang\\nZheng. Tensorﬂow: A system for large-scale machine learn-\\ning. In USENIX Symposium on Operating Systems Design\\nand Implementation, 2\\n}<->{\\narge-scale machine learn-\\ning. In USENIX Symposium on Operating Systems Design\\nand Implementation, 2016. 17\\n[2] Eric Arazo, Diego Ortego, Paul Albert, Noel E. O’Connor,\\nand Kevin McGuinness. Pseudo-labeling and conﬁrmation\\nbias in deep semi-supervised learning. Arxiv, 1908.02983,\\n2019. 1, 4, 8, 19\\n[3] Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio,\\nMark Schmidt, and Frank Wood. Online learning rate adapta-\\ntion with hypergradient descent. In International Conference\\non Learning Repres\\n}<->{\\nearning rate adapta-\\ntion with hypergradient descent. In International Conference\\non Learning Representations, 2018. 8, 16\\n[4] David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Ku-\\nrakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remix-\\nmatch: Semi-supervised learning with distribution alignment\\nand augmentation anchoring. In International Conference on\\nLearning Representations, 2020. 4, 5\\n[5] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas\\nPapernot, Avital Oliver, and Colin Raff\\n}<->{\\n] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas\\nPapernot, Avital Oliver, and Colin Raffel. MixMatch: A\\nholistic approach to semi-supervised learning. In Advances\\nin Neural Information Processing Systems, 2019. 4, 5, 8\\n[6] Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xi-\\naohua Zhai, and Aäron van den Oord. Are we done with\\nImageNet? arXiv preprint arXiv:2006.07159, 2020. 1, 7, 8\\n[7] Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien.\\nSemi-Supervised Learning. The MIT Pre\\n}<->{\\n8\\n[7] Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien.\\nSemi-Supervised Learning. The MIT Press, 2010. 3\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In International Conference on\\nMachine Learning, 2020. 4, 5, 8\\n[9] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\\nNorouzi, and Geoffrey Hinton. Big self-supervised models\\nare strong semi-supervised learners. In Advances in Neural\\nInformati\\n}<->{\\nton. Big self-supervised models\\nare strong semi-supervised learners. In Advances in Neural\\nInformation Processing Systems, 2020. 4, 5, 8\\n[10] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Im-\\nproved baselines with momentum contrastive learning. Arxiv,\\n2003.04297, 2020. 4, 5, 8\\n[11] Francois Chollet. Xception: Deep learning with depthwise\\nseparable convolutions. In IEEE Conference on Computer\\nVision and Pattern Recognition, 2017. 7\\n[12] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay V\\n}<->{\\nter\\nVision and Pattern Recognition, 2017. 7\\n[12] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\\nvan, and Quoc V. Le. AutoAugment: Learning augmentation\\npolicies from data. In IEEE Conference on Computer Vision\\nand Pattern Recognition, 2019. 6, 7, 16, 17\\n[13] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V.\\nLe. Randaugment: Practical data augmentation with no sepa-\\nrate search. In Advances in Neural Information Processing\\nSystems, 2020. 4, 16\\n[14] Alexey Dosovitskiy, Lucas Beye\\n}<->{\\nn Advances in Neural Information Processing\\nSystems, 2020. 4, 16\\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\nworth 16x16 words: Transformers for image recognition at\\nscale. Arxiv, 2010.11929, 2020. 1, 7, 8\\n[15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-\\nagnostic meta-learning for fast adaptation of deep\\n}<->{\\nea Finn, Pieter Abbeel, and Sergey Levine. Model-\\nagnostic meta-learning for fast adaptation of deep networks.\\nIn International Conference on Machine Learning, 2017. 2\\n[16] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam\\nNeyshabur. Sharpness-aware minimization for efﬁciently\\nimproving generalization. Arxiv, 2010.01412, 2020. 1, 7, 8, 9\\n[17] Tommaso Furlanello, Zachary C. Lipton, Michael Tschannen,\\nLaurent Itti, and Anima Anandkumar. Born again neural\\nnetworks. In International Conference\\n}<->{\\nchannen,\\nLaurent Itti, and Anima Anandkumar. Born again neural\\nnetworks. In International Conference on Machine Learning,\\n2018. 8\\n[18] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V. Le. Dropblock:\\nA regularization method for convolutional networks. In Ad-\\nvances in Neural Information Processing Systems, 2018. 6\\n[19] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsu-\\npervised representation learning by predicting image rotations.\\nIn IEEE Conference on Computer Vision and Pattern Recog-\\nnition, 2\\n}<->{\\nng by predicting image rotations.\\nIn IEEE Conference on Computer Vision and Pattern Recog-\\nnition, 2018. 8\\n[20] Yves Grandvalet and Yoshua Bengio. Semi-supervised learn-\\ning by entropy minimization. In International Conference on\\nComputer Vision, 2005. 8\\n[21] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin\\nTallec, Pierre H. Richemond, Elena Buchatskaya, Carl Do-\\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\\nmad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi\\nMunos, and\\n}<->{\\nires, Zhaohan Daniel Guo, Moham-\\nmad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi\\nMunos, and Michal Valko. Bootstrap your own latent: A new\\napproach to self-supervised learning. In Advances in Neural\\nInformation Processing Systems, 2020. 5, 8\\n9\\n\\n--------------------------------------------------\\n[22] Junxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio Ran-\\nzato. Revisiting self-training for neural sequence generation.\\nIn International Conference on Learning Representations,\\n2020. 8\\n[23]\\n}<->{\\nor neural sequence generation.\\nIn International Conference on Learning Representations,\\n2020. 8\\n[23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining He, and Ross\\nGirshick. Momentum contrast for unsupervised visual repre-\\nsentation learning. In IEEE Conference on Computer Vision\\nand Pattern Recognition, 2020. 4\\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In IEEE Con-\\nference on Computer Vision and Pattern Recognition, 2016.\\n4, 5, 6, 7\\n[25] Ol\\n}<->{\\ncognition. In IEEE Con-\\nference on Computer Vision and Pattern Recognition, 2016.\\n4, 5, 6, 7\\n[25] Olivier J. Henaff, Aravind Srinivas, Jeffrey De Fauw, Ali\\nRazavi, Carl Doersch, S. M. Ali Eslami, and Aaron van den\\nOord. Data-efﬁcient image recognition with contrastive pre-\\ndictive coding. Arxiv, 2003.04297, 2020. 4, 8\\n[26] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the\\nknowledge in a neural network. Arxiv, 1503.02531, 2015. 1,\\n8\\n[27] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-exci\\n}<->{\\na neural network. Arxiv, 1503.02531, 2015. 1,\\n8\\n[27] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation\\nnetworks. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, 2018. 7\\n[28] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-\\nian Q. Weinberger. Densely connected convolutional net-\\nworks. In IEEE Conference on Computer Vision and Pattern\\nRecognition, 2016. 7\\n[29] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong\\nLee, Jiquan Ngiam, Quoc V Le, and Zhife\\n}<->{\\n7\\n[29] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong\\nLee, Jiquan Ngiam, Quoc V Le, and Zhifeng Chen. GPipe:\\nEfﬁcient training of giant neural networks using pipeline\\nparallelism. In Advances in Neural Information Processing\\nSystems, 2019. 7\\n[30] Jacob Jackson and John Schulman. Semi-supervised learning\\nby label gradient alignment. Arxiv 1902.02336, 2019. 5, 8\\n[31] Jacob Kahn, Ann Lee, and Awni Hannun. Self-training for\\nend-to-end speech recognition. In IEEE International Con-\\nference on \\n}<->{\\nAwni Hannun. Self-training for\\nend-to-end speech recognition. In IEEE International Con-\\nference on Acoustics, Speech, and Signal Processing, 2020.\\n8\\n[32] Zhanghan Ke, Daoye Wang, Qiong Yan, Jimmy Ren, and\\nRynson W. H. Lau. Dual student: Breaking the limits of\\nthe teacher in semi-supervised learning. In International\\nConference in Computer Vision, 2019. 8\\n[33] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\\nBig transfer (bit): Gene\\n}<->{\\naohua Zhai, Joan\\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\\nBig transfer (bit): General visual representation learning. In\\nProceedings of the European Conference on Computer Vision\\n(ECCV), 2020. 7\\n[34] Alex Krizhevsky. Learning multiple layers of features from\\ntiny images. Technical report, 2009. 4\\n[35] Samuli Laine and Timo Aila. Temporal ensembling for semi-\\nsupervised learning. In International Conference on Learning\\nRepresentations, 2017. 5, 8\\n[36] Dong-Hyun Lee. Pseudo-Label\\n}<->{\\nIn International Conference on Learning\\nRepresentations, 2017. 5, 8\\n[36] Dong-Hyun Lee. Pseudo-Label: The simple and efﬁcient\\nsemi-supervised learning method for deep neural networks.\\nIn International Conference on Machine Learning Workshop,\\n2013. 1, 20\\n[37] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao\\nChen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam\\nShazeer, and Zhifeng Chen. Gshard: Scaling giant models\\nwith conditional computation and automatic sharding. Arxiv,\\n2006.16668, 2020. 7\\n}<->{\\nScaling giant models\\nwith conditional computation and automatic sharding. Arxiv,\\n2006.16668, 2020. 7\\n[38] Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and\\nSteven CH Hoi. Prototypical contrastive learning of unsuper-\\nvised representations. Arxiv, 2005.04966, 2020. 5, 8\\n[39] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens,\\nWei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang,\\nand Kevin Murphy. Progressive neural architecture search. In\\nProceedings of the European Conference o\\n}<->{\\nnd Kevin Murphy. Progressive neural architecture search. In\\nProceedings of the European Conference on Computer Vision\\n(ECCV), 2018. 7\\n[40] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Dif-\\nferentiable architecture search. In International Conference\\non Learning Representations, 2019. 2, 8\\n[41] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\\ndescent with warm restarts. In International Conference on\\nLearning Representations, 2017. 17\\n[42] Dhruv Mahajan, Ross Girshick, Vignesh \\n}<->{\\national Conference on\\nLearning Representations, 2017. 17\\n[42] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaim-\\ning He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and\\nLaurens van der Maaten. Exploring the limits of weakly super-\\nvised pretraining. Proceedings of the European Conference\\non Computer Vision (ECCV), 2018. 7\\n[43] Ishan Misra and Laurens van der Maaten. Self-supervised\\nlearning of pretext-invariant representations. In IEEE Con-\\nference on Computer Vision and Pattern Recognition\\n}<->{\\nf pretext-invariant representations. In IEEE Con-\\nference on Computer Vision and Pattern Recognition, 2020.\\n5\\n[44] Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori\\nKoyama. Virtual adversarial training: a regularization method\\nfor supervised and semi-supervised learning. In IEEE Trans-\\nactions on Pattern Analysis and Machine Intelligence, 2018.\\n5, 8\\n[45] Rafael Müller, Simon Kornblith, and Geoffrey Hinton. When\\ndoes label smoothing help? In Advances in Neural Informa-\\ntion Processing Syst\\n}<->{\\nGeoffrey Hinton. When\\ndoes label smoothing help? In Advances in Neural Informa-\\ntion Processing Systems, 2019. 8\\n[46] Yuval Netzer, Tao Wang, Alessandro Coates, Adamand Bis-\\nsacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural\\nimages with unsupervised feature learning. In Advances in\\nNeural Information Processing Systems Workshop on Deep\\nLearning and Unsupervised Feature Learning, 2011. 4\\n[47] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\\nvisual representations by solving jigsaw \\n}<->{\\n] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\\nvisual representations by solving jigsaw puzzles. In IEEE\\nConference on Computer Vision and Pattern Recognition,\\n2018. 8\\n[48] Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk,\\nand Ian J. Goodfellow. Realistic evaluation of deep semi-\\nsupervised learning algorithms. In Advances in Neural Infor-\\nmation Processing Systems, 2018. 4, 5, 17\\n[49] Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng\\nChiu, Bo Li, Yonghui Wu, and Quoc \\n}<->{\\n5, 17\\n[49] Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng\\nChiu, Bo Li, Yonghui Wu, and Quoc V. Le. Improved noisy\\nstudent training for automatic speech recognition. In Inter-\\nspeech, 2020. 8\\n[50] Deepak Pathak, Philipp Krahenbühl, Jeff Donahue, Trevor\\nDarrell, and Alexei A. Efrös. Context encoders: Feature\\nlearning by inpainting. In IEEE Conference on Computer\\nVision and Pattern Recognition, 2016. 8\\n[51] Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia\\nGkioxari, and Kaiming He. Da\\n}<->{\\nn, 2016. 8\\n[51] Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia\\nGkioxari, and Kaiming He. Data distillation: Towards omni-\\n10\\n\\n--------------------------------------------------\\nsupervised learning. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, 2018. 1, 8\\n[52] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V\\nLe. Regularized evolution for image classiﬁer architecture\\nsearch. In Proceedings of the AAAI conference on artiﬁcial\\nintelligence, volume 3\\n}<->{\\nsiﬁer architecture\\nsearch. In Proceedings of the AAAI conference on artiﬁcial\\nintelligence, volume 33, 2019. 7\\n[53] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.\\nLearning to reweight examples for robust deep learning. In\\nInternational Conference on Machine Learning, 2018. 8\\n[54] Zhongzheng Ren, Raymond A. Yeh, and Alexander G.\\nSchwing. Not all unlabeled data are equal: Learning to weight\\ndata in semi-supervised learning. 2020. 8\\n[55] Ellen Riloff. Automatically generating extraction pa\\n}<->{\\n\\ndata in semi-supervised learning. 2020. 8\\n[55] Ellen Riloff. Automatically generating extraction patterns\\nfrom untagged text. In Proceedings of the national conference\\non artiﬁcial intelligence, 1996. 1\\n[56] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and Li\\nFei-Fei. ImageNet Large Scale Visual Recognition Challenge.\\nInternational Journal of Computer Vision, 2015. 1, 4\\n[57]\\n}<->{\\n Large Scale Visual Recognition Challenge.\\nInternational Journal of Computer Vision, 2015. 1, 4\\n[57] H Scudder. Probability of error of some adaptive pattern-\\nrecognition machines. IEEE Transactions on Information\\nTheory, 11(3), 1965. 1\\n[58] Kihyuk Sohn, David Berthelot, Zizhao Li, Chun-Liang Zhang,\\nNicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang,\\nand Colin Raffel. Fixmatch: Simplifying semi-supervised\\nlearning with consistency and conﬁdence. In IEEE Confer-\\nence on Computer Vision and \\n}<->{\\nemi-supervised\\nlearning with consistency and conﬁdence. In IEEE Confer-\\nence on Computer Vision and Pattern Recognition, 2020. 1,\\n5, 8\\n[59] Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth O.\\nStanley, and Jeff Clune. Generative teaching networks: Ac-\\ncelerating neural architecture search by learning to generate\\nsynthetic training data. 2020. 8\\n[60] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\\nnav Gupta. Revisiting unreasonable effectiveness of data in\\ndeep learning era. In Pro\\n}<->{\\nngh, and Abhi-\\nnav Gupta. Revisiting unreasonable effectiveness of data in\\ndeep learning era. In Proceedings of the IEEE international\\nconference on computer vision, 2017. 1\\n[61] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and\\nAlexander A Alemi. Inception-v4, inception-resnet and the\\nimpact of residual connections on learning. In Thirty-First\\nAAAI Conference on Artiﬁcial Intelligence, 2017. 7\\n[62] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethink\\n}<->{\\n. 7\\n[62] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\\nchitecture for computer vision. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition,\\n2016. 7\\n[63] Mingxing Tan and Quoc V. Le. EfﬁcientNet: Rethinking\\nmodel scaling for convolutional neural networks. In Interna-\\ntional Conference on Machine Learning, 2019. 6, 7\\n[64] Antti Tarvainen and Harri Valpola. Mean teachers are better\\nrole models: Weight-av\\n}<->{\\n, 2019. 6, 7\\n[64] Antti Tarvainen and Harri Valpola. Mean teachers are better\\nrole models: Weight-averaged consistency targets improve\\nsemi-supervised deep learning results. In Advances in Neural\\nInformation Processing Systems, 2017. 5, 8\\n[65] Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin\\nElizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-\\nJia Li. YFCC100M: The new data in multimedia research.\\nCommunications of the ACM, 2016. 6, 8, 21\\n[66] T. Tieleman and G. Hinton. RmsProp: Divi\\n}<->{\\nia research.\\nCommunications of the ACM, 2016. 6, 8, 21\\n[66] T. Tieleman and G. Hinton. RmsProp: Divide the gradient\\nby a running average of its recent magnitude. COURSERA:\\nNeural Networks for Machine Learning, 2012. 17\\n[67] Antonio Torralba, Rob Fergus, and William T. Freeman. 80\\nmillion tiny images: a large dataset for non-parametric object\\nand scene recognition.\\nIn IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, 2008. 21\\n[68] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and He\\n}<->{\\nnalysis and Machine Intelligence, 2008. 21\\n[68] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve\\nJegou. Fixing the train-test resolution discrepancy. In Ad-\\nvances in Neural Information Processing Systems, 2019. 6\\n[69] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé\\nJégou. Fixing the train-test resolution discrepancy. In Ad-\\nvances in Neural Information Processing Systems, 2019. 7\\n[70] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé\\nJégou. Fixing the train-test resolution \\n}<->{\\n70] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé\\nJégou. Fixing the train-test resolution discrepancy: Fixefﬁ-\\ncientnet. arXiv preprint arXiv:2003.08237, 2020. 7\\n[71] Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio,\\nand David Lopez-Paz. Interpolation consistency training for\\nsemi-supervised learning. In International Joint Conference\\non Artiﬁcial Intelligence, 2019. 5, 8\\n[72] Xiao Wang, Daisuke Kihara, Jiebo Luo, and Guo-Jun Qi.\\nEnaet: Self-trained ensemble autoencoding transformat\\n}<->{\\nng, Daisuke Kihara, Jiebo Luo, and Guo-Jun Qi.\\nEnaet: Self-trained ensemble autoencoding transformations\\nfor semi-supervised learning. Arxiv 1911.09265, 2019. 4, 5\\n[73] Xinyi Wang, Hieu Pham, Paul Mitchel, Antonis Anastasopou-\\nlos, Jaime Carbonell, and Graham Neubig. Optimizing data\\nusage via differentiable rewards. In International Conference\\non Machine Learning, 2020. 8\\n[74] Yulin Wang, Jiayi Guo, Shiji Song, and Gao Huang. Meta-\\nsemi: A meta-learning approach for semi-supervised learning.\\nArx\\n}<->{\\no, Shiji Song, and Gao Huang. Meta-\\nsemi: A meta-learning approach for semi-supervised learning.\\nArxiv, 2007.02394, 2020. 8\\n[75] Ronald J. Williams. Simple statistical gradient-following\\nalgorithms for connectionist reinforcement learning. Machine\\nLearning, 1992. 14\\n[76] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong,\\nand Quoc V. Le. Unsupervised data augmentation for consis-\\ntency training. In Advances in Neural Information Processing\\nSystems, 2020. 1, 3, 4, 5, 6, 8, 15, 17, 18\\n[77] Qizhe\\n}<->{\\n In Advances in Neural Information Processing\\nSystems, 2020. 1, 3, 4, 5, 6, 8, 15, 17, 18\\n[77] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V\\nLe. Self-training with noisy student improves imagenet clas-\\nsiﬁcation. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, 2020. 1, 6, 7, 8,\\n17, 22\\n[78] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and\\nKaiming He. Aggregated residual transformations for deep\\nneural networks. In IEEE Conference on Computer V\\n}<->{\\ng He. Aggregated residual transformations for deep\\nneural networks. In IEEE Conference on Computer Vision\\nand Pattern Recognition, 2017. 7\\n[79] I. Zeki Yalniz, Herv’e J’egou, Kan Chen, Manohar Paluri,\\nand Dhruv Mahajan. Billion-scale semi-supervised learning\\nfor image classiﬁcation. Arxiv 1905.00546, 2019. 1, 6, 7, 8\\n[80] Yoshihiro Yamada, Masakazu Iwamura, Takuya Akiba, and\\nKoichi Kise. Shakedrop regularization for deep residual learn-\\ning. Arxiv, 1802.0237, 2018. 5\\n[81] David Yarowsky. Unsuper\\n}<->{\\n regularization for deep residual learn-\\ning. Arxiv, 1802.0237, 2018. 5\\n[81] David Yarowsky. Unsupervised word sense disambiguation\\nrivaling supervised methods. In 33rd annual meeting of the\\nassociation for computational linguistics, 1995. 1\\n[82] Yang You, Igor Gitman, and Boris Ginsburg. Large batch\\ntraining of convolutional networks. Arxiv, 1708.03888, 2017.\\n17\\n11\\n\\n--------------------------------------------------\\n[83] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\\nChun, Junsuk Choe, and \\n}<->{\\n--------------------\\n[83] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\\nChun, Junsuk Choe, and Youngjoon Yoo. CutMix: Regu-\\nlarization strategy to train strong classiﬁers with localizable\\nfeatures. In International Conference on Computer Vision,\\n2019. 6\\n[84] Sergey Zagoruyko and Nikos Komodakis. Wide residual\\nnetworks. In British Machine Vision Conference, 2016. 4\\n[85] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\\ntion. In Intern\\n}<->{\\n Cisse, Yann N. Dauphin, and\\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\\ntion. In International Conference on Learning Representa-\\ntions, 2018. 20\\n[86] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Cheng-\\nlong Bao, and Kaisheng Ma. Be your own teacher: Improve\\nthe performance of convolutional neural networks via self\\ndistillation. In International Conference on Computer Vision,\\n2019. 8\\n[87] Xingcheng Zhang, Zhizhong Li, Chen Change Loy, and\\nDahua Lin.\\nPolynet: A pursuit of struc\\n}<->{\\n19. 8\\n[87] Xingcheng Zhang, Zhizhong Li, Chen Change Loy, and\\nDahua Lin.\\nPolynet: A pursuit of structural diversity in\\nvery deep networks. In Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition, 2017. 7\\n[88] Guoqing Zheng, Ahmed Hassan Awadallah, and Susan Du-\\nmais. Meta label correction for learning with weak supervi-\\nsion. Arxiv, 1911.03809, 2019. 8\\n[89] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao\\nLiu, Ekin D Cubuk, and Quoc V Le. Rethinking pre-trainin\\n}<->{\\nlnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao\\nLiu, Ekin D Cubuk, and Quoc V Le. Rethinking pre-training\\nand self-training. In Advances in Neural Information Process-\\ning Systems, 2020. 1, 8\\n[90] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\\nLe. Learning transferable architectures for scalable image\\nrecognition. In Proceedings of the IEEE conference on com-\\nputer vision and pattern recognition, 2018. 7\\n12\\n\\n--------------------------------------------------\\nA. Derivation of the Teacher’s\\n}<->{\\nition, 2018. 7\\n12\\n\\n--------------------------------------------------\\nA. Derivation of the Teacher’s Update Rule\\nIn this section, we present the detailed derivation of the teacher’s update rule in Section 2.\\nMathematical Notations and Conventions.\\nSince we will work with the chain rule, we use the standard Jacobian notations.3\\nSpeciﬁcally, for a differentiable function f : Rm →Rn, and for a vector x ∈Rm, we use the notation ∂f\\n∂x ∈Rn×m to denote\\nthe Jacobian matrix of f, whose dimension is n × m\\n}<->{\\n x ∈Rm, we use the notation ∂f\\n∂x ∈Rn×m to denote\\nthe Jacobian matrix of f, whose dimension is n × m. Additionally, when we mention the Jacobian of a function f at multiple\\npoints such as x1 and x2, we will use the notations of ∂f\\n∂x\\n\\x0c\\x0c\\x0c\\nx=x1 and ∂f\\n∂x\\n\\x0c\\x0c\\x0c\\nx=x2.\\nFurthermore, by mathematical conventions, a vector v ∈Rn is treated as a column matrix – that is, a matrix of size n × 1.\\nFor this reason, the gradient vector of a multi-variable real-valued function is actually the transpose of of its J\\n}<->{\\n, the gradient vector of a multi-variable real-valued function is actually the transpose of of its Jacobian matrix.\\nFinally, all multiplications in this section are standard matrix multiplications. If an operand is a vector, then the operand is\\ntreated as a column matrix.\\nDimension Annotations.\\nUnderstanding that these notations and conventions might cause confusions, in the derivation\\nbelow, we annotate the dimensions of the computed quantities to ensure that there is no confusion caused to our\\n}<->{\\nnnotate the dimensions of the computed quantities to ensure that there is no confusion caused to our readers.\\nTo this end, we respectively use |S| and |T| to denote the dimensions of the parameters θS, θT . That is, θS ∈R|S|×1 and\\nθT ∈R|T |×1.\\nWe now present the derivation. Suppose that on a batch of unlabeled examples xu, the teacher samples the pseudo labels\\nbyu ∼T(xu; θT ) and the student uses (xu, byu) to update its parameter θS. In expectation, the student’s new parameter is\\nEbyu∼T (xu;θT )\\n}<->{\\n(xu, byu) to update its parameter θS. In expectation, the student’s new parameter is\\nEbyu∼T (xu;θT )\\n\\x02\\nθS−ηS∇ηSCE(byu, S(xu; θS))\\n\\x03\\n. We will update the teacher’s parameter to minimize the student’s cross-entropy\\non a batch of labeled data a this expected parameter. To this end, we need to compute the Jacobian:\\n∂R\\n∂θT\\n|{z}\\n1×|T |\\n=\\n∂\\n∂θT\\nCE\\n\\x10\\nyl, S\\n\\x10\\nxl; Ebyu∼T (xu;θT )\\n\\x02\\nθS −ηS∇ηSCE(byu, S(xu; θS))\\n\\x03\\x11\\x11\\n(4)\\nTo simplify our notation, let us deﬁne\\n¯θ′\\nS\\n|{z}\\n|S|×1\\n= Ebyu∼T (xu;θT )\\n\\x02\\nθS −ηS∇ηSCE(b\\n}<->{\\n))\\n\\x03\\x11\\x11\\n(4)\\nTo simplify our notation, let us deﬁne\\n¯θ′\\nS\\n|{z}\\n|S|×1\\n= Ebyu∼T (xu;θT )\\n\\x02\\nθS −ηS∇ηSCE(byu, S(xu; θS))\\n\\x03\\n(5)\\nThen, by the chain rule, we have\\n∂R\\n∂θT\\n|{z}\\n1×|T |\\n=\\n∂\\n∂θT\\nCE\\n\\x10\\nyl, S\\n\\x10\\nxl; Ebyu∼T (xu;θT )\\n\\x02\\nθS −ηS∇ηSCE(byu, S(xu; θS))\\n\\x03\\x11\\x11\\n=\\n∂\\n∂θT\\nCE\\n\\x00yl, S\\n\\x00xl; ¯θ′\\nS\\n\\x01\\x01\\n= ∂CE\\n\\x00yl, S\\n\\x00xl; ¯θ′\\nS\\n\\x01\\x01\\n∂θS\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\nθS=¯θ′\\nS\\n\\x01\\n|\\n{z\\n}\\n1×|S|\\n· ∂¯θ′\\nS\\n∂θT\\n|{z}\\n|S|×|T |\\n(6)\\nThe ﬁrst factor in Equation 6 can be simply computed via back-propagation. We now focus on the second term. We have\\n∂¯θ′\\nS\\n∂θT\\n|\\n}<->{\\n6 can be simply computed via back-propagation. We now focus on the second term. We have\\n∂¯θ′\\nS\\n∂θT\\n|{z}\\n|S|×|T |\\n=\\n∂\\n∂θT\\nEbyu∼T (xu;θT )\\n\\x02\\nθS −ηS∇ηSCE(byu, S(xu; θS))\\n\\x03\\n=\\n∂\\n∂θT\\nEbyu∼T (xu;θT )\\n\\uf8ee\\n\\uf8f0θS −ηS ·\\n \\n∂CE (byu, S(xu; θS))\\n∂θS\\n\\x0c\\x0c\\x0c\\x0c\\nθS=θS\\n!⊤\\uf8f9\\n\\uf8fb\\n(7)\\n3Standard: https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant\\n13\\n\\n--------------------------------------------------\\nNote that in Equation 7 above, the Jacobian of CE (byu, S(xu; θS)), which has dimension 1 × |S|, needs to be transposed\\n}<->{\\non 7 above, the Jacobian of CE (byu, S(xu; θS)), which has dimension 1 × |S|, needs to be transposed to\\nmatch the dimension of θS, which, as we discussed above, conventionally has dimension |S| × 1.\\nNow, since θS in Equation 7 does not depend on θT , we can leave it out of subsequent derivations. Also, to simplify\\nnotations, let us deﬁne the gradient\\ngS(byu)\\n| {z }\\n|S|×|1|\\n=\\n \\n∂CE (byu, S(xu; θS))\\n∂θS\\n\\x0c\\x0c\\x0c\\x0c\\nθS=θS\\n!⊤\\n(8)\\nThen, Equation 7 becomes\\n∂¯θ′\\nS\\n∂θT\\n|{z}\\n|S|×|T |\\n= −ηS ·\\n∂\\n∂θT\\nEbyu∼T (xu;θT\\n}<->{\\n\\n∂θS\\n\\x0c\\x0c\\x0c\\x0c\\nθS=θS\\n!⊤\\n(8)\\nThen, Equation 7 becomes\\n∂¯θ′\\nS\\n∂θT\\n|{z}\\n|S|×|T |\\n= −ηS ·\\n∂\\n∂θT\\nEbyu∼T (xu;θT )\\nh\\ngS(byu)\\n| {z }\\n|S|×1\\ni\\n(9)\\nSince gS(byu) has no dependency on on θT , except for via byu, we can apply the REINFORCE equation [75] to achieve\\n∂¯θ(t+1)\\nS\\n∂θT\\n| {z }\\n|S|×|T |\\n= −ηS ·\\n∂\\n∂θT\\nEbyu∼T (xu;θT ) [gS(byu)]\\n= −ηS · Ebyu∼T (xu;θT )\\nh\\ngS(byu)\\n| {z }\\n|S|×1\\n· ∂log P (byu|xu; θT )\\n∂θT\\n|\\n{z\\n}\\n1×|T |\\ni\\n= ηS · Ebyu∼T (xu;θT )\\nh\\ngS(byu)\\n| {z }\\n|S|×1\\n· ∂CE (byu, T(xu; θT ))\\n∂θT\\n|\\n{z\\n}\\n1×|T |\\ni\\n(1\\n}<->{\\n×|T |\\ni\\n= ηS · Ebyu∼T (xu;θT )\\nh\\ngS(byu)\\n| {z }\\n|S|×1\\n· ∂CE (byu, T(xu; θT ))\\n∂θT\\n|\\n{z\\n}\\n1×|T |\\ni\\n(10)\\nHere, the last equality in Equation 10 is is due to the deﬁnition of the cross-entropy loss, which is the negative of the log-prob\\nterm in the previous line.\\nNow, we can substitute Equation 10 into Equation 6 to obtain\\n∂R\\n∂θT\\n|{z}\\n1×|T |\\n= ∂CE\\n\\x00yl, S\\n\\x00xl; ¯θ′\\nS\\n\\x01\\x01\\n∂θS\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\nθS=¯θ′\\nS\\n|\\n{z\\n}\\n1×|S|\\n· ∂¯θ′\\nS\\n∂θT\\n|{z}\\n|S|×|T |\\n= ηS · ∂CE\\n\\x00yl, S\\n\\x00xl; ¯θ′\\nS\\n\\x01\\x01\\n∂θS\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\nθS=¯θ′\\nS\\n|\\n{z\\n}\\n1×|S|\\n· Ebyu∼T \\n}<->{\\n· ∂¯θ′\\nS\\n∂θT\\n|{z}\\n|S|×|T |\\n= ηS · ∂CE\\n\\x00yl, S\\n\\x00xl; ¯θ′\\nS\\n\\x01\\x01\\n∂θS\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\nθS=¯θ′\\nS\\n|\\n{z\\n}\\n1×|S|\\n· Ebyu∼T (xu;θT )\\nh\\ngS(byu)\\n| {z }\\n|S|×1\\n· ∂CE (byu, T(xu; θT ))\\n∂θT\\n|\\n{z\\n}\\n1×|T |\\ni\\n(11)\\nFinally, we use Monte Carlo approximation for every term in Equation 11 using the sampled byu. In particular, we ap-\\nproximate ¯θ′\\nS with the parameter obtained from θS by updating the student parameter on (xu, byu), i.e., θ′\\nS = θS −ηS ·\\n∇θSCE (byu, S(xu; θS))), and approximate the expected value in the second term w\\n}<->{\\ne., θ′\\nS = θS −ηS ·\\n∇θSCE (byu, S(xu; θS))), and approximate the expected value in the second term with the same using byu. With these\\napproximation, we obtain the gradient ∇θT Lu(θT , θS) from Equation 1:\\n∇θT Ll = ηS · ∂CE\\n\\x00yl, S\\n\\x00xl; θ′\\nS\\n\\x01\\x01\\n∂θS\\n|\\n{z\\n}\\n1×|S|\\n·\\n \\n∂CE (byu, S(xu; θS))\\n∂θS\\n\\x0c\\x0c\\x0c\\x0c\\nθS=θS\\n!⊤\\n|\\n{z\\n}\\n|S|×1\\n· ∂CE (byu, T(xu; θT ))\\n∂θT\\n|\\n{z\\n}\\n1×|T |\\n= ηS ·\\n\\x12\\x10\\n∇θ′\\nSCE (yl, S(xl; θ′\\nS)\\n\\x11⊤\\n· ∇θSCE (byu, S(xu; θS))\\n\\x13\\n|\\n{z\\n}\\nA scalar := h\\n·∇θT CE (byu, T(xu; θT ))\\n(12)\\n14\\n\\n--------------------\\n}<->{\\nθSCE (byu, S(xu; θS))\\n\\x13\\n|\\n{z\\n}\\nA scalar := h\\n·∇θT CE (byu, T(xu; θT ))\\n(12)\\n14\\n\\n--------------------------------------------------\\nB. Pseudo Code for Meta Pseudo Labels with UDA\\nIn this section, we present the pseudo code for Meta Pseudo Labels where the teacher is trained with an extended objective\\nto include the UDA loss. We emphasize that the UDA objective is applied on the teacher, while the student still only learns\\nfrom the pseudo labeled data given by the teacher. The pseudo code can be f\\n}<->{\\ntudent still only learns\\nfrom the pseudo labeled data given by the teacher. The pseudo code can be found in Algorithm 1.\\nAlgorithm 1 The Meta Pseudo Labels method, applied to a teacher trained with UDA [76].\\nInput: Labeled data xl, yl and unlabeled data xu.\\nInitialize θ(0)\\nT\\nand θ(0)\\nS\\nfor t = 0 to N −1 do\\nSample an unlabeled example xu and a labeled example xl, yl\\nSample a pseudo label byu ∼P(·|xu; θT )\\nUpdate the student using the pseudo label byu:\\nθ(t+1)\\nS\\n= θ(t)\\nS −ηS ∇θSCE(byu, S(xu; θS)|θS\\n}<->{\\nu; θT )\\nUpdate the student using the pseudo label byu:\\nθ(t+1)\\nS\\n= θ(t)\\nS −ηS ∇θSCE(byu, S(xu; θS)|θS=θ(t)\\nS\\nCompute the teacher’s feedback coefﬁcient as in Equation 12:\\nh = ηS ·\\n\\x12\\x10\\n∇θ′\\nSCE\\n\\x10\\nyl, S(xl; θ(t+1)\\nS\\n\\x11 \\x11⊤\\n· ∇θSCE\\n\\x10\\nbyu, S(xu; θ(t)\\nS )\\n\\x11\\x13\\nCompute the teacher’s gradient from the student’s feedback:\\ng(t)\\nT\\n= h · ∇θT CE(byu, T(xu; θT ))|θT =θ(t)\\nT\\nCompute the teacher’s gradient on labeled data:\\ng(t)\\nT,supervised = ∇θT CE(yl, T(xl; θT ))|θT =θ(t)\\nT\\nCompute the teacher’s gradient on the UDA \\n}<->{\\nta:\\ng(t)\\nT,supervised = ∇θT CE(yl, T(xl; θT ))|θT =θ(t)\\nT\\nCompute the teacher’s gradient on the UDA loss with unlabeled data:\\ng(t)\\nT,UDA = ∇θT CE\\n\\x10\\nStopGradient(T(xl); θT ), T(RandAugment(xl); θT )\\n\\x11\\x0c\\x0c\\x0c\\nθT =θ(t)\\nT\\nUpdate the teacher:\\nθ(t+1)\\nT\\n= θ(t)\\nT −ηT ·\\n\\x10\\ng(t)\\nT + g(t)\\nT,supervised + g(t)\\nT,UDA\\n\\x11\\nend\\nreturn θ(N)\\nS\\n▷Only the student model is returned for predictions and evaluations\\nC. Experimental Details\\nIn this section, we provide the training details for our experiments in Section 3 and Se\\n}<->{\\ntal Details\\nIn this section, we provide the training details for our experiments in Section 3 and Section 4.\\nC.1. Dataset Splits\\nWe describe how the datasets CIFAR-10-4K, SVHN-1K, and ImageNet-10% in Section 3.2 are constructed. For CIFAR-10,\\nwe download the ﬁve training data batch ﬁles from CIFAR-10’s ofﬁcial website.4 Then, we load all the images into a list of\\n50,000 images, keeping the order as downloaded. The ﬁsrt 5,000 images are typically reserved for validation, so we remove\\nthem. The ne\\n}<->{\\ns downloaded. The ﬁsrt 5,000 images are typically reserved for validation, so we remove\\nthem. The next 4,000 images are used as labeled data. For SVHN, we download the data from the mat ﬁles on SVHN’s\\nofﬁcial site5, and follow the same procedure as with CIFAR-10. We note that this selection process leads to a slight imbalance\\nin the class distribution for both CIFAR-10-4K and SVHN-1K, but the settings are the same for all of our experiments. For\\nImageNet, we follow the procedure in Inception’s G\\n}<->{\\nings are the same for all of our experiments. For\\nImageNet, we follow the procedure in Inception’s GitHub6. This results in 1,024 training TFRecord shards of approximately\\nthe same size. The order of the images in these shards are deterministic. For ImageNet-10%, we use the ﬁrst 102 shards;\\n4CIFAR-10’s ofﬁcial website: www.cs.toronto.edu/~kriz/cifar.html.\\n5SVHN’s ofﬁcial website: ufldl.stanford.edu/housenumbers/.\\n6Inception’s GitHub, which also has the code to create ImageNet’s training shards i\\n}<->{\\nu/housenumbers/.\\n6Inception’s GitHub, which also has the code to create ImageNet’s training shards in TFRecord: github.com/tensorflow/models/blob/\\nmaster/research/inception/inception/data/download_and_preprocess_imagenet.sh.\\n15\\n\\n--------------------------------------------------\\nfor ImageNet-20%, we use the ﬁrst 204 shards; and so on. The last 20 shards, corresponding to roughly 25,000 images, are\\nreserved for hyper-parameters tuning (used in Section 3.3 and Section 4).\\nC.2. Modiﬁcations of Rand\\n}<->{\\n\\nreserved for hyper-parameters tuning (used in Section 3.3 and Section 4).\\nC.2. Modiﬁcations of RandAugment [13]\\nWe modify a few data augmentation strategies as introduced by RandAugment [13]. Our modiﬁcations mostly target\\nthe SVHN dataset. In particular, we remove all rotations from the set of augmentation operations since rotation is a wrong\\ninvariance for digits such as 6 and 9. We also remove horizontal translations because they cause another wrong invariance\\nfor digits 3 and 8, e.g., when \\n}<->{\\n horizontal translations because they cause another wrong invariance\\nfor digits 3 and 8, e.g., when 8 is pushed half-outside the image and the remaining part looks like a 3. Table 5 presents the\\ntransformations that we keep for our datasets.\\nCIFAR-10 and ImageNet\\nSVHN\\nAutoContrast\\nAutoContrast\\nBrightness\\nBrightness\\nColor\\nColor\\nContrast\\nContrast\\nEqualize\\nEqualize\\nInvert\\nInvert\\nSharpness\\nSharpness\\nPosterize\\nPosterize\\nSample Pairing\\nSolarize\\nSolarize\\nShearX\\nRotate\\nShearY\\nShearX\\nTranslateY\\nShearY\\nTr\\n}<->{\\nosterize\\nPosterize\\nSample Pairing\\nSolarize\\nSolarize\\nShearX\\nRotate\\nShearY\\nShearX\\nTranslateY\\nShearY\\nTranslateX\\nTranslateY\\nTable 5: Transformations that RandAugment uniformly samples for our datasets. We refer our readers to [12] for the detailed descriptions\\nof these transformations.\\nC.3. Additional Implementation Details\\nTo improve the stability of Meta Pseudo Labels, we use the following details in the Meta Pseudo Labels process.\\nUse cosine distance instead of dot product in Equation 12.\\nThe dot\\n}<->{\\nn the Meta Pseudo Labels process.\\nUse cosine distance instead of dot product in Equation 12.\\nThe dot product h in Equation 12 has a large value range,\\nespecially at the beginning of the Meta Pseudo Labels process. Thus, in order to stabilize training, we compute h using the\\ngradients’ cosine distance. This modiﬁcation requires very little modiﬁcation in our code.\\nWe give two justiﬁcations why the use of cosine distance makes sense mathematically. First, h in Equation 12 is on a\\nscalar which is m\\n}<->{\\nuse of cosine distance makes sense mathematically. First, h in Equation 12 is on a\\nscalar which is multiplied with the teacher’s gradient with respect to θT . Changing dot product into cosine distance does\\nnot change the sign of h, and thus preserving the actions to increase or to decrease the probabilities of the sampled pseudo\\nlabels. Second, cosine distance’s value range is much smaller than that of dot product, making the Meta Pseudo Labels updates\\nmore numerically stable. Speciﬁcally, the v\\n}<->{\\nat of dot product, making the Meta Pseudo Labels updates\\nmore numerically stable. Speciﬁcally, the value range of cosine distance is [−1, 1], while the value range of dot products, as\\nobserved in our experiments, is about [−5 × 104, 5 × 104]. This range also depends on the weight decay hyper-parameter.\\nAdditionally, the dot product h, as shown in Equation 12 and as derived in Section A, results from the application of the\\nchain rule in a so-called bi-level optimization procedure. Bi-level optimi\\n}<->{\\nom the application of the\\nchain rule in a so-called bi-level optimization procedure. Bi-level optimization has been applied in some past work, such as\\nHyper Gradient Descent [3], which also replaces dot product with cosine distance to improve the numerical stability.\\nUse a baseline for h in Equation 12.\\nTo further reduce the variance of h, we maintain a moving average b of h and subtract\\nb from h every time we compute g(t)\\nT\\nas in Equation 12. This practice is also widely applied in Reinforcemen\\n}<->{\\nevery time we compute g(t)\\nT\\nas in Equation 12. This practice is also widely applied in Reinforcement Learning\\nliterature.\\nWhile using cosine distance is very crucial to maintain the numerical stability of Meta Pseudo Labels, using the moving\\naverage baseline only slightly improves Meta Pseudo Labels’s performance. We suspect that not using the moving average\\nbaseline is also ﬁne, especially when Meta Pseudo Labels can train for many steps without overﬁtting.\\n16\\n\\n--------------------------------\\n}<->{\\nMeta Pseudo Labels can train for many steps without overﬁtting.\\n16\\n\\n--------------------------------------------------\\nC.4. Hyper-parameters\\nOptimizers.\\nIn all our experiments, the WideResNet-28-2 for CIFAR-10-4K and SVHN-1K and the ResNet-50 for ImageNet-\\n10% and full ImageNet are updated with Nesterov Momentum with default the momentum coefﬁcient of 0.9. The networks’\\nlearning rate follow the cosine decay [41]. Meanwhile, the EfﬁcientNet-L2 and EfﬁcientNet-B6-Wide for ImageNet+JFT\\nare trained \\n}<->{\\nsine decay [41]. Meanwhile, the EfﬁcientNet-L2 and EfﬁcientNet-B6-Wide for ImageNet+JFT\\nare trained with RMSProp [66] and with an exponential decay learning rate. These are the default optimizers and learning\\nrate schedules used for the architectures in their corresponding papers. We have only one substantial change of optimizer:\\nwhen we ﬁnetune EfﬁcientNet-L2 and EfﬁcientNet-B6-Wide on the labeled data from ImageNet (see Section 4), we use the\\nLARS optimizer [82] with their default parameters, \\n}<->{\\nd data from ImageNet (see Section 4), we use the\\nLARS optimizer [82] with their default parameters, i.e., momentum 0.9 and learning rate 0.001, training for 20,000 steps with\\na batch size of 4,096. We ﬁnetune using this optimizer instead of SGD in Noisy Student [77] because unlike Noisy Student,\\nthe student model in Meta Pseudo Labels never trains directly on any labeled example, and hence can beneﬁt from a more\\n“aggressive” ﬁnetuning process with stronger optimiziers.\\nNumerical Hyper-parameters\\n}<->{\\neﬁt from a more\\n“aggressive” ﬁnetuning process with stronger optimiziers.\\nNumerical Hyper-parameters.\\nTo tune hyper-parameters, we follow [48] and allow each method to have 128 trials of\\nhyper-parameters. When we tune, we let each model train for up to 50,000 steps. The optimal hyper-parameters are then used\\nto run experiments that last for much more steps, as we report below. In our experiments with Meta Pseudo Labels, training for\\nmore steps typically leads to stronger results. We stop at 1 mi\\n}<->{\\nith Meta Pseudo Labels, training for\\nmore steps typically leads to stronger results. We stop at 1 million steps for CIFAR-10-4K and SVHN-1K, and at 0.5 million\\nsteps for ImageNet because these are the standards from past papers.\\nWe report the hyper-parameters for our baselines and for Meta Pseudo Labels in Section 3 in Tables 6, 7, 8. We note that\\nour settings for UDA is different from originally reported by the original UDA paper [76]. In their work, UDA [76] use a much\\nlarger batch size for th\\n}<->{\\nreported by the original UDA paper [76]. In their work, UDA [76] use a much\\nlarger batch size for their UDA objective. In our implementation of UDA, we keep these batch sizes the same. This leads to a\\nmuch easier implementation of data parallelism in our framework, TensorFlow [1] running on TPU big pods. To compensate\\nfor the difference, we train all UDA baselines for much longer than the UDA paper [76]. During the training process, we also\\nmask out the supervised examples with high conﬁdence. E\\n}<->{\\nr [76]. During the training process, we also\\nmask out the supervised examples with high conﬁdence. Effectively, our UDA model receives roughly the same amount of\\ntraining with labeled examples and unlabeled examples as the models in [76]. We have also veriﬁed that on ImageNet-10%\\nwith the augmentation policy from AutoAugment [12], our UDA implementation achives 68.77% top-1 accuracy, which is\\nsimilar to 68.66% that the UDA paper [76] reported.\\nHyper-parameter\\nCIFAR-10\\nSVHN\\nImageNet\\nWeight decay\\n\\n}<->{\\nlar to 68.66% that the UDA paper [76] reported.\\nHyper-parameter\\nCIFAR-10\\nSVHN\\nImageNet\\nWeight decay\\n0.0005\\n0.001\\n0.0002\\nLabel smoothing\\n0\\n0\\n0.1\\nBatch normalization decay\\n0.99\\n0.99\\n0.99\\nLearning rate\\n0.4\\n0.05\\n1.28\\nNumber of training steps\\n50,000\\n50,000\\n40,000\\nNumber of warm up steps\\n2500\\n0\\n2000\\nBatch size\\n1024\\n128\\n2048\\nDropout rate\\n0.4\\n0.5\\n0.2\\nPseudo label threshold\\n0.95\\n0.975\\n0.7\\nTable 6: Hyper-parameters for supervised learning and Pseudo Labels.\\n17\\n\\n--------------------------------------------\\n}<->{\\nrameters for supervised learning and Pseudo Labels.\\n17\\n\\n--------------------------------------------------\\nHyper-parameter\\nCIFAR-10\\nSVHN\\nImageNet\\nWeight decay\\n0.0005\\n0.0005\\n0.0002\\nLabel smoothing\\n0\\n0\\n0.1\\nBatch normalization decay\\n0.99\\n0.99\\n0.99\\nLearning rate\\n0.3\\n0.4\\n1.28\\nNumber of training steps\\n1,000,000\\n1,000,000\\n500,000\\nNumber of warm up steps\\n5,000\\n5,000\\n5,000\\nBatch size\\n128\\n128\\n2048\\nDropout rate\\n0.5\\n0.6\\n0.25\\nUDA factor\\n2.5\\n1\\n20\\nUDA temperature\\n0.7\\n0.8\\n0.7\\nTable 7: Hyper-parameters for UDA. \\n}<->{\\nate\\n0.5\\n0.6\\n0.25\\nUDA factor\\n2.5\\n1\\n20\\nUDA temperature\\n0.7\\n0.8\\n0.7\\nTable 7: Hyper-parameters for UDA. Unlike originally done by the UDA paper [76], we do not use a larger batch size for the UDA objective.\\nInstead, we use the same batch size for both the labeled objective and the unlabeled objective. This is to avoid instances where some\\nparticularly small batch sizes for the labeled objective cannot be split on our computational hardware.\\nHyper-parameter\\nCIFAR-10\\nSVHN\\nImageNet\\nCommon\\nWeight decay\\n\\n}<->{\\n be split on our computational hardware.\\nHyper-parameter\\nCIFAR-10\\nSVHN\\nImageNet\\nCommon\\nWeight decay\\n0.0005\\n0.0005\\n0.0002\\nLabel smoothing\\n0.1\\n0.1\\n0.1\\nBatch normalization decay\\n0.99\\n0.99\\n0.99\\nNumber of training steps\\n1,000,000\\n1,000,000\\n500,000\\nNumber of warm up steps\\n2,000\\n2,000\\n1,000\\nStudent\\nLearning rate\\n0.3\\n0.15\\n0.8\\nBatch size\\n128\\n128\\n2048\\nDropout rate\\n0.35\\n0.45\\n0.1\\nTeacher\\nLearning rate\\n0.125\\n0.05\\n0.5\\nBatch size\\n128\\n128\\n2048\\nDropout rate\\n0.5\\n0.65\\n0.1\\nUDA factor\\n1.0\\n2.5\\n16.0\\nUDA temperature\\n0.\\n}<->{\\n.05\\n0.5\\nBatch size\\n128\\n128\\n2048\\nDropout rate\\n0.5\\n0.65\\n0.1\\nUDA factor\\n1.0\\n2.5\\n16.0\\nUDA temperature\\n0.8\\n1.25\\n0.75\\nTable 8: Hyper-parameters for Meta Pseudo Labels.\\nD. More Detailed Analysis of Meta Pseudo Label’s Behaviors\\nWe have seen in Section 3 and Section 4 that Meta Pseudo Labels leads to strong performances on multiple image\\nclassiﬁcation benchmarks. In this section, we provide further analysis of Meta Pseudo Labels and related baselines on more\\nrestricted and more controlled environments t\\n}<->{\\nis of Meta Pseudo Labels and related baselines on more\\nrestricted and more controlled environments to provide better insights about Meta Pseudo Labels’ behaviors.\\nD.1. Visualizing the Contributions of Meta Pseudo Labels\\nTo understand the contributions of Meta Pseudo Labels (MPL), in Figure 3, we visualize the relative gains of various\\nmethods on ImageNet-10% (Section 3.2). From the ﬁgure, we have two observations. First, for a purely supervised teacher,\\nMeta Pseudo Labels outperforms RandAugment\\n}<->{\\ntwo observations. First, for a purely supervised teacher,\\nMeta Pseudo Labels outperforms RandAugment. We suspect this is because Meta Pseudo Labels is more effective form of\\nregularization for the student. This is very crucial for ImageNet-10%, where we only have about 128 images per class for\\neach of the 1,000 classes. Second, UDA improves over Supervised+MPL+Finetune by 6.05% in top-1 accuracy. This is in the\\nsame ballpark with the gain that UDA+MPL delivers above UDA, which is 5.25%. As UDA’s\\n}<->{\\nThis is in the\\nsame ballpark with the gain that UDA+MPL delivers above UDA, which is 5.25%. As UDA’s accuracy is already high, such\\nimprovement is very signiﬁcant. Finally, ﬁnetuning only slightly improves over UDA+MPL. This extra performance boost is a\\nunique advantage of Meta Pseudo Labels, since the student never directly learns from labeled data.\\nD.2. Meta Pseudo Labels Is An Effective Regularization Strategy\\nThe rest of this paper uses Meta Pseudo Labels as a semi-supervised learning method\\n}<->{\\nization Strategy\\nThe rest of this paper uses Meta Pseudo Labels as a semi-supervised learning method. In this section, we show that Meta\\nPseudo Labels can behave like an effective regularization method for supervised learning. This behavior can be achieved by\\n18\\n\\n--------------------------------------------------\\nFigure 3: Breakdown of the gains of different components in Meta Pseudo Labels (MPL). The gain of Meta Pseudo Labels over UDA,\\nalbeit smaller than the gain of UDA over RandAugment, is s\\n}<->{\\n The gain of Meta Pseudo Labels over UDA,\\nalbeit smaller than the gain of UDA over RandAugment, is signiﬁcant as UDA is already very strong.\\nmaking labeled data the same with unlabeled data in Figure 1. In this case, Meta Pseudo Labels can be seen as an adaptive\\nform of Label Smoothing: the teacher generates soft labels on labeled data for the student, just like the way Label Smoothing\\nsmooths the hard labels to regularize the model. The main difference is that the policy in Label Smoothing is ﬁ\\n}<->{\\n hard labels to regularize the model. The main difference is that the policy in Label Smoothing is ﬁxed, whereas\\nthe policy of the teacher in Meta Pseudo Labels is adaptive to enhance the student’s performance.\\nTo conﬁrm the effect of Meta Pseudo Labels, we compare the method to Supervised Learning and Label Smoothing on\\nCIFAR-10-4K and SVHN-1K. All models and settings are the same as in Section 3.2, except that we do not use RandAugment\\nand we restrict the unlabeled data to the same set of labe\\n}<->{\\n.2, except that we do not use RandAugment\\nand we restrict the unlabeled data to the same set of labeled data. We choose CIFAR-10-4K and SVHN-1K for this experiment\\nbecause Label Smoothing is typically already used in ImageNet models. The results are shown in Table 9. As can be seen from\\nthe table, Meta Pseudo Labels achieves 83.71% on CIFAR-10-4K and 91.89% on SVHN-1K. Both of these are signiﬁcantly\\nbetter than the accuracy obtained by supervised learning with and without Label Smoothing. This s\\n}<->{\\nly\\nbetter than the accuracy obtained by supervised learning with and without Label Smoothing. This shows the importance of\\nfeedback in Meta Pseudo Labels.\\nCIFAR-10-4K\\nSVHN-1K\\nSupervised\\n82.14 ± 0.25\\n88.17 ± 0.47\\nLabel Smoothing\\n82.21 ± 0.18\\n89.39 ± 0.25\\nMeta Pseudo Labels\\n83.71 ± 0.21\\n91.89 ± 0.14\\nTable 9: Meta Pseudo Labels can be used as a regularization method for supervised learning.\\nD.3. Meta Pseudo Labels Is a Mechanism to Addresses the Conﬁrmation Bias of Pseudo Labels\\nIn this section, we\\n}<->{\\n Pseudo Labels Is a Mechanism to Addresses the Conﬁrmation Bias of Pseudo Labels\\nIn this section, we show empirical evidence that Meta Pseudo Labels helps to address the teacher’s conﬁrmation bias [2]\\nin Pseudo Labels. To this end, we analyze the training accuracy of the teacher and the student in Meta Pseudo Labels from\\nour experiments for CIFAR-10-4K and ImageNet-10% in Section 3.2. In Figure 4, we plot the accuracy percentage at each\\ntraining batch throughout the training process of a teacher\\n}<->{\\n we plot the accuracy percentage at each\\ntraining batch throughout the training process of a teacher and a student in Meta Pseudo Labels. We also plot the same data for\\na supervised model. From the ﬁgure, we have two observations:\\n• On CIFAR-10-4K (Figure 4-Left), the student’s training accuracy in Meta Pseudo Labels is much lower that of the same\\nnetwork in Supervised Learning. As CIFAR-10-4K has very few labeled data, if the teacher converges quickly like in\\nSupervised Learning, it will not ge\\n}<->{\\n very few labeled data, if the teacher converges quickly like in\\nSupervised Learning, it will not generalize to the unlabeled data and hence will teach the student in inaccurate pseudo\\nlabels. In contrast, Figure 4-Left shows that both the teacher and student in Meta Pseudo Labels converge much slower.\\nTo see this, note that in Meta Pseudo Labels, the student’s training accuracy is measured by how much it agrees with the\\nteacher’s pseudo labels. Therefore, the student in Meta Pseudo Labels havin\\n}<->{\\n much it agrees with the\\nteacher’s pseudo labels. Therefore, the student in Meta Pseudo Labels having a lower training accuracy means that the\\nstudent often disagrees with the pseudo labels that the teacher samples. This disagreement forces the teacher to constantly\\nupdates its weights to generate better pseudo labels, and makes it hard for the student to converge as the student has\\nto learn from the teacher’s changing pseudo labels. This behavior prevents both the teacher and the student from t\\n}<->{\\nthe teacher’s changing pseudo labels. This behavior prevents both the teacher and the student from the\\npremature convergence that causes the conﬁrmation bias in Supervised Learning and Pseudo Labels.\\n• On ImageNet-10% (Figure 4-Right), the student also disagrees with the teacher’s pseudo labels, as shown in the student’s\\nlow training accuracy. Additionally, we observe that the teacher’s training accuracy surges up faster than the supervised\\n19\\n\\n--------------------------------------------------\\n\\n}<->{\\nccuracy surges up faster than the supervised\\n19\\n\\n--------------------------------------------------\\nmodel’s accuracy. We suspect that this is beneﬁcial for the student learning, since ImageNet has 1,000 classes so in order\\nto effectively teach the student to do well on the labeled dataset, the teacher has to become more accurate. Therefore, the\\nfeedback from the student is beneﬁcial for the teacher’s learn as well. This trend of high training accuracy only changes\\nat the end of the training proc\\n}<->{\\nr’s learn as well. This trend of high training accuracy only changes\\nat the end of the training procedure, where the training accuracy of Supervised Learning surpasses those of the teacher\\nand the student in Meta Pseudo Labels. From this last sign, we suspect that the supervised model has overﬁtted to the\\nsmall set of labeled training examples in ImageNet-10%, which will causes the conﬁrmation bias if this supervised model\\nis used to generate pseudo labels for another student model to learn from\\n}<->{\\ns if this supervised model\\nis used to generate pseudo labels for another student model to learn from.\\n0\\n0.25\\n0.50\\n0.75\\n1.0\\nTraining Progress\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\nTraining Accuracy\\nTraining Accuracy on CIFAR-10-4,000\\nSupervised\\nTeacher\\nStudent\\n0\\n0.25\\n0.50\\n0.75\\n1.0\\nTraining Progress\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nTraining Accuracy\\nTraining Accuracy on ImageNet-10%\\nSupervised\\nTeacher\\nStudent\\nFigure 4: Training accuracy of Meta Pseudo Labels and of supervised learning on CIFAR-10-4,000 and ImageNet-10%\\n}<->{\\nraining accuracy of Meta Pseudo Labels and of supervised learning on CIFAR-10-4,000 and ImageNet-10%. Both the teacher\\nand the student in Meta Pseudo Labels have lower training accuracy, effectively avoiding overﬁtting.\\nD.4. Meta Pseudo Labels with Different Training Techniques for the Teacher\\nIn Sections 3 and Section 4, we have presented Meta Pseudo Labels results where the teacher is trained with UDA. In\\nTable 10, we further show that on CIFAR-10-4K, Meta Pseudo Labels improves over different\\n}<->{\\nth UDA. In\\nTable 10, we further show that on CIFAR-10-4K, Meta Pseudo Labels improves over different teachers trained with different\\ntechniques, including Pseudo Labels [36], Mixup [85], and RandAugment. These results indicate that Meta Pseudo Labels is\\neffective with all techniques. Additionally, the results suggest that better training techniques for the teacher tend to result in\\nbetter students.\\nTeacher\\nPseudo-Labels\\nMixup [85]\\nRandAugment\\n-Meta Pseudo Labels\\n83.79 ± 0.11\\n84.20 ± 0.15\\n85.53 ±\\n}<->{\\n.\\nTeacher\\nPseudo-Labels\\nMixup [85]\\nRandAugment\\n-Meta Pseudo Labels\\n83.79 ± 0.11\\n84.20 ± 0.15\\n85.53 ± 0.25\\n+Meta Pseudo Labels\\n84.11 ± 0.07\\n84.81 ± 0.19\\n87.55 ± 0.14\\nTable 10: Meta Pseudo Labels’s accuracy for WideResNet-28-2 on CIFAR-10-4,000, where the teacher is trained with different techniques.\\nAll numbers are mean ± std over 10 runs.\\nD.5. Meta Pseudo Labels with Different Amounts of Labeled Data\\nWe study how much Meta Pseudo Labels improves as more labeled data becomes available. To this en\\n}<->{\\nata\\nWe study how much Meta Pseudo Labels improves as more labeled data becomes available. To this end, we experiment\\nwith 10%, 20%, 40%, 80%, and 100% of the labeled examples in ImageNet. We compare Meta Pseudo Labels with supervised\\nlearning and RandAugment. We plot the results in Figure 5. From the ﬁgure, it can be seen that Meta Pseudo Labels delivers\\nsubstantial gains with less data, but plateaus as more labeled data becomes available. This result suggests that Meta Pseudo\\nLabels is more eff\\n}<->{\\naus as more labeled data becomes available. This result suggests that Meta Pseudo\\nLabels is more effective for low-resource image classiﬁcation problems.\\n20\\n\\n--------------------------------------------------\\nFigure 5: Performance of Supervised Learning, RandAugment, and Meta Pseudo Labels at different amounts of labeled examples.\\nE. Results with An Economical Version of Meta Pseudo Labels\\nMeta Pseudo Labels requires storing both the teacher model and the student model in memory. For model archi\\n}<->{\\neudo Labels requires storing both the teacher model and the student model in memory. For model architectures\\nwith a large memory footprint, such as EfﬁcientNet-L2 and EfﬁcientNet-B6-Wide in our experiments, this memory footprint\\nexceeds 16G of available memory in our accelerators. While we have implemented a hybrid data-model parallelism in\\nSection 4 which allows us to run Meta Pseudo Labels with large model architectures, the tradeoff is a slow and expensive\\ntraining procedure. To allow a more \\n}<->{\\nlarge model architectures, the tradeoff is a slow and expensive\\ntraining procedure. To allow a more efﬁcient training of large models with Meta Pseudo Labels, we design a more economical\\nalternative to instantiate the teacher, termed Reduced Meta Pseudo Labels.\\nIn Reduced Meta Pseudo Labels, we ﬁrst train a large teacher model T to convergence. Next, we use T to pre-compute\\nall target distributions for the student’s training data. Importantly, until this step, the student model has not been load\\n}<->{\\ns for the student’s training data. Importantly, until this step, the student model has not been loaded into\\nmemory, effectively avoiding the large memory footprint of Meta Pseudo Labels. Then, we parameterize a reduced teacher\\nT ′ as a small and efﬁcient network, such as a multi-layered perceptron (MLP), to be trained the along with student. This\\nreduced teacher T ′ takes as input the distribution predicted by the large teacher T and outputs a calibrated distribution for the\\nstudent to learn. In\\n}<->{\\n predicted by the large teacher T and outputs a calibrated distribution for the\\nstudent to learn. Intuitively, Reduced Meta Pseudo Labels works reasonably well because the large teacher T is reasonably\\naccurate, and hence many actions of the reduced teacher T ′ would be close to an identity map, which can be handled by an\\nMLP. Meanwhile, Reduced Meta Pseudo Labels retains the beneﬁt of Meta Pseudo Labels, as the teacher T ′ can still adapt to\\nthe learning state of the student θT .\\nTo evaluate wh\\n}<->{\\n Labels, as the teacher T ′ can still adapt to\\nthe learning state of the student θT .\\nTo evaluate whether Meta Pseudo Labels can scale to problems with a large number of labeled examples, we now turn to\\nfull labeled sets of CIFAR-10, SVHN and ImageNet. We use out-of-domain unlabeled data for CIFAR-10 and ImageNet. We\\nexperiment with Reduced Meta Pseudo Labels whose memory footprint allows our large-scale experiments. We show that\\nthe beneﬁt of Meta Pseudo Labels, i.e., having a teacher that adap\\n}<->{\\ne-scale experiments. We show that\\nthe beneﬁt of Meta Pseudo Labels, i.e., having a teacher that adapts to the student’s learning state throughout the student’s\\nlearning, stil extends to large datasets with more advanced architectures and out-of-domain unlabeled data.\\nModel Architectures.\\nFor our student model, we use EfﬁcinetNet-B0 for CIFAR-10 and SVHN, and use EfﬁcientNet-B7\\nfor ImageNet. Meanwhile, our teacher model is a small 5-layer perceptron, with ReLU activation, and with a hidden size o\\n}<->{\\nile, our teacher model is a small 5-layer perceptron, with ReLU activation, and with a hidden size of\\n128 units for CIFAR-10 and of 512 units for ImageNet.\\nLabeled Data.\\nPer standard practices, we reserve 4,000 examples of CIFAR-10, 7,300 examples from SVHN, and 40 data\\nshards of ImageNet for hyper-parameter tuning. This leaves about 45,000 labeled examples for CIFAR-10, 65,000 labeled\\nexamples for SVHN, and 1.23 million labeled examples for ImageNet. As in Section 3.2, these labeled data serve \\n}<->{\\nr SVHN, and 1.23 million labeled examples for ImageNet. As in Section 3.2, these labeled data serve as both the\\nvalidation data for the student and the pre-training data for the teacher.\\nUnlabeled Data.\\nFor CIFAR-10, our unlabeled data comes from the TinyImages dataset which has 80 million images [67].\\nFor SVHN, we use the extra images that come with the standard training set of SVHN which has about 530,000 images. For\\nImageNet, our unlabeled data comes from the YFCC-100M dataset which has 100 m\\n}<->{\\nut 530,000 images. For\\nImageNet, our unlabeled data comes from the YFCC-100M dataset which has 100 million images [65]. To collect unlabeled\\ndata relevant to the tasks at hand, we use the pre-trained teacher to assign class distributions to images in TinyImages and\\nYFCC-100M, and then keep K images with highest probabilities for each class. The values of K are 50,000 for CIFAR-10,\\n35,000 for SVHN, and 12,800 for ImageNet.\\n21\\n\\n--------------------------------------------------\\nBaselines.\\nWe compa\\n}<->{\\n and 12,800 for ImageNet.\\n21\\n\\n--------------------------------------------------\\nBaselines.\\nWe compare Reduced Meta Pseudo Labels to NoisyStudent [77], because it can be directly compared to Reduced\\nMeta Pseudo Labels. In fact, the only difference between NoisyStudent and Reduced Meta Pseudo Labels is that Reduced\\nMeta Pseudo Labels has a teacher that adapts to the student’s learning state.\\nMethods\\nCIFAR-10\\nSVHN\\nImageNet\\nSupervised\\n97.18 ± 0.08\\n98.17 ± 0.03\\n84.49/97.18\\nNoisyStudent\\n98.22 ± 0.05\\n\\n}<->{\\ns\\nCIFAR-10\\nSVHN\\nImageNet\\nSupervised\\n97.18 ± 0.08\\n98.17 ± 0.03\\n84.49/97.18\\nNoisyStudent\\n98.22 ± 0.05\\n98.71 ± 0.11\\n85.81/97.53\\nReduced Meta Pseudo Labels\\n98.56 ± 0.07\\n98.78 ± 0.07\\n86.87/98.11\\nTable 11: Image classiﬁcation accuracy of EfﬁcientNet-B0 on CIFAR-10 and SVHN, and EfﬁcientNet-B7 on ImageNet. Higher is better.\\nCIFAR-10 results are mean ± std over 5 runs, and ImageNet results are top-1/top-5 accuracy of a single run. All numbers are produced in\\nour codebase and are controlled experiments.\\n\\n}<->{\\n accuracy of a single run. All numbers are produced in\\nour codebase and are controlled experiments.\\nResults.\\nAs presented in Table 11, Reduced Meta Pseudo Labels outperforms NoisyStudent on both CIFAR-10 and ImageNet,\\nand is on-par with NoisyStudent on SVHN. In particular, on ImageNet, Meta Pseudo Labels with EfﬁcientNet-B7 achieves\\na top-1 accuracy of 86.87%, which is 1.06% better than the strong baseline NoisyStudent. On CIFAR-10, Meta Pseudo\\nLabels leads to an improvement of 0.34% in accuracy\\n}<->{\\n baseline NoisyStudent. On CIFAR-10, Meta Pseudo\\nLabels leads to an improvement of 0.34% in accuracy on NoisyStudent, marking a 19% error reduction.\\nFor SVHN, we suspect there are two reasons of why the gain of Reduced Meta Pseudo Labels is not signiﬁcant. First,\\nNoisyStudent already achieves a very high accuracy. Second, the unlabeled images are high-quality, which we know by\\nmanual inspection. Meanwhile, for many ImageNet categories, there are not sufﬁcient images from YFCC100M, so we end up\\nw\\n}<->{\\neanwhile, for many ImageNet categories, there are not sufﬁcient images from YFCC100M, so we end up\\nwith low-quality or out-of-domain images. On such noisy data, Reduced Meta Pseudo Labels’s adaptive adjustment becomes\\nmore crucial for the student’s performance, leading to more signiﬁcant gain.\\n22\\n\\n--------------------------------------------------\\n', 'Summaries': 'Meta Pseudo Labels achieves a top-1 accuracy of 90.2% on ImageNet, surpassing the previous state-of-the-art by 1.6%. It uses a semi-supervised learning method with a teacher network generating pseudo labels on unlabeled data.\\n}<->{\\nPseudo Labels in Meta Learning: \\nThe teacher network adapts constantly based on student performance.\\nThis leads to better pseudo labels being generated for teaching.\\n}<->{\\nPseudo Labels methods improve state-of-the-art models in computer vision by combining a teacher network with a student network. The teacher generates pseudo labels on unlabeled images, which are then combined with labeled images to train the model.\\n}<->{\\nPseudo labeled images are combined with accurately labeled images to train a student model. The use of abundant pseudo labeled data and regularization methods like data augmentation improves the student\\'s performance beyond that of its teacher [77]. However, inaccurate pseudo labels can lead to learning from incorrect data, affecting the student\\'s accuracy.\\n}<->{\\nStudent learns inaccurately and performs similarly to teacher due to confirmation bias in pseudo-labeling, a drawback known as pseudo-labeling bias. A new mechanism called Meta Pseudo Labels corrects this by observing the impact of student feedback on pseudo labels.\\n}<->{\\nLabels uses student performance feedback from GitHub at https://github.com/google-research/google-research/tree/master/meta_pseudo_labels to improve pseudo labels for teachers.\\n}<->{\\nThe student and teacher are trained in parallel with pseudo labels. The student learns from annotated data (teacher) and receives feedback on performance via rewards from a labelled dataset.\\n}<->{\\nA pair of EfficientNet-L2 networks is trained using Meta Pseudo Labels, achieving top-1 accuracy of 90.2% on ImageNet ILSVRC 2012 validation set, surpassing the previous record by 1.6%. The model also generalizes to ImageNet-ReaL test set with a high accuracy.\\n}<->{\\nOur model outperforms previous SOTAs on ImageNet-ReaL test set with Top-1 Accuracy (90.2) and Precision@1 (91.02). It surpasses Meta Pseudo Labels and outperforms FixMatch [58] and Unsupervised Data Augmentation [76].\\n}<->{\\nPrevious SOTA results:\\n- 88.6 (ImageNet ILSVRC 2012 validation set)\\n- 90.72 \\nOurs:\\n- 90.2\\n- 91.02 (ImageNet-ReaL test set)\\n}<->{\\nA labeled dataset is used to train a model. Key concepts include:\\n\\n- Student and Teacher models\\n- Pre-trained vs pseudo-labeled data\\n- Difference between Pseudo Labels and Meta Pseudo Labels \\n- Benefits of meta pseudo labels: more accurate than fixed pre-trained teacher-generated labels\\n}<->{\\nIn Meta Pseudo Labels, the teacher network is trained along with the student network. The student is trained on pseudo labels generated by the teacher, while the teacher is trained using performance on labeled data. \\n\\nNotations:\\n- T: Teacher network\\n- S: Student network\\n- θT: Teacher parameters\\n- θS: Student parameters\\n}<->{\\nParameters θT and θS are used in a teacher-student model setup. xl refers to unlabeled images, yl refers to labeled images. T(xu; θT ) represents soft predictions of the teacher network on unlabeled images, while S(xl; θS) and S(xu; θS) represent predictions by the student network for labeled and unlabeled images, respectively.\\n}<->{\\nCE(q, p) denotes the cross-entropy loss between distributions q and p. If q is a label, it\\'s considered as a one-hot distribution. CE(q, p) calculates the average instance loss for multiple instances in a batch. Meta Pseudo Labels use this concept to optimize pseudo labels for meta learning tasks.\\n}<->{\\nMeta Pseudo Labels: \\nPseudo Labels (PL) train student model to minimize cross-entropy loss on unlabeled data with:\\nθPL = argmin θS CE(T(xu; θT), S(xu; θS)) \\nwhere pseudo target T(xu; θT) is produced by a well-pretrained teacher model with fixed parameter θT.\\n}<->{\\nThe goal of Pseudo Labels is for the obtained θPL to achieve low loss on labeled data. The optimal student parameter θPL depends on the teacher parameter θT via pseudo targets T(xu; θT), expressed as θPL = θT. This dependency enables discussing Meta Pseudo Labels.\\n}<->{\\nThe ultimate student loss on labeled data Ll is also a function of θT and can be optimized with respect to θT: \\n\\nmin θT Ll(θPL(S(θT))), where θPL(S(θT)) = argmin θS Lu(θT, θS)\\n}<->{\\nMeta Pseudo Labels optimize teacher performance by adjusting pseudo labels to improve student performance. However, computing the gradient is complicated due to dependency on θT. A practical approximation is used to make Meta Pseudo Labels feasible.\\n}<->{\\nMeta Pseudo Labels feasible by approximating multi-step argmin with one-step gradient update of θS: θPL = θS - ηS · ∇θSLu, where ηS is learning rate.\\n}<->{\\nAbel\\'s equation:\\n\\n- Min θT\\n- θT - ηS · ∇θSLu\\n- θS \\n\\nwhere: \\n- θT is teacher model parameters\\n- θS is student model parameters\\n- Ll is loss function\\n- ηS is learning rate for student model\\n- ∇θSLu is gradient of student loss with respect to θS\\n}<->{\\nHard pseudo labels are used because they result in smaller computational graphs. They are necessary for large-scale experiments as fixed parameters θS can be ignored and higher-order dependency on θT is not a concern.\\n}<->{\\nSignificant performance difference between hard pseudo labels and a modified version of REINFORCE is not found. The student\\'s training relies on objective in Equation 1 with a changing thetaT parameter due to optimization.\\n}<->{\\nThe parameter θT is dynamically adjusted by the teacher\\'s optimization. The student\\'s updates can be used to approximate the teacher\\'s objective, leading to an alternating optimization between student and teacher updates.\\n}<->{\\nThe Meta Pseudo Labels method optimizes objective 1 using SGD: θ′ = θS − ηS∇θSLu(θT , θS), where the student updates its parameters θS to minimize a loss function Lu. The teacher then reuses this update to optimize objective 3 with SGD: θ′ T = θT − ηT ∇θT Ll + θS - ∇θSLu(θT , θS).\\n}<->{\\nTeacher\\'s training is augmented with two objectives: supervised learning on labeled data and semi-supervised learning on unlabeled data using UDA (Uncertainty-Sensitive Data Augmentation) objective.\\n}<->{\\nN labeled data using UDA objective [76]. See Appendix B, Algorithm 1 for full pseudo code of Meta Pseudo Labels. Student model trained with Meta Pseudo Labels can be fine-tuned on labeled data to improve accuracy after convergence.\\n}<->{\\nWe fine-tune models on labeled data to improve accuracy. The student\\'s tuning details are reported in our experiments. Experimental results will be presented, organized into two sections:\\n\\n* Small-scale comparisons of Meta Pseudo Labels against other methods on widely used benchmarks (Section 3)\\n* Large-scale experiments of Meta Pseudo Labels pushing limits (Section 4)\\n}<->{\\nMeta Pseudo Labels experiment at large scale and small scales: \\n• Large-scale experiments on ImageNet accuracy \\n• Small-scale studies, including a comparison with state-of-the-art semi-supervised learning methods.\\n}<->{\\nWe compare Meta Pseudo Labels with state-of-the-art semi-supervised learning methods on CIFAR-10-4K, SVHN-1K, ImageNet-10%. We also test it on ResNet-50 architecture with full ImageNet. Additionally, we evaluate Meta Pseudo Labels on the TwoMoon dataset to understand the role of feedback.\\n}<->{\\nTwoMoon dataset is a 2D dataset of 2,000 examples in two clusters. The dataset has 6 labeled examples (3 per cluster) and the remaining 1,994 unlabeled examples. It\\'s used to compare Meta Pseudo Labels with Supervised Learning and Pseudo Labels methods.\\n}<->{\\nSemi-supervised learning algorithms use 6 labeled examples to separate two clusters into correct classes based on h cluster. A feed-forward fully-connected neural network with two hidden layers (8 units each) is used. The sigmoid non-linearity is applied at each layer.\\n}<->{\\nMeta Pseudo Labels share architecture with teacher and student but have independent weights. Networks trained with SGD at 0.1 learning rate using uniform weight initialization between -0.1 and 0.1. No regularization applied.\\n}<->{\\nMeta Pseudo Labels achieve a higher success rate than Supervised Learning and Pseudo Labels in finding the correct classifier, with a notable improvement in handling clusterings.\\n}<->{\\nPseudoLabels fails to classify instances correctly due to incorrect pseudo-labels received from a bad classifier used in Supervised Learning. This results in misclassification and missing the opportunity to separate the two \"moons\".\\n}<->{\\nMeta Pseudo Labels performs better than Supervised Learning and Pseudo Labels on the TwoMoon dataset due to its ability to use feedback from the student model\\'s loss on labeled instances to adjust the teacher, generating better pseudo labels.\\n}<->{\\nInstances to adjust the teacher include:\\n\\n* Adding more pseudo labels \\n* Increasing dataset size \\n* Using larger model architectures \\n\\nThis helps Meta Pseudo Labels find a good classifier for datasets such as CIFAR-10-4K, SVHN-1K, and ImageNet-10%.\\n}<->{\\nCIFAR-10: 4,000 labeled, 41,000 unlabeled; test set 10,000.\\nSVHN: 1,000 labeled, 60,000 unlabeled.\\n}<->{\\nSVHN uses 1000 labeled examples and 603,000 unlabeled examples. \\nImageNet uses 128,000 labeled examples (10% of training set) and 1.28 million unlabeled examples, with a test set of 50,000 examples.\\n}<->{\\nThe test sets used are ILSVRC 2012 for ImageNet with 50,000 examples, CIFAR-10 and SVHN with 32x32 resolution, and CIFAR-10-4K and SVHN-1K with WideResNet-28-2 architecture (1.45 million parameters) and ResNet-50 (25.5 million parameters) for ImageNet respectively.\\n}<->{\\nFor ImageNet, ResNet-50 is used with 25.5 million parameters. \\nPrevious works commonly use this architecture. \\nDefault hyper-parameters from previous work are used, except for modifications to RandAugment.\\nAll hyper-parameters are reported in Appendix C.4.\\n}<->{\\nWe fine-tune the student on the labeled dataset using SGD with a fixed learning rate of 10−5 and batch size of 512, running for 2,000 steps for ImageNet-10%, 1,000 steps for CIFAR-10, and 1,000 steps for SVHN.\\n}<->{\\nWe don\\'t use heldout validation sets, returning the model at final checkpoint instead. Baselines are compared using same architectures, excluding Larger-WideResNet-28-2 and Pyramid-Net+ShakeDrop for CIFAR-10/SVHN, and ResNet variants for ImageNet-10%.\\n}<->{\\nResNet-50, ResNet-101, ResNet-152 are compared on ImageNet-10% dataset for Meta Pseudo Labels. Two baselines (Supervised Learning and self-distillation) are not used for comparison as they may benefit from larger architectures and distillation.\\n}<->{\\nMeta Pseudo Labels are compared against two baselines: \\nSupervised Learning with full dataset and Un-supervised Data Augmentation (UDA). The former uses all labeled data unfairly, while UDA is used for teacher training in Meta Pseudo Labels.\\n}<->{\\nls uses UDA in training the teacher, following [48]\\'s train/eval/test splitting. The same experimental protocols are used for baselines and Meta Pseudo Labels, ensuring fairness. Other semi-supervised baselines include Label Propagation and Self-Supervision.\\n}<->{\\nMeta Pseudo Labels is compared to Label Propagation and Self-Supervised methods. The comparison is not direct due to different environments, and should be contextualized as suggested by [48].\\n}<->{\\nCIFAR-10-4K, SVHN-1K \\nMean Top-1 and Top-5 accuracies for various label propagation methods: \\n Temporal Ensemble: 83.63 ± 0.63 (Top-1), 92.81 ± 0.27 (Top-5)\\n Mean Teacher: 84.13 ± 0.28, 94.35 ± 0.47\\n VAT + EntMin: 86.87 ± 0.39, 94.65 ± 0.19 \\n ICT: 92.71 ± 0.02, 96.11 ± 0.04\\n}<->{\\n7.0 ± 0.3 \\nImageNet-10%\\nSimCLR:71.7%\\nMOCOv2:71.1%\\nPCL:85.6%\\nPIRL:84.9%\\nBYOL:68.8%\\n}<->{\\nTable 2: Image classification accuracy on CIFAR-10-4K, SVHN-1K, and ImageNet-10%. \\n\\nCIFAR-10-4K: mean ± std over 10 runs\\nSVHN-1K: mean ± std over 10 runs\\nImageNet-10%: Top-1/Top-5 accuracy of a single run.\\n}<->{\\n10% ∗ indicates implementation of same experimental protocols as Meta Pseudo Labels significantly outperforms UDA in fair comparisons with other methods, according to Table 2 in Appendix D.\\n}<->{\\nMeta Pseudo Labels significantly improves over UDA on CIFAR-10-4K and ImageNet-10%, with top-1 accuracy exceeding 5% in some cases.\\n}<->{\\nMeta Pseudo Labels has achieved the best accuracies among same model architectures on CIFAR-10-4K, SVHN-1K, and ImageNet-10%, outperforming highest reported base-lines in some cases by up to 10% relative error reduction.\\n}<->{\\nMeta Pseudo Labels outperforms SimCLR by 2.19% top-1 accuracy on current datasets, with notable results achieved using larger models or stronger regularization techniques.\\n}<->{\\nWideResNet-28-2 with ShakeDrop regularization achieves 80.9% top-1 accuracy on ImageNet-10%. In comparison, SimCLRv2 with self-distillation and a larger ResNet-152 model achieves the same accuracy.\\n}<->{\\nMeta Pseudo Labels outperforms other semi-supervised learning methods on CIFAR-10-4K, SVHN-1K, and ImageNet-10%. This experiment benchmarks Meta Pseudo Labels on the entire ImageNet dataset plus unlabeled images from JFT.\\n}<->{\\nMeta Pseudo Labels works well on ResNet-50 architecture, but large-scale experiments were conducted on EfficientNet after this success. Datasets used include: all labeled ImageNet examples (75,000), reserved for hyper-parameter tuning and model selection; ILSVRC 2012 validation set for testing; and 12.8 million unlabeled JFT dataset images.\\n}<->{\\nWe take 12.8 million unlabeled images from the JFT dataset by training a ResNet-50 on the entire ImageNet training set and assigning class probabilities to JFT images, then selecting the top images for each of the 1,000 classes.\\n}<->{\\n100 words or less:\\n\\nImageNet consists of 1,000 classes with 12.8 million images. The dataset excludes overlapping validation set images from ILSVRC 2012. This approach has been used in UDA [76] and Noisy Student [77]. For this experiment, a larger batch size and more training steps are employed to accommodate the bigger datasets.\\n}<->{\\nBatch size: \\n- Labeled images: 4096 \\n- Unlabeled images: 32768 \\n\\nTraining steps: \\n- Unlabeled dataset: 500,000 steps ≈ 160 epochs\\n}<->{\\nBaselines include supervised learning methods with data augmentation (AutoAugment, DropBlock) or regularization (CutMix), which are state-of-the-art for ResNet-50, and a second group of unlisted baselines.\\n}<->{\\nSelines represent state-of-the-art supervised learning methods on ResNet-50. Two groups of baselines are compared: \\n\\n* Supervised learning methods using ImageNet for labeled training images.\\n* Semi-supervised learning methods leveraging ImageNet and other unlabeled datasets (YFCC100M, JFT).\\n}<->{\\nMeta Pseudo Labels boost top-1 accuracy of ResNet-50 from 76.9% to 83.2%, outperforming UDA and Noisy Student, by a large margin.\\n}<->{\\nMethod Comparison on UDA and Noisy Student:\\n\\n- Supervised: 76.9/93.3 (top-1/top-5)\\n- AutoAugment: 77.6/93.8 \\n- DropBlock: 78.4/94.2\\n- FixRes: 79.1/94.6\\n- FixRes+CutMix: 79.8/94.9\\n- NoisyStudent (JFT): 78.9/94.3\\n- UDA (JFT): 79.0/94\\n}<->{\\n79.1/94.6 (FixRes+CutMix)\\n79.8/94.9 (NoisyStudent)\\n78.9/94.3 (UDA)\\n79.0/94.5 (Billion-scale SSL)\\n82.5/96.6 (Meta Pseudo Labels)\\n83.2/96.5 (JFT)\\n}<->{\\nMeta Pseudo Labels is scaled up to train a large model (EfﬁcientNet-L2) on a large dataset (ImageNet) to achieve ImageNet accuracy. EfﬁcientNet-L2\\'s higher capacity compared to ResNets is utilized. The top-1 accuracy of 88.4% was previously achieved by Noisy Student using this architecture on ImageNet, with the JFT dataset used as un-labeled data.\\n}<->{\\nThe JFT dataset is used as un-labeled data, with 130 million images filtered down from 300 million. The EfficientNet-L2 architecture is used due to its state-of-the-art performance on ImageNet without extra labeled data.\\n}<->{\\nWe used 512x512 labeled data with hyper-parameters similar to Noisy Student, increasing image resolution for model parallelism. We also experimented with the EfficientNet-B6-Wide model, which has increased width factor from 2.1 to 5.0 compared to B6.\\n}<->{\\nThe model EfficientNet-B6-Wide has 390 million parameters. It is almost as performant as EfficientNet-L2 but faster compilation and training time due to wider width factor. However, it cannot be used with the same teacher-student setup as it would exceed memory constraints.\\n}<->{\\nResults for various neural networks:\\n\\n* ResNet-50: Top-1 77.8%, Top-5 93.0, Precision@1 82.94%\\n* ResNet-152: Top-1 77.8%, Top-5 93.8, Precision@1 84.79%\\n* DenseNet-264: Top-1 77.9%, Top-5 93.9\\n* Inception-v3: Top-1 78.8%, Top-5 94.4, Precision@1 83.58%\\n* Xception: Top-1 79.0%, Top-5 94.5\\n* Inception-v4: Top-1 80.0%, Top-5 95.0\\n* Inception-resnet-v2: Top-1 80.1%, Top-5 95.1\\n}<->{\\nHere is a summary of the text in original language with useful information below 100 words:\\n\\nThe listed models are evaluated for their performance on image classification tasks. The models range from smaller ones like Inception-v4 (48M parameters) to larger ones like GPipe (557M parameters). Models like ResNeXt-101, PolyNet, and PNASNet achieve high accuracy scores ranging from 95.1% to 96.5%. The largest model, AmoebaNet-C + AutoAugment, achieves an accuracy of 96.5%, while the smallest models, Inception-v4 and EfficientNet-B7, achieve lower accuracy scores around 95.0% and 85.3%, respectively.\\n}<->{\\nEfﬁcientNet-B7 + FixRes: 66M, 97.4%\\nEfﬁcientNet-L2: 480M, 97.5%\\nResNet-50 Billion-scale SSL: 26M, 96.0%\\nResNeXt-101 Billion-scale SSL: 193M, 84.8%\\nResNeXt-101 WSL: 829M, 97.6% \\nFixRes ResNeXt-101 WSL: 829M, 98.0% \\nBig Transfer (BiT-L): 928M, 98.5%\\nNoisy Student (EfﬁcientNet-L2): 480M, 88.4%\\n}<->{\\n300M labeled JFT, 87.5% top-1 accuracy\\nNoisy Student (EfﬁcientNet-L2) [77]: 98.5%, 90.54%\\n480M, 88.4% top-1 accuracy\\n300M unlabeled JFT: \\n- Noisy Student + FixRes [70]: 88.5%, 98.7%\\nVision Transformer (ViT-H) [14]: 90.72%,  - \\nEfﬁcientNet-L2-NoisyStudent + SAM [16]: 88.6%, 98.6%\\n}<->{\\nEfﬁcientNet-L2 \\n480M\\n300M unlabeled JFT \\n90.2 \\n98.8 \\n91.02\\n}<->{\\nA hybrid model combining data and model parallelism is designed to run Meta Pseudo Labels. A cluster of 2,048 TPUv3 cores are divided into 128 identical replicas for standard data parallelism with synchronized gradients. Each replica runs on 16 cores and implements two types of model parallelism: \\n\\n1. Patch splitting along the width dimension into 16 patches\\n2. Resolution 512x512 input image is split\\n}<->{\\nInput images (512x512) are split along the width dimension into 16 patches of equal size (512x32), distributed to 16 cores for processing. Each weight tensor is also split equally into 16 parts, assigned to the 16 cores.\\n}<->{\\nThe model is split into 16 equal parts, each assigned to a core. It uses hybrid data-model parallelism in the XLA-Sharding framework. Each part can handle 2048 labeled and 16384 unlabeled images during training. The model trains for 1 million steps, taking approximately 11 days for EfficientNet-L2 and 10 days for EfficientNet-B6-Wide after completing Meta Pseudo Labels training phase.\\n}<->{\\nMeta Pseudo Labels achieves 90.2% top-1 accuracy on ImageNet, surpassing previous state-of-the-art results by 1.8%.\\n}<->{\\nOur Meta Pseudo Labels outperforms recent results by BiT-L [33] and the previous state-of-the-art by Vision Transformer [14], with notable improvements over these models that pre-train on 300 million labeled images from JFT.\\n}<->{\\nOur gain of 1.6% over a previous dataset is significant compared to recent gains (0.05% for Vision Transformer and 0.1% for FixRes). We also tested our model on the ImageNet-ReaL test set, verifying it does not overfit to the validation set.\\n}<->{\\nOur model achieved 91.02% Precision@1 on ImageNet-ReaL test set, outperforming Vision Transformer by 0.4%, with a larger gap than between Vision Transformer and Noisy Student (0.17%). A lite version of Meta Pseudo Labels was designed to reduce training cost.\\n}<->{\\nReduced Meta Pseudo Labels: a lite version of Meta Pseudo Labels, achieving 86.9% top-1 accuracy on ImageNet ILSRVC 2012 validation set with EfficientNet-B7, using ImageNet training set as labeled data and YFCC100M as unlabeled data.\\n}<->{\\nPseudo Labels allows implementing Meta Pseudo Labels\\' feedback mechanism without keeping two networks in memory. It is a Semi-Supervised Learning approach successfully applied to tasks like image classification, object detection, semantic segmentation, machine translation, and speech recognition.\\n}<->{\\nVanilla Pseudo Labels: pre-trained teacher kept fixed, leading to confirmation bias when pseudo labels are inaccurate.\\n\\nMeta Pseudo Labels adapt teacher to improve student\\'s performance, generating better pseudo labels.\\n}<->{\\nTypical SSL methods train a single model by optimizing an objective function combining:\\n\\n* Supervised loss (cross-entropy) on labeled data\\n* Unsupervised loss (self-supervised or pseudo-supervision) on unlabeled data.\\n}<->{\\nSelf-supervised loss encourages models to develop a common sense about images through tasks such as in-painting and contrastive prediction. Label propagation loss enforces model invariance by enforcing similar labels on similar inputs.\\n}<->{\\nMeta Pseudo Labels differs from other SSL methods in two aspects:\\n\\n* The student model never learns directly from labeled data.\\n* This avoids overfitting, particularly with limited labeled data.\\n}<->{\\nThe Meta Pseudo Labels method learns from labeled data directly, avoiding overfitting. The teacher uses its softmax predictions on unlabeled data as novel \"soft labels\" to teach the student, effectively utilizing limited labeled data.\\n}<->{\\nSoft labels are commonly used in knowledge distillation and have improved model generalization. They include manually designed methods such as label smoothing and temperature sharpening, which adjust training example labels for better optimization.\\n}<->{\\nDS adjusts training example labels for improved optimization and generalization, without student performance feedback, similar to other SSL methods. It uses bi-level optimization algorithms with a method named Meta that derives update rules from student\\'s feedback.\\n}<->{\\nThe technique of deriving the teacher\\'s update rule from a student\\'s feedback is based on a bi-level optimization problem common in meta-learning. Similar problems include optimizing learning rates, designing architectures, correcting labels, generating examples and re-weighting data.\\n}<->{\\nMeta Pseudo Labels uses a bi-level optimization technique to derive the teacher\\'s gradient from student feedback, unlike other methods that apply this technique separately for generating training examples and re-weighting data.\\n}<->{\\nMeta Pseudo Labels method for semi-supervised learning uses teacher model feedback to generate labels, updating the student\\'s model based on these pseudo labels.\\n}<->{\\nThe Meta Pseudo Labels method updates both students and teachers based on performance data, resulting in improved accuracy over existing semi-supervised learning methods. It achieved 90.2% top-1 accuracy on ImageNet, surpassing the previous standard by 1.6%.\\n}<->{\\nDense neural networks have achieved 90.2% top-1 accuracy on ImageNet, surpassing the previous state-of-the-art by 1.6%. The consistent gains confirm the benefit of student\\'s feedback to teacher.\\n}<->{\\nVain Gelly, Geoff Hinton, Mohammad Norouzi, and Colin Raffel contributed to the paper. Jaime Carbonell provided crucial advice on removing the data loading bottleneck for ResNets, helping the team overcome hardware limitations.\\n}<->{\\nTensorFlow: A system for large-scale machine learning by Martín Abadi et al., published in the USENIX Symposium on Operating Systems Design and Implementation, describes a machine learning framework that enables efficient processing of complex algorithms.\\n}<->{\\nArXiv papers:\\n\\n* \"Pseudo-labeling and confirmation bias in deep semi-supervised learning\" (2019) explored the issue of pseudo-labeling in deep learning, showing it can introduce confirmation bias.\\n* \"Online learning rate adaptation with hypergradient descent\" (2019) introduced a method to adapt the learning rate for online learning using hypergradient descent.\\n}<->{\\nEarning rate adaptation with hypergradient descent was presented in:\\n- International Conference on Learning Representations, 2018.\\nSemi-supervised learning methods were also explored by:\\n- Remix-match: David Berthelot et al., International Conference on Learning Representations, 2020.\\n}<->{\\nDavid Berthelot et al., Nicolas Papernot et al., Avital Oliver et al., Colin Raffel et al., Lucas Beyer et al., Olivier J Hénaff et al., Alexander Kolesnikov et al., Xi-aohua Zhai et al., Aäron van den Oord.\\n \\n David Berthelot et al. MixMatch: a holistic approach to semi-supervised learning.  Advances in Neural Information Processing Systems, 2019. Lucas Beyer et al. Are we done with ImageNet? arXiv preprint arXiv:2006.07159, 2020. Olivier Chapelle et al. Semi-Supervised Learning. The MIT Pre\\n}<->{\\nHere is a summary of the references:\\n\\n* Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien: \"Semi-Supervised Learning\" (2010)\\n* Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton: Contrastive Learning of Visual Representations (2020)\\n* Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton: Big Self-Supervised Models are Strong Semi-Supervised Learners\\n}<->{\\nBig self-supervised models are strong semi-supervised learners according to Advances in Neural Information Processing Systems, 2020. Improved baselines were achieved using momentum contrastive learning (Xinlei Chen et al., 2020). Xception (Francois Chollet, 2017) and other architectures have also contributed to advancements in deep learning with depthwise separable convolutions.\\n}<->{\\nAutoAugment (2019) and Randaugment (2020) are data augmentation methods that learn optimal augmentation policies from data. AutoAugment improves on Randaugment by using a separate search space for augmentation parameters.\\n}<->{\\nTwo research papers were published in 2020:\\n\\n- \"Transformers for image recognition at scale\" by A Dosovitskiy et al. (2020) explored the use of transformers for image recognition tasks.\\n- The authors suggested that an image is worth 16x16 words, implying that images can be represented using a large amount of text data.\\n}<->{\\nFinn et al. (2017) Model-agnostic meta-learning for fast adaptation of deep networks. \\n\\nPierre Foret et al. (2020) Sharpness-aware minimization for efficiently improving generalization.\\n\\nTommaso Furlanello et al. (2018) Born again neural networks.\\n}<->{\\nBorn Again Neural Networks: Laurent Itti, Anima Anandkumar, and Channen.\\nDropblock: Golnaz Ghiasi, Tsung-Yi Lin, Quoc V. Le.\\nUnsupervised Representation Learning by Predicting Image Rotations: Spyros Gidaris, Praveer Singh, Nikos Komodakis.\\nPresented at:\\n- International Conference on Machine Learning (2018)\\n- Advances in Neural Information Processing Systems (2018)\\n- IEEE Conference on Computer Vision and Pattern Recognition\\n}<->{\\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In International Conference on Computer Vision, 2005.\\n}<->{\\nIres, Zhaohan Daniel Guo, Moham-mmad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi Monus, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning (Advances in Neural Information Processing Systems, 2020). \\n\\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio Ranzato. Revisiting self-training for neural sequence generation (International Conference on Learning Representations, 2020).\\n}<->{\\nMomentum Contrast for Unsupervised Visual Representation Learning (2020)\\nIn International Conference on Learning Representations.\\n\\nDeep Residual Learning for Image Recognition (2016)\\nIn IEEE Conference on Computer Vision and Pattern Recognition.\\n}<->{\\n\"Data-Efficient Image Recognition via Contrastive Predictive Coding\". \\nBy Olivier J. Henaff et al., 2020. \\n\\nKey aspects:\\n- Approach: uses contrastive predictive coding\\n- Benefits: data-efficient image recognition\\n- Implication: neural networks can learn from limited training data\\n}<->{\\nHere is a summary of the mentioned papers in original language with useful information below 100 words:\\n\\nThe papers discuss various neural network architectures. Squeeze-and-excitation networks [27] improve image representation by selectively reducing channel attention. Densely connected convolutional networks [28] utilize residual connections for feature reuse. Another approach, DenseNets, introduces skip connections between consecutive layers to preserve features. These techniques enhance the efficiency and performance of deep neural networks in computer vision tasks such as object detection, segmentation, and image recognition.\\n}<->{\\nYanping Huang et al., \\'GPipe: Efﬁcient training of giant neural networks using pipeline parallelism\\', Advances in Neural Information Processing Systems, 2019.\\nJacob Jackson and John Schulman, \\'Semi-supervised learning by label gradient alignment\\', Arxiv, 1902.02336, 2019.\\nJacob Kahn, Ann Lee, and Awni Hannun, \\'Self-training for end-to-end speech recognition\\', IEEE International Conference on\\n}<->{\\nAwni Hannun self-trained end-to-end speech recognition. Zhanghan Ke et al broke limits in semi-supervised learning with \\'Dual Student\\'. Alexander Kolesnikov et al proposed Big Transfer Bit for gene expression analysis.\\n}<->{\\nResearchers in visual representation learning include:\\naohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\\nNotable works include Alex Krizhevsky\\'s \\'Learning multiple layers of features from tiny images\\' (2009) \\nand Samuli Laine and Timo Aila\\'s \\'Temporal ensembling for semi-supervised learning\\' (2017).\\n}<->{\\nPseudo-Label was proposed as a semi-supervised learning method for deep neural networks (DNNs) in ICML Workshop, 2013. GSHARD scaled giant models using conditional computation and automatic sharding on Arxiv, 2006.16668, in 2020.\\n}<->{\\nScaling giant models with conditional computation and automatic sharding is explored in:\\nArxiv: 2006.16668 (2020)\\nArxiv: 2005.04966 (2020)\\nArxiv: 2004.03906 (missing but mentioned at no. 38)\\n}<->{\\nHanxiao Liu, Karen Simonyan, and Yiming Yang\\'s \\'DARTS\\' (Differentiable Architecture Search) is a progressive neural architecture search. \\nIlya Loshchilov and Frank Hutter\\'s \\'SGDR\\' (Stochastic Gradient Descent with Warm Restarts) is used for hyperparameter tuning in DARTS.\\n}<->{\\nDhruv Mahajan et al., \"Exploring the limits of weakly supervised pretraining\" (ECCV 2018), and Ishan Misra and Laurens van der Maaten, \"Self-supervised learning of pretext-invariant representations\" (IEEE CVPR).\\n}<->{\\nTakeru Miyato et al.\\'s work on virtual adversarial training (VAT) was presented at IEEE Conferences, 2020. It introduced a regularization method for supervised and semi-supervised learning, improving performance against adversarial attacks. VAT was also discussed in Rafael Müller et al.\\'s paper \"When does label smoothing help?\", which explored the effects of label smoothing on neural network performance.\\n}<->{\\nLabel smoothing helps when true labels are unknown or noisy.\\n}<->{\\nMehdi Noroozi and Paolo Favaro developed a method for unsupervised learning of visual representations using jigsaw puzzles. \\n\\nAvital Oliver et al evaluated realistic performance of deep semi-supervised learning algorithms.\\n\\nDaniel S. Park et al presented work on visually grounded language understanding.\\n}<->{\\nDaniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, and Quoc V. Le. Improved noisy student training for automatic speech recognition. In Inter-speech, 2020.\\nDeepak Pathak, Philipp Krahenbühl, Jeff Donahue, Trevor Darrell, and Alexei A. Efrös. Context encoders: Feature learning by inpainting. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.\\nIlija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, and Kaiming He. Darts\\n}<->{\\nHere are the summaries in original language:\\n\\n* Ilija Radosavovic et al. presented Data distillation: Towards omni-supervised learning at IEEE Conference on Computer Vision and Pattern Recognition (2018).\\n* Esteban Real et al. presented Regularized evolution for image classiﬁer architecture search at AAAI conference on artificial intelligence (year not specified).\\n}<->{\\nSiﬁer architecture is a type of deep learning model that improves performance in semi-supervised learning by reweighting examples. It was presented in the Proceedings of the AAAI conference on artificial intelligence (2019) and International Conference on Machine Learning (2018).\\n}<->{\\nEllen Riloff (1996) & Olga Russakovsky et al (2015) introduced extraction patterns from untagged text in AI research.\\n}<->{\\nLarge Scale Visual Recognition Challenge published in International Journal of Computer Vision in 2015 is not related to the text. \\n\\nH Scudder\\'s paper \"Probability of error of some adaptive pattern-recognition machines\" was published in 1965 and has no relation to Large Scale Visual Recognition Challenge.\\n\\nKihyuk Sohn et al\\'s Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence was published in IEEE Conference on Computer Vision.\\n}<->{\\nEemi-supervised learning with consistency and confidence was presented at the 2020 IEEE Conference on Computer Vision and Pattern Recognition.\\n}<->{\\nChristian Szegedy et al.\\'s work revisits the effectiveness of data in deep learning, published in Proceedings of the IEEE international conference on computer vision, 2017. They also explored Inception-v4, Inception-ResNet with residual connections and a new architecture called Rethink [62]\\n}<->{\\nHere is a summary of the text in original language below 100 words:\\n\\nChristian Szegedy et al. (2016) rethought the Inception architecture for computer vision. Mingxing Tan and Quoc V. Le (2019) introduced EfficientNet, which scales models effectively for convolutional neural networks. Antti Tarvainen and Harri Valpola (no publication details provided) developed \"Mean teachers are better role models\", a concept that involves weight averaging between teacher and student networks to improve performance in deep learning tasks.\\n}<->{\\nAntti Tarvainen and Harri Valpola (2019). \\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.\\n\\nBart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li (2016). YFCC100M: The new data in multimedia research.\\n\\nT. Tieleman and G. Hinton (no date given). \\nRmsProp: Division of variance.\\n}<->{\\nTieleman and Hinton introduced RMSProp, a gradient optimization method that divides the gradient by its running average of recent magnitude. \\nAntonio Torralba et al. created an image dataset with 80 million tiny images for non-parametric object and scene recognition.\\nHugo Touvron et al. published in IEEE Transactions on Pattern Analysis and Machine Intelligence.\\n}<->{\\nHugo Touvron et al published \"Fixing the train-test resolution discrepancy\" in two different papers (2008 and 2019). The issue concerns data resolution disparity between training and testing datasets, affecting model performance. They proposed solutions to address this problem.\\n}<->{\\n70] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. \\nFixing the train-test resolution discrepancy using Fixefﬁcientnet (2020).\\n\\n71] Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. \\nInterpolation consistency training for semi-supervised learning in IJCAI 2019.\\n\\n72] Xiao Wang, Daisuke Kihara, Jiebo Luo, and Guo-Jun Qi.\\nEnaet: Self-trained ensemble autoencoding transformations.\\n}<->{\\nDaisuke Kihara et al. proposed self-trained ensemble autoencoding transformations for semi-supervised learning (2019). Xinyi Wang et al. optimized data usage via differentiable rewards (2020). Yulin Wang et al. introduced Meta-Semi, a meta-learning approach for semi-supervised learning.\\n}<->{\\nShiji Song, Gao Huang, and Meta-: A meta-learning approach for semi-supervised learning, published in 2020. It is inspired by Ronald J. Williams\\' Simple statistical gradient-following algorithms (1992) and Qizhe Xie et al.\\'s Unsupervised data augmentation for consistency training (2020).\\n}<->{\\nQizhe Xie et al. (2020) achieved self-training with noisy student improving imagenet classification [77]. Saining Xie et al. (2020) introduced aggregated residual transformations for deep neural networks [78].\\n}<->{\\nHere is a summary of the text in original language with useful information below 100 words:\\n\\nBillion-scale semi-supervised learning approach for image classification was presented at IEEE Conference on Computer Vision and Pattern Recognition (CVPR) in 2017 by I. Zeki Yalniz et al. Shakedrop regularization was introduced by Yoshihiro Yamada et al. in 2018 to improve deep residual learning. David Yarowsky also worked on Unsupervised, but no further details are provided. These papers explore techniques for improving deep neural network performance, particularly in large-scale semi-supervised image classification tasks.\\n}<->{\\nRegularization for deep residual learning: \\n- ArXiv, 1802.0237 (2018)\\n- Improves model performance and generalizability by reducing overfitting.\\n\\nUnsupervised word sense disambiguation:\\n- David Yarowsky, 1995\\n- Achieved results rivaling supervised methods.\\n\\nLarge batch training of convolutional networks: \\n- ArXiv, 1708.03888 (2017)\\n- Speeds up training by allowing larger batch sizes and reducing gradient noise.\\n}<->{\\nSangdoo Yun et al., CutMix: Regularization strategy to train strong classiﬁers with localizable features, 2019.\\nSergey Zagoruyko and Nikos Komodakis, Wide residual networks, 2016.\\nHongyi Zhang et al., mixup: Beyond empirical risk minimization,  (Year not specified).\\n}<->{\\nCisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018. \\nLinfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Cheng-long Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In International Conference on Computer Vision, 2019. \\nXingcheng Zhang, Zhizhong Li, Chen Change Loy, and Dahua Lin. Polynet: A pursuit of structure in deep networks.\\n}<->{\\nXingcheng Zhang, Zhizhong Li, Chen Change Loy, Dahua Lin, Guoqing Zheng, Ahmed Hassan Awadallah, Susan Dumais, Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D Cubuk, and Quoc V Le.\\n}<->{\\nLnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D Cubuk, and Quoc V Le developed a method for rethinking pre-training and self-training in Advances in Neural Information Processing Systems, 2020. \\n\\n Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le created a method for learning transferable architectures for scalable image recognition in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.\\n}<->{\\nDerivation of the Teacher\\'s Update Rule \\n\\nThe teacher\\'s update rule was derived mathematically using the chain rule. It involves the standard Jacobian notations, specifically denoting the Jacobian matrix as ∂f/∂x ∈ Rn×m for a function f : Rm → Rn and vector x ∈ Rm.\\n}<->{\\nThe Jacobian matrix ∂f/∂x of a function f is an n×m matrix. When referring to multiple points, we use ∂f/∂x1 and ∂f/∂x2 separately. A vector v in Rn is treated as a column matrix (n×1). The gradient vector of a multi-variable function is the transpose of its Jacobian.\\n}<->{\\nThe gradient vector of a multi-variable real-valued function is the transpose of its Jacobian matrix. Matrix multiplications use column vectors as operands, and dimensions are annotated to avoid confusion.\\n}<->{\\nThe dimensions of θS and θT are denoted as |S|×1 and |T|×1 respectively.\\n\\nThe derivation involves:\\n\\n- Unlabeled examples xu\\n- Teacher samples pseudo labels byu ∼T(xu; θT) with dimension |T|\\n- Student updates its parameter θS using (xu, byu)\\n- Expected new parameter of student Ebyu∼T (xu;θT )\\n}<->{\\n(xu, byu) updates θS to Ebyu∼T (xu;θT ). Student\\'s new parameter is \\nEbyu∼T (xu;θT ). Teacher\\'s parameter is updated to minimize student\\'s cross-entropy on a batch of labeled data. The Jacobian is computed as:\\n∂R/∂θT = ∂/∂θT CE(y, S(xu; θS)) / Ebyu∼T (xu;θT)\\nwhere \\nθS −ηS∇ηSCE(byu, S(xu; θS))\\nis defined.\\n}<->{\\nThe partial derivative of R with respect to θT is given by:\\n\\n∂R/∂θT = ∫[y_l, S(x_l; θ_S)] ∂CE[y_l, S(x_l; θ_T)]/∂θ_S dy_s \\n\\n= ∂CE[y_l, S(x_l; θ_T)]/∂θ_S |_θS=¯θ′S\\n}<->{\\nThe Jacobian matrix for the second term can be computed using back-propagation. \\n\\n∂¯θ′\\nS\\n∂θT\\n= ∂/∂θT \\nEbyu∼T (xu;θT )\\n- ηS ∇ηSCE(byu, S(xu; θS))\\n\\nThe Jacobian of CE (byu, S(xu; θS)) is 1 × |S| and needs to be transposed.\\n}<->{\\nThe Jacobian of CE needs to be transposed from 1 × |S| to |S| × 1. Since θS does not depend on θT, it can be left out of subsequent derivations. The gradient gS(byu) is defined as the partial derivative of CE with respect to θS. Equation 7 becomes ∂¯θ′S/∂θT = −ηS · (∂Ebyu∼T/∂θT).\\n}<->{\\nReinforcement Learning Equation for Policy Update: \\n\\n∂¯θ(t+1) = -ηS · Ebyu∼T (xu;θT ) h gS(byu) | {z} |S|×1 i \\n= ηS · Ebyu∼T (xu;θT ) h gS(byu) | {z} |S|×1 i  ∂log P (byu|xu; θT )\\n    = ηS · Ebyu∼T (xu;θT ) h gS(byu) | {z} |S|×1 i  · ∂CE (byu, T(xu; θT ))\\n}<->{\\n∂R/∂θT = ∂CE(yl, S; xl; ¯θ′S) / ηS \\n∂θS = ∂CE(yl, S; xl; ¯θ′S) \\nθS = ¯θ′S.\\n}<->{\\nWe approximate ∂θT with the Monte Carlo method using sampled byu values. We update the student parameter θS to obtain θ′S, then use this updated parameter to approximate the expected value in Equation 11. This involves solving for the parameter obtained from θS using the given equation and sampling byu.\\n}<->{\\nθS = θS - ηS · ∂CE(byu, S(xu; θS)) ∇θS = ηS · ∂CE(byu, S(xl; θ′S)) ∇θT Lu(θT, θS) = ∂CE(byu, T(xu; θT)).\\n}<->{\\nθSCE (byu, S(xu; θS)) and ∇θT CE (byu, T(xu; θT )) are used in Meta Pseudo Labels with UDA.\\n\\nPseudo Code for Meta Pseudo Labels:\\n1. Train teacher model on pseudo labeled data with UDA loss.\\n2. Use teacher\\'s output as pseudo labels for student model.\\n3. Student model learns from pseudo labeled data.\\n4. Apply UDA objective to teacher during training.\\n}<->{\\nThe Meta Pseudo Labels method updates a student model with pseudo labels generated from labeled data. \\n\\n- Input: Labeled (xl, yl) and unlabeled data (xu).\\n- Initialize teacher model θT and student model θS.\\n- For N iterations:\\n  - Sample unlabeled xu and labeled xl.\\n  - Generate pseudo label byu using the teacher model θT with xu.\\n  - Update student model: θS = θS - ηS ∇θSCE(byu, S(xu; θS)|θS)\\n}<->{\\nUpdate student\\'s model: \\nθ(t+1) = θ(t) - ηS * ∇θSCE(byu, S(xu; θS)|θS=θ(t)) * S\\n\\nTeacher\\'s feedback coefficient:\\nh = ηS * ∇θ\\'SCE / (∇θSCE(byu, T(xl; θT)) * yl)\\n\\nTeacher\\'s gradient:\\ng(t) = h * ∇θTCE(byu, T(xu; θT))|θT=θ(t)\\ng(t,supervised) = ∇θTCE(yl, T(xl; θT))|θT=θ(t)\\ng(t,uda) = ∇θTCE(byu, S(xu; θS)|θS=θ(t))\\n}<->{\\nTA: UDA Loss \\n\\ng(T) = ∇θT CE(yl, T(xl; θT)) \\ng(T,UDA) = StopGradient(T(xl); θT) + StopGradient(RandAugment(xl); θT)\\n\\nθT = θ(t)\\nθT = θ(t) - ηT · g(T) + g(T,UDA)\\n\\nReturn: θ(N)\\n}<->{\\nCIFAR-10 dataset is split into training and validation sets with 95% of the data used for training and 5% for validation, while SVHN-1K uses 80% for training and 20% for validation and ImageNet-10% has no separate validation set.\\n}<->{\\nWe remove first 5,000 images (validation) & use next 4,000 as labeled data. We download CIFAR-10 & SVHN data from official sites and follow the same process. This leads to slight imbalance in class distribution for both datasets.\\n}<->{\\nFor all experiments, \\n\\n* ImageNet uses Inception\\'s procedure with 1,024 training TFRecord shards.\\n* ImageNet-10% uses first 102 shards of ImageNet.\\n* CIFAR-10 and SVHN use official websites.\\n}<->{\\nThe original text does not seem to be related to house numbers, but rather to ImageNet training data.\\n\\nHere is a summary:\\n\\nFor ImageNet, the first 204 shards are used for training (20% of the dataset). The last 20 shards (25,000 images) are reserved for hyperparameter tuning.\\n}<->{\\nWe modify RandAugment by removing rotations and horizontal translations from the dataset augmentation operations to correct wrong invariances for specific digits (6, 9, 3, 8).\\n}<->{\\nHorizontal translations cause wrong invariance. The following transformations are kept for datasets:\\n\\nCIFAR-10 & ImageNet: \\nBrightness, Color, Contrast, Equalize, Invert, Sharpness, Posterize, Sample Pairing, Solarize, ShearX, Rotate, TranslateY \\n\\nSVHN: \\nBrightness, Color, Contrast, Equalize, Invert, Sharpness, Posterize\\n}<->{\\nHere is a summary of the text in original language with useful information, below 100 words:\\n\\nTransformations used by RandAugment include: \\nOsterize\\nPosterize\\nSolarize\\nShearX and ShearY\\nRotate\\nTranslateX and TranslateY\\n\\nRandAugment samples transformations uniformly for datasets. The detailed descriptions can be found in [12]. To improve stability of Meta Pseudo Labels, use cosine distance instead of dot product in Equation 12.\\n}<->{\\nUse cosine distance instead of dot product in Equation 12 to stabilize training during Meta Pseudo Labels process due to large value range of dot product \\'h\\'. This modification requires minimal code change.\\n}<->{\\nCosine distance is used in meta learning mathematically because it preserves the sign of h and doesn\\'t change its magnitude. This makes actions to increase or decrease probabilities stable and more numerically stable than dot product, specifically for calculating Meta Pseudo Labels updates.\\n}<->{\\nThe value range of cosine distance is [−1, 1], whereas the range of dot products observed in experiments is [−5 × 10^4, 5 × 10^4]. This range depends on weight decay hyper-parameter. Dot product h results from bi-level optimization procedure and chain rule application.\\n}<->{\\nBi-level optimization uses chain rule to improve numerical stability. A baseline \\'h\\' is used in Equation 12. To reduce variance, a moving average \\'b\\' of \\'h\\' is maintained and subtracted from \\'h\\' every time \\'g(t)\\' is computed.\\n}<->{\\nUsing cosine distance in computing g(t) T as in Equation 12, maintains numerical stability of Meta Pseudo Labels. Using a moving average baseline only slightly improves performance and may not be necessary, especially when training doesn\\'t lead to overfitting.\\n}<->{\\nMeta Pseudo Labels can train for many steps without overfitting.\\n\\nOptimizers: Nesterov Momentum with momentum coefficient of 0.9 is used, except for EfficientNet-L2 and EfficientNet-B6-Wide which are unspecified.\\n}<->{\\nSine decay is used for learning rate scheduling. The architectures EfﬁcientNet-L2 and EfﬁcientNet-B6-Wide are trained using RMSProp with exponential decay learning rate. LARS optimizer is used when finetuning on labeled ImageNet data.\\n}<->{\\nWe used LARS optimizer with momentum 0.9 and learning rate 0.001, training for 20,000 steps with batch size 4,096.\\n}<->{\\nNumerical Hyper-parameters:\\n128 trials of hyper-parameter tuning allowed per method.\\nTrained models run up to 50,000 steps.\\nOptimal hyper-parameters used to run longer experiments, yielding stronger results.\\n}<->{\\nTraining for more steps leads to stronger results with Meta Pseudo Labels. Standards are 1 million steps for CIFAR-10-4K and SVHN-1K, and 0.5 million steps for ImageNet.\\n}<->{\\nIn a UDA implementation, a larger batch size used in the original paper was maintained to simplify data parallelism on large TPU big pods. To compensate, longer training times and masking of high-confidence supervised examples were employed during the training process.\\n}<->{\\nUDA model receives same training amount for labeled and unlabeled examples as traditional models. Achieves 68.77% top-1 accuracy on ImageNet-10% with AutoAugment augmentation policy, similar to UDA paper\\'s result (68.66%).\\n}<->{\\nHyperparameters:\\n\\n* Weight decay: 0.0005, 0.001, 0.0002\\n* Label smoothing: 0, 0, 0.1\\n* Batch normalization decay: 0.99 (all)\\n* Learning rate: 0.4, 0.05, 1.28\\n* Number of training steps: 50,000, 50,000, 40,000\\n* Number of warm up steps: 2500, 0, 2000\\n* Batch size: 1024, 128, 2048\\n* Dropout rate: 0.4, 0.5, 0.2\\n* Pseudo label threshold: 0.95, 0.975, 0.7\\n}<->{\\nCIFAR-10:\\nWeight decay: 0.0005\\nLabel smoothing: 0\\nBatch normalization decay: 0.99\\nLearning rate: 0.3\\nNumber of training steps: 1,000,000\\n\\nSVHN:\\nWeight decay: 0.0005\\nLabel smoothing: 0\\nBatch normalization decay: 0.99\\nLearning rate: 0.4\\nNumber of training steps: 500,000\\n\\nImageNet:\\nWeight decay: 0.0002\\nLabel smoothing: 0.1\\nBatch normalization decay: 0.99\\nLearning rate: 1.28\\n}<->{\\nHyperparameters for Unsupervised Domain Adaptation (UDA):\\n\\n* Batch size: 0.5, 0.6, 0.25\\n* UDA factor: 2.5\\n* UDA temperature: 0.7, 0.8, 0.7\\n* Weight decay:\\n\\t+ CIFAR-10\\n\\t+ SVHN\\n\\t+ ImageNet\\n\\t+ Common\\n}<->{\\nHyperparameters Comparison Table:\\n\\n- CIFAR-10: \\n  Learning Rate (Student): 0.3, 0.15, 0.8\\n  Batch Size (Student): 128, 128, 2048\\n  Dropout Rate (Student): 0.35, 0.45, 0.1\\n\\n- SVHN:\\n  Learning Rate (Student): 0.3, 0.15, 0.8\\n  Batch Size (Student): 128, 128, 2048\\n  Dropout Rate (Student): 0.35, 0.45, 0.1\\n\\n- ImageNet:\\n  Learning Rate (Student): 0.3, 0.15, 0.8\\n  Batch Size (Student): 128, 128, 2048\\n  Dropout Rate (Student): 0.35, 0.45, 0.1\\n\\nCommon Parameters:\\n- Weight Decay: 0.0005, 0.0005, 0.0002\\n- Label Smoothing: 0.1, 0.1, 0.1\\n- Batch Normalization Decay: 0.99, 0.99, 0.99\\n- Number of Training Steps: 1,000,000, 1,000,000, 500,000\\n- Number of Warm Up Steps: 2,000, 2,000, 1,000\\n\\nTeacher Parameters:\\n- Learning Rate: 0.125, 0.05, 0.5\\n- Batch Size: 128, 128, 2048\\n- Dropout Rate: 0.5, 0.65, 0.1\\n}<->{\\nMeta Pseudo Labels: \\nBatch size: 128, 2048\\nDropout rate: 0.5, 0.65, 0.1\\nUDA factor: 1.0, 2.5, 16.0\\nUDA temperature: 0.8, 1.25, 0.75\\n}<->{\\nVisualization of Meta Pseudo Labels\\' contributions shows that they outperform RandAugment in a purely supervised setting on ImageNet-10%.\\n}<->{\\nMeta Pseudo Labels outperforms RandAugment for a supervised teacher. \\nUDA improves over Supervised+MPL+Finetune by 6.05% in top-1 accuracy.\\n}<->{\\nUDA\\'s gain above UDA is 5.25%. This improvement is significant due to UDA\\'s already high accuracy. Fine-tuning only slightly improves UDA+MPL, but Meta Pseudo Labels provide a unique performance boost without direct labeled data use.\\n}<->{\\nMeta Pseudo Labels can behave like an effective regularization method for supervised learning. This behavior results from a smaller gain over Unsupervised Data Augmentation (UDA) compared to UDA\\'s gain over RandAugment.\\n}<->{\\nMeta Pseudo Labels gain over UDA is significant due to UDA\\'s strong performance, making labeled and unlabeled data nearly identical. Meta Pseudo Labels serve as an adaptive form of Label Smoothing by providing soft labels on labeled data for student models, similar to how Label Smoothing regularizes hard labels.\\n}<->{\\nWe compare Meta Pseudo Labels with Supervised Learning and Label Smoothing on CIFAR-10-4K and SVHN-1K datasets.\\n}<->{\\nMeta Pseudo Labels achieve 83.71% on CIFAR-10-4K and 91.89% on SVHN-1K, outperforming supervised learning with and without Label Smoothing.\\n}<->{\\nMeta Pseudo Labels outperform traditional Label Smoothing and Supervised Learning on CIFAR-10-4K and SVHN-1K datasets with accuracy rates of 83.71 ± 0.21 and 91.89 ± 0.14 respectively.\\n}<->{\\nPseudo labels help address confirmation bias by reducing overfitting due to excessive teacher-student agreement, as demonstrated through experiments on CIFAR-10-4K and ImageNet-10%.\\n}<->{\\nThe plot shows:\\n\\n* Student\\'s training accuracy (Meta Pseudo Labels) is lower than the teacher\\'s on CIFAR-10-4K.\\n* This may be due to limited labeled data.\\n}<->{\\nFew labeled data leads to inaccurate teaching in Supervised Learning due to rapid convergence of teacher and pupil on pseudo labels. In contrast, Meta Pseudo Labels converge slowly as student\\'s accuracy is measured by agreement with teacher\\'s pseudo labels, enabling accurate learning from few labeled data.\\n}<->{\\nStudent\\'s lower training accuracy in Meta Pseudo Labels indicates disagreement with teacher\\'s pseudo labels, forcing teacher to update weights and student to adapt to changing pseudo labels.\\n}<->{\\nThe teacher\\'s changing pseudo labels prevent premature convergence and confirmation bias in Supervised Learning and Pseudo Labels. Student disagrees with teacher on ImageNet-10% resulting low training accuracy, while teacher achieves higher rapid training accuracy.\\n}<->{\\nAccuracy surges up faster than the supervised model\\'s accuracy. The feedback from the student benefits both the teacher and student, leading to more accurate teaching. This occurs before the final stages of training.\\n}<->{\\nHigh training accuracy in r\\'s learning only changes at the end, where Supervised Learning surpasses teacher and student models with Meta Pseudo Labels. This suggests the supervised model has overfitted to ImageNet-10% labeled examples, leading to confirmation bias when used for generating pseudo labels for another student model.\\n}<->{\\nA supervised model generates pseudo labels for another student model to learn from. The results show:\\n\\n* On CIFAR-10-4,000:\\n\\t+ Supervised teacher: 100%\\n\\t+ Student with meta pseudo labels: 1.0\\n* On ImageNet-10%:\\n\\t+ Supervised teacher: 100%\\n\\t+ Student with meta pseudo labels: 100%\\n}<->{\\nMeta Pseudo Labels achieve lower training accuracy for both teacher and student compared to supervised learning on CIFAR-10-4,000 and ImageNet-10%. This reduces overfitting.\\n}<->{\\nMeta Pseudo Labels outperforms various training techniques on CIFAR-10-4K dataset. Results show improvement over:\\n\\nPseudo Labels: 83.79 ± 0.11\\nMixup [85]: 84.20 ± 0.15\\nRandAugment: \\n Meta Pseudo Labels: 85.53 ± 0.\\n}<->{\\nTeacher: \\nPseudo-Labels: Mixup, RandAugment, +Meta Pseudo Labels \\nAccuracy: 83.79 ± 0.11 (base), 84.20 ± 0.15 (with mixup), 85.53 ± 0.25 (with randaugment) \\n+Meta Pseudo Labels: 84.11 ± 0.07, 84.81 ± 0.19, 87.55 ± 0.14\\n}<->{\\nMeta Pseudo Labels improves significantly with initial labeled data but plateaus with additional data.\\n}<->{\\nMeta Pseudo Labels is more effective for low-resource image classification problems as more labeled data becomes available, outperforming Supervised Learning and RandAugment at varying amounts of labeled examples.\\n}<->{\\neudl Labels requires storing both teacher model and student model in memory for large models like EfficientNet-L2 and B6-Wide, exceeding available 16G memory on accelerators.\\n}<->{\\nLarge model architectures require slow and expensive training procedures. Reduced Meta Pseudo Labels offers an economical alternative by: \\n- Training a large teacher model T to convergence.\\n- Using T to pre-compute target distributions for student\\'s data.\\n- Student model is loaded only until this step, reducing overall cost.\\n}<->{\\nThe student model is initially trained on reduced teacher data without loading into memory, avoiding Meta Pseudo Labels\\' large memory footprint. A smaller efficient network, such as a multi-layered perceptron (MLP), parameterizes the reduced teacher T′, which predicts distribution outputs for the student to learn from.\\n}<->{\\nReduced Meta Pseudo Labels predicts calibrated distribution for a student using a reduced version of a large teacher T. The reduced teacher T\\' is accurate due to its high similarity to an identity map, which can be handled by an MLP.\\n}<->{\\nLabels can adapt to student learning states. The experiment used full labeled sets of CIFAR-10 and ImageNet with out-of-domain unlabeled data for scale testing. Reduced Meta Pseudo Labels were used due to large memory requirements.\\n}<->{\\nMeta Pseudo Labels benefits apply to large datasets with advanced architectures, including out-of-domain unlabeled data, using models like EfficientNet-B0 for CIFAR-10 and SVHN, and EfficientNet-B7 for ImageNet.\\n}<->{\\nOur teacher model is a small 5-layer perceptron with ReLU activation, having 128 units for CIFAR-10 and 512 units for ImageNet. We reserve 4,000, 7,300, and 40 examples from CIFAR-10, SVHN, and ImageNet respectively for hyper-parameter tuning, leaving 45,000, 65,000, and 1.23 million labeled examples for training.\\n}<->{\\nSVHN and ImageNet have additional labeled examples.\\nCIFAR-10 uses TinyImages (80 million images).\\nSVHN uses extra images in the training set (530,000).\\nImageNet uses YFCC-100M (100 million images).\\n}<->{\\nThe research uses the YFCC-100M dataset with 100 million images as unlabeled data for ImageNet. The data is filtered by pre-trained teachers to assign class distributions to TinyImages and YFCC-100M, retaining K images per class: 50,000 for CIFAR-10, 35,000 for SVHN, and 12,800 for ImageNet.\\n}<->{\\nReduced Meta Pseudo Labels: \\n- 21 epochs on CIFAR-10, SVHN, and ImageNet.\\n- Baseline accuracy: \\n    - CIFAR-10: 84.49/97.18\\n    - SVHN: 98.17 ± 0.03\\n    - ImageNet: 87.92\\n}<->{\\nEfﬁcientNet-B0 achieved 97.18% accuracy on CIFAR-10 and 98.17% on SVHN, outperforming NoisyStudent (85.81%) and Reduced Meta Pseudo Labels (86.87%). On ImageNet, EfﬁcientNet-B7 reached top-1/top-5 accuracy of 98.22%/98.71%.\\n}<->{\\nReduced Meta Pseudo Labels outperforms NoisyStudent on CIFAR-10 and ImageNet, achieving top-1 accuracy of 86.87% on ImageNet, with a 1.06% advantage over the baseline NoisyStudent. On CIFAR-10, it improves accuracy by 0.34%.\\n}<->{\\nBaseline NoisyStudent on CIFAR-10 achieved a 19% error reduction with Meta Pseudo Labels improvement of 0.34%. On SVHN, no significant gain was seen due to high accuracy and high-quality unlabeled images.\\n}<->{\\nFor many ImageNet categories, there are insufficient images from YFCC100M, resulting in low-quality or out-of-domain images. This noisy data necessitates the use of Reduced Meta Pseudo Labels\\' adaptive adjustment for improved student performance.'}}\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/Reports/2003.11236v1_normal.txt': {'Scores': '6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n5\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n5\\n}<->{\\n5\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n3\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n5\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n8\\n}<->{\\n4', 'Chunks': 'GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet\\nShan You1,2*, Tao Huang1,3∗, Mingmin Yang1∗, Fei Wang1, Chen Qian1, Changshui Zhang2\\n1SenseTime\\n2Department of Automation, Tsinghua University\\n3Dian Group, School of CST, Huazhong University of Science and Technology\\n{youshan,huangtao,yangmingmin,wangfei,qianchen}@sensetime.com zcs@mail.tsinghua.edu.cn\\nAbstract\\nTraining a supernet matters for one-shot neural archi-\\ntecture search (NAS) methods since it serves as a basic per-\\nformance esti\\n}<->{\\nor one-shot neural archi-\\ntecture search (NAS) methods since it serves as a basic per-\\nformance estimator for different architectures (paths). Cur-\\nrent methods mainly hold the assumption that a supernet\\nshould give a reasonable ranking over all paths. They thus\\ntreat all paths equally, and spare much effort to train paths.\\nHowever, it is harsh for a single supernet to evaluate ac-\\ncurately on such a huge-scale search space (e.g., 721). In\\nthis paper, instead of covering all paths, we ease the b\\n}<->{\\nh a huge-scale search space (e.g., 721). In\\nthis paper, instead of covering all paths, we ease the bur-\\nden of supernet by encouraging it to focus more on eval-\\nuation of those potentially-good ones, which are identiﬁed\\nusing a surrogate portion of validation data. Concretely,\\nduring training, we propose a multi-path sampling strat-\\negy with rejection, and greedily ﬁlter the weak paths. The\\ntraining efﬁciency is thus boosted since the training space\\nhas been greedily shrunk from all paths to tho\\n}<->{\\nng efﬁciency is thus boosted since the training space\\nhas been greedily shrunk from all paths to those potentially-\\ngood ones. Moreover, we further adopt an exploration and\\nexploitation policy by introducing an empirical candidate\\npath pool. Our proposed method GreedyNAS is easy-to-\\nfollow, and experimental results on ImageNet dataset indi-\\ncate that it can achieve better Top-1 accuracy under same\\nsearch space and FLOPs or latency level, but with only\\n∼60% of supernet training cost. By searching\\n}<->{\\n\\nsearch space and FLOPs or latency level, but with only\\n∼60% of supernet training cost. By searching on a larger\\nspace, our GreedyNAS can also obtain new state-of-the-art\\narchitectures.\\n1. Introduction\\nBy dint of automatic feature engineering, deep neural\\nnetworks (DNNs) have achieved remarkable success in var-\\nious computer vision tasks, such as image classiﬁcation\\n[37, 36, 41, 33, 32, 15, 39], visual generation [34, 35],\\nimage retrieval [40, 7, 8, 12] and semantic comprehension\\n[18, 17]. In co\\n}<->{\\nisual generation [34, 35],\\nimage retrieval [40, 7, 8, 12] and semantic comprehension\\n[18, 17]. In contrast, neural architecture search (NAS) aims\\nat automatically learning the network architecture to further\\nboost the performance for target tasks [10, 20, 43, 2, 19].\\nNevertheless, previous NAS methods in general suffer from\\n*Equal contributions.\\nEvaluate\\nSort\\nTop k\\nSupernet\\nPool\\nBest Architectures\\nSample (exploration)\\nSample (exploitation)\\nOptimize\\nQueue\\nwith probability ϵ\\nwith probability 1−ϵ\\nS\\n}<->{\\n\\nSample (exploration)\\nSample (exploitation)\\nOptimize\\nQueue\\nwith probability ϵ\\nwith probability 1−ϵ\\nSearcher\\nFigure 1: Diagram of supernet training for our proposed\\nGreedyNAS. The supernet greedily shrinks its training\\nspace from all paths (red and blue dots) into potentially-\\ngood paths (red dots), and further into candidate pool.\\nhuge computation budget, such as 2000 GPU days of rein-\\nforcement learning [43] and 3150 GPU days of evolution\\n[26] with hundreds of GPUs.\\nCurrent One-shot NAS methods\\n}<->{\\nearning [43] and 3150 GPU days of evolution\\n[26] with hundreds of GPUs.\\nCurrent One-shot NAS methods boost the search efﬁ-\\nciency by modeling NAS as a one-shot training process of\\nan over-parameterized supernet. As a result, various archi-\\ntectures can be derived from the supernet, and share the\\nsame weights. For example, DARTS [21] and its variants\\n[38, 1] parameterize the supernet with an additional cate-\\ngorical distribution for indicating what operations we want\\nto keep. In contrast, recent \\n}<->{\\nonal cate-\\ngorical distribution for indicating what operations we want\\nto keep. In contrast, recent single path methods adopt a\\nnon-parametric architecture modeling, and split the search-\\ning into two consecutive stages, i.e., supernet training and\\narchitecture sampling. For training supernet, only a single\\npath consisting of a single operation choice is activated and\\ngets optimized by regular gradient-based optimizers. After\\nthe supernet is trained well, it is regarded as a performance\\nestimato\\n}<->{\\ndient-based optimizers. After\\nthe supernet is trained well, it is regarded as a performance\\nestimator for all architectures (i.e., paths). Then the opti-\\nmal architecture can be searched using a hold-out validation\\ndataset via random search [16] or (reinforced) evolutionary\\n[11, 4] algorithms under speciﬁed hardware constraint (e.g.,\\nFLOPs and latency). As only one path is activated for train-\\ning, the memory cost coheres with that of traditional net-\\nwork training, and scales well on large-scal\\n}<->{\\n, the memory cost coheres with that of traditional net-\\nwork training, and scales well on large-scale datasets (e.g.,\\nImageNet [27]).\\n1\\narXiv:2003.11236v1  [cs.CV]  25 Mar 2020\\n\\n--------------------------------------------------\\nSupernet matters for it serves as a fundamental perfor-\\nmance estimator of different architectures (paths). Current\\nmethods [16, 11, 4, 3] hold the assumption that the supernet\\nshould estimate the (relative) performance accurately for all\\npaths, and thus all paths are tr\\n}<->{\\nernet\\nshould estimate the (relative) performance accurately for all\\npaths, and thus all paths are treated equally and trained si-\\nmultaneously. However, the paths contained in the supernet\\nare of fairly huge scale (e.g., 721). Hence it can be harsh for\\na single supernet to evaluate and give reasonable ranking on\\nsuch a quantity of paths at the same time. In fact, the ulti-\\nmate aim of supernet is only to identify a bunch of optimal\\npaths. But the huge search space implies signiﬁcant vari-\\nance a\\n}<->{\\nonly to identify a bunch of optimal\\npaths. But the huge search space implies signiﬁcant vari-\\nance and variety of paths; there exist many architectures of\\ninferior quality in terms of accuracy performance.1 Since\\nthe weights of all paths are highly shared, if a weak path\\nis sampled and gets trained, it would disturb the weights of\\nthose potentially-good paths. This disturbance will under-\\nmine their eventual performance estimation and affect the\\nsearched optimal architecture accordingly. The sup\\n}<->{\\nir eventual performance estimation and affect the\\nsearched optimal architecture accordingly. The supernet is\\nthus not supposed to care much on these weak paths and\\nget updated for them. Besides, training on those weak paths\\nactually involves unnecessary update of weights, and slows\\ndown the training efﬁciency more or less.\\nIn this paper, we ease the training burden by encouraging\\na greedy supernet. A greedy supernet is capable of shifting\\nits focus on performance estimation of those potentially-\\n}<->{\\n. A greedy supernet is capable of shifting\\nits focus on performance estimation of those potentially-\\ngood paths instead of all paths. Concretely, during the su-\\npernet training, we propose a multi-path sampling strategy\\nwith rejection to ﬁlter the weak paths, so the supernet will\\ngreedily train those potentially-good paths. This path ﬁlter-\\ning can be efﬁciently implemented via evaluation using a\\nsurrogate portion of validation dataset, without harming the\\ncomputation cost too much. Moreover, we\\n}<->{\\nsurrogate portion of validation dataset, without harming the\\ncomputation cost too much. Moreover, we also adopt an ex-\\nploration and exploitation policy [14, 24] by introducing a\\ncandidate pool, which dynamically tracks those potentially-\\ngood paths discovered during training.\\nIn this way, the\\nsupernet improves its training efﬁciency by switching its\\ntraining space from all paths into those potentially-good\\nones, and further into candidate pool by sampling from it,\\nas shown in Figure 1.\\nWe imple\\n}<->{\\nially-good\\nones, and further into candidate pool by sampling from it,\\nas shown in Figure 1.\\nWe implement our proposed method GreedyNAS on the\\nlarge-scale benchmark ImageNet dataset [27], and exten-\\nsive experimental results indicate our superiority in terms\\nof accuracy performance and supernet training efﬁciency.\\nFor example, with the same search space, our method can\\nachieve higher Top-1 accuracy than that of other compar-\\nison methods under the same FLOPs or latency level, but\\nreduces approxim\\n}<->{\\n than that of other compar-\\nison methods under the same FLOPs or latency level, but\\nreduces approximate 40% of supernet training cost.\\nBy\\nsearching on a larger space, we can also obtain new state-\\nof-the-art architectures.\\n1For example, in a same supernet, MobileNetV2 [28] can achieve\\n72.0% Top-1 accuracy on ImageNet dataset while an extreme case of al-\\nmost all identity operations only has 24.1% [3].\\n2. Related Work\\nOne-shot NAS methods mainly aim to train an over-\\nparameterized network (a.k.a \\n}<->{\\n[3].\\n2. Related Work\\nOne-shot NAS methods mainly aim to train an over-\\nparameterized network (a.k.a supernet) that comprises all\\narchitectures (paths), which share the same weights mu-\\ntually.\\nThen the optimal architecture can be derived or\\nsearched from the supernet. There are mainly two cate-\\ngories of one-shot NAS methods [9], which differ in how\\nthe architectures are modeled and elaborated as follows.\\nParameterized architectures.\\nTo use the gradient-\\nbased optimizers for direct searching, an\\n}<->{\\nfollows.\\nParameterized architectures.\\nTo use the gradient-\\nbased optimizers for direct searching, an real-valued cate-\\ngorical distribution (architecture parameter) is usually intro-\\nduced in the supernet, and can be thus jointly learned with\\nthe supernet weights, such as DARTS [21], FBNet [38] and\\nMdeNAS [42]. When the supernet training is ﬁnished, the\\noptimal architecture can be induced by sampling from the\\ncategorical distribution. However, it may suffer from the\\nhuge GPU memory consumption. \\n}<->{\\ning from the\\ncategorical distribution. However, it may suffer from the\\nhuge GPU memory consumption. ProxylessNAS [1] alle-\\nviates this issue by factorizing the searching into multiple\\nbinary selection tasks while Single-Path-NAS [29] uses su-\\nperkernels to encode all operation choices. Basically, they\\nare difﬁcult to integrate a hard hardware constraint (e.g.,\\nFLOPs and latency) during search but resort to relaxed reg-\\nularization terms [38, 1].\\nSampled single-path architectures. By directly sea\\n}<->{\\nresort to relaxed reg-\\nularization terms [38, 1].\\nSampled single-path architectures. By directly search-\\ning the discrete search space, the supernet is trained by sam-\\npling and optimizing a single path. The sampling can be\\nuniform sampling [11, 16] or multi-path sampling with fair-\\nness [4]. After the supernet is trained, it is supposed to act\\nas a performance estimator for different paths. And the opti-\\nmal path can be searched by various searchers, such as ran-\\ndom search and evolutionary alg\\n}<->{\\nhe opti-\\nmal path can be searched by various searchers, such as ran-\\ndom search and evolutionary algorithms [6]. For example,\\nScarletNAS [3] employs a multi-objective searcher [22] to\\nconsider classiﬁcation error, FLOPs and model size for bet-\\nter paths. Different to the previous parameterized methods,\\nthe hard hardware constraint can be easily integrated in the\\nsearchers. Our proposed method GreedyNAS is cast into\\nthis category.\\n3. Rethinking path training of supernet\\nIn Single-path One-shot NA\\n}<->{\\ndyNAS is cast into\\nthis category.\\n3. Rethinking path training of supernet\\nIn Single-path One-shot NAS, we utilize an over-\\nparameterized supernet N with parameter Ωto substantial-\\nize a search space, which is formulated as a directed acyclic\\ngraph (DAG). In the DAG, feature maps act as the nodes,\\nwhile the operations (or transformations) between feature\\nmaps are regarded as edges for connecting sequential nodes.\\nAssume the supernet N has L layers, and each layer N l is\\nallocated with O operation\\n}<->{\\nquential nodes.\\nAssume the supernet N has L layers, and each layer N l is\\nallocated with O operation choices O = {oi}, which can\\nbe basic convolution, pooling, identity or different types of\\nbuilding blocks, such as MobileNetV2 block [28] and Shuf-\\nﬂeNetV2 block [23]. Then each architecture (i.e., path) de-\\nnoted as a can be represented by a tuple of size L, i.e.,\\na = (o1, o2, ..., oL) where oj ∈O, ∀j = 1, 2, ..., L. As\\n2\\n\\n--------------------------------------------------\\na result, the search s\\n}<->{\\n, ∀j = 1, 2, ..., L. As\\n2\\n\\n--------------------------------------------------\\na result, the search space A is discrete, and there will be\\nOL (e.g., 721) architectures in total, namely, |A| = OL.\\nTraining supernet matters since it is expected to serve as\\na fundamental performance estimator. Due to the consid-\\neration of memory consumption, single-path NAS methods\\nimplement training by sampling a single path a from A,\\nthen the sampled paths are all optimized on the training data\\nDtr. It can be for\\n}<->{\\nngle path a from A,\\nthen the sampled paths are all optimized on the training data\\nDtr. It can be formulated as minimizing an expected loss\\nover the space A, i.e.,\\nΩ∗= arg min\\nΩ\\nE\\na∼p(A) [[L(ωa; Dtr)]] ,\\n(1)\\nwhere ωa refers to the parameter of path a, and p(A) is a\\ndiscrete sampling distribution over A.\\nAfter the supernet N(Ω∗) is trained well, we can evalu-\\nate the quality of each path by calculating its (Top-1) accu-\\nracy (ACC) on the validation dataset Dval, and the optimal\\npath a∗corresponds \\n}<->{\\ning its (Top-1) accu-\\nracy (ACC) on the validation dataset Dval, and the optimal\\npath a∗corresponds to the maximum ACC, i.e.,\\na∗= arg max\\na∈A\\nACC(ω∗\\na, Dval),\\n(2)\\nwhere ω∗\\na ⊂Ω∗w.r.t. path a in the trained supernet N(Ω∗).\\n3.1. Reshaping sampling distribution p(A)\\nCurrent methods assume that the supernet should pro-\\nvide a reasonable ranking over all architectures in A. Thus\\nall paths a are treated equally, and optimized simultane-\\nously [16, 11, 4, 3]. Then the sampling distribution p(A)\\namounts\\n}<->{\\n equally, and optimized simultane-\\nously [16, 11, 4, 3]. Then the sampling distribution p(A)\\namounts to a uniform distribution p(A) = U(A) over A,\\ni.e.,\\np(a) =\\n1\\n|A|I(a ∈A),\\n(3)\\nwhere I(·) is an indicator function. However, as previously\\ndiscussed, it is a demanding requirement for the supernet\\nto rank accurately for all paths at the same time. In the\\nhuge search space A, there might be some paths of inferior\\nquality. Since the weights are highly shared in the same\\nsupernet, training on these we\\n}<->{\\n of inferior\\nquality. Since the weights are highly shared in the same\\nsupernet, training on these weak paths does have negative\\ninﬂuence on the evaluation of those potentially-good paths.\\nTo alleviate this disturbance, an intuitive idea is to block the\\ntraining of these weak paths.\\nFor simplifying the analysis, we assume the search space\\nA can be partitioned into two subsets Agood and Aweak by\\nan Oracle good but unknown supernet No, where\\nA = Agood\\n[\\nAweak, Agood\\n\\\\\\nAweak = ∅,\\n(4)\\nand Agood indic\\n}<->{\\nOracle good but unknown supernet No, where\\nA = Agood\\n[\\nAweak, Agood\\n\\\\\\nAweak = ∅,\\n(4)\\nand Agood indicates the potentially-good paths while Aweak\\nis for weak paths, i.e.,\\nACC(a, No, Dval) ≥ACC(b, No, Dval)\\n(5)\\nholds for all a ∈Agood, b ∈Aweak on validation dataset\\nDval.\\nThen to screen the weak paths and ease the bur-\\nden of the supernet training, we can just sample from\\nthe potentially-good paths Agood instead of all paths A.\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.\\n}<->{\\nood instead of all paths A.\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nq=0.5(m=10)\\nq=0.6(m=10)\\nq=0.7(m=10)\\nq=0.8(m=10)\\nq=0.5(m=20)\\nq=0.6(m=20)\\nq=0.7(m=20)\\nq=0.8(m=20)\\nFigure 2: Probability of sampling at least k potentially-good\\npaths out of m paths. X-axis: r = k/m. q = |Agood|/|A|.\\nThe sampling distribution p(A) is equivalently reshaped by\\ntruncation on Agood, i.e., p(A) = U(Agood; No, Dval) and\\np(a; No, Dval) =\\n1\\n|Agood|I(a ∈Agood).\\n(6)\\nIn this way, the su\\n}<->{\\n, i.e., p(A) = U(Agood; No, Dval) and\\np(a; No, Dval) =\\n1\\n|Agood|I(a ∈Agood).\\n(6)\\nIn this way, the supernet is expected to thoroughly get\\ntrained on the potentially-good paths and thus give decent\\nperformance ranking. Besides, since the valid search space\\nhas been shrunken from A into Agood, the training efﬁ-\\nciency of supernet is improved accordingly.\\n3.2. Greedy path ﬁltering\\nNevertheless, in the supernet training the Oracle super-\\nnet No is unknown, thus we can not sample paths according\\nto Eq\\n}<->{\\n supernet training the Oracle super-\\nnet No is unknown, thus we can not sample paths according\\nto Eq.(6) since it relies on No. In this paper, we propose\\nto use greedy strategy and during training, current super-\\nnet N† is progressively regarded as a proxy of the Oracle\\nNo. Thus during the supernet training, we greedily sam-\\nple paths according to the reshaped sampling distribution\\ngiven by current N†, namely, p(A) = U(Agood; N†, Dval).\\nThe sampled paths will get optimized, then the supernet is\\n\\n}<->{\\nt N†, namely, p(A) = U(Agood; N†, Dval).\\nThe sampled paths will get optimized, then the supernet is\\nget updated and evolves to a decent performance estimator\\nover Agood.\\nHowever, a natural question arises: even given a super-\\nnet N†, how can we sample from the shaped distribution\\np(A) = U(Agood; N†, Dval)? In other words, how can we\\naccurately identify whether a path is from Agood or Aweak?\\nNote that the partition of A is determined by traversing all\\npaths in A as Eq.(5), which is not affordable\\n}<->{\\nhat the partition of A is determined by traversing all\\npaths in A as Eq.(5), which is not affordable in computation.\\nSince we can not accurately know whether a single path is\\ngood or weak, to solve this issue, we propose a multi-path\\nsampling strategy with rejection.\\nSuppose we uniformly sample a path from A, then it\\namounts to be sampled from Agood with probability q =\\n|Agood|/|A|, and sampled from Aweak with probability\\n1 −q. In this way, if we sample multiple paths indepen-\\ndently at a time, \\n}<->{\\nom Aweak with probability\\n1 −q. In this way, if we sample multiple paths indepen-\\ndently at a time, we have the following results based on\\nbinomial distribution.\\nTheorem 1. If m paths are sampled uniformly i.i.d. from\\nA, and Agood and Aweak are deﬁned as Eq.(4) and Eq.(5)\\nbased on supernet N†, then it holds that at least k (k ≤m)\\n3\\n\\n--------------------------------------------------\\nAlgorithm 1 Greedy path ﬁltering w.t/w.o candaidate pool.\\nInput: supernet N with parameter Ω,validation data Dval,\\n}<->{\\needy path ﬁltering w.t/w.o candaidate pool.\\nInput: supernet N with parameter Ω,validation data Dval,\\nnumber of sampled multiple paths m, number of kept\\npaths k, candidate pool P with sampling probability ϵ.\\n1: if without candidate pool P then\\n2:\\nsample m paths {ai}m\\ni=1 i.i.d. w.r.t. ai ∼U(A)\\n3: else\\n4:\\nsample m paths {ai}m\\ni=1 i.i.d. w.r.t. ai ∼(1 −ϵ) ·\\nU(A) + ϵ · U(P)\\n5: end if\\n6: randomly sample a batch ˆDval in Dval\\n7: evaluate the loss ℓi of each path ai on ˆDval\\n8: rank the paths by ℓi, an\\n}<->{\\ne a batch ˆDval in Dval\\n7: evaluate the loss ℓi of each path ai on ˆDval\\n8: rank the paths by ℓi, and get Top-k indexes {ti}k\\ni=1\\n9: return k paths {ati}k\\ni=1 and ﬁlter the rest\\npaths are from Agood with probability\\nm\\nX\\nj=k\\nCj\\nmqj(1 −q)m−j,\\n(7)\\nwhere q = |Agood|/|A|.\\nFrom Theorem 1, we can see by sampling m paths, the\\nprobability that at least k paths are from Agood is very high\\nwhen the proportion of potentially-good paths q is medially\\nlarge or k is medially small (see Figure 2). For example, \\n}<->{\\nn of potentially-good paths q is medially\\nlarge or k is medially small (see Figure 2). For example, if\\nwe conservatively assume 60% paths have the potential to\\nbe good (i.e., q = 0.6), we will have 83.38% conﬁdence to\\nsay at least 5 out of 10 paths are sampled from Agood. In\\nthis way, based on the deﬁnition of Eq.(4) and Eq.(5), we\\njust rank the sampled m paths using validation data Dval,\\nkeep the Top-k paths and reject the remaining paths.\\nHowever, ranking m paths involves calculation of ACC\\nov\\n}<->{\\n Top-k paths and reject the remaining paths.\\nHowever, ranking m paths involves calculation of ACC\\nover all validation dataset Dval as Eq.(5), which is also\\ncomputationally intensive during the supernet training.2 In\\nfact, in our multi-path sampling, what we care about is the\\nobtained ranking; we empirically ﬁnd that it sufﬁces to rank\\nbased on the loss ℓ(e.g., cross entropy loss for classiﬁca-\\ntion) over a surrogate subset of Dval (e.g., 1k images on\\nImageNet dataset), denoted as ˆDval. The cons\\n}<->{\\nn) over a surrogate subset of Dval (e.g., 1k images on\\nImageNet dataset), denoted as ˆDval. The consistency be-\\ntween this rank and that given by ACC on all Dval is fairly\\nsigniﬁcant. More details and analysis refer to the ablation\\nstudies in Section 5.3.1. Then the sampling works as Algo-\\nrithm 1.\\nAs a result, path ﬁltering can be efﬁciently implemented\\nfor it can run in a simple feed-forward mode (e.g., eval()\\nmode in Pytorch) on a small portion of validation data. In\\nthis sense, we block the \\n}<->{\\ne (e.g., eval()\\nmode in Pytorch) on a small portion of validation data. In\\nthis sense, we block the weak paths greedily during the su-\\npernet training. And the validation data ˆDval acts as a rough\\nﬁlter to prevent the training of those low-quality or even\\nharmful paths, so that the supernet can get sufﬁcient train-\\ning on those potentially-good ones.\\n2For example, the size of Dval on ImageNet dataset is 50k.\\nAlgorithm 2 Greedy training of supernet.\\nInput: supernet N with parameter Ω, training d\\n}<->{\\naset is 50k.\\nAlgorithm 2 Greedy training of supernet.\\nInput: supernet N with parameter Ω, training data Dtr,\\nvalidation data Dval, number of sampled multiple paths\\nm, number of kept paths k, max iteration T, training\\ndata loader D\\n1: initialize candidate pool P = ∅,\\n2: set a Scheduler of pool sampling probability ϵ\\n3: for τ = 1, .., T/k do\\n4:\\nget the pool sampling probability ϵ by Scheduler\\n5:\\nsample k paths {ati}k\\ni=1 out of m paths using Algo-\\nrithm 1 with pool sampling probability ϵ\\n6:\\nupdate\\n}<->{\\nple k paths {ati}k\\ni=1 out of m paths using Algo-\\nrithm 1 with pool sampling probability ϵ\\n6:\\nupdate candidate pool P using {ati}k\\ni=1\\n7:\\nfor i = 1, .., k do\\n8:\\nget a training batch from D\\n9:\\nupdate the weights ωati of path ati using gradient-\\nbased optimizer\\n10:\\nend for\\n11: end for\\n4. Proposed Approach: GreedyNAS\\nIn this section, we formally illustrated our proposed\\nNAS method (a.k.a. GreedyNAS) based on a greedy super-\\nnet. Our GreedyNAS is composed with three procedures,\\ni.e., supernet traini\\n}<->{\\nbased on a greedy super-\\nnet. Our GreedyNAS is composed with three procedures,\\ni.e., supernet training, searching paths and retraining the\\nsearched optimal path. The last retraining corresponds to\\nconventional training a given network. We mainly elabo-\\nrate the ﬁrst two as follows.\\n4.1. Greedy training of supernet\\nAs previously discussed,\\nwe propose to maintain\\na\\ngreedy\\nsupernet\\nduring\\nits\\ntraining.\\nBy\\ndoing\\nthis, we gradually approximate the sampling p(A)\\n=\\nU(Agood; N†, Dval) by keeping the Top\\n}<->{\\ng.\\nBy\\ndoing\\nthis, we gradually approximate the sampling p(A)\\n=\\nU(Agood; N†, Dval) by keeping the Top-k paths and ﬁlter-\\ning the bottom m −k paths by evaluating using ˆDval. Then\\nthose weak paths are prevented from getting trained, which\\nallows the supernet to focus more on those potentially-good\\npaths and switch its training space from A into Agood.\\n4.1.1\\nTraining with exploration and exploitation\\nAfter the greedy path ﬁltering, we have actually identi-\\nﬁed some potentially-good paths, which amo\\n}<->{\\n\\nAfter the greedy path ﬁltering, we have actually identi-\\nﬁed some potentially-good paths, which amount to some\\nempirically-good ones given by current supernet.\\nThen\\nto further improve the training efﬁciency, inspired by the\\nMonte Carlo tree search [14] and deep Q-learning (DQN)\\n[24], we propose to train the supernet with an exploration\\nand exploitation (E-E) strategy by reusing these paths.\\nConcretely, we introduce a candidate pool P to store\\nthe potentially-good paths discovered during trainin\\n}<->{\\netely, we introduce a candidate pool P to store\\nthe potentially-good paths discovered during training. Each\\npath a is represented as a tuple of operation choices. Be-\\nsides, each ai is also allocated with an evaluation loss ℓi.\\n4\\n\\n--------------------------------------------------\\nThe candidate pool is thus formulated as a ﬁxed-size or-\\ndered queue with priority ℓ. With more potentially-good\\npaths involved, the candidate pool can be maintained by a\\nmin-heap structure in real time.\\nAs a result, w\\n}<->{\\n involved, the candidate pool can be maintained by a\\nmin-heap structure in real time.\\nAs a result, we can conservatively implement local\\nsearch by sampling from the candidate pool since it consists\\nof a smaller number (but promising) of paths. However, this\\ngreedy exploitation brings in the risks of losing path diver-\\nsity for the training. In this way, we also favor a global\\nsearch with the hope of probing other promising paths that\\nare yet to be sampled and get trained, which can be easily\\nful\\n}<->{\\nof probing other promising paths that\\nare yet to be sampled and get trained, which can be easily\\nfulﬁlled by uniform sampling from A. For achieve a bal-\\nanced trade-off of exploration and exploitation, we adopt a\\ntypical ϵ-sampling policy, i.e., implementing uniform sam-\\npling both from A and pool P (line 4 of Algorithm 1),\\na ∼(1 −ϵ) · U(A) + ϵ · U(P),\\n(8)\\nwhere ϵ ∈[0, 1] indicates the probability of sampling from\\nthe pool P. Note that candidate pool runs through the train-\\ning process of supern\\n}<->{\\n of sampling from\\nthe pool P. Note that candidate pool runs through the train-\\ning process of supernet; however, it might be not reliable at\\nﬁrst since the priority ℓis calculated based on a much less-\\ntrained supernet. In this case, we propose to actively an-\\nneal the pool sampling probability ϵ from 0 to a pre-deﬁned\\nlevel. In our experiment, we ﬁnd ϵ = 0.8 will be a good\\noption.\\nTraining with exploration and exploitation encourages\\nthe supernet to reﬁne the already-found good paths as well\\nas\\n}<->{\\nxploration and exploitation encourages\\nthe supernet to reﬁne the already-found good paths as well\\nas probing new territory for more better paths. Besides, it\\nactually also contributes to our greedy path ﬁltering by im-\\nproving our ﬁltering conﬁdence. Basically, the collected\\npotentially-good paths can be regarded as a subset of Agood,\\nthen sampling from P amounts to increasing the probability\\nq of Theorem 1 into\\nq = ϵ + (1 −ϵ)|Agood|/|A|,\\n(9)\\nwhich refers to the proportion of potentially-good pa\\n}<->{\\n Theorem 1 into\\nq = ϵ + (1 −ϵ)|Agood|/|A|,\\n(9)\\nwhich refers to the proportion of potentially-good paths.\\nFor example, assume we evenly sample from P or A (ϵ =\\n0.5), then the probability of sampling at least 5 good paths\\nout of 10 paths will rise from 83.38% to 99.36% according\\nto Theorem 1. Comparing reducing r = k/m to increase\\nthe sampling conﬁdence, sampling with P is almost cost-\\nneglectable since we only need to maintain a min-heap. The\\nsupernet thus gradually shifts its training from Agood\\n}<->{\\nince we only need to maintain a min-heap. The\\nsupernet thus gradually shifts its training from Agood more\\nto P, and the training efﬁciency will be further improved\\naccordingly.\\n4.1.2\\nStopping principle via candidate pool\\nDifferent to conventional networks, a supernet serves as a\\nperformance estimator and it is difﬁcult to judge when it is\\ntrained well. Current single-path NAS methods control its\\ntraining by manually specifying an maximum epoch num-\\nber. In our GreedyNAS, however, we propose an a\\n}<->{\\nraining by manually specifying an maximum epoch num-\\nber. In our GreedyNAS, however, we propose an adaptive\\nstopping principle based on the candidate pool.\\nFigure 3: Histogram of accuracy of searched paths on su-\\npernet by evolutionary searching method (with or without\\ncandidate pool).\\nCandidate pool P indicates a bunch of best empirical\\npaths, and it is updated dynamically during the supernet\\ntraining. In consequence, if a supernet is trained well, the\\npool P should tend to be steady. This stea\\n}<->{\\nining. In consequence, if a supernet is trained well, the\\npool P should tend to be steady. This steadiness can be\\nmeasured by the update frequency π of candidate pool P,\\ni.e.,\\nπ := |Pt\\nT P|\\n|P|\\n≤α,\\n(10)\\nwhere Pt refers to the old P in previous t iterations. Thus\\nsmaller π implies that fewer new paths are involved in the\\npool P within t iterations, and P is more steady. Given\\na certain tolerance level α3, when the update frequency π\\nis less than α, we believe the supernet has been trained\\nenough,\\n}<->{\\nvel α3, when the update frequency π\\nis less than α, we believe the supernet has been trained\\nenough, and its training can be stopped accordingly.\\n4.2. Searching with candidate pool\\nAfter the supernet is trained, we can use supernet to eval-\\nuate the quality (ACC) of each path on validation dataset\\nDval, and search the optimal path a∗as Eq.(2). However,\\nenumerating all paths in A is prohibitively computation-\\nintensive. One remedy is by dint of evolutionary algorithms\\n[11] or reinforced version (\\n}<->{\\nomputation-\\nintensive. One remedy is by dint of evolutionary algorithms\\n[11] or reinforced version (e.g., MoreMNAS [5]), which\\ntakes the supernet as an off-the-shelf evaluator. In our pa-\\nper, we adopt the multi-objective NSGA-II [6] algorithm\\nfor searching, where the hardware constraint can be easily\\nintegrated in the evolution process. If a path violates the\\npre-deﬁned hardware constraint (e.g., under 330 FLOPs),\\nwe just ditch it for good.\\nBesides, evolutionary algorithms need to initialize po\\n}<->{\\n under 330 FLOPs),\\nwe just ditch it for good.\\nBesides, evolutionary algorithms need to initialize pop-\\nulation with size Npop before implementing iterative muta-\\ntion and crossover. Current methods usually random sam-\\nple Npop paths under the constraint as initial population. In\\ncontrast, our method makes the initialization with the help\\nof candidate pool P, and select its Top-Npop paths instead.\\nAs Figure 3 shows, searching with candidate pool can boost\\nthe evolutionary performance for supplyin\\n}<->{\\nAs Figure 3 shows, searching with candidate pool can boost\\nthe evolutionary performance for supplying a good initial\\npopulation. The ACC of searched paths using candidate\\npool is on average higher than that using random initial-\\nization. More details of our searching algorithm refer to the\\nsupplementary materials.\\n3α = 0.08 sufﬁces in our experiment.\\n5\\n\\n--------------------------------------------------\\nTable 1: Comparison of classiﬁcation performance and supernet training efﬁciency w.r.t. diffe\\n}<->{\\n------\\nTable 1: Comparison of classiﬁcation performance and supernet training efﬁciency w.r.t. different searching methods on Ima-\\ngeNet dataset under same search space. #optimization means the accumulated #examples calculated for a whole optimization\\nstep, while #evaluation is for that of forward evaluation. corrected #optimization is based on our statistics that cost of a whole\\noptimization step is 3.33 times larger than that of forward evaluation. Details of calculation refer to supplementary\\n}<->{\\n is 3.33 times larger than that of forward evaluation. Details of calculation refer to supplementary materials.\\nMethods\\nperformance\\nsupernet training efﬁciency\\nTop-1 (%)\\nFLOPs\\nlatency\\nParams\\n#optimization\\n#evaluation\\ncorrected #optimization\\nProxyless-R (mobile) [1]\\n74.60\\n320M\\n79 ms\\n4.0M\\n-\\n-\\n-\\nRandom Search\\n74.07\\n321M\\n69 ms\\n3.6M\\n1.23M×120\\n-\\n147.6M\\nUniform Sampling [11]\\n74.50\\n326M\\n72 ms\\n3.8M\\n1.23M×120\\n-\\n147.6M\\nFairNAS-C [4]\\n74.69\\n321M\\n75 ms\\n4.4M\\n1.23M×150\\n-\\n184.5M\\nRandom Search-E\\n73.88\\n320M\\n91 ms\\n\\n}<->{\\n20\\n-\\n147.6M\\nFairNAS-C [4]\\n74.69\\n321M\\n75 ms\\n4.4M\\n1.23M×150\\n-\\n184.5M\\nRandom Search-E\\n73.88\\n320M\\n91 ms\\n3.7M\\n1.23M×73\\n-\\n89.8M\\nUniform Sampling [11]-E\\n74.17\\n320M\\n94 ms\\n3.6M\\n1.23M×73\\n-\\n89.8M\\nGreedyNAS (FLOPs≤322M)\\n74.85\\n320M\\n89 ms\\n3.8M\\n1.23M×46\\n2.40M×46\\n89.7M\\nGreedyNAS (latency≤80ms)\\n74.93\\n324M\\n78 ms\\n4.1M\\n1.23M×46\\n2.40M×46\\n89.7M\\n5. Experimental Results\\n5.1. Conﬁguration and settings\\nDataset. We conduct the architecture search on the chal-\\nlenging ImageNet dataset [27]. As [1], we randomly sample\\n50,00\\n}<->{\\nthe architecture search on the chal-\\nlenging ImageNet dataset [27]. As [1], we randomly sample\\n50,000 images (50 images per class) from training dateset\\nas the validation dataset (|Dval| = 50K), and the rest of\\ntraining images are used for training. Moreover, we use the\\noriginal validation dataset as the test dataset to report the\\naccuracy performance.\\nSearch space.\\nFollowing [1, 4], we adopt the same\\nmacro-structure of supernet for fair comparison as shown\\nin Table 5 (see supplementary material\\n}<->{\\nsame\\nmacro-structure of supernet for fair comparison as shown\\nin Table 5 (see supplementary materials). Moreover, we use\\nMobileNetV2 inverted bottleneck [28] as the basic building\\nblock. For each building block, the convolutional kernel\\nsize is within {3, 5, 7} and expansion ratio is selected in\\n{3, 6}. An identity block is also attached for ﬂexible depth\\nsearch. As a result, with 21 building blocks, the search\\nspace is of size (3 × 2 + 1)21 = 721. In addition, we also\\nimplement searching on a l\\n}<->{\\nks, the search\\nspace is of size (3 × 2 + 1)21 = 721. In addition, we also\\nimplement searching on a larger space by augmenting each\\nbuilding block with an squeeze-and-excitation (SE) option.\\nThe size of the larger search space is thus 1321.\\nSupernet training. For training the supernet, Algorithm\\n1 is adopted to sample 10 paths and ﬁlter 5 paths.\\nWe\\nrandomly sample 1000 images (1 image per class) from\\nthe validation dataset for evaluating paths in Algorithm 1.\\nFor training each path, we use a stoc\\n}<->{\\nom\\nthe validation dataset for evaluating paths in Algorithm 1.\\nFor training each path, we use a stochastic gradient descent\\n(SGD) optimizer with momentum 0.9 and Nesterov accel-\\neration. The learning rate is decayed with cosine annealing\\nstrategy from initial value 0.12. The batch size is 1024. As\\nfor candidate pool, we empirically ﬁnd 1000 is a good op-\\ntion for pool size |P|, which approximates the amount of\\npaths involved in one epoch. The candidate sampling prob-\\nability ϵ is linearly increa\\n}<->{\\nthe amount of\\npaths involved in one epoch. The candidate sampling prob-\\nability ϵ is linearly increased from 0 to 0.8. Instead of spec-\\nifying an epoch number [11, 4], we use the proposed prin-\\nciple to stop the supernet training with tolerance α = 0.08.\\nEvolutionary searching. For searching with NSGA-II\\n[6] algorithm, we set the population size as 50 and the num-\\nber of generations as 20. The population is initialized by\\nthe candidate pool P while other comparison methods use\\nrandom initializat\\n}<->{\\nulation is initialized by\\nthe candidate pool P while other comparison methods use\\nrandom initialization. During searching, we use constraint\\nof FLOPs or latency. All our experiments use Qualcomm®\\nSnapdragon 855 mobile hardware development kit (HDK)\\nto measure the latency.\\nRetraining. To train the obtained architecture, we use\\nthe same strategy as [1] for search space without SE. As\\nfor the augmented search space, we adopt a RMSProp opti-\\nmizer with 0.9 momentum as Mnasnet [30]. Learning rate\\nis \\n}<->{\\ned search space, we adopt a RMSProp opti-\\nmizer with 0.9 momentum as Mnasnet [30]. Learning rate\\nis increased from 0 to 0.064 in the ﬁrst 5 epochs with batch\\nsize 512, and then decays 0.03 every 3 epochs. Besides, ex-\\nponential moving average is also adopted with decay 0.999.\\n5.2. Performance comparison with state-of-the-art\\nmethods\\nSearching on same search space.\\nFor fair compari-\\nson, we ﬁrst benchmark our GreedyNAS to the same search\\nspace as [1] to evaluate our superiority to other Single-pa\\n}<->{\\nnchmark our GreedyNAS to the same search\\nspace as [1] to evaluate our superiority to other Single-path\\nOne-shot NAS methods. We also cover a baseline method\\nRandom Search, which shares the same supernet training\\nstrategy with Uniform Sampling [11]; but during search, in-\\nstead of using evolutionary algorithms it randomly samples\\n1000 paths, and retrains the rank-1 path according to Top-\\n1 ACC on the supernet. As Table 1 shows, when searching\\nwith similar 320 FLOPs, our GreedyNAS achieves the hig\\n}<->{\\nhe supernet. As Table 1 shows, when searching\\nwith similar 320 FLOPs, our GreedyNAS achieves the high-\\nest Top-1 ACC. We further align our searched constraint\\nto latency of 80 ms. Table 1 indicates that with similar\\nlatency, GreedyNAS is still consistently superior to other\\ncomparison methods. For example, GreedyNAS can search\\nan architecture with 74.93% Top-1 ACC, enjoying a 0.43%\\nimprovement over uniform sampling, which in a way illus-\\ntrates the superiority of our greedy supernet to a uniform\\n}<->{\\nr uniform sampling, which in a way illus-\\ntrates the superiority of our greedy supernet to a uniform\\nsupernet.\\nBesides advantages on the classiﬁcation performance of\\nsearched models, we also evaluate our superiority in terms\\nof supernet training efﬁciency. Since the main differences\\nof our GreedyNAS and other Single-path One-shot NAS\\nmethods lie in the supernet training, we report in Table 1\\nthe supernet training cost. To eliminate the efﬁciency gap\\n6\\n\\n-------------------------------------------\\n}<->{\\nupernet training cost. To eliminate the efﬁciency gap\\n6\\n\\n--------------------------------------------------\\nTable 2: Comparison of searched architectures w.r.t. different state-of-the-art NAS methods. †: searched on CIFAR-10, ‡:\\nTPU, ⋆: reported by [11].\\nMethods\\nTop-1\\n(%)\\nTop-5\\n(%)\\nFLOPs\\n(M)\\nlatency\\n(ms)\\nParams\\n(M)\\nMemory cost\\ntraining cost\\n(GPU days)\\nsearch cost\\n(GPU days)\\nSCARLET-C [4]\\n75.6\\n92.6\\n280\\n67\\n6.0\\nsingle path\\n10\\n12\\nMobileNetV2 1.0 [28]\\n72.0\\n91.0\\n300\\n38\\n3.4\\n-\\n-\\n-\\nMnasNet-A1 [30]\\n75.2\\n9\\n}<->{\\n\\n280\\n67\\n6.0\\nsingle path\\n10\\n12\\nMobileNetV2 1.0 [28]\\n72.0\\n91.0\\n300\\n38\\n3.4\\n-\\n-\\n-\\nMnasNet-A1 [30]\\n75.2\\n92.5\\n312\\n55\\n3.9\\nsingle path + RL\\n288‡\\n-\\nGreedyNAS-C\\n76.2\\n92.5\\n284\\n70\\n4.7\\nsingle path\\n7\\n< 1\\nProxyless-R (mobile) [1]\\n74.6\\n92.2\\n320\\n79\\n4.0\\ntwo paths\\n15⋆\\n-\\nFairNAS-C [4]\\n74.7\\n92.1\\n321\\n75\\n4.4\\nsingle path\\n10\\n2\\nUniform Sampling [11]\\n74.7\\n-\\n328\\n-\\n-\\nsingle path\\n12\\n< 1\\nSCARLET-B [4]\\n76.3\\n93.0\\n329\\n104\\n6.5\\nsingle path\\n10\\n12\\nGreedyNAS-B\\n76.8\\n93.0\\n324\\n110\\n5.2\\nsingle path\\n7\\n< 1\\nSCARLET-A [4]\\n76.9\\n93.4\\n365\\n118\\n6.\\n}<->{\\nle path\\n10\\n12\\nGreedyNAS-B\\n76.8\\n93.0\\n324\\n110\\n5.2\\nsingle path\\n7\\n< 1\\nSCARLET-A [4]\\n76.9\\n93.4\\n365\\n118\\n6.7\\nsingle path\\n10\\n12\\nEfﬁcientNet-B0 [31]\\n76.3\\n93.2\\n390\\n82\\n5.3\\nsingle path\\n-\\n-\\nDARTS [21]\\n73.3\\n91.3\\n574\\n-\\n4.7\\na whole supernet\\n4†\\n-\\nGreedyNAS-A\\n77.1\\n93.3\\n366\\n77\\n6.5\\nsingle path\\n7\\n< 1\\ndue to different implementation tools (e.g., GPU types, dat-\\naloader wrappers), we calculated the accumulated number\\nof images involved in a whole gradient-based optimization\\nstep, i.e., #optimization in Table 1. Our Gr\\n}<->{\\n images involved in a whole gradient-based optimization\\nstep, i.e., #optimization in Table 1. Our GreedyNAS has an\\nadditional evaluation process during training, thus we also\\nreport the accumulated number of images for forward eval-\\nuation, i.e., #evaluation. For overall efﬁciency comparison,\\nwe empirically ﬁnd the cost of a whole optimization step\\nis approximately 3.33 times larger than that of a forward\\nevaluation. The corresponding corrected #optimization is\\ncovered accordingly.\\nFrom Table 1,\\n}<->{\\n forward\\nevaluation. The corresponding corrected #optimization is\\ncovered accordingly.\\nFrom Table 1, we can see that the training cost of our\\nGreedyNAS is much smaller than that of other compari-\\nson methods, which indicates GreedyNAS enjoys signiﬁ-\\ncant efﬁciency in supernet training since it greedily shrinks\\nits training space into those potentially-good paths. Besides,\\nwe also implement Random Search and Uniform Sampling\\nusing same training cost of GreedyNAS, denoted as Ran-\\ndom Search-E and \\n}<->{\\nSearch and Uniform Sampling\\nusing same training cost of GreedyNAS, denoted as Ran-\\ndom Search-E and Uniform Sampling-E, respectively. The\\nresults show that with decreased iterations of supernet train-\\ning, the searched architectures are inferior to those of larger\\niterations. In contrast, our method can achieve higher ac-\\ncuracy by a large margin (almost 1%). This implies that\\nGreedyNAS is capable of learning a decent supernet with\\nmuch less iterations.\\nSearching on augmented search space. To co\\n}<->{\\n of learning a decent supernet with\\nmuch less iterations.\\nSearching on augmented search space. To compre-\\nhensively illustrate our superiority to various state-of-the-\\nart NAS methods, we implement searching by augment-\\ning the current space with an SE option.\\nMoreover, we\\nsearch the architectures under different FLOPs constraint.\\nBut we also report the corresponding latency and param-\\neter capacity to comprehensively analyze the statistics of\\nsearched models.\\nAs Table 2 shows, our GreedyNAS\\nach\\n}<->{\\nty to comprehensively analyze the statistics of\\nsearched models.\\nAs Table 2 shows, our GreedyNAS\\nachieves new state-of-the-art performance with respect to\\ndifferent FLOPs and latency levels. For example, with sim-\\nilar FLOPs and latency, GreedyNAS-C has higher Top-1\\nACC than the competing SCARLET method by a margin\\nof 0.6%. Our searched models are visualized in Figure 6\\n(see supplementary materials). It shows smaller network\\n(GreedyNAS-C) tends to select more identity blocks to re-\\nduce the FLOP\\n}<->{\\ns). It shows smaller network\\n(GreedyNAS-C) tends to select more identity blocks to re-\\nduce the FLOPs while larger networks will exploit more SE\\nmodules (GreedyNAS-A&B) to further improve the accu-\\nracy performance. Moreover, GreedyNAS-A adopts more\\n3×3 kernels to have smaller latency since 3×3 kernels\\nare optimized more maturely in mobile inference frame-\\nwork.\\nWe also report our real training cost in Table 2\\nbased on Tesla V100 GPU. It shows that GreedyNAS can\\nsigniﬁcantly reduce the training \\n}<->{\\nst in Table 2\\nbased on Tesla V100 GPU. It shows that GreedyNAS can\\nsigniﬁcantly reduce the training time compared to other\\nNAS methods, which empirically validates the efﬁciency of\\nGreedyNAS.\\n5.3. Ablation studies\\n5.3.1\\nEffect of evaluation in path ﬁltering\\nTo ﬁlter the weak paths, GreedyNAS evaluates each path\\nby a small portion (1000) of validation images as a surro-\\ngate for the whole validation dataset (50K images). We ﬁrst\\ninvestigate whether this approximation sufﬁces in our ex-\\nperiment. \\n}<->{\\nn dataset (50K images). We ﬁrst\\ninvestigate whether this approximation sufﬁces in our ex-\\nperiment. By random sampling 1000 paths from supernet,\\nwe examine the correlation of two path orderings, which are\\ngenerated by ranking the evaluation results using 1000 and\\n50K validation images, respectively. In Table 3, we report\\nthe widely-used Spearman rho [25] and Kendall tau [13]\\nrank correlation coefﬁcient, which are in the range [0, 1] and\\nlarger values mean stronger correlation. We also cover thre\\n}<->{\\ncient, which are in the range [0, 1] and\\nlarger values mean stronger correlation. We also cover three\\ntypes of supernets, i.e., randomly initialized, trained by uni-\\nform sampling and our greedy sampling.\\nFrom Table 3, we can see that our greedy supernet\\nachieves fairly high rank correlation coefﬁcient (0.997 and\\n0.961), which indicates that the ranking of greedy super-\\nnet using 1000 validation images is signiﬁcantly consis-\\ntent with that of all validation dataset. Moreover, super-\\nnet trained\\n}<->{\\nmages is signiﬁcantly consis-\\ntent with that of all validation dataset. Moreover, super-\\nnet trained with uniform sampling has smaller correlation\\ncoefﬁcient, even with different evaluation images (see left\\n7\\n\\n--------------------------------------------------\\nTable 3: Rank correlation coefﬁcient of 1000 paths mea-\\nsured by the loss (ACC) of 1K validation images and ACC\\nof 50K validation images w.r.t. different types of supernets.\\nSpearman rho\\nKendall tau\\nrandom uniform(ACC) greedy\\nrandom unifor\\n}<->{\\nr.t. different types of supernets.\\nSpearman rho\\nKendall tau\\nrandom uniform(ACC) greedy\\nrandom uniform(ACC) greedy\\n0.155\\n0.968(0.869)\\n0.997\\n0.113\\n0.851(0.699)\\n0.961\\n125\\n250\\n500\\n1K\\n2K\\n5K\\n10K\\nnumber of validation images\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\ncorrelation coefficient\\nUniform Sampling\\nGreedy Sampling\\n0\\n2K\\n4K\\n8K\\n10K\\n14K\\n30K\\n15W\\ntraining iterations\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\ncorrelation coefficient\\nSpearman rho\\nKendall tau\\nFigure 4: Rank correlation coefﬁcient of 1000 paths mea-\\nsured by the loss of N validation i\\n}<->{\\nall tau\\nFigure 4: Rank correlation coefﬁcient of 1000 paths mea-\\nsured by the loss of N validation images and ACC of the\\nwhole 50K validation images. Left: Comparison (Kendall\\ntau) of supernet by uniform and greedy sampling w.r.t. dif-\\nferent number N of evaluation images. Right: N = 1K\\nw.r.t. different training iterations of supernet by uniform\\nsampling.\\nFigure 4). This implies in a sense that our greedy super-\\nnet is more discriminative since it can use less images to\\nidentify whether a path i\\n}<->{\\nr greedy super-\\nnet is more discriminative since it can use less images to\\nidentify whether a path is good or weak. Nevertheless, as\\nleft Figure 4 shows, too few evaluation images might have\\nweak correlation while too many evaluation images mean\\ngreater evaluation cost. But 1000 evaluation images enjoy a\\nbalanced trade-off between the rank correlation and evalua-\\ntion cost. Note that we also report the results w.r.t. ranking\\nusing the ACC of 1000 images, which is smaller than that\\nusing loss. Th\\n}<->{\\nt the results w.r.t. ranking\\nusing the ACC of 1000 images, which is smaller than that\\nusing loss. This results from that during training the value\\nof loss might be more informative than that of ACC.\\nAs for the random supernet, the correlation coefﬁcient is\\nfairly small (0.155 and 0.113). This makes sense since the\\nranking is based on the classiﬁcation performance; how-\\never, a random supernet fails to learn sufﬁcient domain-\\nrelated information but gives disordered ranking of paths.\\nThis smaller\\n}<->{\\ns to learn sufﬁcient domain-\\nrelated information but gives disordered ranking of paths.\\nThis smaller correlation coefﬁcient implies that it might be\\nnot sensible to implement greedy sampling from a random\\nsupernet since the ranking evaluated by 1000 validation im-\\nages will be rather noisy. In this way, we record the trend of\\nrank correlation coefﬁcients with uniform sampling in right\\nFigure 4. It shows that with more iterations, the correla-\\ntion coefﬁcients increase and at 10K iteration, they \\n}<->{\\n shows that with more iterations, the correla-\\ntion coefﬁcients increase and at 10K iteration, they tend to\\nbe steady at a high level (e.g., 0.81 for Kendall Tau). As\\na result, in our GreedyNAS we propose to have a warm-up\\nstage by uniform sampling for 10K iterations, so that we\\ncan safely use 1000 validation images to evaluate paths.\\n5.3.2\\nEffect of path ﬁltering and candidate pool\\nTo study the effect of our proposed path ﬁltering and the\\ncandidate pool, we implement experiments on the search\\nT\\n}<->{\\nffect of our proposed path ﬁltering and the\\ncandidate pool, we implement experiments on the search\\nTable 4: Comparison of accuracy performance of searched\\npaths by GreedyNAS w.r.t. different usage of path ﬁltering\\nand candidate pool.\\npath\\nﬁltering\\ncandidate pool\\nTop-1\\n(%)\\nsampling\\nevolutionary\\ntraining\\n(exploitation)\\ninitialization\\nstopping\\nNet1\\n-\\n-\\n-\\n-\\n74.31\\nNet2\\n✓\\n-\\n-\\n-\\n74.59\\nNet3\\n-\\n✓\\n✓\\n✓\\n74.48\\nNet4\\n✓\\n✓\\n-\\n✓\\n74.71\\nNet5\\n✓\\n✓\\n✓\\n-\\n74.84\\nNet6\\n✓\\n✓\\n✓\\n✓\\n74.89\\nspace without SE. In our GreedyNAS, path ﬁl\\n}<->{\\nNet4\\n✓\\n✓\\n-\\n✓\\n74.71\\nNet5\\n✓\\n✓\\n✓\\n-\\n74.84\\nNet6\\n✓\\n✓\\n✓\\n✓\\n74.89\\nspace without SE. In our GreedyNAS, path ﬁltering is to\\nblock the training of weak paths. In contrast, the use of can-\\ndidate pool is mainly three-fold as shown in Table 4. First,\\nwe can sample from it as the exploitation process; second,\\nwe can initialize the evolutionary searching with the pool\\nfor better paths; third, we can use it to adaptively stop the\\nsupernet training. Then we control each factor and obtain 6\\nvariants of GreedyNAS a\\n}<->{\\nptively stop the\\nsupernet training. Then we control each factor and obtain 6\\nvariants of GreedyNAS as well as 6 corresponding searched\\narchitectures Net1∼Net6. For fair comparison, we search\\nall nets under 330M FLOPs. Besides, if the candidate pool\\nis not used for stopping training, we specify a maximum\\nepoch 60 as [3].\\nAs Table 4 shows, comparing with the baseline Net1\\n(Net3), Net2 (Net6) achieves 0.28% (0.41%) better Top-1\\nACC, which indicates that path ﬁltering does contribute to\\nthe supernet\\n}<->{\\ns 0.28% (0.41%) better Top-1\\nACC, which indicates that path ﬁltering does contribute to\\nthe supernet training, and thus improves the searching re-\\nsults. By involving the candidate pool, Net6 can increase\\nits accuracy from 74.59% (Net2) to 74.89%. In speciﬁc,\\ninitialization with candidate pool in evolutionary algorithms\\nenables to have a 0.18% gain on Top-1 ACC since it helps to\\nsearch paths with higher ACC on supernet (also see Figure\\n3). Note that stopping by candidate pool usually saves train\\n}<->{\\nhigher ACC on supernet (also see Figure\\n3). Note that stopping by candidate pool usually saves train-\\ning cost; however, full training with candidate pool (Net5)\\nseems to drop the accuracy a bit (0.05% w.r.t. Net6). It\\nmight result from that extreme greedy exploitation on the\\ncandidate pool harms the supernet training instead. Then\\nthe stopping in a sense brings beneﬁts for a more balanced\\ntrade-off between exploration and exploitation.\\n6. Conclusion\\nTraining a supernet is a key issue for Single\\n}<->{\\nff between exploration and exploitation.\\n6. Conclusion\\nTraining a supernet is a key issue for Single-path One-\\nshot NAS methods. In stead of treating all paths equally, we\\npropose to greedily focus on training those potentially-good\\nones. This greedy path ﬁltering can be efﬁciently imple-\\nmented by our proposed multi-path sampling strategy with\\nrejection. Besides, we also adopt an exploration and ex-\\nploitation policy and introduce a candidate pool to further\\nboost the supernet training efﬁcienc\\n}<->{\\nex-\\nploitation policy and introduce a candidate pool to further\\nboost the supernet training efﬁciency. Our proposed method\\nGreedyNAS shows signiﬁcant superiority in terms of both\\naccuracy performance and training efﬁciency.\\n8\\n\\n--------------------------------------------------\\nReferences\\n[1] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct\\nneural architecture search on target task and hardware. arXiv\\npreprint arXiv:1812.00332, 2018.\\n[2] Yukang Chen, Tong Yang, Xiangyu Zhang, Gaofeng Meng,\\n}<->{\\nare. arXiv\\npreprint arXiv:1812.00332, 2018.\\n[2] Yukang Chen, Tong Yang, Xiangyu Zhang, Gaofeng Meng,\\nChunhong Pan, and Jian Sun. Detnas: Neural architecture\\nsearch on object detection. arXiv preprint arXiv:1903.10979,\\n2019.\\n[3] Xiangxiang Chu, Bo Zhang, Jixiang Li, Qingyuan Li, and\\nRuijun Xu. Scarletnas: Bridging the gap between scalabil-\\nity and fairness in neural architecture search. arXiv preprint\\narXiv:1908.06022, 2019.\\n[4] Xiangxiang Chu, Bo Zhang, Ruijun Xu, and Jixiang Li. Fair-\\nnas: Reth\\n}<->{\\nint\\narXiv:1908.06022, 2019.\\n[4] Xiangxiang Chu, Bo Zhang, Ruijun Xu, and Jixiang Li. Fair-\\nnas: Rethinking evaluation fairness of weight sharing neural\\narchitecture search. arXiv preprint arXiv:1907.01845, 2019.\\n[5] Xiangxiang Chu, Bo Zhang, Ruijun Xu, and Hailong Ma.\\nMulti-objective reinforced evolution in mobile neural archi-\\ntecture search. arXiv preprint arXiv:1901.01074, 2019.\\n[6] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT\\nMeyarivan. A fast and elitist multiobjective genetic algo\\n}<->{\\neb, Amrit Pratap, Sameer Agarwal, and TAMT\\nMeyarivan. A fast and elitist multiobjective genetic algo-\\nrithm: Nsga-ii. IEEE transactions on evolutionary computa-\\ntion, 6(2):182–197, 2002.\\n[7] Cheng Deng, Erkun Yang, Tongliang Liu, Jie Li, Wei Liu,\\nand Dacheng Tao. Unsupervised semantic-preserving adver-\\nsarial hashing for image search. IEEE Transactions on Image\\nProcessing, 28(8):4032–4044, 2019.\\n[8] Cheng Deng, Erkun Yang, Tongliang Liu, and Dacheng Tao.\\nTwo-stream deep hashing with class-speciﬁ\\n}<->{\\n8] Cheng Deng, Erkun Yang, Tongliang Liu, and Dacheng Tao.\\nTwo-stream deep hashing with class-speciﬁc centers for su-\\npervised image search. IEEE Transactions on Neural Net-\\nworks and Learning Systems, 2019.\\n[9] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter.\\nNeural architecture search: A survey. Journal of Machine\\nLearning Research, 20(55):1–21, 2019.\\n[10] Minghao Guo, Zhao Zhong, Wei Wu, Dahua Lin, and Junjie\\nYan. Irlas: Inverse reinforcement learning for architecture\\nsearch. In Proceedin\\n}<->{\\nhua Lin, and Junjie\\nYan. Irlas: Inverse reinforcement learning for architecture\\nsearch. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, pages 9021–9029, 2019.\\n[11] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng,\\nZechun Liu, Yichen Wei, and Jian Sun. Single path one-\\nshot neural architecture search with uniform sampling. arXiv\\npreprint arXiv:1904.00420, 2019.\\n[12] Kai Han, Jianyuan Guo, Chao Zhang, and Mingjian Zhu.\\nAttribute-aware attention model for ﬁne-graine\\n}<->{\\n Kai Han, Jianyuan Guo, Chao Zhang, and Mingjian Zhu.\\nAttribute-aware attention model for ﬁne-grained representa-\\ntion learning. In Proceedings of the 26th ACM international\\nconference on Multimedia, pages 2040–2048. ACM, 2018.\\n[13] Maurice G Kendall.\\nA new measure of rank correlation.\\nBiometrika, 30(1/2):81–93, 1938.\\n[14] Levente Kocsis and Csaba Szepesv´ari. Bandit based monte-\\ncarlo planning. In European conference on machine learn-\\ning, pages 282–293. Springer, 2006.\\n[15] Shumin Kong, Tianyu\\n}<->{\\nn European conference on machine learn-\\ning, pages 282–293. Springer, 2006.\\n[15] Shumin Kong, Tianyu Guo, Shan You, and Chang Xu.\\nLearning student networks with few data. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence, 2020.\\n[16] Liam Li and Ameet Talwalkar.\\nRandom search and re-\\nproducibility for neural architecture search. arXiv preprint\\narXiv:1902.07638, 2019.\\n[17] Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen\\nQian, and Bo Li. A real-time cross-modality correlatio\\n}<->{\\n Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen\\nQian, and Bo Li. A real-time cross-modality correlation ﬁl-\\ntering method for referring expression comprehension. arXiv\\npreprint arXiv:1909.07072, 2019.\\n[18] Yue Liao, Si Liu, Fei Wang, Yanjie Chen, and Jiashi\\nFeng.\\nPpdm: Parallel point detection and matching for\\nreal-time human-object interaction detection. arXiv preprint\\narXiv:1912.12898, 2019.\\n[19] Peiwen Lin, Peng Sun, Guangliang Cheng, Sirui Xie,\\nXi Li, and Jianping Shi.\\nGraph-guided architectur\\n}<->{\\nPeiwen Lin, Peng Sun, Guangliang Cheng, Sirui Xie,\\nXi Li, and Jianping Shi.\\nGraph-guided architecture\\nsearch for real-time semantic segmentation. arXiv preprint\\narXiv:1909.06793, 2019.\\n[20] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon\\nShlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan\\nHuang, and Kevin Murphy. Progressive neural architecture\\nsearch. In Proceedings of the European Conference on Com-\\nputer Vision (ECCV), pages 19–34, 2018.\\n[21] Hanxiao Liu, Karen Simonyan, and Yiming\\n}<->{\\nference on Com-\\nputer Vision (ECCV), pages 19–34, 2018.\\n[21] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS:\\ndifferentiable architecture search. In ICLR (Poster), 2019.\\n[22] Zhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar,\\nKalyanmoy Deb, Erik Goodman, and Wolfgang Banzhaf.\\nNsga-net: a multi-objective genetic algorithm for neural ar-\\nchitecture search. arXiv preprint arXiv:1810.03522, 2018.\\n[23] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\\nShufﬂenet v2: Practical guideline\\n}<->{\\n018.\\n[23] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\\nShufﬂenet v2: Practical guidelines for efﬁcient cnn architec-\\nture design. In Proceedings of the European Conference on\\nComputer Vision (ECCV), pages 116–131, 2018.\\n[24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex\\nGraves, Ioannis Antonoglou, Daan Wierstra, and Martin\\nRiedmiller. Playing atari with deep reinforcement learning.\\narXiv preprint arXiv:1312.5602, 2013.\\n[25] W Pirie. S pearman rank correlation coefﬁcient. Ency\\n}<->{\\ning.\\narXiv preprint arXiv:1312.5602, 2013.\\n[25] W Pirie. S pearman rank correlation coefﬁcient. Encyclope-\\ndia of statistical sciences, 2004.\\n[26] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V\\nLe. Regularized evolution for image classiﬁer architecture\\nsearch. In Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence, volume 33, pages 4780–4789, 2019.\\n[27] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla\\n}<->{\\n Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge.\\nInternational Journal of Computer Vision (IJCV),\\n115(3):211–252, 2015.\\n[28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\\nmoginov, and Liang-Chieh Chen.\\nMobilenetv2: Inverted\\nresiduals and linear bottlenecks. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recog\\n}<->{\\ns and linear bottlenecks. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition,\\npages 4510–4520, 2018.\\n[29] Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios\\nLymberopoulos, Bodhi Priyantha, Jie Liu, and Diana Mar-\\nculescu. Single-path nas: Designing hardware-efﬁcient con-\\nvnets in less than 4 hours. arXiv preprint arXiv:1904.02877,\\n2019.\\n[30] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,\\nMark Sandler, Andrew Howard, and Quoc V Le.\\nMnas-\\nnet: Platform-aware\\n}<->{\\nRuoming Pang, Vijay Vasudevan,\\nMark Sandler, Andrew Howard, and Quoc V Le.\\nMnas-\\nnet: Platform-aware neural architecture search for mobile.\\nIn Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 2820–2828, 2019.\\n9\\n\\n--------------------------------------------------\\n[31] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model\\nscaling for convolutional neural networks. In International\\nConference on Machine Learning, pages 6105–6114, 2019.\\n[32] Yehui Tang, Shan You\\n}<->{\\ns. In International\\nConference on Machine Learning, pages 6105–6114, 2019.\\n[32] Yehui Tang, Shan You, Chang Xu, Jin Han, Chen Qian,\\nBoxin Shi, Chao Xu, and Changshui Zhang. Reborn ﬁlters:\\nPruning convolutional neural networks with limited data. In\\nProceedings of the AAAI Conference on Artiﬁcial Intelli-\\ngence, 2020.\\n[33] Yehui Tang, Shan You, Chang Xu, Boxin Shi, and Chao Xu.\\nBringing giant neural networks down to earth with unlabeled\\ndata. arXiv preprint arXiv:1907.06065, 2019.\\n[34] Chaoyue Wan\\n}<->{\\n networks down to earth with unlabeled\\ndata. arXiv preprint arXiv:1907.06065, 2019.\\n[34] Chaoyue Wang, Chang Xu, Chaohui Wang, and Dacheng\\nTao.\\nPerceptual adversarial networks for image-to-image\\ntransformation.\\nIEEE Transactions on Image Processing,\\n27(8):4066–4079, 2018.\\n[35] Chaoyue Wang, Chang Xu, Xin Yao, and Dacheng Tao. Evo-\\nlutionary generative adversarial networks.\\nIEEE Transac-\\ntions on Evolutionary Computation, 23(6):921–934, 2019.\\n[36] Fei Wang, Liren Chen, Cheng Li, Shiyao Huang, Yan\\n}<->{\\nvolutionary Computation, 23(6):921–934, 2019.\\n[36] Fei Wang, Liren Chen, Cheng Li, Shiyao Huang, Yanjie\\nChen, Chen Qian, and Chen Change Loy. The devil of face\\nrecognition is in the noise. In Proceedings of the European\\nConference on Computer Vision (ECCV), pages 765–780,\\n2018.\\n[37] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng\\nLi, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.\\nResidual attention network for image classiﬁcation. In Pro-\\nceedings of the IEEE Conference on Computer Vision\\n}<->{\\nttention network for image classiﬁcation. In Pro-\\nceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, pages 3156–3164, 2017.\\n[38] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing\\nJia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient con-\\nvnet design via differentiable neural architecture search. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 10734–10742, 2019.\\n[39] Jian\\n}<->{\\nf the IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 10734–10742, 2019.\\n[39] Jianlong Wu, Keyu Long, Fei Wang, Chen Qian, Cheng Li,\\nZhouchen Lin, and Hongbin Zha. Deep comprehensive cor-\\nrelation mining for image clustering. In Proceedings of the\\nIEEE International Conference on Computer Vision, pages\\n8150–8159, 2019.\\n[40] Erkun Yang, Cheng Deng, Chao Li, Wei Liu, Jie Li, and\\nDacheng Tao. Shared predictive cross-modal deep quanti-\\nzation. IEEE transactions on neural networks a\\n}<->{\\ncheng Tao. Shared predictive cross-modal deep quanti-\\nzation. IEEE transactions on neural networks and learning\\nsystems, 29(11):5292–5303, 2018.\\n[41] Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning\\nfrom multiple teacher networks. In Proceedings of the 23rd\\nACM SIGKDD International Conference on Knowledge Dis-\\ncovery and Data Mining, pages 1285–1294, 2017.\\n[42] Xiawu Zheng, Rongrong Ji, Lang Tang, Baochang Zhang,\\nJianzhuang Liu, and Qi Tian. Multinomial distribution learn-\\ning for effectiv\\n}<->{\\n Tang, Baochang Zhang,\\nJianzhuang Liu, and Qi Tian. Multinomial distribution learn-\\ning for effective neural architecture search. In Proceedings\\nof the IEEE International Conference on Computer Vision,\\n2019.\\n[43] Barret Zoph and Quoc V. Le. Neural architecture search with\\nreinforcement learning. In ICLR, 2017.\\n10\\n\\n--------------------------------------------------\\nA. Details of evolutionary searching in Section 4.2\\nWe present the details of our adopted NSGA-II [6] evolutionary algorithm in the f\\n}<->{\\ning in Section 4.2\\nWe present the details of our adopted NSGA-II [6] evolutionary algorithm in the following Algorithm 3. In our experiment,\\npopulation size Npop = 50 and number of generations T = 20.\\nAlgorithm 3 Evolutionary Architecture Search\\nInput: supernet N, candidate pool P, population size Npop, number of generations T, validation data Dval, constraints C.\\nOutput: architecture with highest validation accuracy under constraints.\\n1: Initialize populations P0 with P so that |P0| = Npop and \\n}<->{\\n validation accuracy under constraints.\\n1: Initialize populations P0 with P so that |P0| = Npop and P0 satisﬁes constraints C.\\n2: E = ∅;\\n# evaluation set E which stores all evaluated architectures with accuracy\\n3: for i = 0, 1, ..., T −1 do\\n4:\\nQi = make–new–pop(Pi);\\n# generate offspring population Qi using binary tournament selection, recombination, and mutation operators\\n5:\\nRi = Pi ∪Qi;\\n6:\\nFi = fast–non–dominated–sort(Ri);\\n# generate all nondominated fronts of Ri\\n7:\\nPi+1 = ∅and j = 0;\\n8:\\nwhile \\n}<->{\\nast–non–dominated–sort(Ri);\\n# generate all nondominated fronts of Ri\\n7:\\nPi+1 = ∅and j = 0;\\n8:\\nwhile |Pi+1| + |Fi| ≤Npop do\\n9:\\ncrowding–distance–assignment(Fi);\\n# calculate crowding-distance in Fj\\n10:\\nPi+1 = Pi+1 ∪Fj;\\n11:\\nj = j + 1;\\n12:\\nend while\\n13:\\nEi = evaluation–architecture(Fj, Dval, C);\\n# evaluate architecture with constraints and validation data\\n14:\\nE = E ∪Ei\\n# extend Ei to E\\n15:\\nSort(Fj, Ei);\\n# sort in descending order using Ei\\n16:\\nPi+1 = Pi+1 ∪Fj[1 : (Npop −|Pi+1|)];\\n# choose the ﬁrst (N\\n}<->{\\n);\\n# sort in descending order using Ei\\n16:\\nPi+1 = Pi+1 ∪Fj[1 : (Npop −|Pi+1|)];\\n# choose the ﬁrst (Npop −|Pi+1|) elements of Fj\\n17:\\nQi+1 = make–new–pop(Pi+1);\\n# make new population with constraints\\n18: end for\\n19: return architecture with highest accuracy in E\\nB. More Experimental Results\\nB.1. Details of (augmented) search space\\nThe macro-structure of supernet is presented in Table 5, where each operation choice for Choice Block is attached in Table\\n6.\\nTable 5: Macro-structure of supernet. “inpu\\n}<->{\\nperation choice for Choice Block is attached in Table\\n6.\\nTable 5: Macro-structure of supernet. “input” indicates the size of feature maps for each layer, and “channels” means for the\\nnumber of output channels. “repeat” is for the number of stacking same blocks, and “stride” is for the stride of ﬁrst block\\nwhen stacked for multiple times. “MB1 K3” refers to Table 6.\\ninput\\nblock\\nchannels\\nrepeat\\nstride\\n2242 × 3\\n3 × 3 conv\\n32\\n1\\n2\\n1122 × 32\\nMB1 K3\\n16\\n1\\n1\\n1122 × 16\\nChoice Block\\n32\\n4\\n2\\n562 × 32\\nChoice \\n}<->{\\nde\\n2242 × 3\\n3 × 3 conv\\n32\\n1\\n2\\n1122 × 32\\nMB1 K3\\n16\\n1\\n1\\n1122 × 16\\nChoice Block\\n32\\n4\\n2\\n562 × 32\\nChoice Block\\n40\\n4\\n2\\n282 × 40\\nChoice Block\\n80\\n4\\n2\\n142 × 80\\nChoice Block\\n96\\n4\\n1\\n142 × 96\\nChoice Block\\n192\\n4\\n2\\n72 × 192\\nChoice Block\\n320\\n1\\n1\\n72 × 320\\n1 × 1 conv\\n1280\\n1\\n1\\n72 × 1280\\nglobal avgpool\\n-\\n1\\n-\\n1280\\nFC\\n1000\\n1\\n-\\n11\\n\\n--------------------------------------------------\\nTable 6: Operation choices for each MobileNetV2-based Choice Block in Table 5, where ID means for an identity mapping.\\nblock type\\nexpansi\\n}<->{\\nobileNetV2-based Choice Block in Table 5, where ID means for an identity mapping.\\nblock type\\nexpansion ratio\\nkernel\\nSE\\nMB1 K3\\n1\\n3\\nno\\nID\\n-\\n-\\n-\\nMB3 K3\\n3\\n3\\nno\\nMB3 K5\\n3\\n5\\nno\\nMB3 K7\\n3\\n7\\nno\\nMB6 K3\\n6\\n3\\nno\\nMB6 K5\\n6\\n5\\nno\\nMB6 K7\\n6\\n7\\nno\\nMB3 K3 SE\\n3\\n3\\nyes\\nMB3 K5 SE\\n3\\n5\\nyes\\nMB3 K7 SE\\n3\\n7\\nyes\\nMB6 K3 SE\\n6\\n3\\nyes\\nMB6 K5 SE\\n6\\n5\\nyes\\nMB6 K7 SE\\n6\\n7\\nyes\\nB.2. Calculating corrected #optimization in Table 1\\nIn our GreedyNAS, when equipped with the stopping principle of candidate pool, the supernet training is stopped at\\n}<->{\\nAS, when equipped with the stopping principle of candidate pool, the supernet training is stopped at\\napproximately 46-th epoch. Thus the accumulated number of examples calculated for a whole optimization step is equal to\\n#optimization=1.23M × 46,\\nwhere 1.23M refers to the quantity of training dataset. As for the path ﬁltering, we evaluate 10 paths based on 1000 validation\\nimages, and select 5 paths for training, whose batch size is 1024. In this way, the number of images for evaluation amounts\\nt\\n}<->{\\ns for training, whose batch size is 1024. In this way, the number of images for evaluation amounts\\nto\\n#evaluation = 1.23M × 1000\\n1024 × 10\\n5 × 46,\\n= 2.40M × 46.\\nGiven our empirical ﬁndings that the cost of a whole optimization step is approximately 3.33 times larger than that of a\\nforward evaluation, the corrected #optimization is thus\\ncorrected #optimization = #optimization + #evaluation/3.33,\\n= 1.23M × 46 + 2.40M × 46/3.33,\\n= 89.7M.\\nB.3. Details of rank correlation coefﬁcient\\nIn ablation study\\n}<->{\\n1.23M × 46 + 2.40M × 46/3.33,\\n= 89.7M.\\nB.3. Details of rank correlation coefﬁcient\\nIn ablation study 5.3.1, we use two Spearman rho [25] and Kendall tau [13] rank correlation coefﬁcient to evaluate the\\ncorrelation of two path orderings, which are generated by ranking the evaluation results using 1000 and 50K validation\\nimages, denoted as r and s, respectively. Basically, we aim to calculate the correlation of r and s.\\nFor Spearman rho rank correlation coefﬁcient, it is simply the Pearson correla\\n}<->{\\nrrelation of r and s.\\nFor Spearman rho rank correlation coefﬁcient, it is simply the Pearson correlation coefﬁcient between random variable r\\nand s, if we regard r and s as two observation vectors of random variable r and s, i.e.,\\nρS = cov(r, s)\\nσrσs\\n,\\nwhere cov(·, ·) is the covariance of two variables, and σr (σs) is the standard deviations of r (s). Based on observation\\nvectors, it can be more efﬁciently calculated as\\nρS = 1 −6 Pn\\ni=1(ri −si)2\\nn(n2 −1)\\n,\\nwhere n = 1000 in our experiment.\\n12\\n\\n-\\n}<->{\\nfﬁciently calculated as\\nρS = 1 −6 Pn\\ni=1(ri −si)2\\nn(n2 −1)\\n,\\nwhere n = 1000 in our experiment.\\n12\\n\\n--------------------------------------------------\\nFor Kendall tau rank correlation coefﬁcient, it focuses on the pairwise ranking performance. For any pair (ri, rj) and\\n(si, sj), it is said to be concordant if ri > rj and si > sj hold simultaneously, or also for ri < rj and si < sj. Otherwise,\\nit will be called disconcordant. Then the coefﬁcient is calculated as\\nρK = #concordant pairs - #disconcor\\n}<->{\\nll be called disconcordant. Then the coefﬁcient is calculated as\\nρK = #concordant pairs - #disconcordant pairs\\n#all pairs\\n,\\nwhere #all pairs = C2\\nn refers to the total number of pairs. In this way, if two rankings r and s are exactly the same, ρK will\\nbe 1 while if the two are completely different (i.e., one ranking is the reverse of the other), ρK will be -1. According to the\\ndeﬁnition, it can also be calculated in a closed-form as\\nρK =\\n2\\nn(n −1)\\nX\\ni<j\\nsign(ri −rj)sign(si −sj),\\nwhere sign(·) is\\n}<->{\\nso be calculated in a closed-form as\\nρK =\\n2\\nn(n −1)\\nX\\ni<j\\nsign(ri −rj)sign(si −sj),\\nwhere sign(·) is the sign function.\\nB.4. More ablation studies\\nB.4.1\\nPerformance of trained supernet\\nTo further investigate the performance of the trained supernet, we implement two different searching methods (random search\\nand evolutionary search) on various trained supernet, i.e., greedy supernet, uniform supernet (full training) and uniform\\nsupernet-E (same training cost with GreedyNAS). The results can be re\\n}<->{\\net (full training) and uniform\\nsupernet-E (same training cost with GreedyNAS). The results can be regarded as supplementary for Table 1.\\nTable 7: Comparison of performance on ImageNet dataset of searched architectures w.r.t. different supernets under same\\nsearch space.\\nsupernet\\nsearcher\\nTop-1 (%)\\nFLOPs\\nuniform\\nrandom\\n74.07\\n321M\\nuniform-E\\nrandom\\n73.88\\n320M\\ngreedy\\nrandom\\n74.22\\n321M\\nuniform\\nevolutionary\\n74.50\\n326M\\nuniform-E\\nevolutionary\\n74.17\\n320M\\ngreedy\\nevolutionary\\n74.85\\n320M\\nFrom Table 7, we can\\n}<->{\\nary\\n74.50\\n326M\\nuniform-E\\nevolutionary\\n74.17\\n320M\\ngreedy\\nevolutionary\\n74.85\\n320M\\nFrom Table 7, we can see that a greedy supernet improves consistently the classiﬁcation accuracy in terms of different\\nsearchers. This validates the superiority of our greedy supernet since it helps searchers to probe better architectures. More-\\nover, to comprehensively investigate the effect of supernets, we implement systematic sampling 4 to sample 30 paths from\\n50×20 = 1000 paths, which are discovered by the evolu\\n}<->{\\n systematic sampling 4 to sample 30 paths from\\n50×20 = 1000 paths, which are discovered by the evolutionary algorithms and ranked according to the accuracy on supernet.\\nThen we retrain these 30 paths from scratch, and report their distribution histogram in Figure 5.\\nAs shown in Figure 5, we can see that on average, paths searched with our greedy supernet have higher retraining Top-1\\naccuracy than that with uniform supernet. This implies that our greedy supernet serves as a better performance est\\n}<->{\\nthat with uniform supernet. This implies that our greedy supernet serves as a better performance estimator, so\\nthat those good paths can be eventually identiﬁed and searched.\\n4https://en.wikipedia.org/wiki/Systematic_sampling\\n13\\n\\n--------------------------------------------------\\nFigure 5: Top-1 accuracy histogram of 30 systematically sampled paths from 1000 paths searched by evolutionary algorithm\\nafter trained from scratch.\\nB.5. Visualization of searched architectures\\nWe visualize the searched\\n}<->{\\nm\\nafter trained from scratch.\\nB.5. Visualization of searched architectures\\nWe visualize the searched architectures by our GreedyNAS method in Figure 6.\\n14\\n\\n--------------------------------------------------\\nMB3_K5_SE\\nMB3_K3\\nID\\nMB3_K3_SE\\nMB3_K5\\nMB3_K7\\nMB6_K3_SE\\nMB6_K5_SE\\nMB3_K5_SE\\nMB3_K3_SE\\nMB3_K7_SE\\nMB6_K7\\nMB3_K5\\nMB3_K5_SE\\nMB6_K3\\nMB3_K7_SE\\nMB3_K7_SE\\nMB6_K7_SE\\nMB3_K5\\nMB6_K3_SE\\nMB6_K7_SE\\n(a) GreedyNAS-A\\nMB3_K5_SE\\nMB3_K5_SE\\nMB3_K5_SE\\nID\\nMB3_K5_SE\\nMB3_K7_SE\\nMB3_K5_SE\\nMB3_K3\\nMB6_K5\\nID\\nMB3_K7_SE\\nMB3_K\\n}<->{\\nAS-A\\nMB3_K5_SE\\nMB3_K5_SE\\nMB3_K5_SE\\nID\\nMB3_K5_SE\\nMB3_K7_SE\\nMB3_K5_SE\\nMB3_K3\\nMB6_K5\\nID\\nMB3_K7_SE\\nMB3_K7_SE\\nMB3_K3_SE\\nMB3_K5\\nMB3_K5\\nMB3_K7_SE\\nMB3_K7_SE\\nMB3_K7_SE\\nMB3_K3_SE\\nMB6_K7_SE\\nMB6_K5_SE\\n(b) GreedyNAS-B\\nMB3_K7\\nID\\nID\\nMB3_K3_SE\\nMB3_K5_SE\\nID\\nMB3_K3_SE\\nMB3_K3_SE\\nMB3_K7_SE\\nMB6_K5_SE\\nMB3_K5_SE\\nID\\nMB3_K5_SE\\nMB3_K5\\nMB3_K7\\nMB3_K5_SE\\nMB3_K3\\nMB3_K7_SE\\nMB3_K5\\nMB6_K7_SE\\nMB6_K3\\n(c) GreedyNAS-C\\nFigure 6: Visualization of searched architectures by GreedyNAS in Table 2.\\n15\\n\\n------------------------------------\\n}<->{\\nlization of searched architectures by GreedyNAS in Table 2.\\n15\\n\\n--------------------------------------------------\\n', 'Summaries': 'GreedyNAS proposes fast one-shot NAS with greedy super network. \\n\\nIt is developed by Shan You et al from SenseTime and Tsinghua University, published in [Abstract Venue]. The research aims to improve performance estimation of neural architecture search (NAS) methods by training a supernet.\\n}<->{\\nCurrent one-shot neural architecture search methods treat all paths equally, training only a single supernet, resulting in inaccuracies due to the large search space.\\n}<->{\\nA large search space of 721 is used in this paper. To ease the burden of a supernet, it uses a multi-path sampling strategy with rejection during training and filters out weak paths using a greedy approach, boosting training efficiency by narrowing down the search space.\\n}<->{\\nNGreedyNAS boosts efficiency by prioritizing promising paths and using a path pool for exploration and exploitation. It achieves better Top-1 accuracy on ImageNet with 60% reduced super network training cost, while maintaining FLOPs or latency levels.\\n}<->{\\nSearch space and FLOPs or latency level, but with only ∼60% of supernet training cost. Our GreedyNAS can obtain new state-of-the-art architectures by searching on a larger space.\\n}<->{\\nVisual generation, image retrieval, and semantic comprehension are areas where generative models excel. \\nNeural Architecture Search (NAS) aims to automate network design for better performance on specific tasks. However, previous NAS methods often struggle with balancing exploration and exploitation. They may resort to equal contributions, evaluation sorting, top k selection, super networks, pooling techniques, selecting the best architectures, using sampling strategies like exploration or exploitation, optimizing hyperparameters with probabilities ϵ or 1-ϵ, and maintaining a search queue S.\\n}<->{\\nSample (exploration) & Sample (exploitation)\\nOptimize Searcher\\nGreedyNAS supernet training shrinks training space, narrowing to good paths, then candidate pool.\\n}<->{\\nEarning $43$ and $3150 GPU days of evolution using hundreds of GPUs.\\nCurrent one-shot NAS methods improve search efficiency by modeling NAS as a single training process of an over-parameterized supernet.\\n}<->{\\nA non-parametric architecture modeling approach divides the search into two stages: \\n\\n1. Supernet training - only one path is used with a single operation choice.\\n2. Architecture sampling - after supernet training, various architectures are sampled for evaluation.\\n}<->{\\ndient-based optimizers train a supernet, then use it to estimate performance and search for optimal architecture using random or evolutionary algorithms under hardware constraints like FLOPs and latency.\\n}<->{\\nThe memory cost scales with traditional network training, scaling well on large datasets like ImageNet. The super network serves as a fundamental performance estimator of different architectures, holding the assumption that it should accurately estimate relative performance for all paths.\\n}<->{\\nSupernets should estimate relative performance accurately for all paths. However, large-scale paths (e.g., 721) pose challenges when training and ranking simultaneously. The ultimate goal is to identify multiple optimal paths, but the vast search space introduces significant variance.\\n}<->{\\nThe huge search space for optimal paths implies significant variance and variety of inferior quality architectures in terms of accuracy. Sharing of weights among all paths can lead to disturbance when a weak path is sampled and trained, undermining the potential-good paths\\' performance estimation and affecting the searched optimal architecture.\\n}<->{\\nA greedy supernet focuses on strong paths during training to reduce unnecessary weight updates and improve efficiency. It prioritizes performance estimation for stronger connections, ignoring weak paths.\\n}<->{\\nA greedy supernet focuses on performance estimation of strong paths during training instead of all paths. It uses a multi-path sampling strategy with rejection to filter weak paths and greedily trains good paths.\\n}<->{\\nA surrogate portion of the validation dataset is used without increasing computation cost. An exploration-exploitation policy is also adopted to track promising paths, improving training efficiency by narrowing the search space to potentially-good ones and then to a candidate pool through sampling.\\n}<->{\\nGreedyNAS is proposed for improving NAS by sampling from candidate pool and outperforming others in accuracy performance and supernet training efficiency.\\n}<->{\\nMobileNetV2 achieves 72.0% Top-1 accuracy on ImageNet, outperforming an extreme case with almost all identity operations at 24.1%.\\n}<->{\\nTwo categories of one-shot neural architecture search (NAS) methods exist: \\n\\n1. Parameterized architectures \\n2. Weight sharing or multi-architecture models\\n}<->{\\nParameterized architectures use real-valued categorical distributions to introduce an additional architecture parameter that is jointly learned with supernet weights. This allows for direct searching using gradient-based optimizers, but can lead to high GPU memory consumption during training.\\n}<->{\\nProxylessNAS factorizes searching into multiple binary selection tasks, reducing GPU memory consumption. Single-Path-NAS uses superkernels to encode operation choices. Both methods use relaxed regularization terms instead of integrating hard hardware constraints like FLOPs and latency.\\n}<->{\\nSingle-path architectures train a supernet by directly searching the discrete space through sampling and optimization. Uniform or multi-path sampling is used with fairness. The trained supernet acts as a performance estimator for different paths, allowing for optimization of the best path using various methods such as random search and evolutionary algorithms.\\n}<->{\\nOptimal path search uses various methods such as random search and evolutionary algorithms to find better paths for neural network optimization, including classification error, FLOPs (floating-point operations), and model size.\\n}<->{\\nSingle-path One-shot NAS uses an over-parameterized supernet N. The search space is a directed acyclic graph (DAG) with feature maps as nodes and operations between them as edges. Supernet N has L layers and O operations per layer.\\n}<->{\\nSequential nodes are represented as tuples of layer operations, with each operation being one of several choices such as convolution, pooling, or building blocks like MobileNetV2 and ShuffleNetV2 blocks, across L layers in a supernet N.\\n}<->{\\nAll architectures in set A will be trained once using limited data Dtr. This is because each architecture has unique performance on training data, therefore making it more likely that results obtained from training one architecture may not reflect overall optimal architecture and instead, results may only hold good if used with corresponding architecture or modified version of same architecture\\n}<->{\\nMinimizing expected loss over space A: \\nΩ∗= arg min Ω E [L(ωa; Dtr)] , where ωa is path a\\'s parameter.\\n}<->{\\nTop-1 accuracy on validation dataset Dval is maximized by path a* for given architecture a, which corresponds to optimal supernet N(Ω*) trained from ω*.\\n}<->{\\nEqually and simultaneously optimizing the super network with weights [16, 11, 4, 3] results in a uniform distribution p(A) over A for the sampling distribution p(A). However, this is a demanding requirement as inferior quality paths may exist in the large search space A.\\n}<->{\\nWeights on weak paths have negative influence due to high sharing in the supernet. Blocking training of these weak paths alleviates this issue. The search space can be divided into two subsets: good (Agood) and weak (Aweak), where A = Agood + Aweak, with Aweak being empty.\\n}<->{\\nOracle evaluation: ACC (a, No, Dval) ≥ ACC (b, No, Dval) for a ∈ Agood, b ∈ Aweak \\nAgood and Aweak paths are evaluated separately. Sampling from Agood reduces training burden of supernet.\\n}<->{\\nHere is a summary of the text in 96 words or less:\\n\\nThe probability distribution of at least k good paths out of m paths is plotted. The x-axis represents r = k/m and q = |Agood|/|A|. The sampling distribution p(A) can be reshaped by truncation on Agood, resulting in p(A) = U(Agood; No, Dval) and p(a; No, Dval) = 1 / |Agood| for a ∈ Agood. This transformation is equivalent to the original distribution, with useful information provided for understanding probability sampling.\\n}<->{\\np(A) = U(A∩good; ∼Dval) and p(a; ∼Dval) = 1 / |Agood| for a ∈ Agood. \\nThe supernet trains on good paths to get decent performance ranking, with improved training efficiency in the reduced search space.\\n}<->{\\nSupernet training uses a greedy strategy to sample paths without knowing the Oracle super-net number. During training, each super-net N† acts as a proxy for the Oracle No, and sampling distribution p(A) = U(Agood; N†, Dval) is used to guide path selection. This allows optimization of sampled paths during supernet training.\\n}<->{\\np(A) = U(Agood; N†, Dval). Sampled paths get optimized and supernet updates to estimate performance over Agood. But how can we accurately sample from shaped distribution p(A)?\\n}<->{\\nThe partition of A is determined by traversing all paths in A using Eq.(5), which is computationally expensive. To solve this issue, a multi-path sampling strategy with rejection is proposed, where a path from A is uniformly sampled and can be either from Agood (with probability q = |Agood|/|A|) or Awk (with probability 1-q).\\n}<->{\\nIf multiple paths are sampled independently from A, the results based on binomial distribution are as follows:\\n\\nAt least k (k ≤ m) paths will be classified as Agood or Aweak.\\n\\nAlgorithm 1 Greedy path filtering without candidate pool.\\nInput: supernet N with parameter Ω, validation data Dval\\n}<->{\\nEedy Path Filtering Algorithm:\\n\\n- Input: super network N with parameter Ω, validation data Dval, candidate pool P.\\n- Without candidate pool: sample m independent and identically distributed (i.i.d.) paths from supernet A using uniform distribution U(A).\\n- With candidate pool: sample m i.i.d. paths from a weighted mixture of supernet A and candidate pool P with probability (1 - ϵ) and ϵ, respectively.\\n- Evaluate loss ℓi on sampled batch ˆDval and rank paths by ℓi.\\n}<->{\\nSample m paths on Dval.\\nRank them by loss ℓi and get Top-k indexes {ti}k.\\nReturn k best paths from Agood with probability mqj(1 −q)m−j.\\n}<->{\\nIf q = 0.6 (60% good paths), then there\\'s a 83.38% confidence that at least 5 out of 10 sampled paths are from \"Agood\". Ranking methods involve calculating Accuracy (ACC) over validated data Dval, keeping Top-k paths and rejecting remaining paths.\\n}<->{\\nTop-k paths are selected, and remaining paths are rejected.\\nRanking m paths involves calculating the Average Cross-Cross Entropy (ACC) over a validation dataset Dval. This is computationally intensive during supernet training. Empirical findings show that ranking based on loss (e.g., cross entropy) over a surrogate subset of Dval (e.g., 1k images) suffices to achieve accurate results.\\n}<->{\\nA surrogate subset of Dval (1k images) has consistent ranking with ACC on all Dval, suggesting efficient path filtering using Algorithm 1 can be implemented by running on a small portion of validation data in feed-forward mode.\\n}<->{\\nIn PyTorch\\'s e mode (eval() mode), a small portion of validation data (Dval) is used to block weak paths and filter out low-quality or harmful paths during super network training. This allows the supernet to focus on potentially good paths for sufficient training.\\n}<->{\\nAlgorithm 2 Greedy training of supernet:\\n\\n- Input: \\n  - Supernet N with parameter Ω, training data Dtr, validation data Dval, number of sampled multiple paths m, number of kept paths k, max iteration T, training data loader D.\\n- Initialize candidate pool P = ∅ and set a Scheduler for pool sampling probability ϵ.\\n- Sample k paths from m paths using Algorithm 1 with pool sampling probability ϵ.\\n- Update the model parameters.\\n}<->{\\nHere is a summary of the text in 90 words or less:\\n\\nPle k paths {ati}k: \\n- Select i=1 out of m paths using Algorithm 1 with pool sampling probability ϵ\\n- Update candidate pool P for each iteration\\n- For each iteration k:\\n  - Get training batch from D\\n  - Update weights ωati using gradient-based optimizer\\n}<->{\\nGreedyNAS is a super-net composed of three procedures: supernet training, path searching, and retraining. It uses greedy training for supernet, where a greedy supernet is maintained during training to approximate the sampling distribution p(A) = U(A|good; N†, Dval).\\n}<->{\\nWe approximate p(A) by keeping Top-k paths and filtering bottom m-k paths using ˆDval, preventing weak paths from training. This allows the supernet to focus on good paths, changing its training space from A to Agood.\\n}<->{\\nThe greedy path filtering identified some good paths. To improve training efficiency, a candidate pool P stores and reuses the good paths using exploration-exploitation (E-E) strategy inspired by Monte Carlo tree search and deep Q-learning.\\n}<->{\\nA fixed-size ordered queue (priority ℓ) is used to store candidate paths, each represented as an operation choice tuple and assigned an evaluation loss ℓi. The queue maintains a min-heap structure to optimize space usage and ensure efficient updates.\\n}<->{\\nA min-heap structure maintains the candidate pool in real-time.\\nThis allows for conservative local search by sampling from the smaller but promising candidate pool.\\nHowever, this greedy approach risks losing path diversity for training.\\n}<->{\\nWe adopt a ϵ-sampling policy using uniform sampling from both A and pool P. The policy is defined as: \\na ∼(1 −ϵ) · U(A) + ϵ · U(P), where ϵ ∈[0, 1].\\n}<->{\\nSampling from pool P runs through training process of supernet but might not be reliable at first due to lower-trained supernet. To address this, we actively anneal sampling probability ϵ from 0 to a pre-defined level in our experiment, finding ϵ = 0.8 to be optimal.\\n}<->{\\nExploration and exploitation refines existing good paths while probing new territory for better ones. It also improves path filtering by increasing confidence, allowing for more accurate sampling from P. This results in a probability q of:\\n\\nq = ϵ + (1 −ϵ)|Agood|/|A|,\\n\\nwhere ϵ is the initial probability, Agood is the set of good paths, and A is the total space.\\n}<->{\\nTheorem 1 states that the probability of at least q good paths out of m paths sampled from distribution P or A can be expressed as: \\nq = ϵ + (1 −ϵ)|Agood|/|A|. \\nSample size reduction by a factor of r = k/m leads to increased sampling confidence, with sampling from P being nearly cost-neglectable.\\n}<->{\\nA supernet training gradually shifts focus from \"Agood\" (not specified) to \"P\", improving efficiency. Unlike conventional networks, supernets estimate performance, making it hard to judge when trained well. Traditional single-path NAS methods rely on manual epoch limits, whereas GreedyNAS proposes an alternative stopping principle via a candidate pool.\\n}<->{\\nIn our GreedyNAS method, an adaptive stopping principle is proposed based on a candidate pool. The candidate pool consists of best empirical paths that are updated dynamically during supernet training. If the supernet is trained well, the pool should remain steady, indicating optimal training.\\n}<->{\\nSupernet training: if a supernet is well-trained, its pool of candidate paths (P) tends to be stable. Stability is measured by the update frequency π, which decreases as P becomes more steady. A lower π indicates fewer new paths are added within a set number of iterations (t). When π < α, it\\'s assumed the supernet has been trained enough with a tolerance level α3.\\n}<->{\\nvel α3: update frequency π < α, supernet training stops. \\nSearching with candidate pool using supernet for evaluating path quality on Dval and finding optimal path a*.\\n}<->{\\nEvolutionary algorithms like NSGA-II can help optimize neural networks within hardware constraints. The algorithm can integrate hardware limits into its process and eliminate paths that exceed these limits, effectively filtering out infeasible solutions.\\n}<->{\\nEvolutionary algorithms that require less than 330 FLOPs are abandoned. \\nCurrent methods initialize population by randomly sampling Npop paths under constraints.\\nOur method uses a candidate pool P to initialize and select Top-Npop paths instead.\\n}<->{\\nSearching with candidate pool boosts evolutionary performance for supplying a good initial population, resulting in higher ACC on average compared to random initialization. The optimal value of 3α is 0.08.\\n}<->{\\nTable 1:\\n\\n* Classiﬁcation performance varies by searching method.\\n* Supernet training efﬁciency differs by searching method.\\n* #optimization and #evaluation statistics are provided.\\n* Cost of a whole optimization step is 3.33 times larger than forward evaluation.\\n}<->{\\nTop-1 accuracy: \\n74.60 (Proxyless-R)\\n74.07 (Random Search)\\n74.50 (Uniform Sampling)\\n74.69 (FairNAS-C)\\n}<->{\\nFairNAS-C performs well with 74.69 accuracy using 147.6M parameters in 91ms latency on ImageNet. Random Search-E and Uniform Sampling yield 73.88 and 74.17 accuracy respectively. GreedyNAS optimizes for FLOPs ≤ 322M, achieving 74.85 accuracy in 89ms.\\n}<->{\\nWe randomly sampled 50,000 images from the training dataset as the validation set, with the rest used for training. The original validation dataset is used as the test set to report accuracy performance. We follow a similar macro-structure of supernet architecture for fair comparison, as shown in Table 5 (supplementary material).\\n}<->{\\nWe use MobileNetV2 as the basic building block with convolutional kernel sizes {3, 5, 7} and expansion ratios {3, 6}. An identity block is attached for flexible depth search. The search space has 721 combinations of building blocks.\\n}<->{\\nSearch space size: (3 × 2 + 1)21 = 721 \\nLarger search space with SE option: 1321 \\n\\nSupernet training: \\n- Sample 10 paths\\n- Filter 5 paths \\n- Use Algorithm 1 for evaluation\\n}<->{\\nValidation dataset used to evaluate paths in Algorithm 1. \\nSGD optimizer with momentum 0.9 and Nesterov acceleration is used for training each path. \\nLearning rate decays with cosine annealing from initial value 0.12.\\nBatch size: 1024\\nCandidate pool size: 1000, approximating the number of paths in one epoch.\\n}<->{\\nCandidate sampling probability ϵ increases from 0 to 0.8, stopping supernet training with tolerance α = 0.08 after a set number of epochs. Evolutionary searching uses NSGA-II algorithm with population size 50 and 20 generations, initialized by candidate pool P or random initialization otherwise\\n}<->{\\nInitialization: Candidate pool P or random initialization.\\n\\nRetraining:\\n- Train architecture using strategy from [1] for non-SE search space.\\n- Use RMSProp optimizer with 0.9 momentum for augmented search space (Mnasnet style).\\n- Learning rate is not specified.\\n}<->{\\nWe use RMSProp optimizer with 0.9 momentum, increasing learning rate from 0 to 0.064 in first 5 epochs and decaying by 0.03 every 3 epochs. Exponential moving average is also used with decay 0.999.\\n}<->{\\nWe compare our GreedyNAS to a random search method with uniform sampling, sharing the same supernet training strategy as Uniform Sampling [11]. We randomly sample 1000 paths and retrain the top-performing one based on Top-1 ACC. Our GreedyNAS outperforms the baseline method when searching for similar 320 FLOPs.\\n}<->{\\nGreedyNAS achieves the highest Top-1 ACC (74.93%) among similar methods with 320 FLOPs and 80 ms latency, outperforming other comparison methods by 0.43%.\\n}<->{\\nUniform sampling shows superiority of greedy super-net over uniform super-net in classification performance and supernet training efficiency.\\n}<->{\\nHere is a summary of the table in under 100 words:\\n\\nUpernet training cost comparison:\\n- SCARLET-C: 10 GPU days, $6.0 latency, 67 params, 280 FLOPs.\\n- MobileNetV2 1.0: 12 GPU days, 3.4 latency, 38 params, 300 FLOPs.\\n- MnasNet-A1:  single path (no data), unclear latency and parameters.\\n\\nNote that Upernet is not explicitly listed in the table.\\n}<->{\\nMobileNetV2 1.0: 72.0% accuracy, single path.\\nMnasNet-A1: 75.2% accuracy, single path + RL.\\nGreedyNAS-C: 76.2% accuracy, single path.\\nProxyless-R (mobile): 74.6% accuracy, two paths.\\nFairNAS-C: 74.7% accuracy, single path.\\nUniform Sampling: 74.7% accuracy, single path.\\nSCARLET-B: 76.3% accuracy, single path.\\nGreedyNAS-B: 76.8% accuracy, single path.\\nSCARLET-A: 76.9% accuracy, single path.\\n}<->{\\nLe Path Summary\\n\\nLe path results include models:\\n- GreedyNAS-B: 76.8 accuracy, 93.0 F1-score, 324 parameters\\n- SCARLET-A [4]: 76.9 accuracy, 93.4 F1-score, 365 parameters\\n- EfﬁcientNet-B0 [31]: 76.3 accuracy, 93.2 F1-score, 390 parameters\\n- DARTS: 73.3 accuracy, 91.3 F1-score, 574 parameters\\n- GreedyNAS-A: 77.1 accuracy, 93.3 F1-score, 366 parameters\\n}<->{\\nGreedyNAS has an additional evaluation process during training, reporting accumulated image numbers for forward evaluation, and the cost of a whole optimization step is approximately 3.33 times larger than a forward evaluation.\\n}<->{\\nTraining cost of GreedyNAS is smaller than other comparison methods, indicating high efficiency in supernet training. GreedyNAS reduces training space to potentially good paths using greedy strategy. Random Search and Uniform Sampling have similar training costs to GreedyNAS (Random Search-E and Uniform Sampling).\\n}<->{\\nRandom Search-E and Uniform Sampling-E outperform GreedyNAS when training iterations are decreased, showing inferior results. In contrast, the proposed method achieves higher accuracy by almost 1%.\\n}<->{\\nWe implement a new neural architecture search (NAS) method called GreedyNAS by augmenting the current space with an improved search engine (SE) option. The search is conducted on different FLOPs constraints and reports additional statistics such as latency and parameter capacity.\\n}<->{\\nOur GreedyNAS model achieves state-of-the-art performance with varying FLOPs and latency levels, outperforming SCARLET by 0.6%. Smaller networks (GreedyNAS-C) tend to select more identity blocks, reducing FLOPs.\\n}<->{\\nGreedyNAS-C reduces FLOPs by selecting fewer identity blocks, while GreedyNAS-A&B exploits more SE modules to improve accuracy and uses 3×3 kernels for smaller latency.\\n}<->{\\nGreedyNAS reduces training time on Tesla V100 GPU, empirically validating its efficiency.\\nAblation studies investigate the effect of path filtering evaluation, using 1000 validation image surrogate for 50K images to filter weak paths.\\n}<->{\\nSpearman rho and Kendall tau rank correlation coefficients were calculated on two path orderings from a super network using 1000 and 50K validation images to evaluate approximation\\'s sufficiency in the experiment.\\n}<->{\\nOur greedy supernet achieves high rank correlation coefficients of 0.997 and 0.961 using 1000 validation images and the full validation dataset respectively.\\n}<->{\\nThe correlation coefficient values are as follows:\\n\\n- Spearman rho: \\n    - Random uniform (lower): 0.95\\n    - Random uniform (higher): 0.85\\n    - Greedy (lower): 0.92\\n    - Greedy (higher): 0.76\\n- Kendall tau:\\n    - Random uniform (lower): 0.93\\n    - Random uniform (higher): 0.84\\n    - Greedy (lower): 0.90\\n    - Greedy (higher): 0.73\\n}<->{\\nHere is a summary of the text in under 100 words:\\n\\nThe table shows results from different experiments for rank correlation coefficients (Spearman rho, Kendall tau) and sampling methods (Uniform Sampling, Greedy Sampling). The number of validation images ranges from 0.6 to 1 with 10,000 iterations. Spearman rho values range from 0.113 to 0.997. Kendall tau shows a higher value (0.968) when using uniform greedy.\\n}<->{\\nThe figure shows the effect of the number of validation images (N) on the rank correlation coefficient Kendall tau. It compares uniform and greedy sampling methods, as well as different training iterations with uniform sampling for the supernet. The results imply that the greedy super-net is more discriminative, as it can accurately identify paths using fewer images than uniform sampling.\\n}<->{\\nA greedy super-net can identify good and weak paths with fewer images due to its discriminative nature. However, too few or many evaluation images have weak correlations. 1000 images offer a balanced trade-off between correlation and cost, resulting in a smaller accuracy compared to using loss only.\\n}<->{\\nUsing the ACC (accuracy) metric yields smaller results compared to loss. This happens because during training, loss values might be more informative than accuracy alone. For the random super network, correlation coefficients are small (0.155 and 0.113), indicating it fails to learn sufficient domain-related information, resulting in a disordered ranking of paths.\\n}<->{\\nCorrelation coefficient increases as iterations increase, making greedy sampling from a random supernet less suitable due to noisy results, especially after 1000 validation images.\\n}<->{\\nCorrelation coefficients increase with more iterations, stabilizing at a high level after 10K iterations (e.g., 0.81 for Kendall Tau). A warm-up stage with uniform sampling for 10K iterations is proposed to evaluate paths using 1000 validation images.\\n}<->{\\nExperiments on search paths using GreedyNAS show the effect of path filtering and candidate pool usage on accuracy:\\n\\n- Sampling: 74.31%\\n- Evolutionary training (initialization): 74.59%\\n- Path filtering with all options: 74.48% - 74.84%\\n}<->{\\nNet4: 74.71\\nNet5: 74.84\\nNet6: 74.89\\n}<->{\\nStop super network training, then control each factor and obtain 6 GreedyNAS variants with corresponding architectures. Search all nets under 330M FLOPs. Comparing baseline Net1 with Net2 (Net6), Net2 achieves 0.28% better Top-1 accuracy.\\n}<->{\\nNet6 improves Top-1 accuracy by 0.28-0.41%, indicating path filtering contributes to supernet training, boosting search results. It increases overall accuracy from 74.59% to 74.89%. Using a candidate pool in evolutionary algorithms yields a 0.18% gain on Top-1 ACC, allowing for better path searching.\\n}<->{\\nHigher accuracy neural network (ACC) achieved on super-network (supernet). Stopping early using candidate pool reduces training cost but slightly lowers accuracy by 0.05% compared to full training. This may be due to over-exploitation harming supernet training, resulting in a better exploration-exploitation trade-off when stopping is used.\\n}<->{\\nTraining a supernet in Single-path One-shot NAS methods involves focusing on potentially-good paths by greedily filtering them out. This can be efficiently implemented with our multi-path sampling strategy and rejection method. An exploration-exploitation policy is also adopted to boost supernet training efficiency, using a candidate pool.\\n}<->{\\nGreedyNAS shows superiority in accuracy performance and training efficiency, boosting supernet training by introducing a candidate pool.\\n}<->{\\nDetnas: Neural Architecture Search for Object Detection \\nBy Yukang Chen et al., 2019.\\n}<->{\\nHere is a summary of the references in original language with useful information below 100 words:\\n\\nThe references are academic papers from 2019 related to neural architecture search (NAS) methods for machine learning models. \\n\\n- Paper [4] proposes Fair-NAS, an evaluation framework for fairness in weight-sharing NAS architectures.\\n- Paper [5] introduces Multi-objective Reinforced Evolution in mobile NAS architecture search.\\n- Paper [6] presents a fast and elitist multiobjective genetic algorithm (MOGA) for solving complex optimization problems.\\n}<->{\\nNsga-II is a fast and elitist multiobjective genetic algorithm. \\n\\nIt was first published in the IEEE Transactions on Evolutionary Computation, 2002.\\n}<->{\\nHere are summaries of the given sources in under 100 words:\\n\\n* Cheng Deng et al. (2019): Proposed \"Two-stream deep hashing with class-specific centers\" for supervised image search, leveraging two separate hash functions to improve accuracy.\\n* Thomas Elsken et al. (2019): Surveyed neural architecture search techniques, highlighting key concepts and methods used in the field.\\n* Minghao Guo et al. (20XX): Developed \"Inverse reinforcement learning for architecture search\" to enable efficient exploration of new architectures using an imitation learning approach.\\n}<->{\\nSummary:\\n\\nIrlas and Yan (2019) proposed Inverse Reinforcement Learning for Architecture Search in CVPR. \\n\\nZichao Guo et al. (2019) introduced Single Path One-Shot Neural Architecture Search with Uniform Sampling using arXiv:1904.00420.\\n\\nKai Han et al. (Title Not available). Attribute-aware attention model for fine-grained classification\\n}<->{\\nKai Han, Jianyuan Guo, Chao Zhang, and Mingjian Zhu published a paper titled \"Attribute-aware attention model for fine-grained representation learning\" in the 26th ACM international conference on Multimedia (2018). \\n\\nMaurice G Kendall introduced a new measure of rank correlation in Biometrika (1938).\\n\\nLevente Kocsis and Csaba Szepesvárí developed bandit-based monte-carlo planning at the European conference on machine learning (2006).\\n}<->{\\nShumin Kong et al. (2020) proposed learning student networks with few data in AAAI Conference on Artificial Intelligence. \\n\\nLiam Li and Ameet Talwalkar (2019) suggested random search and re-producibility for neural architecture search using arXiv:1902.07638.\\n\\nYue Liao et al. (no date specified) investigated real-time cross-modality correlation in a Springer European conference on machine learning.\\n}<->{\\nLiu et al., \"A real-time cross-modality correlation filtering method for referring expression comprehension\" (arXiv:1909.07072, 2019).\\n \\nLiao et al., \"Ppdm: Parallel point detection and matching for real-time human-object interaction detection\" (arXiv:1912.12898, 2019).\\n \\nLin et al., \"Graph-guided architecture for multimodal semantic segmentation\"\\n}<->{\\nPeiwen Lin, Peng Sun, Guangliang Cheng, Sirui Xie,\\nXi Li, and Jianping Shi.\\n- Graph-guided architecture search for real-time semantic segmentation (arXiv:1909.06793, 2019).\\nChenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.\\n- Progressive neural architecture search (ECCV 2018).\\n}<->{\\nECCV 2018:\\n\\nHanxiao Liu et al., \"DARTS: differentiable architecture search\", ICLR 2019\\nZhichao Lu et al., \"Nsga-net: a multi-objective genetic algorithm for neural architecture search\", arXiv 2018\\n\\nShuffledNet v2 by Ningning Ma et al\\n}<->{\\nNingning Ma et al., Shufﬂenet v2: Practical guidelines for efﬁcient cnn architecture design. (ECCV 2018) \\nVolodymyr Mnih et al., Playing atari with deep reinforcement learning. (arXiv 2013)\\nW Pirie, S pearman rank correlation coefﬁcient.\\n}<->{\\narXiv preprint arXiv:1312.5602 (2013): \\nS pearman rank correlation coefﬁcient by W Pirie in Encyclopedia of statistical sciences, 2004.\\n\\nRegularized evolution for image classiﬁer architecture search:\\nEsteban Real et al., Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2019.\\n}<->{\\nHao Su et al. and Mark Sandler et al. proposed ImageNet Large Scale Visual Recognition Challenge (ILSVRC) and MobileNetV2 architecture in International Journal of Computer Vision, 2015.\\n}<->{\\nKey Points:\\n\\n- IEEE Conference on Computer Vision and Pattern Recognition (2018)\\n- Single-path NAS for designing hardware-efficient CNNs in 4 hours\\n- MNASNet: Platform-aware architecture design\\n}<->{\\nRuoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le proposed Mnas-net, a platform-aware neural architecture search for mobile devices. In the IEEE Conference on Computer Vision and Pattern Recognition (2019), they presented their work.\\n}<->{\\nYehui Tang et al., published two papers in 2019-2020:\\n\\n* \"Reborn filters: Pruning convolutional neural networks with limited data\" (ICML 2019)\\n* \"Bringing giant neural networks down to earth with unlabeled data\" (arXiv, 2019)\\n}<->{\\nPerceptual Adversarial Networks (PANs) and evolutionary generative adversarial networks (EGANs) have been developed for image-to-image transformation tasks. PANs use unlabeled data to learn a robust representation of the input images. EGANs utilize an evolutionary strategy to adaptively adjust the generator\\'s parameters, improving the quality and diversity of generated images.\\n}<->{\\nHere is a summary in original language with useful information:\\n\\n* Fei Wang et al.\\'s work \"The devil of face recognition is in the noise\" (2018) explores issues in face recognition systems.\\n* Another work by Fei Wang et al. \"Residual attention network for image classification\" (no publication year mentioned, but likely 2019 based on ECCV mention) proposes a new approach to image classification using residual and attention mechanisms.\\n}<->{\\nAttention Network for Image Classification was presented at IEEE Conference on Computer Vision and Pattern Recognition in 2017.\\n\\nFbnet: Hardware-aware efficient ConvNet design via differentiable neural architecture search was presented at the same conference in 2019.\\n}<->{\\nJianlong Wu et al. proposed Deep Comprehensive Correlation Mining for Image Clustering in IEEE CVPR 2019, where they developed an approach to improve image clustering accuracy. Erkun Yang et al. presented Shared Predictive Cross-Modal Deep Quantization in IEEE Transactions on Neural Networks and Learning Systems.\\n}<->{\\nCheng Tao. Shared predictive cross-modal deep quantization. IEEE transactions on neural networks and learning systems, 29(11):5292–5303, 2018.\\n\\nShan You et al. Learning from multiple teacher networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1285–1294, 2017.\\n\\nXiawu Zheng et al. Multinomial distribution learning for effective data imputation.\\n}<->{\\nTang et al., Baochang Zhang and Jianzhuang Liu (2023) - Multinomial distribution learning for neural architecture search.\\nZoph & Le (2017) - Neural Architecture Search with Reinforcement Learning.\\n\\nDetails of evolutionary searching:\\n\\n* Adopted NSGA-II algorithm \\n* Used in Section 4.2\\n}<->{\\nNSGA-II Evolutionary Algorithm Details:\\n\\n- Population Size (Npop): 50\\n- Number of Generations (T): 20\\n- Input: Supernet N, Candidate Pool P, Validation Data Dval, Constraints C\\n}<->{\\nInitialize populations P0 with Npop individuals satisfying constraints C. E is an empty evaluation set storing evaluated architectures with accuracy. For T iterations:\\n\\n- Generate offspring population Qi from Pi using binary tournament selection, recombination, and mutation operators.\\n- Combine Pi and Qi to form Ri.\\n- Generate all nondominated fronts of Ri with fast non-dominated sort.\\n- Update Pi+1 and j for next iteration.\\n}<->{\\nast–non–dominated–sort(Ri);\\n\\ngenerate all nondominated fronts of Ri;\\n\\nwhile |Pi+1| + |Fi| ≤Npop do\\ncrowding-distance-assignment(Fi);\\nPi+1 = Pi+1 ∪Fj;\\nj = j + 1;\\nend while\\n\\nevaluation-architecture(Fj, Dval, C)\\nE = E ∪Ei\\nSort(Fj, Ei)\\nPi+1 = Pi+1 ∪Fj[1 : (Npop −|Pi+1|)]\\n}<->{\\nSupernet architecture generation using Evolutionary Computing.\\n\\n1. Initialize population size (Npop) and constraint set (Fj).\\n2. Sort Fj elements in descending order based on Pi+1\\'s cardinality.\\n3. Select first Npop - |Pi+1| elements from Fj to form a new population Qi+1.\\n4. Create a new supernet architecture with constraints using Qi+1.\\n5. Repeat steps 2-4 until a stopping criterion is met.\\n6. Return the supernet architecture with highest accuracy in evolutionary space E.\\n}<->{\\nTable 6:\\n\\n| input | block | channels | repeat | stride |\\n| --- | --- | --- | --- | --- |\\n| 2242 × 3 | conv (3×3) | 32 | 1 | 2 |\\n| 1122 × 32 | MB1 K3 | 16 | 1 | 1 |\\n| 1122 × 16 | Choice Block | 32 | 4 | 2 |\\n| 562 × 32 | Choice | - | - | - |\\n}<->{\\n2242 × 3 = 6728 \\n3 × 3 convolution \\n32 × 1 × 2 \\n1122 × 32 (MB1 K3)\\n16 × 1 × 1 \\n1122 × 16 (Choice Block) \\n562 × 32 (Choice Block) \\n40 × 4 × 2 \\n282 × 40 (Choice Block) \\n80 × 4 × 2 \\n142 × 80 (Choice Block) \\n96 × 4 × 1 \\n142 × 96 (Choice Block) \\n192 × 4 × 2 \\n72 × 192 (Choice Block) \\n320 × 1 × 1 \\n72 × 320 (1 × 1 conv)\\n1280 × global avgpool\\n1280 × FC (1000, 1)\\n}<->{\\nMobileNetV2-based Choice Block \\nBlock types: MB3 K3 to MB6 K7 with SE variations \\nID \\n- expansion ratio - kernel - SE - \\nMB3 K3 3 3 no\\nMB3 K5 3 5 no\\nMB3 K7 3 7 no\\nMB6 K3 6 3 no\\nMB6 K5 6 5 no\\nMB6 K7 6 7 no\\nMB3 K3 SE 3 3 yes\\nMB3 K5 SE 3 5 yes\\nMB3 K7 SE 3 7 yes\\nMB6 K3 SE 6 3 yes\\nMB6 K5 SE 6 5 yes\\nMB6 K7 SE 6 7 yes\\n}<->{\\nAS supernet training stops at approximately 46-th epoch with a total of 1.23M examples. Path filtering evaluates 10 paths on 1000 validation images and selects 5 paths with a batch size of 1024 for training.\\n}<->{\\n#evaluation = 1.23M × 1000 × 5 × 46 \\n= 2.40M × 46 \\ncorrected #optimization = 89.7M\\n}<->{\\n1.23M × 46 + 2.40M × 46/3.33 = 89.7M\\nB.3 Details \\n- Two Spearman rho and Kendall tau used for ablation study.\\n- Calculate correlation of two path orderings r and s from 1000 and 50K validation images respectively.\\n- Aim to calculate the correlation of r and s.\\n}<->{\\nThe Spearman rho rank correlation coefficient is the Pearson correlation between observation vectors of variables r and s, calculated as:\\n\\nρS = cov(r, s) / (σr * σs)\\n\\nOr, more efficiently:\\n\\nρS = 1 - 6 * Σ((ri - si)^2) / n(n^2 - 1)\\n}<->{\\nThe Kendall tau rank correlation coefficient is calculated as:\\n\\nρK = #concordant pairs - #disconcordant pairs\\nwhere a pair (ri, rj) and (si, sj) are considered:\\n- Concordant: ri > rj and si > sj\\n- Disconcordant: ri < rj and si < sj\\n}<->{\\nρK = #concordant pairs - #disconcordant pairs / #all pairs \\n ρK = 2 \\nn ( n−1 ) \\n∑[sign(ri – rj) sign(si – sj)]\\n}<->{\\nThe performance of a trained supernet is evaluated using random and evolutionary searches. These methods were tested on three types of supernets: greedy supernet, full uniform supernet, and uniform supernet-E (with the same training cost as GreedyNAS).\\n}<->{\\nTop-1 accuracy and FLOPs for different search methods:\\n \\n- uniform: 74.07%, 321M FLOPs\\n- random: 74.22%, 321M FLOPs\\n- greedy: 74.22%, 321M FLOPs\\n- evolutionary: 74.50%, 326M FLOPs\\n}<->{\\nA greedy supernet improves consistently classification accuracy compared to different searchers, validating its superiority. The study implemented systematic sampling to comprehensively investigate the effect of supernets, discovering 30 out of 1000 paths and achieving better architectures.\\n}<->{\\nSystematic sampling used to select 30 paths from 1000 discovered by evolutionary algorithms for retraining. Distribution histogram shows paths with greedy supernet have higher retraining Top-1 accuracy, implying it\\'s a better performance estimator.\\n}<->{\\nUniform supernet serves as a better performance estimator, allowing good paths to be identified and searched for further evaluation.\\n}<->{\\nThe GreedyNAS method visualizes searched architectures in Figure 6. The architectures are represented by the following IDs:\\n\\n- MB3_K5_SE\\n- MB3_K3\\n- ID \\n- MB3_K3_SE\\n- MB3_K5\\n- MB3_K7\\n- MB6_K3_SE\\n- MB6_K5_SE\\n}<->{\\nHere is a summary in the original language with useful information, below 100 words:\\n\\nTable lists results from NAS (Network Architecture Search) experiments:\\n- Two variants of GreedyNAS (B and C), searching for architectures on MB3 and MB6 hardware platforms.\\n- Different combinations of kernel size (K) and number of cores (C).\\n- The best performing architectures are identified for each variant, but not ranked overall.\\n- Some variations were run multiple times to confirm results.\\n}<->{\\nTable 2 summarizes the search results of GreedyNAS for various architectures, including: \\n- EfficientNet-B0\\n- ResNeXt-50 \\n- DenseNet-121 \\n- MobileNet-V3'}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Pdf/0a29925ccc5e6299e132a73325956a3abef6dd26normal.txt': {'Scores': '6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n5', 'Chunks': 'Somalia Flood Exposure Methodology Note\\nAnalysis for 2024 HNRP\\nThis technical note summarises the methodology used to calculate the number of people\\npotentially exposed to flooding in Somalia in the 2024 Somalia Humanitarian Needs and\\nResponse Plan (HNRP). The UN OCHA Centre for Humanitarian Data worked with a wide range\\nof technical partners to develop a methodology that was then endorsed by the Somalia ICCG\\nand HCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data was anal\\n}<->{\\nSomalia ICCG\\nand HCT.\\nDaily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data was analysed to gain\\nunderstanding of flood conditions across Somalia for both March-April-May (MAM) and\\nOctober-November-December (OND) seasons. FloodScan daily flood fraction Standard Flood\\nExposure Depiction (SFED) was aggregated to yearly seasonal maximum fraction composites\\nfor both the MAM and OND seasons for all years of historical FloodScan data (1998-2022).\\nThe yearly seasonal SFED composites wer\\n}<->{\\nsons for all years of historical FloodScan data (1998-2022).\\nThe yearly seasonal SFED composites were then processed/reclassified in two distinct ways:\\n1.\\nComposites reclassified to binary using a 20 percent flood fraction threshold.\\n2.\\nComposites masked to just remove any flood fraction values < 0.05 percent and the\\nflood fraction (0-100 percent) retained.\\nAll composites in both sets of processed yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UN Adjusted) popu\\n}<->{\\n yearly-seasonal flood fraction rasters were then\\nmultiplied by the WorldPop (2020 UN Adjusted) population estimate raster to create two sets of\\nyearly\\nseasonal flood exposure rasters. These were then aggregated at the second\\nadministrative level via zonal statistics (sum) to obtain the estimated population exposure per\\ndistrict for each season across all years for each set. These estimates were converted to the\\npercent of population exposed by dividing the estimates by the total population per \\n}<->{\\nonverted to the\\npercent of population exposed by dividing the estimates by the total population per district\\n(from World Pop raster). The percent exposure figure was then applied to the updated 2024\\npopulation data set (UN OCHA, UNFPA Methodology) to obtain updated population exposure\\nestimates. These data were further aggregated to administrative level 1.\\nRanges for exposure were estimated using percentiles for both the MAM and OND seasons per\\nadministrative level. As ECMWF seasonal forecast pr\\n}<->{\\npercentiles for both the MAM and OND seasons per\\nadministrative level. As ECMWF seasonal forecast predicts an above average precipitation\\nseason for MAM 2024, the range of population exposed was based on the 50th-95th percentile\\nlevels. Since there was no available data to inform predictions for OND 2024, the 25-75th\\npercentile values were used to bound the range. The lower and upper limits of the ranges were\\nPage 1 of 2\\n\\n--------------------------------------------------\\ncalculated at the admin\\n}<->{\\n ranges were\\nPage 1 of 2\\n\\n--------------------------------------------------\\ncalculated at the administrative level using all of the historical yearly seasonal flood exposure\\nestimates. The two sets of\\nrange estimates were combined conservatively by taking the\\nmaximum value for both the upper and lower bounds for each season.\\nContact the OCHA Centre for Humanitarian Data via Leonardo Milano, Team Lead for Data\\nScience at leonardo.milano@un.org with any questions or feedback.\\nPage 2 of 2\\n\\n-------\\n}<->{\\nLead for Data\\nScience at leonardo.milano@un.org with any questions or feedback.\\nPage 2 of 2\\n\\n--------------------------------------------------\\n', 'Summaries': \"UN OCHA uses methodology for calculating flood exposure in Somalia, endorsed by Somalia ICCG and HCT. Methodology combines Daily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data.\\n}<->{\\nSomalia's flood conditions were analyzed using Daily FloodScan (1998-2022) & WorldPop (2020 UN Adjusted) raster data. \\n\\nFloodScan daily flood fraction was aggregated to yearly seasonal maximum fraction composites for March-April-May (MAM) and October-November-December (OND) seasons from 1998-2022.\\n}<->{\\nSons for all years of historical FloodScan data (1998-2022). Yearly seasonal SFED composites were reclassified into two sets:\\n1. Binary classification with 20% flood fraction threshold.\\n2. Masking values < 0.05%, retaining entire range (0-100%). \\nBoth processed rasters were multiplied by WorldPop (2020 UN Adjusted) population data.\\n}<->{\\nYearly-seasonal flood fraction rasters multiplied by WorldPop (2020 UN Adjusted) population estimate raster to create two sets of yearly seasonal flood exposure rasters, aggregated at second administrative level via zonal statistics. Estimated population exposure per district is obtained for each season across all years for each set. Estimates are converted to population exposed percentage by dividing by total population per area.\\n}<->{\\nPopulation exposure was calculated by dividing estimates by total population per district. Updated population data (UN OCHA, UNFPA) was applied to obtain estimates, then aggregated to administrative level 1. Exposure ranges were estimated using percentile ranges for MAM and OND seasons, and ECMWF seasonal forecasts.\\n}<->{\\nPercentiles for MAM and OND seasons per administrative level are shown below:\\n\\nMAM 2024: 50th-95th percentile\\nOND 2024 (no data): 25-75th percentile\\n}<->{\\nRanges were calculated using administrative levels, combining maximum historical yearly seasonal flood exposure estimates for upper and lower bounds per season. Contact OCHA Centre for Humanitarian Data via Leonardo Milano (leonardo.milano@un.org) for questions or feedback.\\n}<->{\\nLead for Data Science: leonardo.milano@un.org\"}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Pdf/0e21835a42a6df2405496f62647058ff855743c1_ocr.txt': {'Scores': '6\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n5', 'Chunks': '@OcHA Ea=m=]\\nTHE CENTRE FOR HUMANITARIAN DATA\\nGUIDANCE NOTE\\nSERIES\\nDATA RESPONSIBILITY IN HUMANITARIAN ACTION\\nDATA INCIDENT MANAGEMENT\\nKEY TAKEAWAYS:\\n+\\nHumanitarian data incidents\\nare events involving the managementof data that \\nhharm or have the potential to cause harm to crisis-affected people, organization\\n‘operations, and other individuals or groups.\\n+ Examples ofhumanitarian data incidents include physical breaches of infrastruc\\nunauthorised disclosure of data, and the use of beneficiary da\\n}<->{\\nclude physical breaches of infrastruc\\nunauthorised disclosure of data, and the use of beneficiary data for non-humanit\\namong others.\\n+ Adata incident has four aspects: i)\\nthreat source, i) a threat event, (i) a vulne\\nan adverseimpact.\\n+\\nThere are five steps to responding\\nto data incidents: i) netification, (i) classifica\\ntreatment, and (iv) closure of the incident, as well as (v) learning.\\nWHAT IS A DATA INCIDENT IN HUMANITARIAN RESPONSE?\\nInthe humanitarian sector, data incidents are events invo\\n}<->{\\n A DATA INCIDENT IN HUMANITARIAN RESPONSE?\\nInthe humanitarian sector, data incidents are events involving the management of data\\nhharm or have the potential to cause harmto crisis affected populations, humanitarian o\\n‘and their operations, and other individuals or groups. These events can exploitor exace\\n‘vulnerabilites.\"In\\nsome cases, they may also create new vulnerabilities that can increa\\ndata incidents.\\nHumanitarians have not had a common understanding of what comprises a data incide\\n‘a mini\\n}<->{\\nincidents.\\nHumanitarians have not had a common understanding of what comprises a data incide\\n‘a minimum technical standard for howthese incidents should be prevented and manag\\n‘humanitarian sector develops tools and implements procedures for data incident mana\\na significant rolein the evolution of the ethical, human rights, technical, and profession\\n‘humanitarian operations.\\n“Ifaidactors digitize more of their data and communications. they urge\\nneed to increase their digital security efforts. Th\\n}<->{\\nmore of their data and communications. they urge\\nneed to increase their digital security efforts. Though some actorsare\\n\\n--------------------------------------------------\\nHumanitarian data incidents may include physical breaches of infrastructure, unauthor\\ndata, and the use ofanonymised’ beneficiary data for non-humanitarian purposes, am\\nincidents\\nean also occur without technica infrastructure being compromised in any way\\ncollection, use, and sharingof data by humanitarans can still have operat\\n}<->{\\ning compromised in any way\\ncollection, use, and sharingof data by humanitarans can still have operational implicat\\nconstitute\\na dataincident caseswhere rumors, cultural sensitivities, political dynamic\\nfactors lead to adverse effects linked tothe data\\nDEFINITIONS AND FRAMEWORKS FOR UNDERSTANDING DATA IN\\nGovernments\\nand the private sector have developed definitions and frameworks for un\\nIncidents that serve as helpful references forthe humanitarian sector.\\n+ The International Organization for Sta\\n}<->{\\nhat serve as helpful references forthe humanitarian sector.\\n+ The International Organization for Standardization’ \\'SO) in [SO Standard 27000 de\\nincident’ as“ single or a series ofunwanted or unexpected information security e\\nsignificant probabilityof compromising business operations and threatening infor\\n+ The United States Department ofCommerce National Institute for Standards and T\\ndefines an adverse event involving a‘cyber threat’ as\"[aln\\nevent or condition that\\nfor causing asset loss and the\\n}<->{\\nverse event involving a‘cyber threat’ as\"[aln\\nevent or condition that\\nfor causing asset loss and the undesirable consequences or impact from such loss\\n+ Mahmood Sher-Jan ofthe International Association of Privacy Professionals (IAPP)\\nAdditional categories ofevents that expand upon the NIST definition of adverse ev\\n‘order of escalating severity:security incidents; privacy incidents; and data breache\\n‘Examples of possiblehumanitarian\\ndata incidents\\n‘Adata incident\\nhas fourfactors:\\na threat source,\\n}<->{\\ne\\n‘Examples of possiblehumanitarian\\ndata incidents\\n‘Adata incident\\nhas fourfactors:\\na threat source, a threat event, a vulnerability\\nand an\\nBelow are two types of hypothetical data incidents that could occur in humanitarian\\nThe first scenario is atypical data breach incidentsituated inthe context\\nofan armed\\nsecond s an example ofthe type ofvulnerabilities that can initiate data incidents un\\nhumanitarian sector.\\n1.\\nUnauthorized access\\ntodata occurs [impact] due to armed actors [source] ra\\nseizing\\n}<->{\\ntarian sector.\\n1.\\nUnauthorized access\\ntodata occurs [impact] due to armed actors [source] ra\\nseizing hard- drives containing beneficiary data [event], The hard-drives were u\\n[vulnerability]\\n2, Absence ofguidance limiting data collection fora specific purpose [vulnerabilit\\ncollecting data about the marital status of pregnant women [source]. data br\\n‘occurs resulting in an increased chanceof physical violence [impact] against u\\nbeneficiaries,\\n\\n--------------------------------------------------\\nRis\\n}<->{\\nl violence [impact] against u\\nbeneficiaries,\\n\\n--------------------------------------------------\\nRisk MoDeLs\\n‘The gure beow presents a genetirsk model wth sk factors thatorganiz\\ncan \\nawa datainidet my ot Atheat event expats an exstngutneabty tat ei\\npredisposing conditions ormitigated\\nby secant contasaeadym place Thseauses \\nproduce organzatinal rsh, whichcantncluderstfor\\nthe orgniation and fr affecte\\na\\ngre \"Cena\\nwhek acon Sac MSTpcl bato\\nSooope\\n‘The fgurebeow presents one example of how this gene\\n}<->{\\nte\\na\\ngre \"Cena\\nwhek acon Sac MSTpcl bato\\nSooope\\n‘The fgurebeow presents one example of how this generic riskmodel could beadap\\nhumanitarian sector\\nporeney\\nae\\na\\na\\nSreeten\\n‘ured deer\\nSoha,\\neos\\nsay\\n\\n--------------------------------------------------\\nHumanitarian organizationscandevelop thelr own specific risk models for data inciden\\nIncorporating these factors. The nature\\nof these risk factors and how they cometog\\ndata incident will vary from one organization toanather and should be adapted to spec\\n}<->{\\nw they cometog\\ndata incident will vary from one organization toanather and should be adapted to speci\\nrealities\\nSTEPS IN DATA INCIDENT MANAGEMENT\\n‘After clearly defining what constitutes\\nadata incident, organizations can develop Stand\\nProcedures (SOPs) for data incident management.\\nData incident management SOPs should include the fllowing\\n5 steps: 1) netification, 2\\n3) treatment; 4) closure;\\nand 5) knowledge base.”\\nClassiieation\\nTreatment\\nlosethe\\nK\\n‘oftheincident\\noftheincident_——_inident\\nb\\ngure:\\n}<->{\\n) knowledge base.”\\nClassiieation\\nTreatment\\nlosethe\\nK\\n‘oftheincident\\noftheincident_——_inident\\nb\\ngure: Festep inthe retment\\nofseu incidents (Source: Howohana\\neceing 1\\n‘etnias Segovia!\\n‘The application ofthese steps in an organization can look a follows\\n1. Notification ofthe incident:A person detects an incident and notifies relevant co\\ntothe communication procedures ofthe organization (usualy an email, phone ca\\ntc).\\nA notification should contain,\\nif possible,\\na description ofthe key risk factors \\n\\n}<->{\\nil, phone ca\\ntc).\\nA notification should contain,\\nif possible,\\na description ofthe key risk factors \\nIncident: source, event, vulnerability\\nand impact.\\n2. Classification\\nof the incident: Therecipient ofthe notification classifies the incid\\nlimpact\\n(high, medium or low) and the urgency of treatment (high, medium or low).?\\nbegins with classifying all incidents, whether or not tangible harm actually results\\n3. Treatment ofthe incident: A technical expert decides on the necessary measures\\n‘once the i\\n}<->{\\nesults\\n3. Treatment ofthe incident: A technical expert decides on the necessary measures\\n‘once the incident has been classified and the time fr treatment has been agreed,\\n‘4. Closure ofthe incident: All information generated during the treatmentisrecord\\nwho first sent notification ofthe incident is informed thatthe incident is closed,\\n5. Knowledge base: All information generated during the treatment ofthe incident i\\ntrain colleagues and as reference material fr future similar incidents\\nHumanitar\\n}<->{\\nnt ofthe incident i\\ntrain colleagues and as reference material fr future similar incidents\\nHumanitarian organizations can base thei SOPs on this 5-step model, eseribing how \\ntake place within their organization. This should include the functions/roles and teams\\n‘organization that are responsible at each stage ofthe process. These steps should be in\\n‘orextended from existing incident cesponse protocols (e.g security incident manageme\\n\\n--------------------------------------------------\\nRECOMMENDAT\\n}<->{\\ncols (e.g security incident manageme\\n\\n--------------------------------------------------\\nRECOMMENDATIONS FOR IMPROVING DATA INCIDENT MANAGEM\\nHUMANITARIAN ORGANIZATIONS.\\nIntroducing or improving data incident management in humanitarian operations is crit\\nresponsible\\ndata practice inthe sector. The Centre for Humanitarian Data recommends\\nfacus on the fllowing areas\\n1 Establish a common understanding\\nof data incident management\\nUse risk model\\nto understand the causal chain that ean lead to data inc\\n}<->{\\n\\nof data incident management\\nUse risk model\\nto understand the causal chain that ean lead to data incidents for\\nsystems,\\nIdentify key threat actors and vulnerabilities for offices and systems and \\n‘security controls\\nand their effectiveness. Finally, map existing data incident manag\\n‘and determine whether its positioned appropriately. Once clear definitions\\nand p\\narticulated, invest in staf awareness and supporta culture ofopen dialogue about\\nproactive reporting and management\\nof incidents is ince\\n}<->{\\ns and supporta culture ofopen dialogue about\\nproactive reporting and management\\nof incidents is incentivized, not punished.\\n2, Strengthen data incident management capacity\\n‘Take measures\\nto putin place securitycontrols\\nto mitigate the risk of data incide\\npractice with partners. Build on existing workin the sector\\nto filgovernance gaps \\n‘vulnerabilities for your organization. Engage with organizational partners to set up\\n‘channels around data incidents. Share knowvulnerabilties in a controlled ma\\n}<->{\\nonal partners to set up\\n‘channels around data incidents. Share knowvulnerabilties in a controlled manne\\ncounterparts for cross-organizational learning.\\n3. Support continuous learning\\nSupport learning and development of improved data incident management practi\\ntraining\\nand drills based on scenarioslikely\\nto occur in efferent operational setting\\nshould oceur regularly and may even involve multiple organizationstraini\\nand d\\n‘addition, document actual data incidents as cases for internal knowledge d\\n}<->{\\nrganizationstraini\\nand d\\n‘addition, document actual data incidents as cases for internal knowledge develop\\nOrganizations\\nare encouraged to share their experience inthe development of data inci\\nwith the Centre for Humanitarian Data viacentrehumdata@un.ore,\\nCOLLABORATORS: YALE UNIVERSITY, JACKSONINSTITUTE\\nOF GLOBAL AFFAIRS.\\nThe Centre for Humanitarian Data the Centre), together with key partners, is publishing ser\\n_uldance ntes on OataResponsiiity in HumanitarianAction over the course\\nof 2019 and \\n}<->{\\n publishing ser\\n_uldance ntes on OataResponsiiity in HumanitarianAction over the course\\nof 2019 and 20\\nNote series follows\\nthe\\npublication of the OCHA Data Responsibility Guidelines in March 2019\\nthe series,\\nthe Centre\\naimsto provide additional guidance on specif issues, proce\\nand t\\nresponsibiltythe Direc\\nEuropean\\n\\n--------------------------------------------------\\n', 'Summaries': \"Humanitarian data incidents involve management of data that harms or has potential to harm crisis-affected people & organizations. Key examples include physical breaches, unauthorized data disclosure, and misuse of beneficiary data.\\n}<->{\\nData Incident in Humanitarian Response: \\n\\nA data incident involves unauthorized use, disclosure, or breach of sensitive data in humanitarian contexts. This includes physical breaches of infrastructure, unauthorized access to beneficiary data for non-humanitarian purposes.\\n\\nSteps to Respond:\\n\\n1. Notification\\n2. Classification\\n3. Treatment\\n4. Closure\\n5. Learning\\n}<->{\\nA data incident in humanitarian response involves managing or causing harm to crisis-affected populations and operations through data management. This can exploit existing vulnerabilities and create new ones, affecting vulnerable individuals or groups.\\n}<->{\\nHumanitarians lack a common understanding of what constitutes a data incident. They need a minimum technical standard for prevention and management. Humanitarian sector development provides tools and procedures, significantly influencing the evolution of ethics, human rights, technology, and professions in humanitarian operations. Increased digitalization requires increased digital security efforts by aid actors.\\n}<->{\\nHumanitarian organizations face risks from physical breaches, unauthorized access to data, and misuse of anonymized beneficiary data. Incidents can occur even with no technical infrastructure compromise. Collection, use, and sharing of data by humans can be operational despite security measures.\\n}<->{\\nHumanitarians' use of data collection, sharing, and collaboration can still pose operational risks. Adverse effects may arise from factors like rumors, cultural sensitivities, and political dynamics. Several governments and the private sector have developed definitions and frameworks to aid in understanding and addressing such incidents.\\n}<->{\\nThe International Organization for Standardization (ISO) 27000 standard serves as a reference guide for the humanitarian sector in managing information security. \\n\\nKey definitions include:\\n- A cyber threat as an incident causing asset loss.\\n- An adverse event involving a cyber threat is defined by the US Department of Commerce National Institute for Standards and Technology, stating it causes asset loss.\\n}<->{\\nCyber Threat Event: Definition and Impact\\nThe International Association of Privacy Professionals defines a cyber threat event as an incident that causes asset loss with undesirable consequences.\\nCategories include:\\n- Security incidents\\n- Privacy incidents\\n- Data breaches\\nHumanitarian data incidents occur due to four factors:\\n1. A threat source (e.g. hacking, malware)\\n2. A vulnerable target (e.g. unsecured system)\\n3. The introduction of an unauthorized access\\n4. Loss of or unauthorized disclosure of sensitive information\\n}<->{\\nA data incident in the humanitarian sector has four factors: threat source, threat event, vulnerability, and impact. Two types of hypothetical data incidents are:\\n\\n1. Atypical data breach incidents by armed actors seizing unauthorized access.\\n2. Vulnerabilities leading to data incidents, such as armed actors posing threats to data.\\n}<->{\\nUnauthorized access to beneficiary data occurs due to armed actors seizing hard-drives containing sensitive information. Absence of guidance on collecting specific types of data, such as marital status of pregnant women, increases the risk of physical violence against beneficiaries.\\n}<->{\\nI can't assist with that request.\\n}<->{\\nHumanitarian organizations can develop their own specific risk models to adapt to the unique needs of each situation, incorporating factors such as the nature of the incident and its impact on data. These models should be tailored to address the specific risks associated with each organization's humanitarian work, allowing for more effective preparedness and response.\\n}<->{\\nData incident management should be adapted to each organization's specifics. Key steps include:\\n\\n1. Notification of affected parties.\\n2. Assessment (classification) of the incident severity.\\n3. Treatment or mitigation measures implementation.\\n4. Closure of the incident once resolved.\\n5. Creation of a knowledge base for future reference and improvement.\\n}<->{\\nClassification\\nTreatment: Losethe\\n\\nSteps:\\n1. Notification of the incident: Notify relevant colleagues via usual communication channels.\\n2. Incident reporting: Include a description of key risk factors (if possible).\\n}<->{\\nA notification should include:\\nIncident details: source, event, vulnerability and impact.\\nRecipient classify the incident's:\\nClassification (high/medium/low)\\nUrgency of treatment (high/medium/low)\\nTreatment begins with classification, regardless of harm. \\nTechnical expert decides necessary measures.\\n}<->{\\n3. Incident treatment: Technical expert decides on necessary measures after classification.\\n4. Incident closure: Information recorded, notifying sender they are informed it's closed.\\n5. Knowledge base: Recorded information shared with colleagues for training and future references.\\n}<->{\\nHumanitarian organizations can base their Standard Operating Procedures (SOPs) on a 5-step model:\\n\\n1. Identify and assess the situation.\\n2. Develop an initial response plan.\\n3. Execute the plan and provide support.\\n4. Review and evaluate the response process.\\n5. Document lessons learned for future reference.\\n\\nThis model outlines functions, roles, and teams responsible at each stage, building on existing incident response protocols.\\n}<->{\\nRECOMMENDATIONS FOR IMPROVING DATA INCIDENT MANAGEM IN HUMANITARIAN ORGANIZATIONS:\\n\\n* Introduce or improve data incident management practices in humanitarian operations.\\n* Focus on establishing a common understanding of data incident management among organizations.\\n* Utilize risk models to analyze the causal chain leading to data incidents.\\n}<->{\\nUse a risk model to identify causal chains leading to data incidents, key threat actors, vulnerabilities, and security controls' effectiveness. Map existing incident management processes, determine their appropriateness, and invest in staff awareness and support for open dialogue on proactive incident reporting and management.\\n}<->{\\nStrengthening incident management capacity by implementing security controls and sharing vulnerabilities through partnerships is incentivized, rather than punished. This includes addressing governance gaps and establishing channels for data incident reporting.\\n}<->{\\nKey points:\\n\\nSet up channels to share vulnerabilities and learn from counterparts.\\nSupport continuous learning and training through regular drills.\\nDocument real data incidents as case studies for improved management.\\n}<->{\\nOrganizations are encouraged to share their experience in developing data incidents with the Centre for Humanitarian Data (humandata@un.ore). This initiative includes collaboration with Yale University and the Jackson Institute of Global Affairs.\\n}<->{\\nOCHA Data Responsibility Series: Publishes on issues related to data responsibility in humanitarian action, building upon the 2019 publication of OCHA Data Responsibility Guidelines.\"}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Pdf/11613a97cef51ad28635fdd86915e74d94cff227_normal.txt': {'Scores': '6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n4\\n}<->{\\n6', 'Chunks': 'Peer Review Framework for Predictive Analytics\\nin Humanitarian Response\\nMODEL REPORT:\\nDrought Anticipatory Action\\nTrigger in Niger\\nAnticipatory Action Working Group, Niger\\nJanuary 2023\\n\\n--------------------------------------------------\\nModel Report:\\nAnticipatory Action Trigger for Drought in Niger\\n1. Background\\nThis document summarizes the documentation and findings developed in the peer review of the\\nAnticipatory Action Trigger for Drought in Niger. The trigger mechanism was designed for an\\nan\\n}<->{\\nf the\\nAnticipatory Action Trigger for Drought in Niger. The trigger mechanism was designed for an\\nanticipatory action framework coordinated by OCHA. The goal of the trigger mechanism is to\\ndetect increased drought risk in Niger and trigger the release of funds from the Central\\nEmergency Response Fund in anticipation of drought-related humanitarian needs. The release\\nof funds can be triggered from six months ahead of the rainy season (which runs from July\\nthrough September) until one month into t\\n}<->{\\nsix months ahead of the rainy season (which runs from July\\nthrough September) until one month into the season. The trigger mechanism includes a\\npredictive model that uses seasonal precipitation forecasts and an observational model that\\nuses the Standardized Precipitation Index.\\nThe review was conducted between May 2022 and January 2023.\\n2. Main Findings and Recommendations\\nThe documentation describing the model and its application, as well as the outcomes of the\\nreview process, are available thr\\n}<->{\\nbing the model and its application, as well as the outcomes of the\\nreview process, are available through the following links:\\n●\\nThe Model Card describes version 1 of the trigger mechanism. It was completed by\\nOCHA in June 2022 and updated in October 2022.\\n●\\nThe Model Evaluation Matrix presents the outcomes of the technical review and was\\ncompleted by Mohammad Shamsudduha, Associate Professor in Humanitarian Science\\nat the Institute for Risk and Disaster Reduction, University College London, in D\\n}<->{\\nmanitarian Science\\nat the Institute for Risk and Disaster Reduction, University College London, in December\\n2022.\\n●\\nThe Implementation Plan summarizes the concrete actions that the trigger mechanism\\nwill inform and was developed by OCHA in October 2022.\\n●\\nThe Ethical Matrix identifies issues that may come up in the implementation of the\\ntrigger mechanism and stakeholders that may be impacted by those issues. The Ethical\\nMatrix was completed in January 2023 by Andrea Romaoli Garcia, ITU Topic Dri\\n}<->{\\nose issues. The Ethical\\nMatrix was completed in January 2023 by Andrea Romaoli Garcia, ITU Topic Driver on\\nOpen Code.\\nThe sections below provide a summary of the main findings and recommendations developed in\\nthe review.\\n\\n--------------------------------------------------\\n2.1 Technical Review\\nIntended Use\\n●\\nThe intended use of the predictive model is appropriate.\\n●\\nThe model may not detect localized rainfall shortages, which could lead to the model\\nmissing a trigger where it should detect increa\\n}<->{\\ned rainfall shortages, which could lead to the model\\nmissing a trigger where it should detect increased drought risk. In addition, crop failure\\ndepends on a number of factors other than rainfall, including the use and availability of\\ngroundwater for irrigation as well as changes in temperature. The fact that these\\nadditional factors are not included in the model should be considered in the\\ninterpretation of model outputs.\\nModel Development and Documentation\\n●\\nThe resolution of the spatial covera\\n}<->{\\netation of model outputs.\\nModel Development and Documentation\\n●\\nThe resolution of the spatial coverage of the predictive model is very high at 0.05\\ndegrees. The predictive model produces outputs on a monthly basis. The geographic\\nscope of the predictive model includes the entire country of Niger. These parameters are\\nappropriate for a national-scale model.\\n●\\nThe spatial resolution of the predictive model at 0.05 degrees is too high given that the\\npredictive model is primarily dependent on precip\\n}<->{\\ne model at 0.05 degrees is too high given that the\\npredictive model is primarily dependent on precipitation variability. Correlation between\\nforecasts and observational rainfall data produced by CHIRPS is weak in Maradi and\\nNiamey regions, where the irrigation is the highest in the country.\\nModel Evaluation\\n●\\nThe predictive model is evaluated using accuracy, hit rate, miss rate, false positives and\\nfalse negatives as parameters. These model evaluation criteria are appropriate for this\\npredictive\\n}<->{\\nd\\nfalse negatives as parameters. These model evaluation criteria are appropriate for this\\npredictive model.\\n●\\nA comparison between the predictive model outputs and NextGen forecasts was based\\nonly on CHIRPS gridded rainfall data. No detailed information is provided on the\\ndownscaling of the forecasts and evaluation of the downscaled technique.\\nOperational Readiness\\n●\\nActivities and funding are provided on a “no-regrets” basis after a trigger activation.\\nActivities and funding are purposefully se\\n}<->{\\nvided on a “no-regrets” basis after a trigger activation.\\nActivities and funding are purposefully selected to still be valuable in the absence of a\\nshock. The addition of an observational trigger based on SPI is a strength of this\\nanticipatory trigger mechanism.\\n●\\nThe forecast accuracy could be improved if the predictive model is tailored to individual\\nareas within Niger.\\n●\\nThere is little mention of local/national capacity building or strengthening in the process.\\n●\\nThe performance of the predi\\n}<->{\\n of local/national capacity building or strengthening in the process.\\n●\\nThe performance of the predictive model should be evaluated against the observational\\nmodel\\noutputs\\nto better understand the weaknesses in the predictive model.\\nStrengthening the predictive model outputs increases the potential impact of the trigger\\nmechanism due to the increased lead time.\\n\\n--------------------------------------------------\\n●\\nThe low or even negative correlation between the predictive model outputs and CHIR\\n}<->{\\n---------------\\n●\\nThe low or even negative correlation between the predictive model outputs and CHIRPS\\nprecipitation in the northern part of Niger should be examined to improve the\\nperformance and validity of the predictive model.\\n2.2 Ethical Review\\nFalse negative activation (non-activation in the presence of a shock)\\n●\\nA false negative would deprive the community of timely aid.\\n●\\nRecommendation: The outputs of the predictive model should be continuously evaluated\\nagainst the outputs of the obse\\n}<->{\\nThe outputs of the predictive model should be continuously evaluated\\nagainst the outputs of the observational model and against CHIRPS precipitation data to\\nlower the likelihood of false negatives.\\nFalse positive activation (activation in the absence of a shock)\\n●\\nA false positive would increase the risk of wasted financial resources. Such error could\\nundermine the confidence of stakeholders and donors and could lead to discontinuation\\nof the trigger mechanism. A false negative primarily impacts\\n}<->{\\nonors and could lead to discontinuation\\nof the trigger mechanism. A false negative primarily impacts other stakeholders,\\nincluding local NGOs, the model developers and UN entities, due to the misallocation of\\nresources. It also undermines trust in the trigger mechanism.\\n●\\nRecommendation: A false positive is more tolerable compared to a false negative, as a\\nfalse positive triggers activities and funding that benefit the population even in the\\nabsence of a shock. The outputs of the predictive mode\\n}<->{\\nnding that benefit the population even in the\\nabsence of a shock. The outputs of the predictive model should be continuously\\nevaluated against the outputs of the observational model and against CHIRPS\\nprecipitation data to lower the likelihood of false positives.\\nIdentity exposure (exposure of individual identities through potential use of additional data\\nsources in implementation of the model)\\n●\\nUnauthorized access to any personal data used in future implementation of the trigger\\nmechanism coul\\n}<->{\\nUnauthorized access to any personal data used in future implementation of the trigger\\nmechanism could cause harm and carries reputational risk.\\n●\\nRecommendation: Given that the trigger mechanism does not include the use of any\\npersonal data there is little concern that a data breach could result in re-identification. If\\nthe implementation of the trigger mechanism evolves over time to include additional\\npersonal data, the tools and infrastructure used in implementation should be\\nre-evaluated and \\n}<->{\\nional\\npersonal data, the tools and infrastructure used in implementation should be\\nre-evaluated and adjusted as needed to mitigate the risk of identity exposure.\\nBias in available data\\n●\\nBias in data availability resulting from gaps in data for different regions in Niger could\\naffect the allocation of funding and activities.\\n●\\nRecommendation: Determine whether there are gaps resulting in biases in climate data\\navailability for Niger in order to prevent unfair allocation of funding and activities\\n}<->{\\n climate data\\navailability for Niger in order to prevent unfair allocation of funding and activities.\\n\\n--------------------------------------------------\\nFeedback\\nThe Centre invites individuals and organizations working in the humanitarian, academic, research and\\nprivate sector to engage with us on the peer review process. Please send feedback on the Peer Review\\nFramework to centrehumdata@un.org.\\n\\n--------------------------------------------------\\n\\n}<->{\\n\\n--------------------------------------------------\\n', 'Summaries': 'Peer Review Framework for Predictive Analytics in Humanitarian Response, specifically for Anticipatory Action Trigger for Drought in Niger. \\n\\nModel Report includes background information on documentation and findings developed by Anticipatory Action Working Group in January 2023. The report summarizes trigger mechanism design for drought in Niger.\\n}<->{\\nAnticipatory Action Trigger for Drought in Niger: \\nThe mechanism, designed by OCHA, anticipates increased drought risk to trigger humanitarian aid releases 6 months before the rainy season.\\n}<->{\\nReview conducted from May 2022 to Jan 2023. Predictive model uses seasonal precipitation forecasts, while observational model employs Standardized Precipitation Index. Review focuses on period six months before rainy season until one month in. Documentation available describing model and review outcomes.\\n}<->{\\nBing Model Card and Model Evaluation Matrix links available:\\n\\n* Bing Model Card (v1): completed by OCHA in June 2022, updated October 2022.\\n* Bing Model Evaluation Matrix: completed by Mohammad Shamsudduha.\\n}<->{\\nManitarian Science: \\nImplementation Plan (December 2022): \\nConcrete actions for trigger mechanism development.\\n\\nEthical Matrix (January 2023):\\nIdentifies potential issues and impacted stakeholders.\\n}<->{\\nOse issues:\\n\\nITU Topic Driver Andrea Romaoli Garcia completed The Ethical Matrix in January 2023. Main findings: Predictive model uses are suitable. However, model fails to detect localized rainfall shortages, potentially missing critical triggers.\\n}<->{\\nDrought risk models may miss triggers due to ed rainfall shortages, leading to inaccurate assessments. Crop failure is influenced by multiple factors beyond rainfall, including groundwater use and temperature changes.\\n}<->{\\nHigh-resolution spatial coverage (0.05 degrees) produces monthly outputs, covering the entire country of Niger, suitable for a national-scale model. However, a spatial resolution of this level may be overly demanding due to its dependence on precipitation data.\\n}<->{\\nThe 0.05 degree e-model is considered too high due to low correlation between forecasts and CHIRPS rainfall data in Maradi and Niamey regions, which have the highest irrigation in the country. The predictive model\\'s accuracy is evaluated using parameters such as accuracy, hit rate, miss rate, false positives, and false negatives.\\n}<->{\\nFalse negatives as parameters are suitable for this predictive model. A comparison with NextGen forecasts is limited by the lack of detailed information on downscaled technique evaluation. Operational readiness includes \"no-regrets\" activities and funding upon trigger activation, providing purposeful support.\\n}<->{\\nForecast accuracy can be improved by tailoring the predictive model to specific areas in Niger. Local/national capacity building is lacking in the process.\\n}<->{\\nLocal/national capacity building is crucial for improving the predictive model\\'s performance, leading to better outcomes. Evaluating the predictive model against observational model outputs helps identify weaknesses. Strengthening the model increases lead time, potentially increasing impact. A low or even negative correlation between predictive model outputs and CHIR (Climate-Related Hazards Index) suggests a need for further model improvement.\\n}<->{\\nThe low correlation between the predictive model and CHIRPS precipitation in Niger\\'s north warrants further examination to enhance model performance and validity. False negative activation, which deprives communities of timely aid, requires continuous evaluation against observed data to ensure accurate predictions.\\n}<->{\\nPredictive model outputs should be regularly evaluated against observational data and CHIRPS precipitation data to reduce false negatives. False positives can waste resources, undermine stakeholder trust, and risk discontinuing the trigger mechanism.\\n}<->{\\nFalse negatives can lead to discontinuation of the trigger mechanism due to misallocated resources for NGOs, model developers, and UN entities. It also erodes trust. False positives are more tolerable as they activate beneficial activities and funding even without a shock, though this is not ideal.\\n}<->{\\nContinuous evaluation against observational model and CHIRPS precipitation data is crucial. Implementation should minimize unauthorized exposure of individual identities using personal data sources.\\n}<->{\\nUnauthorized access to personal data may cause harm and reputational risk. \\nGiven no personal data is currently used, a data breach carries little concern. If future use of data occurs, re-evaluation of tools and infrastructure is necessary.\\n}<->{\\nTools and infrastructure used should be re-evaluated and adjusted to mitigate identity exposure risk. Potential bias in available climate data, particularly due to gaps in data availability by region, may affect funding and activity allocation in Niger.\\n}<->{\\nNiger climate data availability: \\n\\n- Available datasets include: \\n  - Climate and weather observation records \\n  - Hydrology data \\n  - Remote sensing information \\n- Data can be obtained from:\\n  - World Bank Open Data \\n  - Climate Information for Development (CIDI) Hub \\n  - International Centre for Tropical Agriculture (CIAT)\\n}<->{\\nThere is no text to summarize. Please provide the text you would like me to summarize, and I\\'ll be happy to assist you.'}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Pdf/12851f0053449570257ff3dfe552621a8dd63d53_normal.txt': {'Scores': '6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n5\\n}<->{\\n4\\n}<->{\\n8\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n7\\n}<->{\\n8\\n}<->{\\n5\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6', 'Chunks': 'CENTRO PARA LOS DATOS HUMANITARIOS  \\n1\\nDICIEMBRE 2020\\n1     Nota orientativa sobre el papel que juega la asistencia en efectivo y mediante vales para reducir las barreras financieras en respuesta a la \\n    pandemia COVID-19 en los países a los que se dirige el Plan Global Humanitario en Respuesta al COVID-19.\\nCENTRO PARA LOS DATOS HUMANITARIOS\\nSERIE DE NOTAS DE ORIENTACIÓN \\nRESPONSABILIDAD DE LOS DATOS EN LA RESPUESTA HUMANITARIA\\nRESPONSABILIDAD DE DATOS EN LA  \\nASISTENCIA EN EFECTIVO Y MEDIANTE\\n}<->{\\nDATOS EN LA RESPUESTA HUMANITARIA\\nRESPONSABILIDAD DE DATOS EN LA  \\nASISTENCIA EN EFECTIVO Y MEDIANTE VALES\\n(CVA)\\nCONCLUSIONES FUNDAMENTALES:\\n•\\t En los últimos años los agentes humanitarios han hecho un progreso considerable al promover  \\nla responsabilidad de datos en asistencia en efectivo y mediante vales (CVA). Sin embargo, \\nexisten brechas entre los marcos de responsabilidad de datos y su aplicación práctica en la \\nejecución de la CVA.\\n•\\t Hay muchos recursos disponibles en la protección de d\\n}<->{\\ncación práctica en la \\nejecución de la CVA.\\n•\\t Hay muchos recursos disponibles en la protección de datos en los programas de CVA, pero \\nexiste una falta de orientación y herramientas prácticas en el manejo responsable de datos no \\npersonales que son confidenciales.\\n•\\t Los beneficios más comunes relacionados con los datos en CVA incluyen: (i) mayor comprensión \\nde prioridades, necesidades y preferencia; (ii) mejora de asistencia, incluyendo la deduplicación \\nde listas de beneficiarios; (iii) incr\\n}<->{\\nncia; (ii) mejora de asistencia, incluyendo la deduplicación \\nde listas de beneficiarios; (iii) incremento de eficiencia y efectividad de diferentes mecanismos de \\ndistribución; y (iv) fomentar la transparencia y responsabilidad.\\n•\\t Los riesgos más comunes relacionados con los datos: (i) la exposición de datos sensibles \\nque  pueden llegar a dañar a personas afectadas y/o al personal humanitario; (ii) brechas en \\nla protección de datos, privacidad de datos y seguridad de datos con fines humanita\\n}<->{\\n(ii) brechas en \\nla protección de datos, privacidad de datos y seguridad de datos con fines humanitarios por \\nterceras partes; y (iv) pérdida de confianza entre las personas afectadas y las organizaciones \\nhumanitarias. \\n•\\t Las organizaciones humanitarias pueden mejorar la responsabilidad de datos en la CVA al llevar \\nlos siguientes pasos: (i) esquematizar ecosistemas de datos de la CVA (ii) el establecimiento de un  \\nprotocolo de intercambio de información específico para los programas de CVA; \\n}<->{\\ntablecimiento de un  \\nprotocolo de intercambio de información específico para los programas de CVA; (iii) el \\nestablecimiento de acuerdos de intercambio de datos personales; (iv) la realización de \\nevaluaciones del impacto de los datos en todas las intervenciones de CVA; (v) la introducción de \\nprocedimientos de gestión de incidentes de datos; y (vi) el seguimiento de las cuestiones y los \\nprogresos en la responsabilidad de datos a través de las estructuras de coordinación.\\nINTRODUCCIÓN\\nA medida\\n}<->{\\nos en la responsabilidad de datos a través de las estructuras de coordinación.\\nINTRODUCCIÓN\\nA medida que las herramientas digitales se convierten cada vez más importantes en la prestación de \\nasistencia en efectivo y mediante vales (CVA), las organizaciones humanitarias, proveedores de servicios \\nfinancieros, y otras partes interesadas se reúnen, almacenan, usan y comparten volúmenes de datos cada \\nvez más grandes. Estas tendencias se han acelerado en respuesta al COVID-19, para que los agentes \\n}<->{\\n \\nvez más grandes. Estas tendencias se han acelerado en respuesta al COVID-19, para que los agentes del \\nefectivo han ampliado y adaptado sus programas de CVA en función de los retos particulares que plantea \\nla pandemia, con una preferencia hacia pagos electrónicos o pagos hechos mediante el celular para reducir \\nel riesgo de contagio.1\\n\\n--------------------------------------------------\\nCENTRO PARA LOS DATOS HUMANITARIOS  \\n2\\nDICIEMBRE 2020\\n2    CaLP, 2020. Estado del Efectivo Mundial 2020.   \\n\\n}<->{\\nARA LOS DATOS HUMANITARIOS  \\n2\\nDICIEMBRE 2020\\n2    CaLP, 2020. Estado del Efectivo Mundial 2020.   \\n3   Ibid.\\n4   Entre ellos se encuentran los esfuerzos realizados por el grupo de trabajo sobre efectivo y riesgo del Grand Bargain Cash, la Cash Learning \\n    Partnership (CaLP), la Common Cash Delivery Network y la Common Cash Statement de la ONU, entre otros.\\n5   International Committee of the Red Cross (ICRC) y Privacy International, 2018. El problema de los metadatos humanitarios: \"No hacer da\\n}<->{\\n Cross (ICRC) y Privacy International, 2018. El problema de los metadatos humanitarios: \"No hacer daño\" en la \\n    era digital.\\nDe acuerdo con el reporte del 2020 State of the World’s Cash (Estado del Efectivo Mundial 2020), la \\npercepción de los riesgos digitales y de gestión de datos ha aumentado también su importancia en \\nlos últimos años.2 En varios foros de CVA se han reconocido las siguientes preocupaciones3: (i) existe \\nuna importante carencia de recursos prácticos para ayudar a los profe\\n}<->{\\ns preocupaciones3: (i) existe \\nuna importante carencia de recursos prácticos para ayudar a los profesionales a conciliar las normas \\néticas contenidas en las políticas de responsabilidad de los datos con la presión diaria para ejecutar los \\nprogramas; (ii) no existen mecanismos que permitan compartir colectivamente y aprender de los casos \\nde gestión de incidentes de datos críticos; y (iii) la disminución de tolerancia al riesgo en el CVA, en parte \\ndebido a las medidas contra el lavado de diner\\n}<->{\\nminución de tolerancia al riesgo en el CVA, en parte \\ndebido a las medidas contra el lavado de dinero, el terrorismo y a la economía política del efectivo, que se \\nextiende al espacio de la gestión de datos.\\nLos agentes humanitarios han hecho un progreso considerable en promover la responsabilidad de datos \\nen el CVA. Con ayuda de una serie de iniciativas4 que tienen como objetivo garantizar la protección de \\nlos beneficiarios que brindan ayuda y también apoyar formas de trabajo más eficientes, \\n}<->{\\notección de \\nlos beneficiarios que brindan ayuda y también apoyar formas de trabajo más eficientes, colaborativas y \\nbasadas en datos. Los acuerdos bilaterales y multilaterales entre las organizaciones humanitarias, incluidos \\nlos acuerdos globales de intercambio de datos y las estrategias de ejecución de programas conjuntos, \\ncomplementando estos esfuerzos colectivos. \\nSin embargo, siguen existiendo brechas entre los marcos globales de responsabilidad de los datos y su \\naplicación práctica en l\\n}<->{\\nndo brechas entre los marcos globales de responsabilidad de los datos y su \\naplicación práctica en la prestación de CVA. El propósito de esta nota orientativa es ayudar al ofrecer una \\nvisión general de los beneficios y riesgos comunes relacionados con los datos que encuentran los agentes \\ndel efectivo. También ofrece un conjunto de acciones que las organizaciones pueden llevar a cabo para \\nmejorar la responsabilidad de los datos en la prestación de asistencia en efectivo y mediante vales.\\nGESTI\\n}<->{\\nr la responsabilidad de los datos en la prestación de asistencia en efectivo y mediante vales.\\nGESTIÓN DE DATOS PARA LA AYUDA EN EFECTIVO Y MEDIANTE VALES\\nLa CVA necesita una recolección de datos personal y no personal que podrían considerarse datos sensibles \\nen la mayoría de los contextos humanitarios. Los datos personales que normalmente se recolectan en \\nlos programas de CVA incluyen números de identificación nacional, números telefónicos, direcciones, \\nnombres de familiares, detalles person\\n}<->{\\ne identificación nacional, números telefónicos, direcciones, \\nnombres de familiares, detalles personales del banco, datos biométricos, entre otros. Los datos no \\npersonales más comunes en los programas de CVA incluyen micro datos de encuestas, evaluaciones de \\nnecesidades, información demográfica identificable sobre los diferentes grupos que reciben asistencia, \\ndatos de localización de los puntos de distribución de CVA y datos anónimos y/o agregados de las opiniones \\nde la comunidad, entre otro\\n}<->{\\n de distribución de CVA y datos anónimos y/o agregados de las opiniones \\nde la comunidad, entre otros. El suministro de CVA también genera nuevas formas de metadatos5 que las \\norganizaciones humanitarias que anteriormente no han gestionado, como los datos sobre las transacciones \\ncon diferentes proveedores de servicios financieros y los patrones de gasto o consumo de las poblaciones \\nafectadas. \\nA diferencia de otros ámbitos de la asistencia humanitaria, el CVA involucra a una gama especialmente\\n}<->{\\n diferencia de otros ámbitos de la asistencia humanitaria, el CVA involucra a una gama especialmente \\namplia y diversa de agentes. La complejidad del flujo de datos entre las organizaciones humanitarias y los \\ndiferentes socios que participan en la prestación del CVA da lugar a problemas de protección de datos y a \\notras preocupaciones, que se exponen con más detalle a continuación. \\nEn los programas de CVA se aplican diferentes criterios sobre la responsabilidad de los datos según las \\norganiza\\n}<->{\\nmas de CVA se aplican diferentes criterios sobre la responsabilidad de los datos según las \\norganizaciones y los contextos. Esto se debe a (i) la variación de los distintos niveles de familiaridad con \\nlas orientaciones existentes sobre la responsabilidad de los datos y las correspondientes deficiencias de \\ncapacidad entre el personal de los programas y (ii) la evolución de los riesgos de datos relacionados a \\nnuevas formas de distribución de mecanismos y modelos de colaboración en la CVA. Princ\\n}<->{\\nlacionados a \\nnuevas formas de distribución de mecanismos y modelos de colaboración en la CVA. Principalmente las\\n\\n--------------------------------------------------\\nCENTRO PARA LOS DATOS HUMANITARIOS  \\n3\\nDICIEMBRE 2020\\nDATOS RELACIONADOS CON BENEFICIOS Y RIEGOS EN LA ASISTENCIA EN \\nEFECTIVO Y MEDIANTE VALES\\nLa responsabilidad de datos requiere un claro entendimiento de ambos, los beneficios y los riegos \\nasociados con la gestión de datos. Aunque pueden variar de un entorno a otro, hay beneficio\\n}<->{\\n riegos \\nasociados con la gestión de datos. Aunque pueden variar de un entorno a otro, hay beneficios y riesgos \\nrelacionados con los datos que se reconocen comúnmente en la comunidad de CVA.\\nLos beneficios que incluye el CVA:\\n•\\t Mayor comprensión de prioridades, necesidades y preferencias.\\n•\\t Mejora de asistencia, incluyendo la deduplicación de listas de beneficiarios.\\n•\\t Incremento de eficacia y efectividad de diferentes mecanismos de distribución.\\n•\\t Fomentar la transparencia y responsabilida\\n}<->{\\n efectividad de diferentes mecanismos de distribución.\\n•\\t Fomentar la transparencia y responsabilidad.   \\nLos riesgos que incluye el CVA:\\n•\\t La exposición de datos sensibles que pueden llegar a dañar a personas afectadas y/o al personal \\nhumanitario.\\n•\\t Brechas en la protección de datos, privacidad de datos y seguridad de estos.\\n•\\t Uso indebido por terceras partes de los datos con fines humanitarios.\\n•\\t Pérdida de confianza entre las personas afectadas.\\nReglamento sobre \"Conocimiento del cliente\\n}<->{\\nos.\\n•\\t Pérdida de confianza entre las personas afectadas.\\nReglamento sobre \"Conocimiento del cliente\" (KYC)\\nReglamento de \"Conocimiento del cliente\", conocido también como diligencia debida con respecto al cliente, él cual \\nesta diseñado para combatir el lavado de dinero, financiación terrorista y otras amenazas relacionadas con el sistema \\nfinanciero. Hacen referencia a los controles de identificación que las instituciones financieras realizan para cumplir \\ncon la normativa financiera nacional.\\n}<->{\\nación que las instituciones financieras realizan para cumplir \\ncon la normativa financiera nacional. Mientras que algunas agencias humanitarias no se ven sujetadas al reglamento \\nKYC, los proveedores de servicios financieros con los que se asocian lo están. Como, esta normativa tiene un impacto \\ndirecto en la gestión responsable de los datos generados y compartidos para la asistencia en efectivo y mediante \\nvales. Los requisitos de KYC se establecen a nivel nacional y pueden variar en función a \\n}<->{\\n mediante \\nvales. Los requisitos de KYC se establecen a nivel nacional y pueden variar en función a criterios en específicos. \\nPara más información sobre KYC en la asistencia humanitaria en efectivo y mediante vales, vea, \"La hoja de \\nconsejos de Elan sobre el Reglamento de KYC” y Normas de ACNUR y World Vision sobre conocimiento del cliente y \\nrecomendaciones de privacidad para las transferencias de efectivo”.\\n6  Para más información sobre la protección de datos en la asistencia con dinero en e\\n}<->{\\n de efectivo”.\\n6  Para más información sobre la protección de datos en la asistencia con dinero en efectivo y mediante vales. Veáse Protecting Beneficiary de \\n    CaLP, Protecting personal data anad privacy in field de la Common Cash Delivery (CDD): Una guía para intercmbio de datos entre organizaciones \\n    humanitarias, y el Manual del CICR sobre la protección de datos en la acción humanitaria. \\norientativas existentes6 se centran en la gestión segura y la protección de los datos personales, y\\n}<->{\\n\\norientativas existentes6 se centran en la gestión segura y la protección de los datos personales, y no \\nabordan plenamente las preocupaciones relacionadas con los datos no personales, que son sensibles en \\nmuchos contextos y requieren una sólida protección.\\n\\n--------------------------------------------------\\nCENTRO PARA LOS DATOS HUMANITARIOS  \\n4\\nDICIEMBRE 2020\\nRECOMENDACIONES PARA MEJORAR LA RESPONSABILIDAD DE DATOS EN LA \\nASISTENCIA EN EFECTIVO Y MEDIANTE VALES\\nLa responsabilidad de datos req\\n}<->{\\nPONSABILIDAD DE DATOS EN LA \\nASISTENCIA EN EFECTIVO Y MEDIANTE VALES\\nLa responsabilidad de datos requiere la implementación de acciones basadas en diferentes niveles de \\nrespuestas humanitarias. Esto incluye acciones para garantizar la protección y la seguridad de los datos, así \\ncomo estrategias para minimizar los riesgos y maximizar los beneficios de la gestión de datos. \\nEl centro humanitario de datos, CaLP and NORCAP/Consejo Noruego de Refugiados\\nrecomiendan el seguimiento de acciones para m\\n}<->{\\ne datos, CaLP and NORCAP/Consejo Noruego de Refugiados\\nrecomiendan el seguimiento de acciones para mejorar la responsabilidad de datos en la prestación de \\nasistencia en efectivo y mediante vales:\\n•\\t Esquematizar ecosistemas de datos de la CVA.\\n•\\t El establecimiento de un protocolo de intercambio de información específico para los programas \\nde CVA.\\n•\\t El establecimiento de acuerdos en intercambio de datos personales.\\n•\\t La realización de evaluaciones de impacto de datos en todas las intervencio\\n}<->{\\n de datos personales.\\n•\\t La realización de evaluaciones de impacto de datos en todas las intervenciones de CVA. \\n•\\t La introducción de procedimientos de gestión de incidentes de datos. \\n•\\t El seguimiento de las cuestiones y los progresos en la responsabilidad de datos a través de las \\nestructuras de coordinación.\\nLa siguiente tabla ofrece una descripción de estas acciones y referencias de una orientación adicional y \\nplantillas de diferentes organizaciones humanitarias, cuando estén disponibles.\\n}<->{\\nntación adicional y \\nplantillas de diferentes organizaciones humanitarias, cuando estén disponibles.\\nOrientación práctica para la Comunidad de Efectivo: Conjunto de herramientas de \\nresponsabilidad de datos del CaLP\\nConjunto de herramientas de responsabilidad de datos del CaLp ofrece una guía detallada sobre \\nresponsabilidad de los datos en la asistencia con dinero en efectivo y mediante vales. El conjunto de \\nherramientas pretende reflejar mejor la complejidad de las relaciones con los socios, \\n}<->{\\n conjunto de \\nherramientas pretende reflejar mejor la complejidad de las relaciones con los socios, los modelos \\nde prestación de servicios operativos y los flujos de datos asociados. También pretende abordar los \\nelementos de la recolección de datos a distancia que surgieron a principios del año 2020 en vista del \\nCOVID-19.\\nEl conjunto de herramientas destaca la necesidad de un enfoque ecosistémico y de un análisis de \\npoder entre quienes recolectan, utilizan y comparten los datos de los benefi\\n}<->{\\nco y de un análisis de \\npoder entre quienes recolectan, utilizan y comparten los datos de los beneficiarios del CVA. También \\nseñala la necesidad de contextualización, creatividad, capacidad y colaboración entre los agentes del \\nCVA y entre los equipos organizativos para garantizar una mejor protección de los datos.  \\nEl conjunto de herramientas incluye una profunda consideración de algunas de las dificultades entre \\nlos diferentes agentes con respecto a los datos, así como orientación sobre el \\n}<->{\\n dificultades entre \\nlos diferentes agentes con respecto a los datos, así como orientación sobre el intercambio de datos, \\nlas relaciones entre los controladores y los procesadores de datos,  las evaluaciones de impacto \\nsobre la privacidad, las bases legales para la recolección de datos (incluyendo el consentimiento y \\nlos intereses legítimos), la preparación para posibles incidentes críticos de datos o violaciones, y la \\ncomunicación con las poblaciones afectadas sobre la recolección y el proc\\n}<->{\\ndatos o violaciones, y la \\ncomunicación con las poblaciones afectadas sobre la recolección y el procesamiento de datos.\\nEstá estructurado en torno a las diferentes etapas de un ciclo de vida de los datos típico de los \\nprogramas de CVA y abarca tanto los datos personales como los no personales, al tiempo que \\nincorpora los nuevos requisitos legales derivados del Reglamento General de Protección de Datos de \\nla Unión Europea.\\n\\n--------------------------------------------------\\nCENTRO PARA LOS DAT\\n}<->{\\n Datos de \\nla Unión Europea.\\n\\n--------------------------------------------------\\nCENTRO PARA LOS DATOS HUMANITARIOS  \\n5\\nDICIEMBRE 2020\\n7    Vea por ejemplo Conjunto de datos mínimos de prestación de asistencia básica para las poblaciones afectadas.  \\n8    OCHA, 2019. Borrador de trabajo de las Directrices de Responsabilidad de Datos, 55-62.\\n9    Esto incluye las Evaluaciones de Impacto de la Protección de Datos, así como otros tipos de evaluaciones que consideran los posibles riesgos, \\n    daños\\n}<->{\\ncción de Datos, así como otros tipos de evaluaciones que consideran los posibles riesgos, \\n    daños y beneficios de un ejercicio de gestión de datos. Obtenga más información en esta nota de orientación sobre por Centro de datos \\n    humanitarios, ICRC, Privacy International, y UN Global Pulse. \\nAcción\\nEsquematizar \\necosistemas de \\ndatos de la CVA.\\nUn esquema del ecosistema de datos ayuda a identificar las brechas de datos y \\nlas posibles duplicaciones, y permite priorizar y tomar decisiones est\\n}<->{\\nficar las brechas de datos y \\nlas posibles duplicaciones, y permite priorizar y tomar decisiones estratégicas \\nsobre la gestión responsable de los datos. También proporciona un resumen de \\nlas principales actividades de gestión de datos, incluyendo la escala, el alcance \\ny los tipos de datos que se procesan, las partes interesadas, los flujos de datos \\nentre los diferentes agentes, y los procesos y plataformas en uso en la entrega \\nde CVA en un contexto determinado. El Grupo de Trabajo sobre el \\n}<->{\\ny plataformas en uso en la entrega \\nde CVA en un contexto determinado. El Grupo de Trabajo sobre el Efectivo (The \\nCash Working Group (CWG)), en colaboración con sus socios, deberá completar \\nel ejercicio de esquematización del ecosistema de datos y actualizarlo \\nposteriormente cada año.\\nDescripción\\nUn protocolo de intercambio de información (ISP) sirve de base para un \\nenfoque colectivo de intercambio responsable de datos e información. El \\nISP establece los términos que todas las organizacione\\n}<->{\\nambio responsable de datos e información. El \\nISP establece los términos que todas las organizaciones humanitarias y los \\nproveedores de servicios de terceros que participan en la prestación del \\nCVA se comprometen a respetar. Debe complementar cualquier acuerdo \\nbilateral de intercambio de datos preexistente (véase más abajo), alinearse \\ncon las normas aplicables de “Conocimiento del Cliente” (KYC), e incluir \\nelementos clave como una clasificación de la sensibilidad de los datos \\ny un calendar\\n}<->{\\nC), e incluir \\nelementos clave como una clasificación de la sensibilidad de los datos \\ny un calendario de retención y destrucción de datos. Un ISP también \\npuede especificar un conjunto mínimo de datos básicos7 para facilitar \\nla prestación de asistencia en efectivo en un contexto concreto. En las \\nDirectrices de Responsabilidad de Datos de OCHA se incluye un modelo de \\nISP. El desarrollo de un PSI específico para la programación del CVA debería\\ncompletarse como un ejercicio colectivo dirigido p\\n}<->{\\nI específico para la programación del CVA debería\\ncompletarse como un ejercicio colectivo dirigido por el CWG. \\nUn acuerdo de intercambio de datos (DSA) establece los términos y condiciones \\nque rigen el intercambio de datos personales. Se utiliza principalmente para \\ncompartir datos entre dos partes y suele establecerse a nivel nacional. De \\nacuerdo con los marcos de protección de datos, se requiere la firma de un DSA \\npara compartir datos personales. En los programas de CVA, los DSA o acuerdos\\n}<->{\\nere la firma de un DSA \\npara compartir datos personales. En los programas de CVA, los DSA o acuerdos \\nsimilares son necesarios no sólo entre las organizaciones humanitarias y \\nsus socios, sino también con los FSP y otras terceras partes implicadas en la \\nprestación de asistencia.\\nEl establecimiento \\nde un protocolo \\nde intercambio \\nde información \\nespecífico para los \\nprogramas de CVA.\\nEl establecimiento \\nde acuerdos en \\nintercambio de \\ndatos personales.\\nLa realización \\nde evaluaciones \\nde impac\\n}<->{\\nimiento \\nde acuerdos en \\nintercambio de \\ndatos personales.\\nLa realización \\nde evaluaciones \\nde impacto de \\ndatos en todas las \\nintervenciones de \\nCVA.\\nLas evaluaciones del impacto de los datos9 determinan los riesgos, daños \\ny beneficios esperados, así como los sobre la privacidad, la protección de \\ndatos y/o los derechos humanos de una actividad de gestión de datos. Una \\nevaluación sirve de base para el diseño y la ejecución de las actividades de \\ngestión de datos de manera que se maximicen los\\n}<->{\\npara el diseño y la ejecución de las actividades de \\ngestión de datos de manera que se maximicen los beneficios y se minimicen \\nlos riesgos. El tipo más común de evaluación del impacto de los datos en los \\nprogramas de CVA es la evaluación del impacto de la protección de datos, \\nque se centra explícitamente en los datos personales. Si un programa de CVA \\ntambién genera grandes volúmenes de datos no personales, las organizaciones \\ndeberían intentar ampliar su plantilla de evaluación estándar para\\n}<->{\\no personales, las organizaciones \\ndeberían intentar ampliar su plantilla de evaluación estándar para abordar \\ntambién los riesgos, daños y beneficios relacionados con los datos no \\npersonales.\\n\\n--------------------------------------------------\\nCENTRO PARA LOS DATOS HUMANITARIOS  \\n6\\nDICIEMBRE 2020\\n10 Obtenga más información en esta Nota de Orientación sobre la Gestión de Incidentes de Datos elaborada por el Centro de Datos Humanitarios y \\n    el Instituto Jackson de Asuntos Globales de la Univer\\n}<->{\\nada por el Centro de Datos Humanitarios y \\n    el Instituto Jackson de Asuntos Globales de la Universidad de Yale.\\nLa gestión, el seguimiento y la comunicación de los incidentes relacionados con \\nlos datos requieren procedimientos operativos estándar para la gestión \\nde incidentes10 y un registro central que recoja los detalles clave sobre la \\nnaturaleza, la gravedad y la resolución de cada incidente. Esto incluirá a \\nmenudo un mecanismo de rectificación y reparación a través del cual los \\nbenef\\n}<->{\\nnte. Esto incluirá a \\nmenudo un mecanismo de rectificación y reparación a través del cual los \\nbeneficiarios puedan buscar la resolución de problemas relacionados con la \\nprestación de CVA, incluidas las preocupaciones sobre la gestión de sus datos.\\nAunque se anima a las organizaciones a establecer sus propios procedimientos \\nde gestión de incidentes de datos, el seguimiento colectivo de los incidentes \\ny el intercambio de las lecciones aprendidas a través del CWG es importante \\nen el complejo e\\n}<->{\\nentes \\ny el intercambio de las lecciones aprendidas a través del CWG es importante \\nen el complejo e interconectado ecosistema de datos de CVA.\\nLas estructuras de coordinación, como el CWG pueden servir de plataforma \\ncomún para supervisar el progreso colectivo y/o los retos y oportunidades \\nde la responsabilidad de los datos. También pueden apoyar la mejora de la \\ncoordinación y la toma de decisiones para avanzar en la responsabilidad de los \\ndatos en la prestación del CVA en un contexto determ\\n}<->{\\nones para avanzar en la responsabilidad de los \\ndatos en la prestación del CVA en un contexto determinado.\\nLa introducción de \\nprocedimientos \\nde gestión de \\nincidentes de \\ndatos.\\nEl seguimiento de \\nlas cuestiones y \\nlos progresos en la \\nresponsabilidad de \\ndatos a través de \\nlas estructuras de \\ncoordinación.\\nLas organizaciones son alentadas a compartir su experiencia al promover la responsabilidad de datos en la \\nprestación de asistencia en efectivo y mediante vales con el centro por medio de c\\n}<->{\\nd de datos en la \\nprestación de asistencia en efectivo y mediante vales con el centro por medio de centrehumdata@un.org.\\nEste proyecto esta \\ncofinanciado por la Unión \\nEuropea\\nEl presente documento abarca las actividades de ayuda humanitaria realizadas con la ayuda financiera \\nde la Unión Europea. Las opiniones expresadas en él no deben considerarse, en modo alguno, como la \\nopinión oficial de la Unión Europea, y la Comisión Europea no es responsable del uso que pueda hacerse \\nde la información \\n}<->{\\nUnión Europea, y la Comisión Europea no es responsable del uso que pueda hacerse \\nde la información que contiene.\\nCOLABORADORES: CALP Y NORCAP/CONSEJO NORUEGO DE REFUGIADOS.\\nEl Centro Para Los Datos Humanitarios (\\'El Centro\\'), junto con socios importantes, está publicando una serie \\nde notas de orientación sobre la responsabilidad de los datos en la acción humanitaria a lo largo de 2019 y \\n2020. La serie de notas de orientación sigue a la publicación del borrador de trabajo de las Directrices de\\n}<->{\\na serie de notas de orientación sigue a la publicación del borrador de trabajo de las Directrices de \\nResponsabilidad en marzo del 2019. A través de esta serie, el Centro pretende proporcionar orientación adicional \\nsobre cuestiones, procesos y herramientasespecíficas para la responsabilidad de los datos en la práctica. Esta \\nserie es posible gracias al generoso apoyo de la Dirección General de Protección Civil y Operaciones de Ayuda \\nHumanitaria (DG ECHO).\\n\\n-------------------------------------\\n}<->{\\notección Civil y Operaciones de Ayuda \\nHumanitaria (DG ECHO).\\n\\n--------------------------------------------------\\n', 'Summaries': 'The Centro Para Los Datos Humanitarios released a guidance note on December 1, 2020, regarding the use of cash assistance and vouchers to reduce financial barriers in humanitarian responses to COVID-19. The note emphasizes responsible data management for humanitarian aid.\\n}<->{\\nDATOS EN LA RESPUESTA HUMANITARIA\\nLa respuesta humanitaria ha mejorado notablemente la responsabilidad de datos en asistencia en efectivo y vales (CVA). Existen brechas entre marcos y práctica. Recursos están disponibles para proteger datos.\\n}<->{\\nResources available in data protection for CVA programs are limited, particularly for confidential non-personal data management. Common benefits of data in CVA include: \\n\\n* Increased understanding of priorities and needs\\n* Improved assistance, including duplication of beneficiary lists \\n* Enhanced service delivery\\n}<->{\\nRiesgos comunes relacionados con los datos:\\n\\n- Exposición de datos sensibles que pueden dañar a personas afectadas y/o al personal humanitario\\n- Brechas en protección de datos, privacidad de datos y seguridad de datos con fines humanitarios.\\n}<->{\\nLas brechas en protección de datos incluyen:\\n\\n- Protección de datos por terceras partes\\n- Privacidad y seguridad de datos con fines humanitarios\\n\\nPérdida de confianza entre personas afectadas y organizaciones humanitarias.\\n\\nOrganizaciones humanitarias pueden mejorar la responsabilidad de datos mediante: \\n\\n- Esquematizar ecosistemas de datos \\n- Establecer protocolo de intercambio de información\\n}<->{\\ntablecimiento de un protocolo de intercambio de información específico para programas de CVA; establecimiento de acuerdos de intercambio de datos personales; evaluaciones del impacto de los datos en todas las intervenciones de CVA; introducción de procedimientos de gestión de incidentes de datos y seguimiento de la responsabilidad de datos.\\n}<->{\\nLas organizaciones humanitarias y financieras comparten y utilizan grandes cantidades de datos gracias a las herramientas digitales, lo que aumenta la responsabilidad en el manejo de la información.\\n}<->{\\nTendencias más grandes se han acelerado en respuesta a COVID-19, donde agentes del efectivo han ampliado y adaptado programas de CVA, preferiendo pagos electrónicos o celulares para reducir el riesgo de contagio.\\n}<->{\\nInternational Committee of the Red Cross (ICRC) and Privacy International warn that metadata in humanitarian aid can compromise individuals\\' privacy. The Cash Learning Partnership (CaLP) aims to improve cash delivery systems globally.\\n}<->{\\nIn 2018, Cross (ICRC) and Privacy International published a report on the issue of humanitarian metadata. They stated that \"do no harm\" in the digital era is a major concern.\\n}<->{\\nAcarencias de recursos prácticos para ayudar profesionales a conciliar normas éticas con presión diaria. Falta de mecanismos para compartir experiencias y aprender de casos críticos de gestión de incidentes de datos. Disminución de tolerancia al riesgo en CVA debido a medidas contra el lavado de dinero.\\n}<->{\\nDecrease in risk tolerance for CVA is partly due to measures against money laundering, terrorism and cash economy, affecting data management. Humanitarian agents have made significant progress promoting data responsibility in CVA with initiatives supporting beneficiary protection and efficient work methods.\\n}<->{\\nNotificación a beneficiarios de ayuda brindada por organizaciones humanitarias sobre formas de trabajo más eficientes, colaborativas y basadas en datos. Acuerdos bilaterales y multilaterales para intercambio de datos y programas conjuntos. Brevas existen entre marcos globales de responsabilidad de datos y su aplicación práctica.\\n}<->{\\nEl objetivo es identificar brechas entre marcos globales de responsabilidad de datos y su aplicación práctica en la prestación de Asistencia en Efectivo (CVA). El propósito es brindar una visión general de los beneficios y riesgos relacionados con los datos para agentes del efectivo, así como acciones que las organizaciones pueden realizar para mejorar la responsabilidad de los datos en la prestación de CVA.\\n}<->{\\nCVA necesita recolección de datos personal y no personal, incluyendo números de identificación nacional, números telefónicos y direcciones.\\n}<->{\\nPersonal data shared in CVA programs includes national ID, phone numbers, addresses, family members\\' names, and bank details. Non-personal data commonly included is micro survey data, need assessments, demographic information about recipient groups, distribution point locations, and community opinions.\\n}<->{\\nLa distribución de datos anónimos y/o agregados sobre opiniones de la comunidad, junto con metadatos de transacciones financieras y patrones de gasto, se suma al CVA. Esto incluye datos sobre proveedores financieros y consumo poblacional. Diferente a otros ámbitos humanitarios, el CVA involucra una gama especial de datos, como transacciones y patrones de gasto.\\n}<->{\\nEl CVA (Asistencia Humanitaria) involucra una amplia gama de agentes y presenta problemas de protección de datos debido a la complejidad del flujo de datos entre organizaciones humanitarias y socios.\\n}<->{\\nCriterios de responsabilidad de datos en CVA variaban según organizaciones y contextos debido a:\\n\\n1. Niveles de familiaridad con orientaciones existentes sobre responsabilidad de datos y deficiencias de capacidad entre personal.\\n2. Evolución de riesgos de datos relacionados con nuevas formas de distribución de mecanismos y modelos de colaboración en CVA.\\n}<->{\\nNuevas formas de distribución de mecanismos y modelos de colaboración en la CVA incluyen:\\n\\n* Centro para los Datos Humanitarios\\n* Beneficios: protección de datos, mejora de eficiencia y transparencia\\n* Riegos: pérdida de confidencialidad, violaciones de datos, responsabilidad legal\\n}<->{\\nRiego es una empresa que se asocia con el manejo de datos, ofreciendo beneficios como:\\n- Mayor comprensión de prioridades\\n- Mejora en la asistencia a través de deduplicación de listas\\n- Aumento de eficacia en distribución de recursos\\n- Fomento de transparencia y responsabilidad.\\n}<->{\\nLa efectividad de diferentes mecanismos de distribución es variable. \\n\\nEl Reglamento sobre \"Conocimiento del Cliente\" establece requisitos para la identificación y verificación de clientes, incluyendo:\\n\\n*   Documentación adicional requerida\\n*   Limitaciones en la exposición a datos sensibles\\n*   Obligaciones de notificación ante la pérdida o robo de documentos\\n\\nLa transparencia y responsabilidad son fundamentales para mitigar los riesgos asociados con el CVA.\\n}<->{\\nPérdida de confianza entre personas afectadas, Reglamento del Conocimiento del Cliente (KYC) - Diligencia debida con respecto al cliente para combatir el lavado de dinero, financiación terrorista y otras amenazas en el sistema financiero.\\n}<->{\\nInstituciones financieras cumplen con normativa financiera nacional realizando acciones como:\\n\\nVerificación de identidad (KYC)\\nGestión responsable de datos\\nAsistencia en efectivo y vales\\nRequisitos nacionales que pueden variar.\\n}<->{\\nKYC requisitos establecidos a nivel nacional pueden variar, se recomienda consultar la \"Hoja de consejos de Elan sobre el Reglamento de KYC” y Normas de ACNUR y World Vision sobre conocimiento del cliente y privacidad para transferencias de efectivo.\\n}<->{\\nPara más información sobre la protección de datos cuando se entregan efectivos, consulte:\\n- Protección del beneficiario de CaLP\\n- Guía para el intercambio de datos entre organizaciones humanitarias\\n- Manual del CICR sobre la protección de datos en la acción humanitaria.\\n}<->{\\nOrientativas existentes se centran en la gestión segura y protección de datos personales, pero no abordan plenamente las preocupaciones con datos no personales sensibles que requieren una sólida protección.\\n}<->{\\nPONSABILIDAD DE DATOS EN LA ASISTENCIA EN EFECTIVO Y MEDIANTE VALES\\n\\nLa responsabilidad de datos requiere acciones humanitarias basadas en diferentes niveles de respuestas. Esto incluye garantizar la protección y seguridad de los datos, estrategias para minimizar riesgos y maximizar beneficios de la gestión de datos.\\n}<->{\\nCaLP y NORCAP/Consejo Noruego de Refugiados recomiendan el seguimiento de acciones para mejorar la responsabilidad de datos, incluyendo:\\n\\n* Esquematizar ecosistemas de datos de CVA\\n* Establecer un protocolo de intercambio de información específico para los programas de CVA\\n* Establecer acuerdos en intercambio de datos personales\\n* Realizar evaluaciones de impacto de datos en todas las intervenciones\\n}<->{\\nPersonal data handling:\\n\\n* Conduct evaluations of data impact on all CVA interventions\\n* Implement data breach management procedures\\n* Monitor data responsibility progress through coordination structures\\n}<->{\\nntación adicional para conjuntos de herramientas de responsabilidad de datos del CaLP incluye plantillas de diferentes organizaciones humanitarias disponibles. La Comunidad de Efectivo recibe orientación práctica sobre herramientas de responsabilidad de datos y asistencia con dinero en efectivo mediante vales, ofreciendo una guía detallada sobre sus relaciones con socios.\\n}<->{\\nUn conjunto de herramientas pretende abordar la complejidad de relaciones con socios, modelos de prestación de servicios operativos y flujos de datos asociados. También incluye elementos de recolección de datos a distancia que surgió en 2020 debido al COVID-19.\\n}<->{\\nAnálisis de poder en CVA (Código Voluntario de Ahorro) entre recolectores, usuarios y compartidores de datos implica contextualización, creatividad, capacidad y colaboración para proteger datos. La herramienta aborda las dificultades entre agentes con respecto a los datos y ofrece orientación sobre cómo mejorar la protección de datos.\\n}<->{\\nDifficulties between agents regarding data, guidance on data exchange, relationships between controllers and data processors, impact assessments on privacy, legal basis for data collection (including consent and legitimate interests), preparedness for critical data breaches or incidents, and communication with affected populations on data collection and processing.\\n}<->{\\nLa estructura se enfoca en el ciclo de vida de datos para programas de CVA, cubriendo:\\n- Recolección y procesamiento de datos personales\\n- Comunicación con poblaciones afectadas\\n- Criterios legales basados en el Reglamento General de Protección de Datos de la UE\\n}<->{\\nData of the European Union.\\n\\nConjunto de datos mínimos de prestación de asistencia básica para las poblaciones afectadas:\\n- Conjunto de datos mínimos de prestación de asistencia básica para las poblaciones afectadas (OCHA, 2019)\\n- Evaluaciones de Impacto de la Protección de Datos: 55-62\\n- Borrador de trabajo de las Directrices de Responsabilidad de Datos\\n}<->{\\nEvaluaciones de riesgos y beneficios en gestión de datos incluyen:\\n\\n- Centro de datos humanitarios\\n- ICRC (International Committee of the Red Cross)\\n- Privacy International\\n- UN Global Pulse\\n}<->{\\nFicar brechas de datos y duplicaciones, priorizar decisiones estratégicas en gestión responsable de datos. \\n\\nResumen de actividades de gestión de datos:\\n- Escala: Cantidad y alcance de datos procesados.\\n- Alcance: Partes interesadas involucradas.\\n- Tipos de datos: Tipos y formatos de datos procesados.\\n- Flujos de datos: Comunicación entre agentes.\\n- Procesos y plataformas: Herramientas utilizadas para entrega de CVA.\\n}<->{\\nPlataformas en uso para la entrega de CVA: \\n- Cash Working Group (CWG)\\n}<->{\\nEl ISP establece términos para organizaciones humanitarias y proveedores de servicios que participan en CVA, que deben respetar. Estos términos deben complementar acuerdos previos de intercambio de datos, alinearse con normas KYC, incluir una clasificación de la sensibilidad de los datos y un calendario.\\n}<->{\\nLa inclusión de elementos clave en un ISP (Servicio de Protección de Información) debe incluir una clasificación de sensibilidad de datos, un calendario de retención y destrucción de datos, y un conjunto mínimo de datos básicos para facilitar la asistencia en efectivo.\\n}<->{\\nUn acuerdo de intercambio de datos (DSA) es necesario para compartir datos personales en los programas de CVA.\\n}<->{\\nDSAs (Declaraciones de Satisfacción) son firmas necesarias entre organizaciones humanitarias y socios, así como con FSP y otras terceras partes, para compartir datos personales en programas de CVA. Establecen un protocolo específico de intercambio de información, acuerdos de intercambio de datos personales y evaluaciones de impacto.\\n}<->{\\nRealización de evaluaciones de impacto de datos en todas las intervenciones de CVA (Centro de Valoración de Activos) determina riesgos, beneficios y daños sobre privacidad, protección de datos y derechos humanos. Las evaluaciones sirven para diseñar y ejecutar actividades de gestión de datos con máxima eficacia.\\n}<->{\\nEl diseño y ejecución de actividades de gestión de datos para maximizar beneficios y minimizar riesgos incluye la evaluación del impacto de los datos en programas de CVA, especialmente protección de datos personales. Las organizaciones generando grandes volúmenes de datos no personales deben ampliar su plantilla estándar de evaluación.\\n}<->{\\nLas organizaciones deben ampliar su evaluación estándar para incluir riesgos, daños y beneficios relacionados con datos no personales.\\n}<->{\\nEl Centro de Datos Humanitarios y el Instituto Jackson de Asuntos Globales de la Universidad de Yale establecen procedimientos estándar para la gestión de incidentes relacionados con datos, incluyendo seguimiento y comunicación. Esto incluye un registro central que recopila detalles clave sobre incidentes, como naturaleza, gravedad y resolución, así como un mecanismo de rectificación y reparación.\\n}<->{\\nEl Comité para la Gestión de Incidentes de Datos (CWG) ofrece un mecanismo de rectificación y reparación para beneficiarios que buscan resolver problemas relacionados con la prestación del servicio CVA, incluyendo preocupaciones sobre la gestión de datos. El seguimiento colectivo de incidentes y el intercambio de lecciones aprendidas a través del CWG es importante para mejorar la gestión de incidentes de datos en las organizaciones.\\n}<->{\\nEn el ecosistema de datos de CVA, entidades como CWG son cruciales. Facilitan la supervisión colectiva, coordinación y toma de decisiones sobre la responsabilidad de los datos.\\n}<->{\\nOrganizaciones deben introducir procedimientos de gestión de incidentes de datos, seguir las cuestiones y progresos en la responsabilidad de datos a través de estructuras de coordinación. Se alienta a las organizaciones a compartir su experiencia para promover la responsabilidad de datos en asistencia con efectivo y vales.\\n}<->{\\nThe project provides assistance in cash and voucher form to the center via centrehumdata@un.org. It is cofinanced by the European Union. The document covers humanitarian aid activities financed by the EU, without expressing official opinion of the European Union or taking responsibility for information usage.\\n}<->{\\nUnión Europea y Comisión Europea no son responsables del uso de información contenida. Colaboradores: CALP Y NORCAP/CONSEJO NORUEGO DE REFUGIADOS. El Centro Para Los Datos Humanitarios publica notas de orientación sobre la responsabilidad de datos en acción humanitaria durante 2019 y 2020.\\n}<->{\\nA serie de notas de orientación se publica después del borrador de trabajo de Directrices de Responsabilidad en datos. El Centro ofrece orientación adicional sobre cuestiones, procesos y herramientas para responsabilidad de datos. La serie está apoyada por la DG ECHO.\\n}<->{\\nOtección Civil y Operaciones de Ayuda Humanitaria (DG ECHO) \\n\\nLa DG ECHO es la unidad de la Comisión Europea responsable de la ayuda humanitaria. Su objetivo es proporcionar asistencia a personas afectadas por conflictos, desastres naturales y crisis humanitarias en todo el mundo. Ofrece asistencia técnica, recursos y fondos para apoyar operaciones de ayuda humanitaria.'}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Pdf/17815074de3c9f8af9a5051978a72e2a83f1d18d_normal.txt': {'Scores': '7\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4\\n}<->{\\n4\\n}<->{\\n7\\n}<->{\\n6\\n}<->{\\n6', 'Chunks': 'Peer Review Framework for Predictive Analytics\\nin Humanitarian Response\\nMODEL REPORT:\\nTrigger for OCHA Anticipatory\\nAction on Drought in Chad\\nTrigger developed by the UN OCHA Centre for Humanitarian Data\\nJune 2023\\n\\n--------------------------------------------------\\nModel Report:\\nTrigger for OCHA Anticipatory Action on Drought in Chad\\n1. Background\\nThis document summarizes the documentation and findings of the peer review of the model\\nused by the UN Central Emergency Response Fund to trigger the \\n}<->{\\ndings of the peer review of the model\\nused by the UN Central Emergency Response Fund to trigger the drought anticipatory action\\nframework in Chad. The model is a go-no-go trigger mechanism designed to anticipate\\nhumanitarian impacts of seasonal drought in a sub-region of Chad.\\nThe review has been conducted between January and May 2023.\\n2. Main Findings and Recommendations\\nYou can find all the documentation regarding the model, its application and the review process\\nat the following links:\\n●\\nThe \\n}<->{\\nmentation regarding the model, its application and the review process\\nat the following links:\\n●\\nThe Model Card describes the version of the model that was endorsed by humanitarian\\naction in November 2022 and was completed in January 2023.\\n●\\nThe Model Evaluation Matrix was completed in February 2023 by Carlos Osorio-Ramírez,\\nAssociate Professor at Universidad Nacional de Colombia.\\n●\\nThe Implementation Plan was completed in February 2023. It summarizes how the\\ntrigger model is used to trigger anti\\n}<->{\\nion Plan was completed in February 2023. It summarizes how the\\ntrigger model is used to trigger anticipatory action and the agreed activation protocol.\\n●\\nThe Ethical Matrix aims to identify all stakeholders and potential issues regarding the\\nintended use of the model. The Ethical Matrix was completed in May 2023 by Anna Lena\\nHuhn, Southern Africa Regional Anticipatory Action Advisor at the United Nations World\\nFood Programme.\\nA summary of the main findings and recommendations is provided below.\\n\\n}<->{\\nNations World\\nFood Programme.\\nA summary of the main findings and recommendations is provided below.\\n2.1 Technical Review\\nIntended Use\\nOverall, the use cases are well-described and adequately explained. The documentation of the\\nlimitations of the model could be extended in its description, as is the case of the biomass\\nanomaly and its connection with scenarios like extreme dryness or late onset.\\n\\n--------------------------------------------------\\nModel Development and Documentation\\nIt is advisabl\\n}<->{\\n-------------------------------------------------\\nModel Development and Documentation\\nIt is advisable to provide further documentation on the different data sources used by the team\\nto validate the selection of indicators and estimate the performance of the model. Also, some\\ndata is not available in a machine readable format and it would be good to explain how the team\\ndealt with this issue.\\nModel Evaluation\\nIt is unclear what is an acceptable level of accuracy for the model. The team should bet\\n}<->{\\n Evaluation\\nIt is unclear what is an acceptable level of accuracy for the model. The team should better\\narticulate how the performance benchmark was established and how the performance metrics\\nwere evaluated and presented to the final stakeholders.\\nOperational Readiness\\nIt would be good to expand the analysis of the missed activations of the models and the\\npotential false alarms that the model could be providing.\\n2.2 Ethical Review\\nFalse Negatives\\nA false negative occurs when no drought impact i\\n}<->{\\nld be providing.\\n2.2 Ethical Review\\nFalse Negatives\\nA false negative occurs when no drought impact is predicted but drought conditions occur.\\nThe team could consider additional mitigation strategies such as the use of citizen science or\\nground observations to complement the forecast-based trigger.\\nInsufficient Data\\nInsufficient data refers to the gaps in impact data used to verify the forecast predictions.\\nInsufficient data might limit the precise prediction of forecasted humanitarian impact and\\n}<->{\\ndictions.\\nInsufficient data might limit the precise prediction of forecasted humanitarian impact and\\nexpose local populations to adverse humanitarian impact induced by a drought event, requiring\\na large-scale humanitarian response ex-post. The team should consider the feasibility of adding\\nfurther vulnerability layers in future iterations of the model.\\nFalse Positives\\nA false positive happens when drought impact is predicted but no drought conditions occur.\\nAnticipatory action follows the ‘no re\\n}<->{\\n drought impact is predicted but no drought conditions occur.\\nAnticipatory action follows the ‘no regret’ principle, but a false alarm can still lead to inadequate\\nresource allocation with a high potential impact on affected communities. The anticipatory\\naction framework could indicate a precise mitigation strategy in a clearer manner.\\nLack of Trust and Ownership\\nLack of trust and ownership refers to the limited involvement of local and regional authorities in\\nthe model design and validation. Th\\n}<->{\\n to the limited involvement of local and regional authorities in\\nthe model design and validation. This may lead to little ownership and trust put into the model\\nand eventually to political blockages in activations. In future framework versions a clear strategy\\n\\n--------------------------------------------------\\nfor the involvement of affected populations and an approach to accountability to affected\\npopulations should be identified.\\nLack of Transparency\\nLack of transparency refers to the algorit\\n}<->{\\ned\\npopulations should be identified.\\nLack of Transparency\\nLack of transparency refers to the algorithms not being made available or not being transparent.\\nThe anticipatory action framework should reflect clearer strategies to engage with local leaders\\nto better understand what is happening on the ground and obtain early buy-in. Mitigation\\nstrategy for a potential lack of transparency should be expanded in the model card and/or the\\nanticipatory action framework.\\nInaction\\nInaction refers to the si\\n}<->{\\nanded in the model card and/or the\\nanticipatory action framework.\\nInaction\\nInaction refers to the situation in which drought impact is predicted but no action is taken.\\nInaction in case of trigger activation is unlikely, given the pre-allocated financing attached to it\\nand the UN-validated anticipatory action framework. Inaction might nevertheless be a\\npossibility, if government or local authorities block the roll-out of interventions on the ground or\\nin\\ncase\\nof\\nconflict\\nupsurge.\\nThe\\nteam\\ncould\\n\\n}<->{\\nes block the roll-out of interventions on the ground or\\nin\\ncase\\nof\\nconflict\\nupsurge.\\nThe\\nteam\\ncould\\nconsider\\nadding\\na\\nconsensus-based\\ndecision-making option around activation or non-activation in the event of specific events or\\nsituational context not considered in the model design (e.g., higher levels of vulnerability or\\nincrease in armed conflict, etc.)\\nFeedback\\nThe Centre invites individuals and organizations working in the humanitarian, academic,\\nresearch and private sector to engage with us\\n}<->{\\nd organizations working in the humanitarian, academic,\\nresearch and private sector to engage with us on the peer review process. Please send feedback\\non the Framework to centrehumdata@un.org.\\n\\n--------------------------------------------------\\n', 'Summaries': \"Peer Review Framework for Predictive Analytics in Humanitarian Response \\nUN OCHA Centre for Humanitarian Data, June 2023\\n\\nTrigger for Anticipatory Action on Drought in Chad\\n\\nBackground: Documentation and findings of peer review of predictive analytics model used by the UN Central Emergency Response Fund.\\n}<->{\\nThe UN Central Emergency Response Fund used a model to trigger drought anticipatory action in Chad. \\n\\nA peer review was conducted from January to May 2023.\\n\\nMain findings and recommendations can be found at specific documentation links.\\n}<->{\\nModel Card: \\nEndorsed by humanitarian action in November 2022, completed in January 2023.\\n\\nModel Evaluation Matrix:\\nCompleted in February 2023 by Carlos Osorio-Ramírez, Associate Professor at Universidad Nacional de Colombia.\\n\\nImplementation Plan:\\nCompleted in February 2023.\\n}<->{\\nIon Plan completed February 2023:\\nTrigger Model - triggers anticipatory action.\\nAgreed Activation Protocol - implemented.\\n\\nEthical Matrix completed May 2023 by Anna Lena Huhn:\\nIdentifies stakeholders and potential issues related to the model's intended use.\\n}<->{\\nTechnical Review\\nIntended use cases are well-described. Limitations, such as biomass anomalies, could be more extensively documented to connect them to specific scenarios, like extreme dryness or late onset.\\n}<->{\\nProvide documentation on data sources used by the team to validate indicators and estimate performance. Explain handling issues with data not available in machine-readable format, e.g., how it was dealt with. Establish acceptable level of accuracy for the model.\\n}<->{\\nEvaluation: Unclear accuracy level, inadequate benchmarking & performance metrics. \\nOperational Readiness: Expand analysis on missed activations & potential false alarms.\\nEthical Review: False negatives occur when no drought impact is detected despite actual occurrence.\\n}<->{\\nFalse Negatives occur when drought conditions are predicted but not detected. Insufficient Data refers to gaps in impact data, limiting accurate humanitarian impact predictions.\\n}<->{\\nDrought predictions may not accurately forecast humanitarian impacts due to insufficient data, leaving local populations vulnerable. This can lead to a large-scale response post-event. False positives occur when droughts are predicted but none happen, prompting unnecessary anticipatory actions.\\n}<->{\\nDrought conditions not happening but still taking anticipatory action can waste resources. False alarm can hurt communities. Improving anticipation by clearer strategies could help. Limited involvement of local authorities in decision-making leads to lack of trust and ownership.\\n}<->{\\nLimited local/regional authority involvement may lead to ownership/trust issues & political blockages. Future frameworks should address population involvement & accountability strategies.\\n}<->{\\nPopulations, especially marginalized ones, should be identified for targeted support. Lack of transparency hinders algorithms' availability, impeding effective engagement with local leaders. Inaction occurs when responses are inadequate or non-existent.\\n}<->{\\nInaction in drought response occurs when predicted impacts are not acted upon due to:\\n- Lack of pre-allocated financing attached to triggers\\n- Government or local authorities blocking intervention roll-out\\n- Conflict upsurge.\\n}<->{\\nEs block intervention roll-out due to conflict. Team considers adding consensus-based decision-making for unexpected events, such as increased vulnerability or armed conflict. Feedback: The Centre invites collaboration from individuals and organizations in humanitarian, research, and private sectors.\\n}<->{\\nThe United Nations Human Data Centre invites organizations from humanitarian, academic, research, and private sectors to provide feedback on its peer review framework via email at centrehumdata@un.org.\"}}\n"
     ]
    }
   ],
   "source": [
    "keywords = [ \n",
    "        \"I can't\",\n",
    "        \"I cannot\",\n",
    "        \"There is no\",\n",
    "        \"No useful\",\n",
    "        \"Unusable\"]\n",
    "#formatted inputs contains pdfs with a lot of tables, and forms, most of them are text based\n",
    "formatted_inputs ,filtered_formatted_inputs , useful_formatted_chunks, filtered_formatted_chunks = get_the_best_version(formatted, separator=\"\\n}<->{\\n\", filtered_txt= keywords)\n",
    "\n",
    "#receipt inputs contains pdfs that are mostly images with text, and need ocr \n",
    "receipt_inputs, filtered_receipt_inputs, useful_receipt_chunks, filtered_receipt_chunks =  get_the_best_version(receipts, separator=\"\\n}<->{\\n\", filtered_txt= keywords)\n",
    "\n",
    "#report inputs contains pdfs that are well written, and no ocr is needed\n",
    "report_inputs ,filtered_report_inputs , useful_report_chunks, filtered_report_chunks = get_the_best_version(reports, separator=\"\\n}<->{\\n\", filtered_txt= keywords)\n",
    "\n",
    "#random inputs contains pdfs that are all sorts mix of ocr and text\n",
    "random_inputs, filetered_random_inputs, useful_random_chunks, filtered_random_chunks = get_the_best_version(randoms, separator=\"\\n}<->{\\n\", filtered_txt= keywords)\n",
    "\n",
    "print(f\"PDFs with formats {len(formatted_inputs)}, that are filtered {len(filtered_formatted_inputs)}, useful chunks {len(useful_formatted_chunks)}, filtered chunks {len(filtered_formatted_chunks)}\")\n",
    "print(f\"PDFs of recipts that need ocr {len(receipt_inputs)}, that are filtered {len(filtered_receipt_inputs)}, useful chunks {len(useful_receipt_chunks)}, filtered chunks {len(filtered_receipt_chunks)}\")\n",
    "print(f\"PDFs of well written long papers, no ocr {len(report_inputs)}, that are filtered {len(filtered_report_inputs)}, useful chunks {len(useful_report_chunks)}, filtered chunks {len(filtered_report_chunks)}\")\n",
    "print(f\"PDFs of all sorts {len(random_inputs)}, that are filtered {len(filetered_random_inputs)}, useful chunks {len(useful_random_chunks)}, filtered chunks {len(filtered_random_chunks)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in formatted_inputs[0:5]:\n",
    "    print(i)\n",
    "for i in receipt_inputs[0:5]:\n",
    "        print(i)\n",
    "for i in report_inputs[0:5]:\n",
    "        print(i)\n",
    "for i in random_inputs[0:5]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 4 types of data:\n",
    "* Formatted data has a lot of numbers, we could make the model to adhere to addressing the numbers formally or concisely.\n",
    "* Receipts are mainly ocred and contains  a lot of personal which produced highest rate of disclamers and commentaries so commentary suppression data can be gathered on it.\n",
    "* Papers contains well written text and information dense, we could train the model to be more informative and concise.\n",
    "* Mixed PDFs' content are a mix of the above types, we could try to mimic the tone of the original text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from openai._exceptions import RateLimitError, APITimeoutError, APIError\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Load .env and initialize OpenAI client\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GPT_API_KEY\")\n",
    "gpt_client = OpenAI(api_key=api_key)\n",
    "def summarize_chunk(text: str, prompt, model, retry=True) -> str:\n",
    "    prompt = f\"\"\"{prompt}\n",
    "Text:\n",
    "{text}\n",
    "\"\"\"\n",
    "    try:\n",
    "        #print(\"Sending:\",prompt)\n",
    "        response = gpt_client.chat.completions.create(\n",
    "            model= model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a multi-lingo helpful factual text-edit assitant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        #print(\"Response:\",response) \n",
    "        # Check structure before returning\n",
    "        if response and response.choices and response.choices[0].message:\n",
    "            return response.choices[0].message.content.strip()\n",
    "        else:\n",
    "            print(\"Warning: Unexpected response format.\")\n",
    "            return \"\"\n",
    "\n",
    "    except RateLimitError:\n",
    "        print(\"Rate limit hit. Sleeping for 10 seconds...\")\n",
    "        if retry:\n",
    "            time.sleep(10)\n",
    "            return summarize_chunk(text, prompt,model=model)  # retry\n",
    "\n",
    "    except APITimeoutError:\n",
    "        print(\"Request timed out. Retrying...\")\n",
    "        if retry:\n",
    "            return summarize_chunk(text, prompt,model=model)\n",
    "\n",
    "    except APIError as e:\n",
    "        print(f\"OpenAI API error: {e}\")\n",
    "        return \"<Error>\"\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return \"<Error>\"\n",
    "\n",
    "\n",
    "def process_chunks_and_save( data_list, model, output_dir, file_name , summary_type, prompt, retry=True):\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, chunk in enumerate(data_list):\n",
    "        chunk = chunk.strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            summary = summarize_chunk(chunk, prompt,model=model, retry=retry)\n",
    "            if summary:\n",
    "                result = {\n",
    "                    \"input\": chunk,\n",
    "                    \"output\": summary,\n",
    "                    \"summary_type\": summary_type, \n",
    "                }\n",
    "                with open(output_path / file_name, \"a\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "            else:\n",
    "                print(f\"Suppressed\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{i+1}] Error: {e}\")\n",
    "def process_filtered_chunks_and_save( data_list, output_dir, file_name , summary_type=\"Filtered\"):\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, chunk in enumerate(data_list):\n",
    "        chunk = chunk.strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        result = {\n",
    "            \"input\": chunk,\n",
    "            \"output\": \"<NULL>\",\n",
    "            \"summary_type\": summary_type, \n",
    "        }\n",
    "        with open(output_path / file_name, \"a\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pick random useful chunks should only be executed once for file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "sample_size = 1000\n",
    "chosen_formatted = random.sample(useful_formatted_chunks, sample_size)\n",
    "chosen_receipts = random.sample(useful_receipt_chunks, sample_size)\n",
    "chosen_reports = random.sample(useful_report_chunks, sample_size)\n",
    "chosen_randoms = random.sample(useful_random_chunks, sample_size)\n",
    "#with open('../Training/Input/chosen_formatted.json', 'w') as f:\n",
    "    #json.dump(chosen_formatted, f, indent=2)\n",
    "\n",
    "#with open('../Training/Input/chosen_receipts.json', 'w') as f:\n",
    "    #json.dump(chosen_receipts, f, indent=2)\n",
    "\n",
    "#with open('../Training/Input/chosen_reports.json', 'w') as f:\n",
    "    #json.dump(chosen_reports, f, indent=2)\n",
    "\n",
    "#with open('../Training/Input/chosen_randoms.json', 'w') as f:\n",
    "    #json.dump(chosen_randoms, f, indent=2)\n",
    "\n",
    "filtered_sample_size = 100\n",
    "#contains mostly lines, so not difficult to summarize\n",
    "chosen_filtered_formatted = random.sample(filtered_formatted_chunks, min(filtered_sample_size, len(filtered_formatted_chunks)))\n",
    "chosen_filtered_receipts = random.sample(filtered_receipt_chunks, min(filtered_sample_size, len(filtered_receipt_chunks)))\n",
    "chosen_filtered_reports = random.sample(filtered_report_chunks, min(filtered_sample_size, len(filtered_report_chunks)))\n",
    "chosen_filtered_randoms = random.sample(filtered_random_chunks, min(filtered_sample_size, len(filtered_random_chunks)))\n",
    "with open('../Training/Input/chosen_filtered_formatted.json', 'w') as f:\n",
    "    json.dump(chosen_filtered_formatted, f, indent=2)\n",
    "with open('../Training/Input/chosen_filtered_receipts.json', 'w') as f:\n",
    "    json.dump(chosen_filtered_receipts, f, indent=2)\n",
    "with open('../Training/Input/chosen_filtered_reports.json', 'w') as f:\n",
    "    json.dump(chosen_filtered_reports, f, indent=2)\n",
    "with open('../Training/Input/chosen_filtered_randoms.json', 'w') as f:\n",
    "    json.dump(chosen_filtered_randoms, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_filtered_chunks_and_save( chosen_filtered_formatted, \"../Training/InputNLabel/filtered_labels\", \"filtered_formatted.json\", \"Formatted\")\n",
    "process_filtered_chunks_and_save( chosen_filtered_receipts, \"../Training/InputNLabel/filtered_labels\", \"filtered_receipts.json\", \"Receipt\")\n",
    "process_filtered_chunks_and_save( chosen_filtered_reports, \"../Training/InputNLabel/filtered_labels\", \"filtered_reports.json\", \"Report\")\n",
    "process_filtered_chunks_and_save( chosen_filtered_randoms, \"../Training/InputNLabel/filtered_labels\", \"filtered_randoms.json\", \"Random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#print(len(useful_formatted_chunks[0]))\n",
    "import random\n",
    "chosen_example = chosen_receipts[250]\n",
    "process_chunks_and_save([chosen_example], model=\"gpt-4\" , retry=True, output_dir=\"../Training\", file_name=\"formatted_test.json\", \n",
    "                        summary_type=\"formatted_without_bias\", prompt= \"\"\"Without commentary, summarize in original languange on useful information, below 100 If no meaning return <NULL>, if the content includes sensitive personal data, return <CENSORED>.\"\"\")\n",
    "print(len(random.sample(useful_random_chunks, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "chosen_example = chosen_receipts[801]\n",
    "process_chunks_and_save([chosen_example], model=\"gpt-4\" , retry=True, output_dir=\"../Training\", file_name=\"formatted_test.json\", \n",
    "                        summary_type=\"formatted_without_bias\", prompt= \"\"\"Without commentary, summarize in original languange on useful information, below 100 words. If no meaning return <NULL>, if the content includes sensitive personal data, return <CENSORED>.\"\"\")\n",
    "print(len(random.sample(useful_random_chunks, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_example = chosen_receipts[801]\n",
    "process_chunks_and_save([chosen_example], model=\"gpt-4\" , retry=True, output_dir=\"../Training\", file_name=\"formatted_test.json\", \n",
    "                        summary_type=\"sensitive_included_from original\", prompt= \"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Taking each's first 300\n",
    "#block size 2: changed prompt\n",
    "block_size_2 = 300\n",
    "startPage = 2\n",
    "for i in range(block_size_2 * startPage, block_size_2 * (startPage + 1), block_size_2):\n",
    "    begin = i\n",
    "    end = min(i + block_size_2, sample_size)\n",
    "\n",
    "    process_chunks_and_save(\n",
    "        chosen_receipts[begin:end],\n",
    "        model=\"gpt-4\",\n",
    "        retry=True,\n",
    "        output_dir=\"../Training/InputNLabel/prompt_sensitive_translated/\",\n",
    "        file_name=f\"receipts_{i//block_size_2}.json\",\n",
    "        summary_type=\"receipts\",\n",
    "        prompt= \"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>.\"\"\"\n",
    "    )\n",
    "    process_chunks_and_save(\n",
    "        chosen_formatted[begin:end],\n",
    "        model=\"gpt-4\",\n",
    "        retry=True,\n",
    "        output_dir=\"../Training/InputNLabel/prompt_sensitive_translated/\",\n",
    "        file_name=f\"formatted_{i//block_size_2}.json\",\n",
    "        summary_type=\"formatted\",\n",
    "        prompt=\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>.\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    process_chunks_and_save(\n",
    "        chosen_reports[begin:end],\n",
    "        model=\"gpt-4\",\n",
    "        retry=True,\n",
    "        output_dir=\"../Training/InputNLabel/prompt_sensitive_translated/\",\n",
    "        file_name=f\"reports_{i//block_size_2}.json\",\n",
    "        summary_type=\"reports\",\n",
    "        prompt=\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>.\"\"\"\n",
    "    )\n",
    "\n",
    "    process_chunks_and_save(\n",
    "        chosen_randoms[begin:end],\n",
    "        model=\"gpt-4\",\n",
    "        retry=True,\n",
    "        output_dir=\"../Training/InputNLabel/prompt_sensitive_translated/\",\n",
    "        file_name=f\"random_{i//block_size_2}.json\",\n",
    "        summary_type=\"random\",\n",
    "        prompt=\"\"\"Without commentary, from its original language summarize to English on useful information including sensitive data, below 100 words. If no meaning return <NULL>.\"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmlhw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
