{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask GPT for answers for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/home/yxuhuang/adapter/LLMAdapter/SummaryNScores/formatted_scores.json\n",
    "formatted_pdfs = \"/home/yxuhuang/adapter/LLMAdapter/SummaryNScores/formatted_scores.json\"\n",
    "receipt_pdfs = \"/home/yxuhuang/adapter/LLMAdapter/SummaryNScores/receipt_scores.json\"\n",
    "report_pdfs = \"/home/yxuhuang/adapter/LLMAdapter/SummaryNScores/report_scores.json\"\n",
    "random_pdfs = \"/home/yxuhuang/adapter/LLMAdapter/SummaryNScores/random_scores.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs with formats 4840\n",
      "PDFs of recipts that need ocr 2441\n",
      "PDFs of well written long papers, no ocr 98\n",
      "PDFs of all sorts 600\n",
      "{'/home/yxuhuang/adapter/Data/Extracted/WithFormat/Company Documents Dataset/CompanyDocuments/Inventory Report/monthly-Category/monthly-Category/StockReport_2016-07_1_normal.txt': {'Scores': '8', 'Chunks': 'Stock Report for 2016-07 \\n Category : Beverages  \\n id category : 1  \\nProduct\\nUnits Sold\\nUnits in Stock\\nUnit Price\\nChang\\n105\\n17\\n19\\nGuaraná Fantástica\\n43\\n20\\n4.5\\nSteeleye Stout\\n20\\n20\\n18\\nChartreuse verte\\n48\\n69\\n18\\nOutback Lager\\n41\\n15\\n15\\nLakkalikööri\\n15\\n57\\n18\\n\\n--------------------------------------------------\\n', 'Summaries': 'Stock Report for 2016-07 \\n id category : 1 \\nBeverages: \\n- Guaraná Fantástica: 43 units sold, 20 in stock, $4.5/unit \\n- Steeleye Stout: 20 units sold, 20 in stock, $18/unit \\n- Chartreuse verte: 48 units sold, 69 in stock, $18/unit \\n- Outback Lager: 41 units sold, 15 in stock, $15/unit \\n- Lakkalikööri: 15 units sold, 57 in stock, $18/unit'}}\n",
      "{'/Users/seanhuang/Grad/adapter/Data/Extracted/Receipts/2017/de/public transport/118NP8_normal.txt': {'Scores': '8\\n}<->{\\n4\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n6\\n}<->{\\n4', 'Chunks': 'Online-Ticket\\nIC/EC Fahrkarte\\nFahrtantritt am 10.11.2017\\nFlexpreis (Einfache Fahrt)\\nKlasse:\\n1\\nErw:\\n1, mit 1 BC50\\nHinfahrt:\\nHamburg+City \\n Bonn-Beuel+City, mit IC/EC\\nÜber:\\nVIA: HB*MS*(HA/D)*K\\nUmtausch/Erstattung kostenlos bis 1 Tag vor Reiseantritt (Hinfahrt).\\nZahlungspositionen und Preis\\nPositionen\\nPreis\\nMwSt (D) 19%\\nMwSt D: 7%\\nIC/EC Fahrkarte\\n1\\n78,00€\\n78,00€\\n12,45€\\nZahlungsmittelentgelt 1\\n0,75€\\n0,75€\\n0,12€\\nSumme\\n78,75€\\n78,75€\\n12,57€\\nKreditkartenzahlung\\nBetrag\\n78,75€\\nVU-Nr\\n4556695619\\nTransaktion\\n}<->{\\n,75€\\n0,12€\\nSumme\\n78,75€\\n78,75€\\n12,57€\\nKreditkartenzahlung\\nBetrag\\n78,75€\\nVU-Nr\\n4556695619\\nTransaktions-Nr\\n705427\\nDatum\\n10.11.2017\\nGen-Nr\\nN49HF8\\nIhre Kreditkarte wurde mit dem oben genannten Betrag belastet. Die Buchung Ihres\\nOnline-Tickets erfolgte am 10.11.2017 14:25 Uhr. DB Fernverkehr AG/DB Regio AG,\\nStephensonstr. 1, 60326 Frankfurt, Steuernummer: 29/001/60002.\\nBarcode bitte nicht knicken!               \\nHinfahrt:\\nGültig ab:\\n10.11.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\n118NP8\\nIh\\n}<->{\\n          \\nHinfahrt:\\nGültig ab:\\n10.11.2017\\nZangenabdruck\\nHerr  Jens walter\\nAuftragsnummer:\\n118NP8\\nIhre Reiseverbindung und Reservierung Hinfahrt am 10.11.2017\\nHalt\\nDatum\\nZeit\\nGleis\\nProdukte\\nReservierung\\nHamburg Hbf\\n10.11.\\nab 14:46 11\\nBonn-Beuel\\n10.11.\\nan 19:10 1\\nIC 2213\\nWichtige Nutzungshinweise:\\n- Ihre Fahrkarte gilt nur zusammen mit einem amtlichen Lichtbildausweis (z.B. Personalausweis) oder Ihrer BahnCard.\\n- Mit Ihrer Fahrkarte zum Flexpreis können Sie jeden Zug der gewählten Verbindung nutz\\n}<->{\\nrer BahnCard.\\n- Mit Ihrer Fahrkarte zum Flexpreis können Sie jeden Zug der gewählten Verbindung nutzen: mit einer IC/EC-Fahrkarte alle IC- und EC-Züge, mit\\neiner ICE-Fahrkarte auch alle anderen Züge.\\n- Das Online-Ticket gilt nur für den unter \"Fahrkarte\" angegebenen Reiseabschnitt. Die Übersicht \"Ihre Reiseverbindung\" enthält gegebenenfalls\\nReiseinformationen zu Teilstrecken (z.B. Bus oder Straßenbahn), für die eine weitere Fahrkarte erforderlich sein kann.\\n- Wenn Ihr Ticket den Zusatz \"+City\" o\\n}<->{\\nbahn), für die eine weitere Fahrkarte erforderlich sein kann.\\n- Wenn Ihr Ticket den Zusatz \"+City\" oder \"City mobil\" zeigt, gilt dieser nur am Tag der Hinfahrt bzw. am Tag der Rückfahrt.\\n- Es gelten die nationalen und internationalen Beförderungsbedingungen der DB AG. Innerhalb von Verkehrsverbünden und Tarifgemeinschaften\\ngelten deren Bedingungen. Alle Bedingungen finden Sie unter: www.bahn.de/agb und www.diebefoerderer.de.\\nIhre Reisedaten können sich kurzfristig durch Bauarbeiten oder andere e\\n}<->{\\nb und www.diebefoerderer.de.\\nIhre Reisedaten können sich kurzfristig durch Bauarbeiten oder andere erforderliche Fahrplananpassungen ändern.\\nBitte informieren Sie sich kurz vor Ihrer Reise über mögliche Änderungen Ihrer Reisedaten unter www.bahn.de/reiseplan oder mobil über die\\nApp DB Navigator. Achten Sie auch auf Informationen und Ansagen im Zug und am Bahnhof. Wir danken Ihnen für Ihre Buchung und wünschen\\nIhnen eine angenehme Reise!.\\n118NP8\\nSeite 1 / 1\\n\\n--------------------------------------\\n}<->{\\nund wünschen\\nIhnen eine angenehme Reise!.\\n118NP8\\nSeite 1 / 1\\n\\n--------------------------------------------------\\n', 'Summaries': 'Online-Ticket \\n- Fahrkarte: IC/EC \\n- Reiseantritt: 10.11.2017 \\n- Klasse: Erw. Klasse 1 \\n- Preis: 78,75€ (mit MwSt) \\n- Zahlungsmittelentgelt: 0,75€\\n}<->{\\nA German receipt summary:\\n\\nYou were charged €78,75 for a ticket purchase.\\nDate: 10.11.2017\\nTime: 14:25 Uhr\\nPayment method: Credit card (Kreditkartenzahlung)\\nVendor: DB Fernverkehr AG/DB Regio AG\\nAddress: Stephensonstr. 1, 60326 Frankfurt\\n}<->{\\nDeine Hinfahrt:\\n- Gültig: 10.11.2017\\n- Reservierung: Auftragsnummer 118NP8, Herr Jens Walter\\n- Halt:\\n  - Hamburg Hauptbahnhof: 14:46 Uhr, Gleis 11\\n  - Bonn-Beuel: 19:10 Uhr, Gleis 1\\n- Produkte:\\n  - IC 2213\\n}<->{\\nRer BahnCard: Mit Flexpreis-Ticket nutzen Sie jeden Zug der gewählten Verbindung, entweder alle IC/EC-Züge mit IC/EC-Fahrkarte oder andere Züge mit ICE-Fahrkarte. Online-Ticket gilt nur für den angegebenen Reiseabschnitt und kann bei Teilstrecken (z.B. Bus) zusätzliche Fahrkarten erfordern.\\n}<->{\\nFür eine weitere Fahrkarte kann die Bahn gebucht werden. Das Ticket zeigt nur am Tag der Hinfahrt bzw. Rückfahrt den Zusatz \"+City\" oder \"City mobil\". Die Beförderungsbedingungen gelten nach DB AG und Verkehrsverbünden/Tarifgemeinschaften.\\n}<->{\\nDie Deutsche Bahn (DB) kann Ihre Reisedaten kurzfristig ändern wegen Bauarbeiten oder Fahrplananpassungen. Informieren Sie sich vor Ihrer Reise unter www.bahn.de/reiseplan oder in der DB-App über mögliche Änderungen und achten auch auf Informationen im Zug und am Bahnhof.\\n}<->{\\nThere is not enough text to provide a summary. The given text appears to be part of an email or message in German, but it lacks relevant information for summarization. \\n\\nIf you could provide more context or the actual content, I would be happy to assist you further.'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#load and sort the data inferenced by on device model\n",
    "with open(formatted_pdfs, \"r\") as f1, open(receipt_pdfs, \"r\") as f2, open(report_pdfs, \"r\") as f3, open(random_pdfs, \"r\") as f4:\n",
    "    formatted = json.load(f1)\n",
    "    receipts = json.load(f2)\n",
    "    reports = json.load(f3)\n",
    "    randoms = json.load(f4)\n",
    "    print(\"PDFs with formats\",len(formatted))\n",
    "    print(\"PDFs of recipts that need ocr\", len(receipts))\n",
    "    print(\"PDFs of well written long papers, no ocr\",len(reports))\n",
    "    print(\"PDFs of all sorts\",len(randoms))\n",
    "formatted = sorted(formatted, key=lambda x: list(x.keys())[0])\n",
    "receipts = sorted(receipts, key=lambda x: list(x.keys())[0])\n",
    "reports = sorted(reports, key=lambda x: list(x.keys())[0])\n",
    "randoms = sorted(randoms, key=lambda x: list(x.keys())[0])\n",
    "print (formatted[0])\n",
    "print (receipts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training will happen after the first summary for both ocr detection and text extraction versions. \n",
    "##### The input now is no longer gibberish and not natural language.  \n",
    "##### Now we need to suppress the apologies/commentaries on sensitive data and gibberish text. There is no need to train on the initial summary process since it will skew the model into detecting random gibberish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the scores to average and omit the scores when summaries are not usable\n",
    "def get_average_scores(pdf, separator : str, filtered_txt = [\"\"]):\n",
    "    #print(filtered_txt)\n",
    "    file_name = list(pdf.keys())[0]\n",
    "    values = list(pdf.values())[0]\n",
    "    scoretxt =  values.get(\"Scores\")\n",
    "    scoretxt = scoretxt.strip()\n",
    "    scores = scoretxt.split(separator)\n",
    "    score = 0.0\n",
    "    count = 0\n",
    "    summaries = values.get(\"Summaries\")\n",
    "    summaries = summaries.split(separator)\n",
    "    for s, txt in zip(scores, summaries): \n",
    "        lower_txt = txt.lower()\n",
    "        #print(s , txt)\n",
    "        for filt in filtered_txt:\n",
    "            if filt.lower() in lower_txt:\n",
    "                #print(f\"In file {file_name} filtered\", txt)\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    score += float(s)\n",
    "                    count += 1\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return 0 if count == 0 else score/count\n",
    "\n",
    "def count_pdf_available_and_filtered_chunks(pdf, separator : str, filtered_txt = [\"\"]):\n",
    "    file_name = list(pdf.keys())[0]\n",
    "    available_chunks = []\n",
    "    filtered_chunks = []\n",
    "    values = list(pdf.values())[0]\n",
    "    chunks = values.get(\"Chunks\").split(separator)\n",
    "    summaries = values.get(\"Summaries\").split(separator)\n",
    "    for c ,summary  in zip(chunks,summaries): \n",
    "        lower_txt = summary.lower()\n",
    "        if any(filt.lower() in lower_txt for filt in filtered_txt):\n",
    "            #print(f\"In file {file_name} filtered\", txt)\n",
    "            filtered_chunks.append(c)\n",
    "        else:\n",
    "            available_chunks.append(c)\n",
    "    #print(\"ava vs filt:\",len(available_chunks), len(filtered_chunks))\n",
    "    return available_chunks, filtered_chunks\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higher_scored_pdf(pdfs, separator : str, filtered_txt = [\"\"]):\n",
    "    if len(pdfs) == 0:\n",
    "        return None\n",
    "    solution = None\n",
    "    score = 0 \n",
    "    for pdf in pdfs:\n",
    "        if solution == None:\n",
    "            solution = pdf\n",
    "            score = get_average_scores(pdf, separator,filtered_txt=filtered_txt)\n",
    "        else:\n",
    "            new_score = get_average_scores(pdf, separator, filtered_txt=filtered_txt)\n",
    "            if new_score > score:\n",
    "                solution = pdf\n",
    "                score = new_score\n",
    "\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "## get sorted adjacent pdfs that originated from the same source\n",
    "def get_adjacent_pdfs(pdfs, start, reg = r'_(normal|ocr)\\.txt$'):\n",
    "    adjacent = []\n",
    "    i = start\n",
    "    while i < len(pdfs):\n",
    "        if i == start:\n",
    "            adjacent.append(pdfs[i])\n",
    "        else:\n",
    "            prev = re.sub(reg , '', list(adjacent[-1].keys())[0])\n",
    "            current = re.sub(reg , '', list(pdfs[i].keys())[0])\n",
    "            if prev == current:\n",
    "                adjacent.append(pdfs[i])\n",
    "            else:\n",
    "                break\n",
    "        i += 1\n",
    "    return adjacent, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_best_version(pdfs, separator: str, filtered_txt):\n",
    "    higher_scored_pdfs = []\n",
    "    higher_scored_filtered = []\n",
    "    i = 0\n",
    "    useful_chunks = []\n",
    "    filtered_chunks = []\n",
    "    while i < len(pdfs):\n",
    "        start = i\n",
    "        adjacents, i = get_adjacent_pdfs(pdfs, start)\n",
    "        # for f in adjacents:\n",
    "        #     print(f\"PDF from: {start} to {i}:\",f)\n",
    "        winner = get_higher_scored_pdf(adjacents, separator=separator, filtered_txt=filtered_txt)\n",
    "        useful, filtered = count_pdf_available_and_filtered_chunks(winner, separator=separator, filtered_txt=filtered_txt) \n",
    "        if len(filtered) != 0:\n",
    "            higher_scored_filtered.append(winner)\n",
    "        higher_scored_pdfs.append(winner)\n",
    "        #print(\"Incoming:\", len(useful), len(filtered))\n",
    "        print(\"useful\",useful)\n",
    "        useful_chunks.extend(useful)\n",
    "        filtered_chunks.extend(filtered)\n",
    "        #print(\"Current Lenghts:\",len(useful_chunks), len(filtered_chunks))\n",
    "        \n",
    "        #print(\"Winner\", winner)\n",
    "    return higher_scored_pdfs, higher_scored_filtered, useful_chunks, filtered_chunks \n",
    "def get_censored_text(text):\n",
    "    return text.replace(\"CENSORED\", \" \").replace(\"censored\", \" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The resulted data will still contain appologies and commentaries but it should contain the best availale information from the original pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [ \n",
    "        \"I can't\",\n",
    "        \"I cannot\",\n",
    "        \"There is no\",\n",
    "        \"No useful\",\n",
    "        \"Unusable\"]\n",
    "#formatted inputs contains pdfs with a lot of tables, and forms, most of them are text based\n",
    "formatted_inputs ,filtered_formatted_inputs , useful_formatted_chunks, filtered_formatted_chunks = get_the_best_version(formatted, separator=\"\\n}<->{\\n\", filtered_txt= keywords)\n",
    "\n",
    "#receipt inputs contains pdfs that are mostly images with text, and need ocr \n",
    "receipt_inputs, filtered_receipt_inputs, useful_receipt_chunks, filtered_receipt_chunks =  get_the_best_version(receipts, separator=\"\\n}<->{\\n\", filtered_txt= keywords)\n",
    "\n",
    "#report inputs contains pdfs that are well written, and no ocr is needed\n",
    "report_inputs ,filtered_report_inputs , useful_report_chunks, filtered_report_chunks = get_the_best_version(reports, separator=\"\\n}<->{\\n\", filtered_txt= keywords)\n",
    "\n",
    "#random inputs contains pdfs that are all sorts mix of ocr and text\n",
    "random_inputs, filetered_random_inputs, useful_random_chunks, filtered_random_chunks = get_the_best_version(randoms, separator=\"\\n}<->{\\n\", filtered_txt= keywords)\n",
    "\n",
    "print(f\"PDFs with formats {len(formatted_inputs)}, that are filtered {len(filtered_formatted_inputs)}, useful chunks {len(useful_formatted_chunks)}, filtered chunks {len(filtered_formatted_chunks)}\")\n",
    "print(f\"PDFs of recipts that need ocr {len(receipt_inputs)}, that are filtered {len(filtered_receipt_inputs)}, useful chunks {len(useful_receipt_chunks)}, filtered chunks {len(filtered_receipt_chunks)}\")\n",
    "print(f\"PDFs of well written long papers, no ocr {len(report_inputs)}, that are filtered {len(filtered_report_inputs)}, useful chunks {len(useful_report_chunks)}, filtered chunks {len(filtered_report_chunks)}\")\n",
    "print(f\"PDFs of all sorts {len(random_inputs)}, that are filtered {len(filetered_random_inputs)}, useful chunks {len(useful_random_chunks)}, filtered chunks {len(filtered_random_chunks)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in formatted_inputs[0:5]:\n",
    "    print(i)\n",
    "for i in receipt_inputs[0:5]:\n",
    "        print(i)\n",
    "for i in report_inputs[0:5]:\n",
    "        print(i)\n",
    "for i in random_inputs[0:5]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 4 types of data:\n",
    "* Formatted data has a lot of numbers, we could make the model to adhere to addressing the numbers formally or concisely.\n",
    "* Receipts are mainly ocred and contains  a lot of personal which produced highest rate of disclamers and commentaries so commentary suppression data can be gathered on it.\n",
    "* Papers contains well written text and information dense, we could train the model to be more informative and concise.\n",
    "* Mixed PDFs' content are a mix of the above types, we could try to mimic the tone of the original text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Load .env and initialize OpenAI client\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GPT_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "def summarize_chunk(text: str, prompt, model = \"gpt-3.5-turbo\", retry=True) -> str:\n",
    "    prompt = f\"\"\"{prompt}\n",
    "Text:\n",
    "{text}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model= model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        print(\"Response:\",response) \n",
    "        # Check structure before returning\n",
    "        if response and response.choices and response.choices[0].message:\n",
    "            return response.choices[0].message.content.strip()\n",
    "        else:\n",
    "            print(\"Warning: Unexpected response format.\")\n",
    "            return \"\"\n",
    "\n",
    "    except openai.RateLimitError:\n",
    "        print(\"Rate limit hit. Sleeping for 10 seconds...\")\n",
    "        if retry:\n",
    "            time.sleep(10)\n",
    "            return summarize_chunk(text, prompt)  # retry\n",
    "\n",
    "    except openai.Timeout:\n",
    "        print(\"Request timed out. Retrying...\")\n",
    "        if retry:\n",
    "            return summarize_chunk(text, prompt)\n",
    "\n",
    "    except openai.APIError as e:\n",
    "        print(f\"OpenAI API error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def process_chunks_and_save(data_list, output_dir, file_name , summary_type, prompt, retry=True):\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, chunk in enumerate(data_list):\n",
    "        chunk = chunk.strip()\n",
    "        print(\"Sending:\", chunk)\n",
    "        if not chunk:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            summary = summarize_chunk(chunk, prompt,retry=retry)\n",
    "            if summary:\n",
    "                result = {\n",
    "                    \"input\": chunk,\n",
    "                    \"output\": summary,\n",
    "                    \"summary_type\": summary_type, \n",
    "                }\n",
    "                with open(output_path / file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "            else:\n",
    "                print(f\"Suppressed\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{i+1}] Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Report for 2016-07 \n",
      " Category : Beverages  \n",
      " id category : 1  \n",
      "Product\n",
      "Units Sold\n",
      "Units in Stock\n",
      "Unit Price\n",
      "Chang\n",
      "105\n",
      "17\n",
      "19\n",
      "Guaraná Fantástica\n",
      "43\n",
      "20\n",
      "4.5\n",
      "Steeleye Stout\n",
      "20\n",
      "20\n",
      "18\n",
      "Chartreuse verte\n",
      "48\n",
      "69\n",
      "18\n",
      "Outback Lager\n",
      "41\n",
      "15\n",
      "15\n",
      "Lakkalikööri\n",
      "15\n",
      "57\n",
      "18\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Sending: Stock Report for 2016-07 \n",
      " Category : Beverages  \n",
      " id category : 1  \n",
      "Product\n",
      "Units Sold\n",
      "Units in Stock\n",
      "Unit Price\n",
      "Chang\n",
      "105\n",
      "17\n",
      "19\n",
      "Guaraná Fantástica\n",
      "43\n",
      "20\n",
      "4.5\n",
      "Steeleye Stout\n",
      "20\n",
      "20\n",
      "18\n",
      "Chartreuse verte\n",
      "48\n",
      "69\n",
      "18\n",
      "Outback Lager\n",
      "41\n",
      "15\n",
      "15\n",
      "Lakkalikööri\n",
      "15\n",
      "57\n",
      "18\n",
      "\n",
      "--------------------------------------------------\n",
      "Rate limit hit. Sleeping for 10 seconds...\n",
      "[1] Suppressed\n"
     ]
    }
   ],
   "source": [
    "print((useful_formatted_chunks[0]))\n",
    "process_chunks_and_save([useful_formatted_chunks[0]], retry=False, output_dir=\"../Training/Formatted\", file_name=\"formatted_0.json\", \n",
    "                        summary_type=\"formatted_without_bias\", prompt = \"\"\"Summarize the following text concisely, without adding commentary, disclaimers, or opinions. \n",
    "If the text contains sensitive or personal information (e.g. names, SSNs, medical data), return an empty response.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmlhw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
